{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SUMMARY Several lines of evidence support the involvement of inflammatory '\n",
      " 'and immunologic abnormalities in chronic fatigue syndrome CFS Since recent '\n",
      " 'studies have shown that Î±1 antitrypsin AAT possesses antiinflammatory '\n",
      " 'properties the potential therapeutic effect of AAT treatment on CFS has been '\n",
      " 'investigated A 49yearold woman diagnosed with CFS was treated with '\n",
      " 'intravenous infusions of a human plasmaderived AAT concentrate 60 mgkg body '\n",
      " 'weight weekly for 8 consecutive weeks The patients monocyte elastase a '\n",
      " 'regulator of inflammatory processes was 1170 Umg At completion of treatment '\n",
      " 'improvement in maximal workload was observed 540717 of predicted '\n",
      " 'Additionally amelioration in working memory scores 8394 and perceptual '\n",
      " 'organization scores 7583 were detected on the Wechsler Adult Intelligence '\n",
      " 'ScaleIII test Monocyte elastase decreased to a normal range 150 Umg '\n",
      " 'Improvement in functional capacity allowed the patient to work in parttime '\n",
      " 'employment These findings suggest a possible role for AAT in the treatment '\n",
      " 'of CFS',\n",
      " 'Acute inflammation is a severe medical condition defined as an inflammatory '\n",
      " 'response of the body to an infection Its rapid progression requires quick '\n",
      " 'and accurate decisions from clinicians Inadequate and delayed decisions '\n",
      " 'makes acute inflammation the 10th leading cause of death overall in United '\n",
      " 'States with the estimated cost of treatment about 17 billion annually '\n",
      " 'However despite the need there are limited number of methods that could '\n",
      " 'assist clinicians to determine optimal therapies for acute inflammation We '\n",
      " 'developed a datadriven method for suggesting optimal therapy by using '\n",
      " 'machine learning model that is learned on historical patients behaviors To '\n",
      " 'reduce both the risk of failure and the expense for clinical trials our '\n",
      " 'method is evaluated on a virtual patients generated by a mathematical model '\n",
      " 'that emulates inflammatory response In conducted experiments acute '\n",
      " 'inflammation was handled with two complimentary pro and antiinflammatory '\n",
      " 'medications which adequate timing and doses are crucial for the successful '\n",
      " 'outcome Our experiments show that the dosage regimen assigned with our '\n",
      " 'datadriven method significantly improves the percentage of healthy patients '\n",
      " 'when compared to results by other methods used in clinical practice and '\n",
      " 'found in literature Our method saved 88 of patients that would otherwise die '\n",
      " 'within a week while the best method found in literature saved only 73 of '\n",
      " 'patients At the same time our method used lower doses of medications than '\n",
      " 'alternatives In addition our method achieved better results than '\n",
      " 'alternatives when only incomplete or noisy measurements were available over '\n",
      " 'time as well as it was less affected by therapy delay The presented results '\n",
      " 'provide strong evidence that models from the artificial intelligence '\n",
      " 'community have a potential for development of personalized treatment '\n",
      " 'strategies for acute inflammation',\n",
      " 'Human brain connectivity can be studied using graph theory Many connectivity '\n",
      " 'studies parcellate the brain into regions and count fibres extracted between '\n",
      " 'them The resulting network analyses require validation of the tractography '\n",
      " 'as well as region and parameter selection Here we investigate whole brain '\n",
      " 'connectivity from a different perspective We propose a mathematical '\n",
      " 'formulation based on studying the eigenvalues of the Laplacian matrix of the '\n",
      " 'diffusion tensor field at the voxel level This voxelwise matrix has over a '\n",
      " 'million parameters but we derive the Kirchhoff complexity and eigenspectrum '\n",
      " 'through elegant mathematical theorems without heavy computation We use these '\n",
      " 'novel measures to accurately estimate the voxelwise connectivity in multiple '\n",
      " 'biomedical applications such as Alzheimers disease and intelligence '\n",
      " 'prediction']\n"
     ]
    }
   ],
   "source": [
    "# Read the two CSV files\n",
    "df_part1 = pd.read_csv('../data/processed_data_part1.csv')\n",
    "df_part2 = pd.read_csv('../data/processed_data_part2.csv')\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "df = pd.concat([df_part1, df_part2], ignore_index=True)\n",
    "\n",
    "# Read in abstracts\n",
    "abstracts = df['Abstract'].tolist()\n",
    "pprint.pprint(abstracts[:3])\n",
    "#print(abstracts[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load BioBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "model = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Convert abstracts to embeddings\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m [abstract_to_embedding(abstract) \u001b[38;5;28;01mfor\u001b[39;00m abstract \u001b[38;5;129;01min\u001b[39;00m abstracts]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(embeddings[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Convert abstracts to embeddings\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m [\u001b[43mabstract_to_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabstract\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m abstract \u001b[38;5;129;01min\u001b[39;00m abstracts]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(embeddings[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m, in \u001b[0;36mabstract_to_embedding\u001b[0;34m(abstract)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mabstract_to_embedding\u001b[39m(abstract):\n\u001b[0;32m----> 3\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabstract\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      5\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2802\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2800\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2801\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2802\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2804\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2860\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2861\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2862\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2863\u001b[0m     )\n\u001b[1;32m   2865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2866\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2867\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2868\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2869\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "# Function to convert abstract to embedding\n",
    "def abstract_to_embedding(abstract):\n",
    "    inputs = tokenizer(abstract, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Convert abstracts to embeddings\n",
    "embeddings = [abstract_to_embedding(abstract) for abstract in abstracts]\n",
    "print(embeddings[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Is there a correlation between intelligence and genetics?\n",
      "Most similar abstracts:\n",
      "Abstract 8219 (Similarity: 0.89):\n",
      "Does general intelligence exist across species, and has it been a target of natural selection? These questions can be addressed with genomic data, which can rule out artifacts by demonstrating that distinct cognitive abilities are genetically correlated and thus share a biological substrate. This work has begun with data from humans and can be extended to other species; it should focus not only on general intelligence but also specific capacities like language and spatial ability.\n",
      "\n",
      "\n",
      "Abstract 8205 (Similarity: 0.89):\n",
      "Burkart et al. consider that the relationship between general intelligence and socio-cognitive abilities is poorly understood in animals and humans. We examine this conclusion in the perspective of an already substantial evidence base on the relationship among general intelligence, theory of mind, and emotional intelligence. We propose a link between general intelligence and socio-cognitive abilities within humans.\n",
      "\n",
      "\n",
      "Abstract 14365 (Similarity: 0.89):\n",
      "The brain is an information machine equipped with mind and consciousness, acquired through a long history of evolution. Artificial intelligence (AI) aims at the development of intelligent functions in computers. We show mechanisms of deep learning in AI, and compare them with brain functions. In particular, we consider the function of consciousness in the brain, and its relation to AI. Mathematical neuroscience provides a powerful method to both AI and brain science. We conclude by emphasizing that further interactions are important for both AI and brain science.\n",
      "\n",
      "\n",
      "Query: Is there a correlation between spiritual intelligence and the success of addiction treatment with methadone?\n",
      "Most similar abstracts:\n",
      "Abstract 3964 (Similarity: 0.93):\n",
      "Type 1 diabetes is a lifelong physical and emotional challenge. The concept of emotional intelligence may offer better understanding of personal resources facilitating management of such challenges. We therefore hypothesized that emotional intelligence will negatively associate with two measures of diabetic management: HA1c and blood sugar levels. A total of 78 young adults with type 1 diabetes mellitus reported their last HA1c test result and their blood sugar level, as well as demographics and took the audio-visual test of emotional intelligence. The results showed a negative association between emotional intelligence and HA1c and marginal results in the same direction with blood sugar levels even when controlling for demographics.\n",
      "\n",
      "\n",
      "Abstract 15429 (Similarity: 0.93):\n",
      "Individuals with intellectual disability (ID) and traumatic brain injury experience mental health issues at a higher rate than the general population. They are typically more vulnerable to stress, have fewer coping skills, and possess a smaller system of natural supports. It is clear that level of intelligence is not the sole indicator of the appropriateness of psychotherapy and that the full range of mental health services are able to help improve the quality of life for patients with intellectual disability. Special issues related to motivational interviewing, cognitive behavioral therapy (CBT), and supportive psychotherapy are described with specific attention to special issues for the intellectual disability population and effective adaptations addressed.\n",
      "\n",
      "\n",
      "Abstract 2574 (Similarity: 0.93):\n",
      "The concept of Emotional Intelligence (EI) was developed as a way to evaluate and highlight the importance of emotional health as it relates to overall quality of life. This study examines the predictive nature of EI with standardized measures of mental health to create a model that can be utilized to create more effective health promotion interventions. Step-wise multiple regression was used to predict mental health (Kessler K-6 scale) and Type D personality (Denollett's Scale of Negative Affectivity and Social Inhibition) with five dimensions of EI. The results revealed that while not all of the dimensions of EI regressed significantly in each model, mood management was highly predictive of all mental health measures under investigation. Cut-off points for each scale were also helpful in interpreting the relatedness of EI to mental health.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define some test queries for evaluation\n",
    "test_queries = [\"Is there a correlation between intelligence and genetics?\", \n",
    "                \"Is there a correlation between spiritual intelligence and the success of addiction treatment with methadone?\"]\n",
    "\n",
    "def queries_to_embeddings(queries, tokenizer, model):\n",
    "    embeddings = []\n",
    "    for query in queries:\n",
    "        inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "# Convert queries to embeddings\n",
    "query_embeddings = queries_to_embeddings(test_queries, tokenizer, model)\n",
    "\n",
    "\n",
    "# Function to find most similar abstracts for a given query embedding\n",
    "def find_most_similar(query_emb, abstract_embs, top_n=3):\n",
    "    similarities = cosine_similarity([query_emb], abstract_embs)[0]\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_n]\n",
    "    top_similarities = np.sort(similarities)[::-1][:top_n]\n",
    "    return top_indices, top_similarities\n",
    "\n",
    "\n",
    "# Evaluate each query\n",
    "for query, query_emb in zip(test_queries, query_embeddings):\n",
    "    print(f\"Query: {query}\")\n",
    "    top_indices, top_similarities = find_most_similar(query_emb, embeddings)\n",
    "    print(\"Most similar abstracts:\")\n",
    "    for index, similarity in zip(top_indices, top_similarities):\n",
    "        print(f\"Abstract {index + 1} (Similarity: {similarity:.2f}):\")\n",
    "        print(df.iloc[index]['Abstract'])\n",
    "        print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of abstracts longer than 512 tokens: 1174\n",
      "Average excess length of abstracts longer than 512 tokens: 128.27683134582622\n"
     ]
    }
   ],
   "source": [
    "def count_long_abstracts(df, max_length=512):\n",
    "    long_abstract_count = 0\n",
    "    for abstract in df['Abstract']:\n",
    "        tokens = tokenizer.encode(abstract, add_special_tokens=True)\n",
    "        if len(tokens) > max_length:\n",
    "            long_abstract_count += 1\n",
    "    return long_abstract_count\n",
    "\n",
    "# Count how many abstracts are longer than 512 tokens\n",
    "num_long_abstracts = count_long_abstracts(df)\n",
    "print(\"Number of abstracts longer than 512 tokens:\", num_long_abstracts)\n",
    "\n",
    "def average_length_excess(df, max_length=512):\n",
    "    total_excess_length = 0\n",
    "    long_abstract_count = 0\n",
    "    for abstract in df['Abstract']:\n",
    "        tokens = tokenizer.encode(abstract, add_special_tokens=True)\n",
    "        if len(tokens) > max_length:\n",
    "            long_abstract_count += 1\n",
    "            total_excess_length += len(tokens) - max_length\n",
    "    return total_excess_length / long_abstract_count if long_abstract_count > 0 else 0\n",
    "\n",
    "# Calculate the average length by which abstracts exceed 512 tokens\n",
    "average_excess_length = average_length_excess(df)\n",
    "print(\"Average excess length of abstracts longer than 512 tokens:\", average_excess_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embeddings.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
