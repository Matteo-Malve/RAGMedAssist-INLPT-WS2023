{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import pandas as pd\n",
    "import glob\n",
    "import ast\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load generated answers\n",
    "\n",
    "def process_files_to_aggregated_dfs(directory_path):\n",
    "    # initialize dicts to store QAs\n",
    "    data_dicts = {\n",
    "        \"qa_eval_set\": {},\n",
    "        \"chat_gpt_questions\": {},\n",
    "        \"unrelated_topic_questions\": {}\n",
    "    }\n",
    "\n",
    "    file_pattern = f\"{directory_path}/query_response_pairs_hybrid_search*.txt\"\n",
    "\n",
    "    for filename in glob.glob(file_pattern):\n",
    "        with open(filename, 'r') as file:\n",
    "            contents = file.read()\n",
    "            data_list = ast.literal_eval(contents)\n",
    "            \n",
    "            # extract unique id from filename\n",
    "            file_id = os.path.basename(filename).split('.')[0].split('_')[5:]\n",
    "\n",
    "            if len(data_list) >= 17:\n",
    "                # Process for qa_eval_set\n",
    "                for pair in data_list[:10]:  # First 10 for qa_eval_set\n",
    "                    query, answer = pair\n",
    "                    if query not in data_dicts[\"qa_eval_set\"]:\n",
    "                        data_dicts[\"qa_eval_set\"][query] = {\"query\": query}\n",
    "                    data_dicts[\"qa_eval_set\"][query][f\"answer with hybrid search weights: {file_id[0]}, {file_id[1]}\"] = answer\n",
    "                \n",
    "                # Process for chat_gpt_questions\n",
    "                for pair in data_list[10:15]:  # Next 5 for chat_gpt_questions\n",
    "                    query, answer = pair\n",
    "                    if query not in data_dicts[\"chat_gpt_questions\"]:\n",
    "                        data_dicts[\"chat_gpt_questions\"][query] = {\"query\": query}\n",
    "                    data_dicts[\"chat_gpt_questions\"][query][f\"answer with hybrid search weights: {file_id[0]}, {file_id[1]}\"] = answer\n",
    "                \n",
    "                # Process for unrelated_topic_questions\n",
    "                for pair in data_list[-2:]:  # Last 2 for unrelated_topic_questions\n",
    "                    query, answer = pair\n",
    "                    if query not in data_dicts[\"unrelated_topic_questions\"]:\n",
    "                        data_dicts[\"unrelated_topic_questions\"][query] = {\"query\": query}\n",
    "                    data_dicts[\"unrelated_topic_questions\"][query][f\"answer with hybrid search weights: {file_id[0]}, {file_id[1]}\"] = answer\n",
    "\n",
    "    # Convert dictionaries to DataFrames\n",
    "    for category, data_dict in data_dicts.items():\n",
    "        data_dicts[category] = pd.DataFrame.from_dict(data_dict, orient='index').fillna('').reset_index(drop=True)\n",
    "\n",
    "    return data_dicts[\"qa_eval_set\"], data_dicts[\"chat_gpt_questions\"], data_dicts[\"unrelated_topic_questions\"]\n",
    "\n",
    "directory_path = './results/hybrid_search'\n",
    "qa_eval_set_df, chat_gpt_qa_df, unrelated_topic_qa_df = process_files_to_aggregated_dfs(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"QA Eval Set:\")\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "print(len(qa_eval_set_df))\n",
    "qa_eval_set_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nChatGPT QA:\")\n",
    "print(len(chat_gpt_qa_df))\n",
    "#chat_gpt_qa_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nUnrelated Topic QA:\")\n",
    "print(len(unrelated_topic_qa_df))\n",
    "#unrelated_topic_qa_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract queries\n",
    "qa_eval_queries = qa_eval_set_df[\"query\"].to_list()\n",
    "# chat_gpt_queries = chat_gpt_qa_df[\"query\"].to_list()\n",
    "# unrelated_queries = unrelated_topic_qa_df[\"query\"].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gold truth answers\n",
    "qa_set = pd.read_csv(\"../../data/questions_answers/questions_answers.csv\")\n",
    "gold_truth_qa = qa_set[[\"QUESTION\", \"LONG_ANSWER\"]][qa_set['QUESTION'].isin(qa_eval_queries)]\n",
    "\n",
    "gold_answers = gold_truth_qa[\"LONG_ANSWER\"].to_list()\n",
    "\n",
    "gold_truth_qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ℹ️ One of the tested questions is unfortunately not available in the QA evaluation set, thus only 9 of our 10 asked question can be evaluated using ground truth results.**\n",
    "\n",
    "➡️ \"What are the effects of α1-antitrypsin (AAT) treatment on chronic fatigue syndrome (CFS) based on a case study involving a 49-year-old woman?\" will be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_answers_list = [\n",
    "    qa_eval_set_df[\"answer with hybrid search weights: 0, 1\"].to_list(),\n",
    "    qa_eval_set_df[\"answer with hybrid search weights: 01, 09\"].to_list(),\n",
    "    qa_eval_set_df[\"answer with hybrid search weights: 02, 08\"].to_list(),\n",
    "    qa_eval_set_df[\"answer with hybrid search weights: 03, 07\"].to_list(),\n",
    "    qa_eval_set_df[\"answer with hybrid search weights: 04, 06\"].to_list(),\n",
    "    qa_eval_set_df[\"answer with hybrid search weights: 05, 05\"].to_list(),\n",
    "    qa_eval_set_df[\"answer with hybrid search weights: 06, 04\"].to_list(),\n",
    "    qa_eval_set_df[\"answer with hybrid search weights: 07, 03\"].to_list(),\n",
    "    qa_eval_set_df[\"answer with hybrid search weights: 08, 02\"].to_list(),\n",
    "    qa_eval_set_df[\"answer with hybrid search weights: 09, 01\"].to_list(),\n",
    "    qa_eval_set_df[\"answer with hybrid search weights: 1, 0\"].to_list()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "bleu = evaluate.load('bleu')\n",
    "rouge = evaluate.load('rouge')\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "list_names = [\"0.0, 1.0\", \"0.1, 0.9\", \"0.2, 0.8\", \"0.3, 0.7\", \"0.4, 0.6\", \"0.5, 0.5\", \"0.6, 0.4\", \"0.7, 0.3\", \"0.8, 0.2\", \"0.9, 0.1\", \"1.0, 0.0\" ]\n",
    "\n",
    "def compute_scores(gen_answers_list, gold_truths, names):\n",
    "    scores = []\n",
    "    \n",
    "    for name, gen_answers in zip(names, gen_answers_list):\n",
    "        gen_answers_copy = gen_answers[:]\n",
    "        del gen_answers_copy[8]\n",
    "\n",
    "        bleu_score = bleu.compute(predictions=gen_answers_copy, references=gold_truths)\n",
    "        rouge_score = rouge.compute(predictions=gen_answers_copy, references=gold_truths)\n",
    "        bert = bertscore.compute(predictions=gen_answers_copy, references=gold_truths, lang=\"en\")\n",
    "        bertscore_averaged = {}\n",
    "\n",
    "        for key in bert.keys():\n",
    "            if key!='hashcode':\n",
    "                bertscore_averaged[key]=np.mean(bert[key])\n",
    "\n",
    "        # save results to file\n",
    "        with open(\"results/hybrid_search/bleu_rouge_bert_scores.txt\", \"a\") as file:\n",
    "            file.write(f\"Results for hybrid search weights: {name}\\n\")\n",
    "            file.write(f\"BLEU Score: {bleu_score}\\n\")\n",
    "            file.write(f\"ROUGE Score: {rouge_score}\\n\")\n",
    "            file.write(f\"BERTScore Averaged: {bertscore_averaged}\\n\\n\")\n",
    "\n",
    "        # return scores for plotting\n",
    "        scores.append({\n",
    "            'name': name,\n",
    "            'bleu': bleu_score,\n",
    "            'rouge': rouge_score,\n",
    "            'bertscore': bertscore_averaged\n",
    "        })\n",
    "\n",
    "    return scores\n",
    "\n",
    "scores = compute_scores(generated_answers_list, gold_answers, list_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "weights = [score['name'] for score in scores]\n",
    "bleu_scores = [score['bleu']['bleu'] for score in scores]\n",
    "rouge1_scores = [score['rouge']['rouge1'] for score in scores]\n",
    "bertscore_f1_scores = [score['bertscore']['f1'] for score in scores]\n",
    "\n",
    "# BLEU scores\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.subplot(3, 1, 1)  # 3 rows, 1 column, 1st subplot\n",
    "plt.plot(weights, bleu_scores, marker='o', linestyle='-', color='blue', label='BLEU Score')\n",
    "plt.title('Evaluation Scores Across Different Hybrid Search Weights')\n",
    "plt.xlabel('Hybrid Search Weights')\n",
    "plt.ylabel('BLEU Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "\n",
    "# ROUGE-1 scores\n",
    "plt.subplot(3, 1, 2)  # 3 rows, 1 column, 2nd subplot\n",
    "plt.plot(weights, rouge1_scores, marker='s', linestyle='--', color='red', label='ROUGE-1 Score')\n",
    "plt.xlabel('Hybrid Search Weights')\n",
    "plt.ylabel('ROUGE-1 Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "\n",
    "# BERTScore F1 scores\n",
    "plt.subplot(3, 1, 3)  # 3 rows, 1 column, 3rd subplot\n",
    "plt.plot(weights, bertscore_f1_scores, marker='^', linestyle='-.', color='green', label='BERTScore F1')\n",
    "plt.xlabel('Hybrid Search Weights')\n",
    "plt.ylabel('BERTScore F1')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(\"../../docs/images/bleu_rouge_bert_hybrid_search.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
