{"cells":[{"cell_type":"markdown","metadata":{"id":"be9f7653"},"source":["**Heidelberg University**\n","\n","**Data Science  Group**\n","    \n","Prof. Dr. Michael Gertz  \n","\n","Ashish Chouhan, Satya Almasian, John Ziegler, Jayson Salazar, Nicolas Reuter\n","    \n","January 16, 2024\n","    \n","Natural Language Processing with Transformers\n","\n","Winter Semster 2023/2024     \n","***"],"id":"be9f7653"},{"cell_type":"markdown","metadata":{"id":"258e9648"},"source":["# **Assignment 4: Question Answering**\n","**Due**: Monday, January 29, 2024, 2pm, via [Moodle](https://moodle.uni-heidelberg.de/course/view.php?id=19251)\n","\n"],"id":"258e9648"},{"cell_type":"markdown","metadata":{"id":"fc27ad9e"},"source":["### **Submission Guidelines**\n","\n","- Solutions need to be uploaded as a **single** Jupyter notebook. You will find several pre-filled code segments in the notebook, your task is to fill in the missing cells.\n","- For the written solution, use LaTeX in markdown inside the same notebook. Do **not** hand in a separate file for it.\n","- Download the .zip file containing the dataset but do **not** upload it with your solution.\n","- It is sufficient if one person per group uploads the solution to Moodle, but make sure that the full names of all team members are given in the notebook.\n","\n","***"],"id":"fc27ad9e"},{"cell_type":"markdown","metadata":{"id":"HETm7VsBkmLq"},"source":["## **Task 1: Retrieval Augmented Generation (RAG)** ( 4.5 + 3 + 4 + 3 + 1.5 = 16 points)"],"id":"HETm7VsBkmLq"},{"cell_type":"markdown","metadata":{"id":"0ODkKBIRkrfe"},"source":["In this task, we look at using the open source `Llama-13b-chat` model for creating a RAG system. You must first apply for access to Llama 2 models via [this](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) form (access is typically granted within a few hours). etrieval augmented generation you also need to request to use the model on Hugging Face by going to the [model](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) card. ***Note that the emails you provide for your Hugging Face account must match the email you used to request Llama 2.***\n","\n","The final piece that you need is a Hugging Face authentication token. You can find such a token by going to the `setting` in your Hugging Face profile, under the `Access Token` menu you can generate a new token.\n","\n","To store the document you will need a free Pinecone [API key](https://app.pinecone.io/).\n","Make sure you have these pieces ready before starting to work on this task.\n","\n","----\n","When ready, let's start by downloading the necessary packages.\n","\n","It is advised to proceed with this notebook with a GPU (if you are on Colab make sure that a GPU environment is activated.)\n"],"id":"0ODkKBIRkrfe"},{"cell_type":"markdown","metadata":{"id":"yO1OwuHTMo75"},"source":["Place all the access tokens in the `.env` file and upload it to the working directory (if you are running this notebook locally, you can change the path to fit your working directory). Please use the following format:\n","\n","\n","```\n","HF_AUTH= \"Hugging Face Authentication Key\"\n","PINECONE_API_KEY=\"Pincone API Key\"\n","PINECONE_ENVIRONMENT=\"Pinecone Environment\"\n","```\n","\n","Run the cell below to load the access tokens into the environment variables."],"id":"yO1OwuHTMo75"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9786,"status":"ok","timestamp":1705049745330,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"1Dxt-e1bg73b","outputId":"ad99334c-759d-4a9c-f862-245f26eabec5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python-dotenv\n","  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n","Installing collected packages: python-dotenv\n","Successfully installed python-dotenv-1.0.0\n"]}],"source":["%pip install python-dotenv"],"id":"1Dxt-e1bg73b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"EqmnxgpALbVV"},"outputs":[],"source":["import os\n","from dotenv import load_dotenv\n","\n","# load environment variables from .env file\n","from dotenv import load_dotenv, find_dotenv\n","_ = load_dotenv(find_dotenv())"],"id":"EqmnxgpALbVV"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":658601,"status":"ok","timestamp":1705050403928,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"yE3EOlhTrKo6","outputId":"a3c20e9d-b209-4fc7-8667-1a4d7f7d800e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting litellm\n","  Downloading litellm-1.17.3-py3-none-any.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from litellm) (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from litellm) (8.1.7)\n","Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (7.0.1)\n","Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from litellm) (3.1.2)\n","Collecting openai>=1.0.0 (from litellm)\n","  Downloading openai-1.7.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (1.0.0)\n","Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (2.31.0)\n","Collecting tiktoken>=0.4.0 (from litellm)\n","  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from litellm) (0.15.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm) (3.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm) (2.1.3)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->litellm) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.0.0->litellm) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from openai>=1.0.0->litellm)\n","  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->litellm) (1.10.13)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->litellm) (1.3.0)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->litellm) (4.66.1)\n","Collecting typing-extensions<5,>=4.7 (from openai>=1.0.0->litellm)\n","  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (2023.11.17)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.4.0->litellm) (2023.6.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (4.0.3)\n","Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers->litellm) (0.20.2)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.0.0->litellm) (1.2.0)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.0.0->litellm)\n","  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0.0->litellm)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->litellm) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->litellm) (2023.6.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->litellm) (23.2)\n","Installing collected packages: typing-extensions, h11, tiktoken, httpcore, httpx, openai, litellm\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 litellm-1.17.3 openai-1.7.1 tiktoken-0.5.2 typing-extensions-4.9.0\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m638.4/638.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.0/798.0 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m680.8/680.8 kB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m635.4/635.4 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.8/250.8 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for millify (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n","Collecting sentence-transformers\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n","Collecting sentencepiece (from sentence-transformers)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.9.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=5d5ed28d8d9839e6dc602e4a5c3100a8233cf32e33fa4837d7e40d922f442118\n","  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n","Successfully built sentence-transformers\n","Installing collected packages: sentencepiece, sentence-transformers\n","Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.99\n","Collecting pinecone-client\n","  Downloading pinecone_client-2.2.4-py3-none-any.whl (179 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.4/179.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (6.0.1)\n","Collecting loguru>=0.5.0 (from pinecone-client)\n","  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.9.0)\n","Collecting dnspython>=2.0.0 (from pinecone-client)\n","  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.8.2)\n","Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.7)\n","Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.1)\n","Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.23.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (3.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (2023.11.17)\n","Installing collected packages: loguru, dnspython, pinecone-client\n","Successfully installed dnspython-2.4.2 loguru-0.7.2 pinecone-client-2.2.4\n","Collecting datasets\n","  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: multiprocess, datasets\n","Successfully installed datasets-2.16.1 multiprocess-0.70.15\n","Collecting accelerate\n","  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.26.1\n","Collecting einops\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m830.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.7.0\n","Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.0)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.3)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n","Requirement already satisfied: langchain-community<0.1,>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.11)\n","Requirement already satisfied: langchain-core<0.2,>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.10)\n","Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.80)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.5.3)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n","Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.7->langchain) (3.7.1)\n","Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.7->langchain) (23.2)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.14.6 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.14.6)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (1.2.0)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n","Collecting xformers\n","  Downloading xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl (213.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.23.5)\n","Collecting torch==2.1.2 (from xformers)\n","  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.2->xformers)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.2->xformers)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.2->xformers)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.2->xformers)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.2->xformers)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.2->xformers)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.2->xformers)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.2->xformers)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.2->xformers)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.2->xformers)\n","  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.2->xformers)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (2.1.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->xformers)\n","  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.2->xformers) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.2->xformers) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, xformers\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.1.0+cu121\n","    Uninstalling torch-2.1.0+cu121:\n","      Successfully uninstalled torch-2.1.0+cu121\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n","torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n","torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n","torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 torch-2.1.2 xformers-0.0.23.post1\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.11.4)\n","Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->bitsandbytes) (1.23.5)\n","Installing collected packages: bitsandbytes\n","Successfully installed bitsandbytes-0.42.0\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.5.3)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->seaborn) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Collecting chromadb\n","  Downloading chromadb-0.4.22-py3-none-any.whl (509 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.0/509.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.0.3)\n","Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n","Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.5.3)\n","Collecting chroma-hnswlib==0.7.3 (from chromadb)\n","  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.109.0)\n","Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.25.0)\n","Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n","Collecting posthog>=2.4.0 (from chromadb)\n","  Downloading posthog-3.3.1-py2.py3-none-any.whl (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.9.0)\n","Collecting pulsar-client>=3.1.0 (from chromadb)\n","  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n","  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_api-1.22.0-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.22.0-py3-none-any.whl (18 kB)\n","Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n","  Downloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)\n","Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_sdk-1.22.0-py3-none-any.whl (105 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.0)\n","Collecting pypika>=0.48.9 (from chromadb)\n","  Downloading PyPika-0.48.9.tar.gz (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.1)\n","Collecting overrides>=7.3.1 (from chromadb)\n","  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.60.0)\n","Collecting bcrypt>=4.0.1 (from chromadb)\n","  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n","Collecting kubernetes>=28.1.0 (from chromadb)\n","  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n","Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n","Collecting mmh3>=4.0.1 (from chromadb)\n","  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (23.2)\n","Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.0.0)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n","Requirement already satisfied: starlette<0.36.0,>=0.35.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.35.1)\n","Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2023.11.17)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.17.3)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n","Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n","Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n","Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (6.11.0)\n","Requirement already satisfied: backoff<3.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.22.0-py3-none-any.whl (17 kB)\n","Collecting opentelemetry-proto==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.22.0-py3-none-any.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl (14 kB)\n","Collecting opentelemetry-instrumentation==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl (28 kB)\n","Collecting opentelemetry-semantic-conventions==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_semantic_conventions-0.43b0-py3-none-any.whl (36 kB)\n","Collecting opentelemetry-util-http==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_util_http-0.43b0-py3-none-any.whl (6.9 kB)\n","Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n","Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n","Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n","Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n","  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.14.6 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.14.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.6)\n","Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.20.2)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n","Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n","Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n","Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n","Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.36.0,>=0.35.0->fastapi>=0.95.2->chromadb) (3.7.1)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.36.0,>=0.35.0->fastapi>=0.95.2->chromadb) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.36.0,>=0.35.0->fastapi>=0.95.2->chromadb) (1.2.0)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n","Building wheels for collected packages: pypika\n","  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=eb53719d58bb25cddcf7a22aeae764c7a8a14a69a33a863c4a7f9025f7836a08\n","  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n","Successfully built pypika\n","Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, pulsar-client, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, humanfriendly, httptools, chroma-hnswlib, bcrypt, asgiref, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, kubernetes, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n","Successfully installed asgiref-3.7.2 bcrypt-4.1.2 chroma-hnswlib-0.7.3 chromadb-0.4.22 coloredlogs-15.0.1 httptools-0.6.1 humanfriendly-10.0 kubernetes-29.0.0 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.16.3 opentelemetry-api-1.22.0 opentelemetry-exporter-otlp-proto-common-1.22.0 opentelemetry-exporter-otlp-proto-grpc-1.22.0 opentelemetry-instrumentation-0.43b0 opentelemetry-instrumentation-asgi-0.43b0 opentelemetry-instrumentation-fastapi-0.43b0 opentelemetry-proto-1.22.0 opentelemetry-sdk-1.22.0 opentelemetry-semantic-conventions-0.43b0 opentelemetry-util-http-0.43b0 overrides-7.4.0 posthog-3.3.1 pulsar-client-3.4.0 pypika-0.48.9 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n","Collecting evaluate\n","  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n","Collecting responses<0.19 (from evaluate)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (10.0.1)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.11.17)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n","Installing collected packages: responses, evaluate\n","Successfully installed evaluate-0.4.1 responses-0.18.0\n","Collecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.23.5)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.1)\n","Building wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=f3f19ed2e08b00962e1dc73bfb99e696b85f680015a796730899d1f752509147\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge_score\n","Installing collected packages: rouge_score\n","Successfully installed rouge_score-0.1.2\n","Collecting bert_score\n","  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m910.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.1.2)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.5.3)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.35.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.31.0)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.66.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (23.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2023.3.post1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2.18.1)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (12.1.105)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2.1.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert_score) (12.3.101)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.20.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2023.6.3)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2023.11.17)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n","Installing collected packages: bert_score\n","Successfully installed bert_score-0.3.13\n"]}],"source":["%pip install litellm\n","%pip install -qU trulens_eval pydantic fastapi kaleido python-multipart uvicorn cohere openai tiktoken \"llama-index\"\n","%pip install transformers\n","%pip install sentence-transformers\n","%pip install pinecone-client\n","%pip install datasets\n","%pip install accelerate\n","%pip install einops\n","%pip install langchain\n","%pip install xformers\n","%pip install bitsandbytes\n","%pip install matplotlib seaborn tqdm\n","%pip install chromadb\n","%pip install evaluate\n","%pip install rouge_score\n","%pip install bert_score"],"id":"yE3EOlhTrKo6"},{"cell_type":"markdown","metadata":{"id":"dLDAPMVST2Ds"},"source":["\n","\n","## Subtask 1.1: Data Preparation\n","\n"],"id":"dLDAPMVST2Ds"},{"cell_type":"markdown","metadata":{"id":"9b9odZyFLDin"},"source":["We need a collection of documents to perform our retrieval on. To make it closer to your final project, you will be downloading and using a subset of the LangChain documentation. We get some of the `.html` files located on the site. The code below will download all HTML files from the links on the webpage into a `docs` directory. `-l1` limits the download to only the first level of depth.\n"],"id":"9b9odZyFLDin"},{"cell_type":"code","source":["import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\""],"metadata":{"id":"JXe86ZKVFfrm"},"id":"JXe86ZKVFfrm","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56212,"status":"ok","timestamp":1705053388937,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"qiodFLkaLUsF","outputId":"3ebfcce1-06ce-4389-fa43-f821c19f3693"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-01-12 09:55:32--  https://api.python.langchain.com/en/stable/langchain_api_reference.html\n","Resolving api.python.langchain.com (api.python.langchain.com)... 104.17.32.82, 104.17.33.82, 2606:4700::6811:2152, ...\n","Connecting to api.python.langchain.com (api.python.langchain.com)|104.17.32.82|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/langchain_api_reference.html’\n","\n","\r          api.pytho     [<=>                 ]       0  --.-KB/s               \rapi.python.langchai     [ <=>                ] 256.55K  --.-KB/s    in 0.03s   \n","\n","2024-01-12 09:55:32 (8.92 MB/s) - ‘docs/api.python.langchain.com/en/stable/langchain_api_reference.html’ saved [262709]\n","\n","Loading robots.txt; please ignore errors.\n","--2024-01-12 09:55:32--  https://api.python.langchain.com/robots.txt\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 99 [text/plain]\n","Saving to: ‘docs/api.python.langchain.com/robots.txt.tmp’\n","\n","api.python.langchai 100%[===================>]      99  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:33 (59.0 MB/s) - ‘docs/api.python.langchain.com/robots.txt.tmp’ saved [99/99]\n","\n","Removing docs/api.python.langchain.com/robots.txt.tmp.\n","--2024-01-12 09:55:33--  https://api.python.langchain.com/en/latest/langchain_api_reference.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/latest/langchain_api_reference.html’\n","\n","api.python.langchai     [ <=>                ] 255.69K  --.-KB/s    in 0.01s   \n","\n","2024-01-12 09:55:33 (25.2 MB/s) - ‘docs/api.python.langchain.com/en/latest/langchain_api_reference.html’ saved [261827]\n","\n","--2024-01-12 09:55:33--  https://api.python.langchain.com/en/stable/core_api_reference.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/core_api_reference.html’\n","\n","api.python.langchai     [ <=>                ] 171.08K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:33 (47.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/core_api_reference.html’ saved [175186]\n","\n","--2024-01-12 09:55:33--  https://api.python.langchain.com/en/stable/community_api_reference.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/community_api_reference.html’\n","\n","api.python.langchai     [ <=>                ] 664.56K  --.-KB/s    in 0.02s   \n","\n","2024-01-12 09:55:33 (36.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/community_api_reference.html’ saved [680512]\n","\n","--2024-01-12 09:55:33--  https://api.python.langchain.com/en/stable/experimental_api_reference.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/experimental_api_reference.html’\n","\n","api.python.langchai     [ <=>                ] 129.77K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:33 (39.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/experimental_api_reference.html’ saved [132886]\n","\n","--2024-01-12 09:55:33--  https://api.python.langchain.com/en/stable/google_genai_api_reference.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/google_genai_api_reference.html’\n","\n","api.python.langchai     [ <=>                ]  15.17K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:33 (53.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/google_genai_api_reference.html’ saved [15539]\n","\n","--2024-01-12 09:55:33--  https://api.python.langchain.com/en/stable/anthropic_api_reference.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/anthropic_api_reference.html’\n","\n","api.python.langchai     [ <=>                ]  11.35K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:33 (214 MB/s) - ‘docs/api.python.langchain.com/en/stable/anthropic_api_reference.html’ saved [11623]\n","\n","--2024-01-12 09:55:33--  https://api.python.langchain.com/en/stable/nvidia_trt_api_reference.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/nvidia_trt_api_reference.html’\n","\n","api.python.langchai     [ <=>                ]  12.16K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:33 (42.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/nvidia_trt_api_reference.html’ saved [12456]\n","\n","--2024-01-12 09:55:33--  https://api.python.langchain.com/en/stable/mistralai_api_reference.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/mistralai_api_reference.html’\n","\n","api.python.langchai     [ <=>                ]  11.79K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:34 (238 MB/s) - ‘docs/api.python.langchain.com/en/stable/mistralai_api_reference.html’ saved [12070]\n","\n","--2024-01-12 09:55:34--  https://api.python.langchain.com/en/stable/together_api_reference.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/together_api_reference.html’\n","\n","api.python.langchai     [ <=>                ]  10.95K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:34 (137 MB/s) - ‘docs/api.python.langchain.com/en/stable/together_api_reference.html’ saved [11208]\n","\n","--2024-01-12 09:55:34--  https://api.python.langchain.com/en/stable/nvidia_ai_endpoints_api_reference.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/nvidia_ai_endpoints_api_reference.html’\n","\n","api.python.langchai     [ <=>                ]  13.13K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:34 (64.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/nvidia_ai_endpoints_api_reference.html’ saved [13446]\n","\n","--2024-01-12 09:55:34--  https://api.python.langchain.com/en/stable/agents/langchain.agents.agent.Agent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent.Agent.html’\n","\n","api.python.langchai     [ <=>                ]  58.94K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:34 (46.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent.Agent.html’ saved [60351]\n","\n","--2024-01-12 09:55:34--  https://api.python.langchain.com/en/stable/agents/langchain.agents.agent.AgentExecutor.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent.AgentExecutor.html’\n","\n","api.python.langchai     [ <=>                ] 179.57K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:55:34 (37.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent.AgentExecutor.html’ saved [183881]\n","\n","--2024-01-12 09:55:34--  https://api.python.langchain.com/en/stable/agents/langchain.agents.agent.AgentOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent.AgentOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 123.14K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:34 (42.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent.AgentOutputParser.html’ saved [126099]\n","\n","--2024-01-12 09:55:34--  https://api.python.langchain.com/en/stable/agents/langchain.agents.agent.BaseMultiActionAgent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent.BaseMultiActionAgent.html’\n","\n","api.python.langchai     [ <=>                ]  49.49K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:35 (41.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent.BaseMultiActionAgent.html’ saved [50677]\n","\n","--2024-01-12 09:55:35--  https://api.python.langchain.com/en/stable/agents/langchain.agents.agent.BaseSingleActionAgent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent.BaseSingleActionAgent.html’\n","\n","api.python.langchai     [ <=>                ]  52.27K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:35 (55.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent.BaseSingleActionAgent.html’ saved [53526]\n","\n","--2024-01-12 09:55:35--  https://api.python.langchain.com/en/stable/agents/langchain.agents.agent.ExceptionTool.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent.ExceptionTool.html’\n","\n","api.python.langchai     [ <=>                ] 138.25K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:35 (35.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent.ExceptionTool.html’ saved [141565]\n","\n","--2024-01-12 09:55:35--  https://api.python.langchain.com/en/stable/agents/langchain.agents.agent.LLMSingleActionAgent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent.LLMSingleActionAgent.html’\n","\n","api.python.langchai     [ <=>                ]  55.49K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:35 (55.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent.LLMSingleActionAgent.html’ saved [56817]\n","\n","--2024-01-12 09:55:35--  https://api.python.langchain.com/en/stable/agents/langchain.agents.agent.MultiActionAgentOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent.MultiActionAgentOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 123.67K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:35 (27.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent.MultiActionAgentOutputParser.html’ saved [126640]\n","\n","--2024-01-12 09:55:35--  https://api.python.langchain.com/en/stable/agents/langchain.agents.agent.RunnableAgent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent.RunnableAgent.html’\n","\n","api.python.langchai     [ <=>                ]  52.91K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:35 (43.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent.RunnableAgent.html’ saved [54182]\n","\n","--2024-01-12 09:55:35--  https://api.python.langchain.com/en/stable/agents/langchain.agents.agent.RunnableMultiActionAgent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent.RunnableMultiActionAgent.html’\n","\n","api.python.langchai     [ <=>                ]  51.14K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:35 (35.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent.RunnableMultiActionAgent.html’ saved [52367]\n","\n","--2024-01-12 09:55:35--  https://api.python.langchain.com/en/stable/agents/langchain.agents.agent_iterator.AgentExecutorIterator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent_iterator.AgentExecutorIterator.html’\n","\n","api.python.langchai     [ <=>                ]  26.32K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:35 (73.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent_iterator.AgentExecutorIterator.html’ saved [26948]\n","\n","--2024-01-12 09:55:35--  https://api.python.langchain.com/en/stable/agents/langchain.agents.agent_toolkits.vectorstore.toolkit.VectorStoreInfo.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent_toolkits.vectorstore.toolkit.VectorStoreInfo.html’\n","\n","api.python.langchai     [ <=>                ]  41.05K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:36 (322 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent_toolkits.vectorstore.toolkit.VectorStoreInfo.html’ saved [42032]\n","\n","--2024-01-12 09:55:36--  https://api.python.langchain.com/en/stable/agents/langchain.agents.agent_toolkits.vectorstore.toolkit.VectorStoreRouterToolkit.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent_toolkits.vectorstore.toolkit.VectorStoreRouterToolkit.html’\n","\n","api.python.langchai     [ <=>                ]  42.60K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:36 (28.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent_toolkits.vectorstore.toolkit.VectorStoreRouterToolkit.html’ saved [43621]\n","\n","--2024-01-12 09:55:36--  https://api.python.langchain.com/en/stable/agents/langchain.agents.agent_toolkits.vectorstore.toolkit.VectorStoreToolkit.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent_toolkits.vectorstore.toolkit.VectorStoreToolkit.html’\n","\n","api.python.langchai     [ <=>                ]  42.23K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:36 (57.7 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent_toolkits.vectorstore.toolkit.VectorStoreToolkit.html’ saved [43244]\n","\n","--2024-01-12 09:55:36--  https://api.python.langchain.com/en/stable/agents/langchain.agents.agent_types.AgentType.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent_types.AgentType.html’\n","\n","api.python.langchai     [ <=>                ]  29.59K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:36 (47.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent_types.AgentType.html’ saved [30303]\n","\n","--2024-01-12 09:55:36--  https://api.python.langchain.com/en/stable/agents/langchain.agents.chat.base.ChatAgent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.chat.base.ChatAgent.html’\n","\n","api.python.langchai     [ <=>                ]  71.35K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:36 (232 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.chat.base.ChatAgent.html’ saved [73067]\n","\n","--2024-01-12 09:55:36--  https://api.python.langchain.com/en/stable/agents/langchain.agents.chat.output_parser.ChatOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.chat.output_parser.ChatOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 123.71K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:36 (103 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.chat.output_parser.ChatOutputParser.html’ saved [126680]\n","\n","--2024-01-12 09:55:36--  https://api.python.langchain.com/en/stable/agents/langchain.agents.conversational.base.ConversationalAgent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.conversational.base.ConversationalAgent.html’\n","\n","api.python.langchai     [ <=>                ]  82.75K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:55:37 (38.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.conversational.base.ConversationalAgent.html’ saved [84735]\n","\n","--2024-01-12 09:55:37--  https://api.python.langchain.com/en/stable/agents/langchain.agents.conversational.output_parser.ConvoOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.conversational.output_parser.ConvoOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 125.16K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:37 (36.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.conversational.output_parser.ConvoOutputParser.html’ saved [128160]\n","\n","--2024-01-12 09:55:37--  https://api.python.langchain.com/en/stable/agents/langchain.agents.conversational_chat.base.ConversationalChatAgent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.conversational_chat.base.ConversationalChatAgent.html’\n","\n","api.python.langchai     [ <=>                ]  81.03K  --.-KB/s    in 0.007s  \n","\n","2024-01-12 09:55:37 (11.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.conversational_chat.base.ConversationalChatAgent.html’ saved [82973]\n","\n","--2024-01-12 09:55:37--  https://api.python.langchain.com/en/stable/agents/langchain.agents.conversational_chat.output_parser.ConvoOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.conversational_chat.output_parser.ConvoOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 125.20K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:37 (36.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.conversational_chat.output_parser.ConvoOutputParser.html’ saved [128209]\n","\n","--2024-01-12 09:55:37--  https://api.python.langchain.com/en/stable/agents/langchain.agents.mrkl.base.ChainConfig.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.mrkl.base.ChainConfig.html’\n","\n","api.python.langchai     [ <=>                ]  15.46K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:37 (81.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.mrkl.base.ChainConfig.html’ saved [15826]\n","\n","--2024-01-12 09:55:37--  https://api.python.langchain.com/en/stable/agents/langchain.agents.mrkl.base.MRKLChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.mrkl.base.MRKLChain.html’\n","\n","api.python.langchai     [ <=>                ] 177.32K  --.-KB/s    in 0.009s  \n","\n","2024-01-12 09:55:37 (18.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.mrkl.base.MRKLChain.html’ saved [181573]\n","\n","--2024-01-12 09:55:37--  https://api.python.langchain.com/en/stable/agents/langchain.agents.mrkl.base.ZeroShotAgent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.mrkl.base.ZeroShotAgent.html’\n","\n","api.python.langchai     [ <=>                ]  68.16K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:38 (59.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.mrkl.base.ZeroShotAgent.html’ saved [69792]\n","\n","--2024-01-12 09:55:38--  https://api.python.langchain.com/en/stable/agents/langchain.agents.mrkl.output_parser.MRKLOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.mrkl.output_parser.MRKLOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 123.01K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:38 (37.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.mrkl.output_parser.MRKLOutputParser.html’ saved [125967]\n","\n","--2024-01-12 09:55:38--  https://api.python.langchain.com/en/stable/agents/langchain.agents.openai_assistant.base.OpenAIAssistantAction.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.openai_assistant.base.OpenAIAssistantAction.html’\n","\n","api.python.langchai     [ <=>                ]  51.32K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:38 (40.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.openai_assistant.base.OpenAIAssistantAction.html’ saved [52552]\n","\n","--2024-01-12 09:55:38--  https://api.python.langchain.com/en/stable/agents/langchain.agents.openai_assistant.base.OpenAIAssistantFinish.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.openai_assistant.base.OpenAIAssistantFinish.html’\n","\n","api.python.langchai     [ <=>                ]  49.71K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:38 (38.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.openai_assistant.base.OpenAIAssistantFinish.html’ saved [50903]\n","\n","--2024-01-12 09:55:38--  https://api.python.langchain.com/en/stable/agents/langchain.agents.openai_assistant.base.OpenAIAssistantRunnable.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.openai_assistant.base.OpenAIAssistantRunnable.html’\n","\n","api.python.langchai     [ <=>                ] 136.40K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:38 (43.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.openai_assistant.base.OpenAIAssistantRunnable.html’ saved [139670]\n","\n","--2024-01-12 09:55:38--  https://api.python.langchain.com/en/stable/agents/langchain.agents.openai_functions_agent.agent_token_buffer_memory.AgentTokenBufferMemory.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.openai_functions_agent.agent_token_buffer_memory.AgentTokenBufferMemory.html’\n","\n","api.python.langchai     [ <=>                ]  60.18K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:38 (357 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.openai_functions_agent.agent_token_buffer_memory.AgentTokenBufferMemory.html’ saved [61629]\n","\n","--2024-01-12 09:55:38--  https://api.python.langchain.com/en/stable/agents/langchain.agents.openai_functions_agent.base.OpenAIFunctionsAgent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.openai_functions_agent.base.OpenAIFunctionsAgent.html’\n","\n","api.python.langchai     [ <=>                ]  63.42K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:38 (53.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.openai_functions_agent.base.OpenAIFunctionsAgent.html’ saved [64941]\n","\n","--2024-01-12 09:55:38--  https://api.python.langchain.com/en/stable/agents/langchain.agents.openai_functions_multi_agent.base.OpenAIMultiFunctionsAgent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.openai_functions_multi_agent.base.OpenAIMultiFunctionsAgent.html’\n","\n","api.python.langchai     [ <=>                ]  63.06K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:55:39 (40.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.openai_functions_multi_agent.base.OpenAIMultiFunctionsAgent.html’ saved [64572]\n","\n","--2024-01-12 09:55:39--  https://api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.json.JSONAgentOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.json.JSONAgentOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 124.26K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:39 (42.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.json.JSONAgentOutputParser.html’ saved [127239]\n","\n","--2024-01-12 09:55:39--  https://api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.openai_functions.OpenAIFunctionsAgentOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.openai_functions.OpenAIFunctionsAgentOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 128.23K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:39 (43.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.openai_functions.OpenAIFunctionsAgentOutputParser.html’ saved [131304]\n","\n","--2024-01-12 09:55:39--  https://api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.openai_tools.OpenAIToolAgentAction.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.openai_tools.OpenAIToolAgentAction.html’\n","\n","api.python.langchai     [ <=>                ]  51.67K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:39 (60.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.openai_tools.OpenAIToolAgentAction.html’ saved [52907]\n","\n","--2024-01-12 09:55:39--  https://api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.openai_tools.OpenAIToolsAgentOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.openai_tools.OpenAIToolsAgentOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 127.64K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:55:39 (27.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.openai_tools.OpenAIToolsAgentOutputParser.html’ saved [130708]\n","\n","--2024-01-12 09:55:39--  https://api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.react_json_single_input.ReActJsonSingleInputOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.react_json_single_input.ReActJsonSingleInputOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 129.59K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:39 (39.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.react_json_single_input.ReActJsonSingleInputOutputParser.html’ saved [132702]\n","\n","--2024-01-12 09:55:39--  https://api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.react_single_input.ReActSingleInputOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.react_single_input.ReActSingleInputOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 127.70K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:40 (44.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.react_single_input.ReActSingleInputOutputParser.html’ saved [130767]\n","\n","--2024-01-12 09:55:40--  https://api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.self_ask.SelfAskOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.self_ask.SelfAskOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 126.88K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:55:40 (50.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.self_ask.SelfAskOutputParser.html’ saved [129922]\n","\n","--2024-01-12 09:55:40--  https://api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.xml.XMLAgentOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.xml.XMLAgentOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 124.38K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:40 (27.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.xml.XMLAgentOutputParser.html’ saved [127360]\n","\n","--2024-01-12 09:55:40--  https://api.python.langchain.com/en/stable/agents/langchain.agents.react.base.DocstoreExplorer.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.react.base.DocstoreExplorer.html’\n","\n","api.python.langchai     [ <=>                ]  15.60K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:40 (175 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.react.base.DocstoreExplorer.html’ saved [15978]\n","\n","--2024-01-12 09:55:40--  https://api.python.langchain.com/en/stable/agents/langchain.agents.react.base.ReActChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.react.base.ReActChain.html’\n","\n","api.python.langchai     [ <=>                ] 174.77K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:55:40 (34.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.react.base.ReActChain.html’ saved [178962]\n","\n","--2024-01-12 09:55:40--  https://api.python.langchain.com/en/stable/agents/langchain.agents.react.base.ReActDocstoreAgent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.react.base.ReActDocstoreAgent.html’\n","\n","api.python.langchai     [ <=>                ]  58.35K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:40 (49.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.react.base.ReActDocstoreAgent.html’ saved [59750]\n","\n","--2024-01-12 09:55:40--  https://api.python.langchain.com/en/stable/agents/langchain.agents.react.base.ReActTextWorldAgent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.react.base.ReActTextWorldAgent.html’\n","\n","api.python.langchai     [ <=>                ]  58.47K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:40 (351 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.react.base.ReActTextWorldAgent.html’ saved [59870]\n","\n","--2024-01-12 09:55:40--  https://api.python.langchain.com/en/stable/agents/langchain.agents.react.output_parser.ReActOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.react.output_parser.ReActOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 123.06K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:55:41 (63.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.react.output_parser.ReActOutputParser.html’ saved [126012]\n","\n","--2024-01-12 09:55:41--  https://api.python.langchain.com/en/stable/agents/langchain.agents.schema.AgentScratchPadChatPromptTemplate.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.schema.AgentScratchPadChatPromptTemplate.html’\n","\n","api.python.langchai     [ <=>                ] 146.16K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:41 (41.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.schema.AgentScratchPadChatPromptTemplate.html’ saved [149668]\n","\n","--2024-01-12 09:55:41--  https://api.python.langchain.com/en/stable/agents/langchain.agents.self_ask_with_search.base.SelfAskWithSearchAgent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.self_ask_with_search.base.SelfAskWithSearchAgent.html’\n","\n","api.python.langchai     [ <=>                ]  59.60K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:41 (45.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.self_ask_with_search.base.SelfAskWithSearchAgent.html’ saved [61029]\n","\n","--2024-01-12 09:55:41--  https://api.python.langchain.com/en/stable/agents/langchain.agents.self_ask_with_search.base.SelfAskWithSearchChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.self_ask_with_search.base.SelfAskWithSearchChain.html’\n","\n","api.python.langchai     [ <=>                ] 179.12K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:41 (38.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.self_ask_with_search.base.SelfAskWithSearchChain.html’ saved [183424]\n","\n","--2024-01-12 09:55:41--  https://api.python.langchain.com/en/stable/agents/langchain.agents.structured_chat.base.StructuredChatAgent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.structured_chat.base.StructuredChatAgent.html’\n","\n","api.python.langchai     [ <=>                ]  72.52K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:55:41 (47.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.structured_chat.base.StructuredChatAgent.html’ saved [74259]\n","\n","--2024-01-12 09:55:41--  https://api.python.langchain.com/en/stable/agents/langchain.agents.structured_chat.output_parser.StructuredChatOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.structured_chat.output_parser.StructuredChatOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 125.62K  --.-KB/s    in 0.006s  \n","\n","2024-01-12 09:55:41 (19.7 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.structured_chat.output_parser.StructuredChatOutputParser.html’ saved [128640]\n","\n","--2024-01-12 09:55:41--  https://api.python.langchain.com/en/stable/agents/langchain.agents.structured_chat.output_parser.StructuredChatOutputParserWithRetries.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.structured_chat.output_parser.StructuredChatOutputParserWithRetries.html’\n","\n","api.python.langchai     [ <=>                ] 131.83K  --.-KB/s    in 0.02s   \n","\n","2024-01-12 09:55:42 (7.36 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.structured_chat.output_parser.StructuredChatOutputParserWithRetries.html’ saved [134995]\n","\n","--2024-01-12 09:55:42--  https://api.python.langchain.com/en/stable/agents/langchain.agents.tools.InvalidTool.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.tools.InvalidTool.html’\n","\n","api.python.langchai     [ <=>                ] 138.99K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:42 (45.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.tools.InvalidTool.html’ saved [142321]\n","\n","--2024-01-12 09:55:42--  https://api.python.langchain.com/en/stable/agents/langchain.agents.xml.base.XMLAgent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.xml.base.XMLAgent.html’\n","\n","api.python.langchai     [ <=>                ]  55.30K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:42 (40.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.xml.base.XMLAgent.html’ saved [56627]\n","\n","--2024-01-12 09:55:42--  https://api.python.langchain.com/en/stable/agents/langchain.agents.agent_toolkits.conversational_retrieval.openai_functions.create_conversational_retrieval_agent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent_toolkits.conversational_retrieval.openai_functions.create_conversational_retrieval_agent.html’\n","\n","api.python.langchai     [ <=>                ]  16.62K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:42 (52.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent_toolkits.conversational_retrieval.openai_functions.create_conversational_retrieval_agent.html’ saved [17015]\n","\n","--2024-01-12 09:55:42--  https://api.python.langchain.com/en/stable/agents/langchain.agents.agent_toolkits.vectorstore.base.create_vectorstore_agent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent_toolkits.vectorstore.base.create_vectorstore_agent.html’\n","\n","api.python.langchai     [ <=>                ]  19.31K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:42 (185 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent_toolkits.vectorstore.base.create_vectorstore_agent.html’ saved [19778]\n","\n","--2024-01-12 09:55:42--  https://api.python.langchain.com/en/stable/agents/langchain.agents.agent_toolkits.vectorstore.base.create_vectorstore_router_agent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent_toolkits.vectorstore.base.create_vectorstore_router_agent.html’\n","\n","api.python.langchai     [ <=>                ]  19.20K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:42 (47.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent_toolkits.vectorstore.base.create_vectorstore_router_agent.html’ saved [19661]\n","\n","--2024-01-12 09:55:42--  https://api.python.langchain.com/en/stable/agents/langchain.agents.format_scratchpad.log.format_log_to_str.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.format_scratchpad.log.format_log_to_str.html’\n","\n","api.python.langchai     [ <=>                ]  12.37K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:42 (163 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.format_scratchpad.log.format_log_to_str.html’ saved [12666]\n","\n","--2024-01-12 09:55:42--  https://api.python.langchain.com/en/stable/agents/langchain.agents.format_scratchpad.log_to_messages.format_log_to_messages.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.format_scratchpad.log_to_messages.format_log_to_messages.html’\n","\n","api.python.langchai     [ <=>                ]  12.45K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:42 (122 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.format_scratchpad.log_to_messages.format_log_to_messages.html’ saved [12752]\n","\n","--2024-01-12 09:55:42--  https://api.python.langchain.com/en/stable/agents/langchain.agents.format_scratchpad.openai_functions.format_to_openai_function_messages.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.format_scratchpad.openai_functions.format_to_openai_function_messages.html’\n","\n","api.python.langchai     [ <=>                ]  12.53K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:43 (164 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.format_scratchpad.openai_functions.format_to_openai_function_messages.html’ saved [12833]\n","\n","--2024-01-12 09:55:43--  https://api.python.langchain.com/en/stable/agents/langchain.agents.format_scratchpad.openai_functions.format_to_openai_functions.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.format_scratchpad.openai_functions.format_to_openai_functions.html’\n","\n","api.python.langchai     [ <=>                ]  12.25K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:43 (124 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.format_scratchpad.openai_functions.format_to_openai_functions.html’ saved [12540]\n","\n","--2024-01-12 09:55:43--  https://api.python.langchain.com/en/stable/agents/langchain.agents.format_scratchpad.openai_tools.format_to_openai_tool_messages.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.format_scratchpad.openai_tools.format_to_openai_tool_messages.html’\n","\n","api.python.langchai     [ <=>                ]  12.45K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:43 (180 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.format_scratchpad.openai_tools.format_to_openai_tool_messages.html’ saved [12745]\n","\n","--2024-01-12 09:55:43--  https://api.python.langchain.com/en/stable/agents/langchain.agents.format_scratchpad.xml.format_xml.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.format_scratchpad.xml.format_xml.html’\n","\n","api.python.langchai     [ <=>                ]  11.73K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:43 (162 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.format_scratchpad.xml.format_xml.html’ saved [12009]\n","\n","--2024-01-12 09:55:43--  https://api.python.langchain.com/en/stable/agents/langchain.agents.initialize.initialize_agent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.initialize.initialize_agent.html’\n","\n","api.python.langchai     [ <=>                ]  27.76K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:43 (233 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.initialize.initialize_agent.html’ saved [28428]\n","\n","--2024-01-12 09:55:43--  https://api.python.langchain.com/en/stable/agents/langchain.agents.json_chat.base.create_json_chat_agent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.json_chat.base.create_json_chat_agent.html’\n","\n","api.python.langchai     [ <=>                ]  16.03K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:43 (4.04 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.json_chat.base.create_json_chat_agent.html’ saved [16412]\n","\n","--2024-01-12 09:55:43--  https://api.python.langchain.com/en/stable/agents/langchain.agents.load_tools.get_all_tool_names.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.load_tools.get_all_tool_names.html’\n","\n","api.python.langchai     [ <=>                ]  10.83K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:43 (216 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.load_tools.get_all_tool_names.html’ saved [11090]\n","\n","--2024-01-12 09:55:43--  https://api.python.langchain.com/en/stable/agents/langchain.agents.load_tools.load_huggingface_tool.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.load_tools.load_huggingface_tool.html’\n","\n","api.python.langchai     [ <=>                ]  13.54K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:43 (265 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.load_tools.load_huggingface_tool.html’ saved [13862]\n","\n","--2024-01-12 09:55:43--  https://api.python.langchain.com/en/stable/agents/langchain.agents.load_tools.load_tools.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.load_tools.load_tools.html’\n","\n","api.python.langchai     [ <=>                ]  22.03K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:44 (30.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.load_tools.load_tools.html’ saved [22562]\n","\n","--2024-01-12 09:55:44--  https://api.python.langchain.com/en/stable/agents/langchain.agents.loading.load_agent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.loading.load_agent.html’\n","\n","api.python.langchai     [ <=>                ]  12.29K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:44 (163 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.loading.load_agent.html’ saved [12590]\n","\n","--2024-01-12 09:55:44--  https://api.python.langchain.com/en/stable/agents/langchain.agents.loading.load_agent_from_config.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.loading.load_agent_from_config.html’\n","\n","api.python.langchai     [ <=>                ]  13.81K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:44 (99.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.loading.load_agent_from_config.html’ saved [14139]\n","\n","--2024-01-12 09:55:44--  https://api.python.langchain.com/en/stable/agents/langchain.agents.openai_functions_agent.base.create_openai_functions_agent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.openai_functions_agent.base.create_openai_functions_agent.html’\n","\n","api.python.langchai     [ <=>                ]  16.39K  --.-KB/s    in 0.006s  \n","\n","2024-01-12 09:55:44 (2.79 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.openai_functions_agent.base.create_openai_functions_agent.html’ saved [16779]\n","\n","--2024-01-12 09:55:44--  https://api.python.langchain.com/en/stable/agents/langchain.agents.openai_tools.base.create_openai_tools_agent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.openai_tools.base.create_openai_tools_agent.html’\n","\n","api.python.langchai     [ <=>                ]  16.02K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:44 (40.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.openai_tools.base.create_openai_tools_agent.html’ saved [16403]\n","\n","--2024-01-12 09:55:44--  https://api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.openai_tools.parse_ai_message_to_openai_tool_action.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.openai_tools.parse_ai_message_to_openai_tool_action.html’\n","\n","api.python.langchai     [ <=>                ]  12.18K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:44 (181 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.output_parsers.openai_tools.parse_ai_message_to_openai_tool_action.html’ saved [12473]\n","\n","--2024-01-12 09:55:44--  https://api.python.langchain.com/en/stable/agents/langchain.agents.react.agent.create_react_agent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.react.agent.create_react_agent.html’\n","\n","api.python.langchai     [ <=>                ]  15.58K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:44 (73.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.react.agent.create_react_agent.html’ saved [15953]\n","\n","--2024-01-12 09:55:44--  https://api.python.langchain.com/en/stable/agents/langchain.agents.self_ask_with_search.base.create_self_ask_with_search_agent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.self_ask_with_search.base.create_self_ask_with_search_agent.html’\n","\n","api.python.langchai     [ <=>                ]  15.34K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:44 (66.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.self_ask_with_search.base.create_self_ask_with_search_agent.html’ saved [15706]\n","\n","--2024-01-12 09:55:44--  https://api.python.langchain.com/en/stable/agents/langchain.agents.structured_chat.base.create_structured_chat_agent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.structured_chat.base.create_structured_chat_agent.html’\n","\n","api.python.langchai     [ <=>                ]  16.16K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:45 (44.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.structured_chat.base.create_structured_chat_agent.html’ saved [16551]\n","\n","--2024-01-12 09:55:45--  https://api.python.langchain.com/en/stable/agents/langchain.agents.utils.validate_tools_single_input.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.utils.validate_tools_single_input.html’\n","\n","api.python.langchai     [ <=>                ]  11.43K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:45 (176 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.utils.validate_tools_single_input.html’ saved [11706]\n","\n","--2024-01-12 09:55:45--  https://api.python.langchain.com/en/stable/agents/langchain.agents.xml.base.create_xml_agent.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.xml.base.create_xml_agent.html’\n","\n","api.python.langchai     [ <=>                ]  15.53K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:45 (42.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/agents/langchain.agents.xml.base.create_xml_agent.html’ saved [15907]\n","\n","--2024-01-12 09:55:45--  https://api.python.langchain.com/en/stable/callbacks/langchain.callbacks.file.FileCallbackHandler.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/callbacks/langchain.callbacks.file.FileCallbackHandler.html’\n","\n","api.python.langchai     [ <=>                ]  67.21K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:55:45 (30.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/callbacks/langchain.callbacks.file.FileCallbackHandler.html’ saved [68820]\n","\n","--2024-01-12 09:55:45--  https://api.python.langchain.com/en/stable/callbacks/langchain.callbacks.streaming_aiter.AsyncIteratorCallbackHandler.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/callbacks/langchain.callbacks.streaming_aiter.AsyncIteratorCallbackHandler.html’\n","\n","api.python.langchai     [ <=>                ]  70.84K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:45 (47.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/callbacks/langchain.callbacks.streaming_aiter.AsyncIteratorCallbackHandler.html’ saved [72540]\n","\n","--2024-01-12 09:55:45--  https://api.python.langchain.com/en/stable/callbacks/langchain.callbacks.streaming_aiter_final_only.AsyncFinalIteratorCallbackHandler.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/callbacks/langchain.callbacks.streaming_aiter_final_only.AsyncFinalIteratorCallbackHandler.html’\n","\n","api.python.langchai     [ <=>                ]  78.66K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:55:45 (40.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/callbacks/langchain.callbacks.streaming_aiter_final_only.AsyncFinalIteratorCallbackHandler.html’ saved [80546]\n","\n","--2024-01-12 09:55:45--  https://api.python.langchain.com/en/stable/callbacks/langchain.callbacks.streaming_stdout_final_only.FinalStreamingStdOutCallbackHandler.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/callbacks/langchain.callbacks.streaming_stdout_final_only.FinalStreamingStdOutCallbackHandler.html’\n","\n","api.python.langchai     [ <=>                ]  58.60K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:55:45 (30.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/callbacks/langchain.callbacks.streaming_stdout_final_only.FinalStreamingStdOutCallbackHandler.html’ saved [60007]\n","\n","--2024-01-12 09:55:45--  https://api.python.langchain.com/en/stable/callbacks/langchain.callbacks.tracers.logging.LoggingCallbackHandler.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/callbacks/langchain.callbacks.tracers.logging.LoggingCallbackHandler.html’\n","\n","api.python.langchai     [ <=>                ]  73.64K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:45 (50.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/callbacks/langchain.callbacks.tracers.logging.LoggingCallbackHandler.html’ saved [75409]\n","\n","--2024-01-12 09:55:45--  https://api.python.langchain.com/en/stable/chains/langchain.chains.api.base.APIChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.api.base.APIChain.html’\n","\n","api.python.langchai     [ <=>                ] 169.28K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:46 (42.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.api.base.APIChain.html’ saved [173338]\n","\n","--2024-01-12 09:55:46--  https://api.python.langchain.com/en/stable/chains/langchain.chains.api.openapi.chain.OpenAPIEndpointChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.api.openapi.chain.OpenAPIEndpointChain.html’\n","\n","api.python.langchai     [ <=>                ] 172.80K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:46 (37.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.api.openapi.chain.OpenAPIEndpointChain.html’ saved [176944]\n","\n","--2024-01-12 09:55:46--  https://api.python.langchain.com/en/stable/chains/langchain.chains.api.openapi.requests_chain.APIRequesterChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.api.openapi.requests_chain.APIRequesterChain.html’\n","\n","api.python.langchai     [ <=>                ] 200.40K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:55:46 (41.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.api.openapi.requests_chain.APIRequesterChain.html’ saved [205206]\n","\n","--2024-01-12 09:55:46--  https://api.python.langchain.com/en/stable/chains/langchain.chains.api.openapi.requests_chain.APIRequesterOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.api.openapi.requests_chain.APIRequesterOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 124.29K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:46 (46.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.api.openapi.requests_chain.APIRequesterOutputParser.html’ saved [127271]\n","\n","--2024-01-12 09:55:46--  https://api.python.langchain.com/en/stable/chains/langchain.chains.api.openapi.response_chain.APIResponderChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.api.openapi.response_chain.APIResponderChain.html’\n","\n","api.python.langchai     [ <=>                ] 200.12K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:55:46 (43.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.api.openapi.response_chain.APIResponderChain.html’ saved [204926]\n","\n","--2024-01-12 09:55:46--  https://api.python.langchain.com/en/stable/chains/langchain.chains.api.openapi.response_chain.APIResponderOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.api.openapi.response_chain.APIResponderOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 124.29K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:46 (42.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.api.openapi.response_chain.APIResponderOutputParser.html’ saved [127273]\n","\n","--2024-01-12 09:55:46--  https://api.python.langchain.com/en/stable/chains/langchain.chains.base.Chain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.base.Chain.html’\n","\n","api.python.langchai     [ <=>                ] 161.37K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:47 (40.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.base.Chain.html’ saved [165241]\n","\n","--2024-01-12 09:55:47--  https://api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.base.AnalyzeDocumentChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.base.AnalyzeDocumentChain.html’\n","\n","api.python.langchai     [ <=>                ] 160.87K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:55:47 (34.7 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.base.AnalyzeDocumentChain.html’ saved [164735]\n","\n","--2024-01-12 09:55:47--  https://api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.base.BaseCombineDocumentsChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.base.BaseCombineDocumentsChain.html’\n","\n","api.python.langchai     [ <=>                ] 167.72K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:47 (39.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.base.BaseCombineDocumentsChain.html’ saved [171741]\n","\n","--2024-01-12 09:55:47--  https://api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain.html’\n","\n","api.python.langchai     [ <=>                ] 182.66K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:47 (142 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain.html’ saved [187048]\n","\n","--2024-01-12 09:55:47--  https://api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.map_rerank.MapRerankDocumentsChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.map_rerank.MapRerankDocumentsChain.html’\n","\n","api.python.langchai     [ <=>                ] 177.67K  --.-KB/s    in 0.008s  \n","\n","2024-01-12 09:55:47 (21.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.map_rerank.MapRerankDocumentsChain.html’ saved [181932]\n","\n","--2024-01-12 09:55:47--  https://api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.reduce.AsyncCombineDocsProtocol.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.reduce.AsyncCombineDocsProtocol.html’\n","\n","api.python.langchai     [ <=>                ]  12.35K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:47 (171 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.reduce.AsyncCombineDocsProtocol.html’ saved [12648]\n","\n","--2024-01-12 09:55:47--  https://api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.reduce.CombineDocsProtocol.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.reduce.CombineDocsProtocol.html’\n","\n","api.python.langchai     [ <=>                ]  12.28K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:47 (123 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.reduce.CombineDocsProtocol.html’ saved [12573]\n","\n","--2024-01-12 09:55:47--  https://api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.reduce.ReduceDocumentsChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.reduce.ReduceDocumentsChain.html’\n","\n","api.python.langchai     [ <=>                ] 179.00K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:48 (47.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.reduce.ReduceDocumentsChain.html’ saved [183294]\n","\n","--2024-01-12 09:55:48--  https://api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.refine.RefineDocumentsChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.refine.RefineDocumentsChain.html’\n","\n","api.python.langchai     [ <=>                ] 178.39K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:48 (39.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.refine.RefineDocumentsChain.html’ saved [182675]\n","\n","--2024-01-12 09:55:48--  https://api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.stuff.StuffDocumentsChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.stuff.StuffDocumentsChain.html’\n","\n","api.python.langchai     [ <=>                ] 175.79K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:48 (201 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.stuff.StuffDocumentsChain.html’ saved [180007]\n","\n","--2024-01-12 09:55:48--  https://api.python.langchain.com/en/stable/chains/langchain.chains.constitutional_ai.base.ConstitutionalChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.constitutional_ai.base.ConstitutionalChain.html’\n","\n","api.python.langchai     [ <=>                ] 233.25K  --.-KB/s    in 0.007s  \n","\n","2024-01-12 09:55:48 (34.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.constitutional_ai.base.ConstitutionalChain.html’ saved [238851]\n","\n","--2024-01-12 09:55:48--  https://api.python.langchain.com/en/stable/chains/langchain.chains.constitutional_ai.models.ConstitutionalPrinciple.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.constitutional_ai.models.ConstitutionalPrinciple.html’\n","\n","api.python.langchai     [ <=>                ]  40.61K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:48 (35.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.constitutional_ai.models.ConstitutionalPrinciple.html’ saved [41580]\n","\n","--2024-01-12 09:55:48--  https://api.python.langchain.com/en/stable/chains/langchain.chains.conversation.base.ConversationChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.conversation.base.ConversationChain.html’\n","\n","api.python.langchai     [ <=>                ] 200.77K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:48 (44.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.conversation.base.ConversationChain.html’ saved [205589]\n","\n","--2024-01-12 09:55:48--  https://api.python.langchain.com/en/stable/chains/langchain.chains.conversational_retrieval.base.BaseConversationalRetrievalChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.conversational_retrieval.base.BaseConversationalRetrievalChain.html’\n","\n","api.python.langchai     [ <=>                ] 170.51K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:49 (58.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.conversational_retrieval.base.BaseConversationalRetrievalChain.html’ saved [174600]\n","\n","--2024-01-12 09:55:49--  https://api.python.langchain.com/en/stable/chains/langchain.chains.conversational_retrieval.base.ChatVectorDBChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.conversational_retrieval.base.ChatVectorDBChain.html’\n","\n","api.python.langchai     [ <=>                ] 176.27K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:49 (45.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.conversational_retrieval.base.ChatVectorDBChain.html’ saved [180497]\n","\n","--2024-01-12 09:55:49--  https://api.python.langchain.com/en/stable/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html’\n","\n","api.python.langchai     [ <=>                ] 186.08K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:55:49 (36.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html’ saved [190547]\n","\n","--2024-01-12 09:55:49--  https://api.python.langchain.com/en/stable/chains/langchain.chains.conversational_retrieval.base.InputType.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.conversational_retrieval.base.InputType.html’\n","\n","api.python.langchai     [ <=>                ]  40.33K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:49 (73.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.conversational_retrieval.base.InputType.html’ saved [41300]\n","\n","--2024-01-12 09:55:49--  https://api.python.langchain.com/en/stable/chains/langchain.chains.elasticsearch_database.base.ElasticsearchDatabaseChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.elasticsearch_database.base.ElasticsearchDatabaseChain.html’\n","\n","api.python.langchai     [ <=>                ] 173.63K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:55:49 (33.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.elasticsearch_database.base.ElasticsearchDatabaseChain.html’ saved [177799]\n","\n","--2024-01-12 09:55:49--  https://api.python.langchain.com/en/stable/chains/langchain.chains.flare.base.FlareChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.flare.base.FlareChain.html’\n","\n","api.python.langchai     [ <=>                ] 166.71K  --.-KB/s    in 0.008s  \n","\n","2024-01-12 09:55:49 (21.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.flare.base.FlareChain.html’ saved [170712]\n","\n","--2024-01-12 09:55:49--  https://api.python.langchain.com/en/stable/chains/langchain.chains.flare.base.QuestionGeneratorChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.flare.base.QuestionGeneratorChain.html’\n","\n","api.python.langchai     [ <=>                ] 198.38K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:50 (44.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.flare.base.QuestionGeneratorChain.html’ saved [203142]\n","\n","--2024-01-12 09:55:50--  https://api.python.langchain.com/en/stable/chains/langchain.chains.flare.prompts.FinishedOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.flare.prompts.FinishedOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 123.78K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:50 (43.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.flare.prompts.FinishedOutputParser.html’ saved [126751]\n","\n","--2024-01-12 09:55:50--  https://api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.arangodb.ArangoGraphQAChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.arangodb.ArangoGraphQAChain.html’\n","\n","api.python.langchai     [ <=>                ] 190.19K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:55:50 (38.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.arangodb.ArangoGraphQAChain.html’ saved [194757]\n","\n","--2024-01-12 09:55:50--  https://api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.base.GraphQAChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.base.GraphQAChain.html’\n","\n","api.python.langchai     [ <=>                ] 166.39K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:50 (39.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.base.GraphQAChain.html’ saved [170387]\n","\n","--2024-01-12 09:55:50--  https://api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.cypher.GraphCypherQAChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.cypher.GraphCypherQAChain.html’\n","\n","api.python.langchai     [ <=>                ] 172.95K  --.-KB/s    in 0.008s  \n","\n","2024-01-12 09:55:50 (21.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.cypher.GraphCypherQAChain.html’ saved [177101]\n","\n","--2024-01-12 09:55:50--  https://api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.cypher_utils.CypherQueryCorrector.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.cypher_utils.CypherQueryCorrector.html’\n","\n","api.python.langchai     [ <=>                ]  32.04K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:50 (32.7 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.cypher_utils.CypherQueryCorrector.html’ saved [32811]\n","\n","--2024-01-12 09:55:50--  https://api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.cypher_utils.Schema.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.cypher_utils.Schema.html’\n","\n","api.python.langchai     [ <=>                ]  14.65K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:50 (32.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.cypher_utils.Schema.html’ saved [15005]\n","\n","--2024-01-12 09:55:50--  https://api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.falkordb.FalkorDBQAChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.falkordb.FalkorDBQAChain.html’\n","\n","api.python.langchai     [ <=>                ] 170.70K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:55:51 (76.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.falkordb.FalkorDBQAChain.html’ saved [174792]\n","\n","--2024-01-12 09:55:51--  https://api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.hugegraph.HugeGraphQAChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.hugegraph.HugeGraphQAChain.html’\n","\n","api.python.langchai     [ <=>                ] 168.60K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:55:51 (35.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.hugegraph.HugeGraphQAChain.html’ saved [172644]\n","\n","--2024-01-12 09:55:51--  https://api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.kuzu.KuzuQAChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.kuzu.KuzuQAChain.html’\n","\n","api.python.langchai     [ <=>                ] 168.85K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:51 (44.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.kuzu.KuzuQAChain.html’ saved [172905]\n","\n","--2024-01-12 09:55:51--  https://api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.nebulagraph.NebulaGraphQAChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.nebulagraph.NebulaGraphQAChain.html’\n","\n","api.python.langchai     [ <=>                ] 171.75K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:55:51 (31.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.nebulagraph.NebulaGraphQAChain.html’ saved [175867]\n","\n","--2024-01-12 09:55:51--  https://api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.neptune_cypher.NeptuneOpenCypherQAChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.neptune_cypher.NeptuneOpenCypherQAChain.html’\n","\n","api.python.langchai     [ <=>                ] 172.61K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:51 (43.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.neptune_cypher.NeptuneOpenCypherQAChain.html’ saved [176750]\n","\n","--2024-01-12 09:55:51--  https://api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.sparql.GraphSparqlQAChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.sparql.GraphSparqlQAChain.html’\n","\n","api.python.langchai     [ <=>                ] 181.49K  --.-KB/s    in 0.007s  \n","\n","2024-01-12 09:55:51 (26.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.sparql.GraphSparqlQAChain.html’ saved [185846]\n","\n","--2024-01-12 09:55:51--  https://api.python.langchain.com/en/stable/chains/langchain.chains.hyde.base.HypotheticalDocumentEmbedder.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.hyde.base.HypotheticalDocumentEmbedder.html’\n","\n","api.python.langchai     [ <=>                ] 171.10K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:51 (40.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.hyde.base.HypotheticalDocumentEmbedder.html’ saved [175206]\n","\n","--2024-01-12 09:55:51--  https://api.python.langchain.com/en/stable/chains/langchain.chains.llm.LLMChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.llm.LLMChain.html’\n","\n","api.python.langchai     [ <=>                ] 202.29K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:55:51 (41.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.llm.LLMChain.html’ saved [207150]\n","\n","--2024-01-12 09:55:51--  https://api.python.langchain.com/en/stable/chains/langchain.chains.llm_checker.base.LLMCheckerChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.llm_checker.base.LLMCheckerChain.html’\n","\n","api.python.langchai     [ <=>                ] 172.15K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:52 (39.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.llm_checker.base.LLMCheckerChain.html’ saved [176280]\n","\n","--2024-01-12 09:55:52--  https://api.python.langchain.com/en/stable/chains/langchain.chains.llm_math.base.LLMMathChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.llm_math.base.LLMMathChain.html’\n","\n","api.python.langchai     [ <=>                ] 165.94K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:55:52 (30.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.llm_math.base.LLMMathChain.html’ saved [169919]\n","\n","--2024-01-12 09:55:52--  https://api.python.langchain.com/en/stable/chains/langchain.chains.llm_requests.LLMRequestsChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.llm_requests.LLMRequestsChain.html’\n","\n","api.python.langchai     [ <=>                ] 159.55K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:52 (50.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.llm_requests.LLMRequestsChain.html’ saved [163381]\n","\n","--2024-01-12 09:55:52--  https://api.python.langchain.com/en/stable/chains/langchain.chains.llm_summarization_checker.base.LLMSummarizationCheckerChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.llm_summarization_checker.base.LLMSummarizationCheckerChain.html’\n","\n","api.python.langchai     [ <=>                ] 188.75K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:52 (44.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.llm_summarization_checker.base.LLMSummarizationCheckerChain.html’ saved [193282]\n","\n","--2024-01-12 09:55:52--  https://api.python.langchain.com/en/stable/chains/langchain.chains.mapreduce.MapReduceChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.mapreduce.MapReduceChain.html’\n","\n","api.python.langchai     [ <=>                ] 163.39K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:55:52 (31.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.mapreduce.MapReduceChain.html’ saved [167314]\n","\n","--2024-01-12 09:55:52--  https://api.python.langchain.com/en/stable/chains/langchain.chains.moderation.OpenAIModerationChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.moderation.OpenAIModerationChain.html’\n","\n","api.python.langchai     [ <=>                ] 161.48K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:52 (44.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.moderation.OpenAIModerationChain.html’ saved [165356]\n","\n","--2024-01-12 09:55:52--  https://api.python.langchain.com/en/stable/chains/langchain.chains.natbot.base.NatBotChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.natbot.base.NatBotChain.html’\n","\n","api.python.langchai     [ <=>                ] 164.60K  --.-KB/s    in 0.008s  \n","\n","2024-01-12 09:55:53 (19.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.natbot.base.NatBotChain.html’ saved [168546]\n","\n","--2024-01-12 09:55:53--  https://api.python.langchain.com/en/stable/chains/langchain.chains.natbot.crawler.Crawler.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.natbot.crawler.Crawler.html’\n","\n","api.python.langchai     [ <=>                ]  19.74K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:53 (78.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.natbot.crawler.Crawler.html’ saved [20210]\n","\n","--2024-01-12 09:55:53--  https://api.python.langchain.com/en/stable/chains/langchain.chains.natbot.crawler.ElementInViewPort.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.natbot.crawler.ElementInViewPort.html’\n","\n","api.python.langchai     [ <=>                ]  15.70K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:53 (37.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.natbot.crawler.ElementInViewPort.html’ saved [16077]\n","\n","--2024-01-12 09:55:53--  https://api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.citation_fuzzy_match.FactWithEvidence.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.citation_fuzzy_match.FactWithEvidence.html’\n","\n","api.python.langchai     [ <=>                ]  41.60K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:53 (61.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.citation_fuzzy_match.FactWithEvidence.html’ saved [42595]\n","\n","--2024-01-12 09:55:53--  https://api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.citation_fuzzy_match.QuestionAnswer.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.citation_fuzzy_match.QuestionAnswer.html’\n","\n","api.python.langchai     [ <=>                ]  40.61K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:53 (281 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.citation_fuzzy_match.QuestionAnswer.html’ saved [41582]\n","\n","--2024-01-12 09:55:53--  https://api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.openapi.SimpleRequestChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.openapi.SimpleRequestChain.html’\n","\n","api.python.langchai     [ <=>                ] 162.18K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:53 (42.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.openapi.SimpleRequestChain.html’ saved [166068]\n","\n","--2024-01-12 09:55:53--  https://api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.qa_with_structure.AnswerWithSources.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.qa_with_structure.AnswerWithSources.html’\n","\n","api.python.langchai     [ <=>                ]  40.15K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:53 (54.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.qa_with_structure.AnswerWithSources.html’ saved [41117]\n","\n","--2024-01-12 09:55:53--  https://api.python.langchain.com/en/stable/chains/langchain.chains.prompt_selector.BasePromptSelector.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.prompt_selector.BasePromptSelector.html’\n","\n","api.python.langchai     [ <=>                ]  39.63K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:53 (401 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.prompt_selector.BasePromptSelector.html’ saved [40578]\n","\n","--2024-01-12 09:55:53--  https://api.python.langchain.com/en/stable/chains/langchain.chains.prompt_selector.ConditionalPromptSelector.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.prompt_selector.ConditionalPromptSelector.html’\n","\n","api.python.langchai     [ <=>                ]  43.37K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:53 (47.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.prompt_selector.ConditionalPromptSelector.html’ saved [44412]\n","\n","--2024-01-12 09:55:53--  https://api.python.langchain.com/en/stable/chains/langchain.chains.qa_generation.base.QAGenerationChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.qa_generation.base.QAGenerationChain.html’\n","\n","api.python.langchai     [ <=>                ] 166.02K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:54 (60.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.qa_generation.base.QAGenerationChain.html’ saved [170009]\n","\n","--2024-01-12 09:55:54--  https://api.python.langchain.com/en/stable/chains/langchain.chains.qa_with_sources.base.BaseQAWithSourcesChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.qa_with_sources.base.BaseQAWithSourcesChain.html’\n","\n","api.python.langchai     [ <=>                ] 197.51K  --.-KB/s    in 0.009s  \n","\n","2024-01-12 09:55:54 (21.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.qa_with_sources.base.BaseQAWithSourcesChain.html’ saved [202250]\n","\n","--2024-01-12 09:55:54--  https://api.python.langchain.com/en/stable/chains/langchain.chains.qa_with_sources.base.QAWithSourcesChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.qa_with_sources.base.QAWithSourcesChain.html’\n","\n","api.python.langchai     [ <=>                ] 196.68K  --.-KB/s    in 0.006s  \n","\n","2024-01-12 09:55:54 (31.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.qa_with_sources.base.QAWithSourcesChain.html’ saved [201398]\n","\n","--2024-01-12 09:55:54--  https://api.python.langchain.com/en/stable/chains/langchain.chains.qa_with_sources.loading.LoadingCallable.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.qa_with_sources.loading.LoadingCallable.html’\n","\n","api.python.langchai     [ <=>                ]  12.22K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:54 (237 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.qa_with_sources.loading.LoadingCallable.html’ saved [12510]\n","\n","--2024-01-12 09:55:54--  https://api.python.langchain.com/en/stable/chains/langchain.chains.qa_with_sources.retrieval.RetrievalQAWithSourcesChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.qa_with_sources.retrieval.RetrievalQAWithSourcesChain.html’\n","\n","api.python.langchai     [ <=>                ] 202.78K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:54 (79.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.qa_with_sources.retrieval.RetrievalQAWithSourcesChain.html’ saved [207646]\n","\n","--2024-01-12 09:55:54--  https://api.python.langchain.com/en/stable/chains/langchain.chains.qa_with_sources.vector_db.VectorDBQAWithSourcesChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.qa_with_sources.vector_db.VectorDBQAWithSourcesChain.html’\n","\n","api.python.langchai     [ <=>                ] 202.96K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:55:54 (41.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.qa_with_sources.vector_db.VectorDBQAWithSourcesChain.html’ saved [207832]\n","\n","--2024-01-12 09:55:54--  https://api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.base.StructuredQueryOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.base.StructuredQueryOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 129.90K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:55 (41.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.base.StructuredQueryOutputParser.html’ saved [133018]\n","\n","--2024-01-12 09:55:55--  https://api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.Comparator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.Comparator.html’\n","\n","api.python.langchai     [ <=>                ]  16.68K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:55 (39.7 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.Comparator.html’ saved [17076]\n","\n","--2024-01-12 09:55:55--  https://api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.Comparison.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.Comparison.html’\n","\n","api.python.langchai     [ <=>                ]  41.48K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:55 (34.7 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.Comparison.html’ saved [42478]\n","\n","--2024-01-12 09:55:55--  https://api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.Expr.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.Expr.html’\n","\n","api.python.langchai     [ <=>                ]  39.06K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:55 (288 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.Expr.html’ saved [39994]\n","\n","--2024-01-12 09:55:55--  https://api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.FilterDirective.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.FilterDirective.html’\n","\n","api.python.langchai     [ <=>                ]  39.54K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:55 (51.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.FilterDirective.html’ saved [40492]\n","\n","--2024-01-12 09:55:55--  https://api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.Operation.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.Operation.html’\n","\n","api.python.langchai     [ <=>                ]  41.14K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:55 (51.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.Operation.html’ saved [42124]\n","\n","--2024-01-12 09:55:55--  https://api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.Operator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.Operator.html’\n","\n","api.python.langchai     [ <=>                ]  13.33K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:55 (48.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.Operator.html’ saved [13647]\n","\n","--2024-01-12 09:55:55--  https://api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.StructuredQuery.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.StructuredQuery.html’\n","\n","api.python.langchai     [ <=>                ]  42.07K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:55 (45.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.StructuredQuery.html’ saved [43081]\n","\n","--2024-01-12 09:55:55--  https://api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.Visitor.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.Visitor.html’\n","\n","api.python.langchai     [ <=>                ]  16.96K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:55 (26.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.ir.Visitor.html’ saved [17366]\n","\n","--2024-01-12 09:55:55--  https://api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.parser.ISO8601Date.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.parser.ISO8601Date.html’\n","\n","api.python.langchai     [ <=>                ]  25.43K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:56 (40.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.parser.ISO8601Date.html’ saved [26037]\n","\n","--2024-01-12 09:55:56--  https://api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.schema.AttributeInfo.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.schema.AttributeInfo.html’\n","\n","api.python.langchai     [ <=>                ]  42.44K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:56 (42.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.schema.AttributeInfo.html’ saved [43455]\n","\n","--2024-01-12 09:55:56--  https://api.python.langchain.com/en/stable/chains/langchain.chains.retrieval_qa.base.BaseRetrievalQA.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.retrieval_qa.base.BaseRetrievalQA.html’\n","\n","api.python.langchai     [ <=>                ] 165.56K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:56 (38.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.retrieval_qa.base.BaseRetrievalQA.html’ saved [169537]\n","\n","--2024-01-12 09:55:56--  https://api.python.langchain.com/en/stable/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html’\n","\n","api.python.langchai     [ <=>                ] 169.76K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:56 (37.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html’ saved [173833]\n","\n","--2024-01-12 09:55:56--  https://api.python.langchain.com/en/stable/chains/langchain.chains.retrieval_qa.base.VectorDBQA.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.retrieval_qa.base.VectorDBQA.html’\n","\n","api.python.langchai     [ <=>                ] 167.90K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:56 (41.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.retrieval_qa.base.VectorDBQA.html’ saved [171925]\n","\n","--2024-01-12 09:55:56--  https://api.python.langchain.com/en/stable/chains/langchain.chains.router.base.MultiRouteChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.router.base.MultiRouteChain.html’\n","\n","api.python.langchai     [ <=>                ] 159.78K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:56 (45.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.router.base.MultiRouteChain.html’ saved [163613]\n","\n","--2024-01-12 09:55:56--  https://api.python.langchain.com/en/stable/chains/langchain.chains.router.base.Route.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.router.base.Route.html’\n","\n","api.python.langchai     [ <=>                ]  14.31K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:56 (104 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.router.base.Route.html’ saved [14655]\n","\n","--2024-01-12 09:55:56--  https://api.python.langchain.com/en/stable/chains/langchain.chains.router.base.RouterChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.router.base.RouterChain.html’\n","\n","api.python.langchai     [ <=>                ] 162.83K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:55:57 (64.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.router.base.RouterChain.html’ saved [166742]\n","\n","--2024-01-12 09:55:57--  https://api.python.langchain.com/en/stable/chains/langchain.chains.router.embedding_router.EmbeddingRouterChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.router.embedding_router.EmbeddingRouterChain.html’\n","\n","api.python.langchai     [ <=>                ] 169.69K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:57 (41.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.router.embedding_router.EmbeddingRouterChain.html’ saved [173766]\n","\n","--2024-01-12 09:55:57--  https://api.python.langchain.com/en/stable/chains/langchain.chains.router.llm_router.LLMRouterChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.router.llm_router.LLMRouterChain.html’\n","\n","api.python.langchai     [ <=>                ] 166.16K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:57 (36.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.router.llm_router.LLMRouterChain.html’ saved [170149]\n","\n","--2024-01-12 09:55:57--  https://api.python.langchain.com/en/stable/chains/langchain.chains.router.llm_router.RouterOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.router.llm_router.RouterOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 125.82K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:57 (43.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.router.llm_router.RouterOutputParser.html’ saved [128839]\n","\n","--2024-01-12 09:55:57--  https://api.python.langchain.com/en/stable/chains/langchain.chains.router.multi_prompt.MultiPromptChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.router.multi_prompt.MultiPromptChain.html’\n","\n","api.python.langchai     [ <=>                ] 164.39K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:57 (41.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.router.multi_prompt.MultiPromptChain.html’ saved [168340]\n","\n","--2024-01-12 09:55:57--  https://api.python.langchain.com/en/stable/chains/langchain.chains.router.multi_retrieval_qa.MultiRetrievalQAChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.router.multi_retrieval_qa.MultiRetrievalQAChain.html’\n","\n","api.python.langchai     [ <=>                ] 167.25K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:55:57 (35.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.router.multi_retrieval_qa.MultiRetrievalQAChain.html’ saved [171268]\n","\n","--2024-01-12 09:55:57--  https://api.python.langchain.com/en/stable/chains/langchain.chains.sequential.SequentialChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.sequential.SequentialChain.html’\n","\n","api.python.langchai     [ <=>                ] 158.42K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:55:58 (38.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.sequential.SequentialChain.html’ saved [162220]\n","\n","--2024-01-12 09:55:58--  https://api.python.langchain.com/en/stable/chains/langchain.chains.sequential.SimpleSequentialChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.sequential.SimpleSequentialChain.html’\n","\n","api.python.langchai     [ <=>                ] 159.76K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:58 (44.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.sequential.SimpleSequentialChain.html’ saved [163590]\n","\n","--2024-01-12 09:55:58--  https://api.python.langchain.com/en/stable/chains/langchain.chains.sql_database.query.SQLInput.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.sql_database.query.SQLInput.html’\n","\n","api.python.langchai     [ <=>                ]  10.99K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:58 (161 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.sql_database.query.SQLInput.html’ saved [11254]\n","\n","--2024-01-12 09:55:58--  https://api.python.langchain.com/en/stable/chains/langchain.chains.sql_database.query.SQLInputWithTables.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.sql_database.query.SQLInputWithTables.html’\n","\n","api.python.langchai     [ <=>                ]  11.74K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:58 (119 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.sql_database.query.SQLInputWithTables.html’ saved [12025]\n","\n","--2024-01-12 09:55:58--  https://api.python.langchain.com/en/stable/chains/langchain.chains.transform.TransformChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.transform.TransformChain.html’\n","\n","api.python.langchai     [ <=>                ] 162.12K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:55:58 (83.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.transform.TransformChain.html’ saved [166012]\n","\n","--2024-01-12 09:55:58--  https://api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.reduce.acollapse_docs.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.reduce.acollapse_docs.html’\n","\n","api.python.langchai     [ <=>                ]  13.25K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:58 (113 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.reduce.acollapse_docs.html’ saved [13571]\n","\n","--2024-01-12 09:55:58--  https://api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.reduce.collapse_docs.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.reduce.collapse_docs.html’\n","\n","api.python.langchai     [ <=>                ]  13.12K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:58 (85.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.reduce.collapse_docs.html’ saved [13437]\n","\n","--2024-01-12 09:55:58--  https://api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.reduce.split_list_of_docs.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.reduce.split_list_of_docs.html’\n","\n","api.python.langchai     [ <=>                ]  13.12K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:55:58 (4.29 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.reduce.split_list_of_docs.html’ saved [13440]\n","\n","--2024-01-12 09:55:58--  https://api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html’\n","\n","api.python.langchai     [ <=>                ]  18.44K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:59 (37.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html’ saved [18880]\n","\n","--2024-01-12 09:55:59--  https://api.python.langchain.com/en/stable/chains/langchain.chains.ernie_functions.base.convert_python_function_to_ernie_function.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.ernie_functions.base.convert_python_function_to_ernie_function.html’\n","\n","api.python.langchai     [ <=>                ]  11.75K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:59 (127 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.ernie_functions.base.convert_python_function_to_ernie_function.html’ saved [12029]\n","\n","--2024-01-12 09:55:59--  https://api.python.langchain.com/en/stable/chains/langchain.chains.ernie_functions.base.convert_to_ernie_function.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.ernie_functions.base.convert_to_ernie_function.html’\n","\n","api.python.langchai     [ <=>                ]  12.51K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:59 (163 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.ernie_functions.base.convert_to_ernie_function.html’ saved [12808]\n","\n","--2024-01-12 09:55:59--  https://api.python.langchain.com/en/stable/chains/langchain.chains.ernie_functions.base.create_ernie_fn_chain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.ernie_functions.base.create_ernie_fn_chain.html’\n","\n","api.python.langchai     [ <=>                ]  21.56K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:55:59 (41.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.ernie_functions.base.create_ernie_fn_chain.html’ saved [22079]\n","\n","--2024-01-12 09:55:59--  https://api.python.langchain.com/en/stable/chains/langchain.chains.ernie_functions.base.create_ernie_fn_runnable.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.ernie_functions.base.create_ernie_fn_runnable.html’\n","\n","api.python.langchai     [ <=>                ]  21.66K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:59 (128 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.ernie_functions.base.create_ernie_fn_runnable.html’ saved [22180]\n","\n","--2024-01-12 09:55:59--  https://api.python.langchain.com/en/stable/chains/langchain.chains.ernie_functions.base.create_structured_output_chain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.ernie_functions.base.create_structured_output_chain.html’\n","\n","api.python.langchai     [ <=>                ]  19.72K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:59 (167 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.ernie_functions.base.create_structured_output_chain.html’ saved [20190]\n","\n","--2024-01-12 09:55:59--  https://api.python.langchain.com/en/stable/chains/langchain.chains.ernie_functions.base.create_structured_output_runnable.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.ernie_functions.base.create_structured_output_runnable.html’\n","\n","api.python.langchai     [ <=>                ]  19.47K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:59 (83.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.ernie_functions.base.create_structured_output_runnable.html’ saved [19941]\n","\n","--2024-01-12 09:55:59--  https://api.python.langchain.com/en/stable/chains/langchain.chains.ernie_functions.base.get_ernie_output_parser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.ernie_functions.base.get_ernie_output_parser.html’\n","\n","api.python.langchai     [ <=>                ]  13.33K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:59 (33.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.ernie_functions.base.get_ernie_output_parser.html’ saved [13655]\n","\n","--2024-01-12 09:55:59--  https://api.python.langchain.com/en/stable/chains/langchain.chains.example_generator.generate_example.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.example_generator.generate_example.html’\n","\n","api.python.langchai     [ <=>                ]  11.97K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:55:59 (53.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.example_generator.generate_example.html’ saved [12256]\n","\n","--2024-01-12 09:55:59--  https://api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.cypher.construct_schema.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.cypher.construct_schema.html’\n","\n","api.python.langchai     [ <=>                ]  11.85K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:00 (162 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.cypher.construct_schema.html’ saved [12137]\n","\n","--2024-01-12 09:56:00--  https://api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.cypher.extract_cypher.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.cypher.extract_cypher.html’\n","\n","api.python.langchai     [ <=>                ]  11.18K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:00 (229 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.cypher.extract_cypher.html’ saved [11450]\n","\n","--2024-01-12 09:56:00--  https://api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.falkordb.extract_cypher.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.falkordb.extract_cypher.html’\n","\n","api.python.langchai     [ <=>                ]  11.12K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:00 (101 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.falkordb.extract_cypher.html’ saved [11384]\n","\n","--2024-01-12 09:56:00--  https://api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.neptune_cypher.extract_cypher.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.neptune_cypher.extract_cypher.html’\n","\n","api.python.langchai     [ <=>                ]  11.00K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:00 (105 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.neptune_cypher.extract_cypher.html’ saved [11269]\n","\n","--2024-01-12 09:56:00--  https://api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.neptune_cypher.trim_query.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.neptune_cypher.trim_query.html’\n","\n","api.python.langchai     [ <=>                ]  10.97K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:00 (14.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.neptune_cypher.trim_query.html’ saved [11231]\n","\n","--2024-01-12 09:56:00--  https://api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.neptune_cypher.use_simple_prompt.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.neptune_cypher.use_simple_prompt.html’\n","\n","api.python.langchai     [ <=>                ]  11.28K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:00 (63.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.graph_qa.neptune_cypher.use_simple_prompt.html’ saved [11549]\n","\n","--2024-01-12 09:56:00--  https://api.python.langchain.com/en/stable/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html’\n","\n","api.python.langchai     [ <=>                ]  17.62K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:00 (35.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html’ saved [18043]\n","\n","--2024-01-12 09:56:00--  https://api.python.langchain.com/en/stable/chains/langchain.chains.loading.load_chain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.loading.load_chain.html’\n","\n","api.python.langchai     [ <=>                ]  12.02K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:00 (142 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.loading.load_chain.html’ saved [12310]\n","\n","--2024-01-12 09:56:00--  https://api.python.langchain.com/en/stable/chains/langchain.chains.loading.load_chain_from_config.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.loading.load_chain_from_config.html’\n","\n","api.python.langchai     [ <=>                ]  11.30K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:01 (149 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.loading.load_chain_from_config.html’ saved [11567]\n","\n","--2024-01-12 09:56:01--  https://api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.base.convert_python_function_to_openai_function.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.base.convert_python_function_to_openai_function.html’\n","\n","api.python.langchai     [ <=>                ]  11.77K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:01 (76.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.base.convert_python_function_to_openai_function.html’ saved [12052]\n","\n","--2024-01-12 09:56:01--  https://api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.base.convert_to_openai_function.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.base.convert_to_openai_function.html’\n","\n","api.python.langchai     [ <=>                ]  12.53K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:01 (61.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.base.convert_to_openai_function.html’ saved [12833]\n","\n","--2024-01-12 09:56:01--  https://api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.base.create_openai_fn_chain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.base.create_openai_fn_chain.html’\n","\n","api.python.langchai     [ <=>                ]  22.70K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:01 (30.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.base.create_openai_fn_chain.html’ saved [23248]\n","\n","--2024-01-12 09:56:01--  https://api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.base.create_openai_fn_runnable.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.base.create_openai_fn_runnable.html’\n","\n","api.python.langchai     [ <=>                ]  22.48K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:01 (42.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.base.create_openai_fn_runnable.html’ saved [23016]\n","\n","--2024-01-12 09:56:01--  https://api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.base.create_structured_output_chain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.base.create_structured_output_chain.html’\n","\n","api.python.langchai     [ <=>                ]  19.92K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:01 (56.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.base.create_structured_output_chain.html’ saved [20398]\n","\n","--2024-01-12 09:56:01--  https://api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.base.create_structured_output_runnable.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.base.create_structured_output_runnable.html’\n","\n","api.python.langchai     [ <=>                ]  19.68K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:01 (199 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.base.create_structured_output_runnable.html’ saved [20148]\n","\n","--2024-01-12 09:56:01--  https://api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.base.get_openai_output_parser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.base.get_openai_output_parser.html’\n","\n","api.python.langchai     [ <=>                ]  13.36K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:01 (185 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.base.get_openai_output_parser.html’ saved [13678]\n","\n","--2024-01-12 09:56:01--  https://api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.citation_fuzzy_match.create_citation_fuzzy_match_chain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.citation_fuzzy_match.create_citation_fuzzy_match_chain.html’\n","\n","api.python.langchai     [ <=>                ]  12.41K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:02 (19.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.citation_fuzzy_match.create_citation_fuzzy_match_chain.html’ saved [12708]\n","\n","--2024-01-12 09:56:02--  https://api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.extraction.create_extraction_chain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.extraction.create_extraction_chain.html’\n","\n","api.python.langchai     [ <=>                ]  14.64K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:02 (186 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.extraction.create_extraction_chain.html’ saved [14993]\n","\n","--2024-01-12 09:56:02--  https://api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.extraction.create_extraction_chain_pydantic.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.extraction.create_extraction_chain_pydantic.html’\n","\n","api.python.langchai     [ <=>                ]  14.05K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:02 (26.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.extraction.create_extraction_chain_pydantic.html’ saved [14384]\n","\n","--2024-01-12 09:56:02--  https://api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.openapi.get_openapi_chain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.openapi.get_openapi_chain.html’\n","\n","api.python.langchai     [ <=>                ]  16.74K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:02 (184 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.openapi.get_openapi_chain.html’ saved [17141]\n","\n","--2024-01-12 09:56:02--  https://api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.openapi.openapi_spec_to_openai_fn.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.openapi.openapi_spec_to_openai_fn.html’\n","\n","api.python.langchai     [ <=>                ]  12.42K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:02 (10.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.openapi.openapi_spec_to_openai_fn.html’ saved [12713]\n","\n","--2024-01-12 09:56:02--  https://api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.qa_with_structure.create_qa_with_sources_chain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.qa_with_structure.create_qa_with_sources_chain.html’\n","\n","api.python.langchai     [ <=>                ]  13.24K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:02 (149 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.qa_with_structure.create_qa_with_sources_chain.html’ saved [13561]\n","\n","--2024-01-12 09:56:02--  https://api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.qa_with_structure.create_qa_with_structure_chain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.qa_with_structure.create_qa_with_structure_chain.html’\n","\n","api.python.langchai     [ <=>                ]  15.13K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:02 (111 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.qa_with_structure.create_qa_with_structure_chain.html’ saved [15493]\n","\n","--2024-01-12 09:56:02--  https://api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.tagging.create_tagging_chain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.tagging.create_tagging_chain.html’\n","\n","api.python.langchai     [ <=>                ]  13.71K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:02 (61.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.tagging.create_tagging_chain.html’ saved [14040]\n","\n","--2024-01-12 09:56:02--  https://api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.tagging.create_tagging_chain_pydantic.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.tagging.create_tagging_chain_pydantic.html’\n","\n","api.python.langchai     [ <=>                ]  13.59K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:03 (51.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.tagging.create_tagging_chain_pydantic.html’ saved [13914]\n","\n","--2024-01-12 09:56:03--  https://api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.utils.get_llm_kwargs.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.utils.get_llm_kwargs.html’\n","\n","api.python.langchai     [ <=>                ]  11.27K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:03 (102 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_functions.utils.get_llm_kwargs.html’ saved [11544]\n","\n","--2024-01-12 09:56:03--  https://api.python.langchain.com/en/stable/chains/langchain.chains.openai_tools.extraction.create_extraction_chain_pydantic.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_tools.extraction.create_extraction_chain_pydantic.html’\n","\n","api.python.langchai     [ <=>                ]  14.72K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:03 (61.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.openai_tools.extraction.create_extraction_chain_pydantic.html’ saved [15076]\n","\n","--2024-01-12 09:56:03--  https://api.python.langchain.com/en/stable/chains/langchain.chains.prompt_selector.is_chat_model.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.prompt_selector.is_chat_model.html’\n","\n","api.python.langchai     [ <=>                ]  11.45K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:03 (230 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.prompt_selector.is_chat_model.html’ saved [11723]\n","\n","--2024-01-12 09:56:03--  https://api.python.langchain.com/en/stable/chains/langchain.chains.prompt_selector.is_llm.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.prompt_selector.is_llm.html’\n","\n","api.python.langchai     [ <=>                ]  11.36K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:04 (71.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.prompt_selector.is_llm.html’ saved [11633]\n","\n","--2024-01-12 09:56:04--  https://api.python.langchain.com/en/stable/chains/langchain.chains.qa_with_sources.loading.load_qa_with_sources_chain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.qa_with_sources.loading.load_qa_with_sources_chain.html’\n","\n","api.python.langchai     [ <=>                ]  13.91K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:04 (47.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.qa_with_sources.loading.load_qa_with_sources_chain.html’ saved [14247]\n","\n","--2024-01-12 09:56:04--  https://api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.base.construct_examples.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.base.construct_examples.html’\n","\n","api.python.langchai     [ <=>                ]  11.79K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:04 (214 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.base.construct_examples.html’ saved [12078]\n","\n","--2024-01-12 09:56:04--  https://api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.base.fix_filter_directive.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.base.fix_filter_directive.html’\n","\n","api.python.langchai     [ <=>                ]  14.70K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:04 (109 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.base.fix_filter_directive.html’ saved [15049]\n","\n","--2024-01-12 09:56:04--  https://api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.base.get_query_constructor_prompt.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.base.get_query_constructor_prompt.html’\n","\n","api.python.langchai     [ <=>                ]  14.44K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:04 (174 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.base.get_query_constructor_prompt.html’ saved [14791]\n","\n","--2024-01-12 09:56:04--  https://api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.base.load_query_constructor_chain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.base.load_query_constructor_chain.html’\n","\n","api.python.langchai     [ <=>                ]  14.52K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:04 (179 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.base.load_query_constructor_chain.html’ saved [14872]\n","\n","--2024-01-12 09:56:04--  https://api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.base.load_query_constructor_runnable.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.base.load_query_constructor_runnable.html’\n","\n","api.python.langchai     [ <=>                ]  14.92K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:04 (83.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.base.load_query_constructor_runnable.html’ saved [15281]\n","\n","--2024-01-12 09:56:04--  https://api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.parser.get_parser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.parser.get_parser.html’\n","\n","api.python.langchai     [ <=>                ]  13.45K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:04 (59.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.parser.get_parser.html’ saved [13775]\n","\n","--2024-01-12 09:56:04--  https://api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.parser.v_args.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.parser.v_args.html’\n","\n","api.python.langchai     [ <=>                ]  11.23K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:04 (226 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.query_constructor.parser.v_args.html’ saved [11503]\n","\n","--2024-01-12 09:56:04--  https://api.python.langchain.com/en/stable/chains/langchain.chains.retrieval.create_retrieval_chain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.retrieval.create_retrieval_chain.html’\n","\n","api.python.langchai     [ <=>                ]  16.27K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:05 (30.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.retrieval.create_retrieval_chain.html’ saved [16659]\n","\n","--2024-01-12 09:56:05--  https://api.python.langchain.com/en/stable/chains/langchain.chains.sql_database.query.create_sql_query_chain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.sql_database.query.create_sql_query_chain.html’\n","\n","api.python.langchai     [ <=>                ]  15.80K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:05 (44.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/chains/langchain.chains.sql_database.query.create_sql_query_chain.html’ saved [16182]\n","\n","--2024-01-12 09:56:05--  https://api.python.langchain.com/en/stable/embeddings/langchain.embeddings.cache.CacheBackedEmbeddings.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/embeddings/langchain.embeddings.cache.CacheBackedEmbeddings.html’\n","\n","api.python.langchai     [ <=>                ]  26.34K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:05 (254 MB/s) - ‘docs/api.python.langchain.com/en/stable/embeddings/langchain.embeddings.cache.CacheBackedEmbeddings.html’ saved [26977]\n","\n","--2024-01-12 09:56:05--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.loading.load_evaluators.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.loading.load_evaluators.html’\n","\n","api.python.langchai     [ <=>                ]  16.00K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:05 (47.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.loading.load_evaluators.html’ saved [16380]\n","\n","--2024-01-12 09:56:05--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.loading.load_evaluator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.loading.load_evaluator.html’\n","\n","api.python.langchai     [ <=>                ]  15.79K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:05 (75.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.loading.load_evaluator.html’ saved [16170]\n","\n","--2024-01-12 09:56:05--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.schema.EvaluatorType.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.schema.EvaluatorType.html’\n","\n","api.python.langchai     [ <=>                ]  24.16K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:05 (29.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.schema.EvaluatorType.html’ saved [24737]\n","\n","--2024-01-12 09:56:05--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.loading.load_dataset.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.loading.load_dataset.html’\n","\n","api.python.langchai     [ <=>                ]  12.32K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:05 (134 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.loading.load_dataset.html’ saved [12615]\n","\n","--2024-01-12 09:56:05--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.qa.eval_chain.QAEvalChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.qa.eval_chain.QAEvalChain.html’\n","\n","api.python.langchai     [ <=>                ] 211.43K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:56:05 (43.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.qa.eval_chain.QAEvalChain.html’ saved [216501]\n","\n","--2024-01-12 09:56:05--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.comparison.eval_chain.PairwiseStringEvalChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.comparison.eval_chain.PairwiseStringEvalChain.html’\n","\n","api.python.langchai     [ <=>                ] 217.40K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:56:05 (45.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.comparison.eval_chain.PairwiseStringEvalChain.html’ saved [222620]\n","\n","--2024-01-12 09:56:05--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.comparison.eval_chain.LabeledPairwiseStringEvalChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.comparison.eval_chain.LabeledPairwiseStringEvalChain.html’\n","\n","api.python.langchai     [ <=>                ] 215.14K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:56:06 (42.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.comparison.eval_chain.LabeledPairwiseStringEvalChain.html’ saved [220302]\n","\n","--2024-01-12 09:56:06--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain.html’\n","\n","api.python.langchai     [ <=>                ] 186.15K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:56:06 (40.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain.html’ saved [190619]\n","\n","--2024-01-12 09:56:06--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.criteria.eval_chain.CriteriaEvalChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.criteria.eval_chain.CriteriaEvalChain.html’\n","\n","api.python.langchai     [ <=>                ] 225.49K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:56:06 (41.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.criteria.eval_chain.CriteriaEvalChain.html’ saved [230905]\n","\n","--2024-01-12 09:56:06--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain.html’\n","\n","api.python.langchai     [ <=>                ] 218.82K  --.-KB/s    in 0.007s  \n","\n","2024-01-12 09:56:06 (32.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain.html’ saved [224069]\n","\n","--2024-01-12 09:56:06--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.embedding_distance.base.EmbeddingDistanceEvalChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.embedding_distance.base.EmbeddingDistanceEvalChain.html’\n","\n","api.python.langchai     [ <=>                ] 173.28K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:56:06 (44.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.embedding_distance.base.EmbeddingDistanceEvalChain.html’ saved [177441]\n","\n","--2024-01-12 09:56:06--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.embedding_distance.base.PairwiseEmbeddingDistanceEvalChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.embedding_distance.base.PairwiseEmbeddingDistanceEvalChain.html’\n","\n","api.python.langchai     [ <=>                ] 174.19K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:56:06 (42.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.embedding_distance.base.PairwiseEmbeddingDistanceEvalChain.html’ saved [178366]\n","\n","--2024-01-12 09:56:06--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.string_distance.base.StringDistanceEvalChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.string_distance.base.StringDistanceEvalChain.html’\n","\n","api.python.langchai     [ <=>                ] 175.17K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:56:06 (43.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.string_distance.base.StringDistanceEvalChain.html’ saved [179377]\n","\n","--2024-01-12 09:56:06--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.string_distance.base.PairwiseStringDistanceEvalChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.string_distance.base.PairwiseStringDistanceEvalChain.html’\n","\n","api.python.langchai     [ <=>                ] 175.41K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:56:07 (39.7 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.string_distance.base.PairwiseStringDistanceEvalChain.html’ saved [179615]\n","\n","--2024-01-12 09:56:07--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.schema.StringEvaluator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.schema.StringEvaluator.html’\n","\n","api.python.langchai     [ <=>                ]  19.89K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:07 (38.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.schema.StringEvaluator.html’ saved [20370]\n","\n","--2024-01-12 09:56:07--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.schema.PairwiseStringEvaluator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.schema.PairwiseStringEvaluator.html’\n","\n","api.python.langchai     [ <=>                ]  20.49K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:07 (31.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.schema.PairwiseStringEvaluator.html’ saved [20983]\n","\n","--2024-01-12 09:56:07--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.schema.AgentTrajectoryEvaluator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.schema.AgentTrajectoryEvaluator.html’\n","\n","api.python.langchai     [ <=>                ]  20.95K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:07 (45.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.schema.AgentTrajectoryEvaluator.html’ saved [21452]\n","\n","--2024-01-12 09:56:07--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEval.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEval.html’\n","\n","api.python.langchai     [ <=>                ]  11.88K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:07 (170 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEval.html’ saved [12163]\n","\n","--2024-01-12 09:56:07--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.agents.trajectory_eval_chain.TrajectoryOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.agents.trajectory_eval_chain.TrajectoryOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 126.11K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:07 (37.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.agents.trajectory_eval_chain.TrajectoryOutputParser.html’ saved [129134]\n","\n","--2024-01-12 09:56:07--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.comparison.eval_chain.PairwiseStringResultOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.comparison.eval_chain.PairwiseStringResultOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 126.68K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:07 (39.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.comparison.eval_chain.PairwiseStringResultOutputParser.html’ saved [129717]\n","\n","--2024-01-12 09:56:07--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.criteria.eval_chain.Criteria.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.criteria.eval_chain.Criteria.html’\n","\n","api.python.langchai     [ <=>                ]  19.29K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:07 (86.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.criteria.eval_chain.Criteria.html’ saved [19757]\n","\n","--2024-01-12 09:56:07--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.criteria.eval_chain.CriteriaResultOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.criteria.eval_chain.CriteriaResultOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 125.02K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:56:07 (31.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.criteria.eval_chain.CriteriaResultOutputParser.html’ saved [128021]\n","\n","--2024-01-12 09:56:07--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.embedding_distance.base.EmbeddingDistance.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.embedding_distance.base.EmbeddingDistance.html’\n","\n","api.python.langchai     [ <=>                ]  16.51K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:08 (173 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.embedding_distance.base.EmbeddingDistance.html’ saved [16907]\n","\n","--2024-01-12 09:56:08--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.exact_match.base.ExactMatchStringEvaluator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.exact_match.base.ExactMatchStringEvaluator.html’\n","\n","api.python.langchai     [ <=>                ]  24.08K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:08 (39.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.exact_match.base.ExactMatchStringEvaluator.html’ saved [24661]\n","\n","--2024-01-12 09:56:08--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.parsing.base.JsonEqualityEvaluator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.parsing.base.JsonEqualityEvaluator.html’\n","\n","api.python.langchai     [ <=>                ]  26.43K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:08 (241 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.parsing.base.JsonEqualityEvaluator.html’ saved [27069]\n","\n","--2024-01-12 09:56:08--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.parsing.base.JsonValidityEvaluator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.parsing.base.JsonValidityEvaluator.html’\n","\n","api.python.langchai     [ <=>                ]  24.15K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:08 (46.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.parsing.base.JsonValidityEvaluator.html’ saved [24725]\n","\n","--2024-01-12 09:56:08--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.parsing.json_distance.JsonEditDistanceEvaluator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.parsing.json_distance.JsonEditDistanceEvaluator.html’\n","\n","api.python.langchai     [ <=>                ]  27.84K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:08 (49.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.parsing.json_distance.JsonEditDistanceEvaluator.html’ saved [28507]\n","\n","--2024-01-12 09:56:08--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.parsing.json_schema.JsonSchemaEvaluator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.parsing.json_schema.JsonSchemaEvaluator.html’\n","\n","api.python.langchai     [ <=>                ]  24.13K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:08 (96.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.parsing.json_schema.JsonSchemaEvaluator.html’ saved [24711]\n","\n","--2024-01-12 09:56:08--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.qa.eval_chain.ContextQAEvalChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.qa.eval_chain.ContextQAEvalChain.html’\n","\n","api.python.langchai     [ <=>                ] 212.79K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:56:08 (44.7 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.qa.eval_chain.ContextQAEvalChain.html’ saved [217894]\n","\n","--2024-01-12 09:56:08--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.qa.eval_chain.CotQAEvalChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.qa.eval_chain.CotQAEvalChain.html’\n","\n","api.python.langchai     [ <=>                ] 210.86K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:56:09 (39.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.qa.eval_chain.CotQAEvalChain.html’ saved [215922]\n","\n","--2024-01-12 09:56:09--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.qa.generate_chain.QAGenerateChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.qa.generate_chain.QAGenerateChain.html’\n","\n","api.python.langchai     [ <=>                ] 199.64K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:56:09 (45.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.qa.generate_chain.QAGenerateChain.html’ saved [204432]\n","\n","--2024-01-12 09:56:09--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.regex_match.base.RegexMatchStringEvaluator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.regex_match.base.RegexMatchStringEvaluator.html’\n","\n","api.python.langchai     [ <=>                ]  23.45K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:09 (46.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.regex_match.base.RegexMatchStringEvaluator.html’ saved [24013]\n","\n","--2024-01-12 09:56:09--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.schema.LLMEvalChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.schema.LLMEvalChain.html’\n","\n","api.python.langchai     [ <=>                ] 158.90K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:56:09 (64.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.schema.LLMEvalChain.html’ saved [162715]\n","\n","--2024-01-12 09:56:09--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.scoring.eval_chain.LabeledScoreStringEvalChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.scoring.eval_chain.LabeledScoreStringEvalChain.html’\n","\n","api.python.langchai     [ <=>                ] 216.31K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:56:09 (40.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.scoring.eval_chain.LabeledScoreStringEvalChain.html’ saved [221501]\n","\n","--2024-01-12 09:56:09--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.scoring.eval_chain.ScoreStringEvalChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.scoring.eval_chain.ScoreStringEvalChain.html’\n","\n","api.python.langchai     [ <=>                ] 217.80K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:56:09 (96.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.scoring.eval_chain.ScoreStringEvalChain.html’ saved [223030]\n","\n","--2024-01-12 09:56:09--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.scoring.eval_chain.ScoreStringResultOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.scoring.eval_chain.ScoreStringResultOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 125.91K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:09 (45.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.scoring.eval_chain.ScoreStringResultOutputParser.html’ saved [128934]\n","\n","--2024-01-12 09:56:09--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.string_distance.base.StringDistance.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.string_distance.base.StringDistance.html’\n","\n","api.python.langchai     [ <=>                ]  17.01K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:10 (44.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.string_distance.base.StringDistance.html’ saved [17417]\n","\n","--2024-01-12 09:56:10--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.comparison.eval_chain.resolve_pairwise_criteria.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.comparison.eval_chain.resolve_pairwise_criteria.html’\n","\n","api.python.langchai     [ <=>                ]  14.25K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:10 (103 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.comparison.eval_chain.resolve_pairwise_criteria.html’ saved [14591]\n","\n","--2024-01-12 09:56:10--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.criteria.eval_chain.resolve_criteria.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.criteria.eval_chain.resolve_criteria.html’\n","\n","api.python.langchai     [ <=>                ]  13.81K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:10 (167 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.criteria.eval_chain.resolve_criteria.html’ saved [14146]\n","\n","--2024-01-12 09:56:10--  https://api.python.langchain.com/en/stable/evaluation/langchain.evaluation.scoring.eval_chain.resolve_criteria.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.scoring.eval_chain.resolve_criteria.html’\n","\n","api.python.langchai     [ <=>                ]  13.97K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:10 (55.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/evaluation/langchain.evaluation.scoring.eval_chain.resolve_criteria.html’ saved [14308]\n","\n","--2024-01-12 09:56:10--  https://api.python.langchain.com/en/stable/hub/langchain.hub.pull.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/hub/langchain.hub.pull.html’\n","\n","api.python.langchai     [ <=>                ]  12.35K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:10 (75.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/hub/langchain.hub.pull.html’ saved [12646]\n","\n","--2024-01-12 09:56:10--  https://api.python.langchain.com/en/stable/hub/langchain.hub.push.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/hub/langchain.hub.push.html’\n","\n","api.python.langchai     [ <=>                ]  14.27K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:10 (238 MB/s) - ‘docs/api.python.langchain.com/en/stable/hub/langchain.hub.push.html’ saved [14611]\n","\n","--2024-01-12 09:56:10--  https://api.python.langchain.com/en/stable/indexes/langchain.indexes.base.RecordManager.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/indexes/langchain.indexes.base.RecordManager.html’\n","\n","api.python.langchai     [ <=>                ]  40.48K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:10 (36.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/indexes/langchain.indexes.base.RecordManager.html’ saved [41449]\n","\n","--2024-01-12 09:56:10--  https://api.python.langchain.com/en/stable/indexes/langchain.indexes.graph.GraphIndexCreator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/indexes/langchain.indexes.graph.GraphIndexCreator.html’\n","\n","api.python.langchai     [ <=>                ]  53.41K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:10 (59.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/indexes/langchain.indexes.graph.GraphIndexCreator.html’ saved [54691]\n","\n","--2024-01-12 09:56:10--  https://api.python.langchain.com/en/stable/indexes/langchain.indexes.vectorstore.VectorStoreIndexWrapper.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/indexes/langchain.indexes.vectorstore.VectorStoreIndexWrapper.html’\n","\n","api.python.langchai     [ <=>                ]  44.33K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:10 (43.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/indexes/langchain.indexes.vectorstore.VectorStoreIndexWrapper.html’ saved [45390]\n","\n","--2024-01-12 09:56:10--  https://api.python.langchain.com/en/stable/indexes/langchain.indexes.vectorstore.VectorstoreIndexCreator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/indexes/langchain.indexes.vectorstore.VectorstoreIndexCreator.html’\n","\n","api.python.langchai     [ <=>                ]  46.45K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:10 (51.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/indexes/langchain.indexes.vectorstore.VectorstoreIndexCreator.html’ saved [47561]\n","\n","--2024-01-12 09:56:10--  https://api.python.langchain.com/en/stable/memory/langchain.memory.buffer.ConversationBufferMemory.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.buffer.ConversationBufferMemory.html’\n","\n","api.python.langchai     [ <=>                ]  58.23K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:11 (379 MB/s) - ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.buffer.ConversationBufferMemory.html’ saved [59629]\n","\n","--2024-01-12 09:56:11--  https://api.python.langchain.com/en/stable/memory/langchain.memory.buffer.ConversationStringBufferMemory.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.buffer.ConversationStringBufferMemory.html’\n","\n","api.python.langchai     [ <=>                ]  53.15K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:11 (64.7 MB/s) - ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.buffer.ConversationStringBufferMemory.html’ saved [54426]\n","\n","--2024-01-12 09:56:11--  https://api.python.langchain.com/en/stable/memory/langchain.memory.buffer_window.ConversationBufferWindowMemory.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.buffer_window.ConversationBufferWindowMemory.html’\n","\n","api.python.langchai     [ <=>                ]  57.88K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:56:11 (32.7 MB/s) - ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.buffer_window.ConversationBufferWindowMemory.html’ saved [59267]\n","\n","--2024-01-12 09:56:11--  https://api.python.langchain.com/en/stable/memory/langchain.memory.chat_memory.BaseChatMemory.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.chat_memory.BaseChatMemory.html’\n","\n","api.python.langchai     [ <=>                ]  51.93K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:56:11 (31.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.chat_memory.BaseChatMemory.html’ saved [53177]\n","\n","--2024-01-12 09:56:11--  https://api.python.langchain.com/en/stable/memory/langchain.memory.combined.CombinedMemory.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.combined.CombinedMemory.html’\n","\n","api.python.langchai     [ <=>                ]  49.86K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:11 (58.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.combined.CombinedMemory.html’ saved [51056]\n","\n","--2024-01-12 09:56:11--  https://api.python.langchain.com/en/stable/memory/langchain.memory.entity.BaseEntityStore.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.entity.BaseEntityStore.html’\n","\n","api.python.langchai     [ <=>                ]  43.43K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:11 (15.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.entity.BaseEntityStore.html’ saved [44475]\n","\n","--2024-01-12 09:56:11--  https://api.python.langchain.com/en/stable/memory/langchain.memory.entity.ConversationEntityMemory.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.entity.ConversationEntityMemory.html’\n","\n","api.python.langchai     [ <=>                ]  73.15K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:56:11 (46.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.entity.ConversationEntityMemory.html’ saved [74903]\n","\n","--2024-01-12 09:56:11--  https://api.python.langchain.com/en/stable/memory/langchain.memory.entity.InMemoryEntityStore.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.entity.InMemoryEntityStore.html’\n","\n","api.python.langchai     [ <=>                ]  44.32K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:11 (29.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.entity.InMemoryEntityStore.html’ saved [45386]\n","\n","--2024-01-12 09:56:11--  https://api.python.langchain.com/en/stable/memory/langchain.memory.entity.RedisEntityStore.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.entity.RedisEntityStore.html’\n","\n","api.python.langchai     [ <=>                ]  47.53K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:12 (40.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.entity.RedisEntityStore.html’ saved [48669]\n","\n","--2024-01-12 09:56:12--  https://api.python.langchain.com/en/stable/memory/langchain.memory.entity.SQLiteEntityStore.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.entity.SQLiteEntityStore.html’\n","\n","api.python.langchai     [ <=>                ]  45.46K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:12 (344 MB/s) - ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.entity.SQLiteEntityStore.html’ saved [46556]\n","\n","--2024-01-12 09:56:12--  https://api.python.langchain.com/en/stable/memory/langchain.memory.entity.UpstashRedisEntityStore.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.entity.UpstashRedisEntityStore.html’\n","\n","api.python.langchai     [ <=>                ]  44.17K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:12 (32.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.entity.UpstashRedisEntityStore.html’ saved [45231]\n","\n","--2024-01-12 09:56:12--  https://api.python.langchain.com/en/stable/memory/langchain.memory.kg.ConversationKGMemory.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.kg.ConversationKGMemory.html’\n","\n","api.python.langchai     [ <=>                ]  78.20K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:12 (66.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.kg.ConversationKGMemory.html’ saved [80077]\n","\n","--2024-01-12 09:56:12--  https://api.python.langchain.com/en/stable/memory/langchain.memory.motorhead_memory.MotorheadMemory.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.motorhead_memory.MotorheadMemory.html’\n","\n","api.python.langchai     [ <=>                ]  59.40K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:12 (54.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.motorhead_memory.MotorheadMemory.html’ saved [60826]\n","\n","--2024-01-12 09:56:12--  https://api.python.langchain.com/en/stable/memory/langchain.memory.readonly.ReadOnlySharedMemory.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.readonly.ReadOnlySharedMemory.html’\n","\n","api.python.langchai     [ <=>                ]  49.91K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:12 (58.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.readonly.ReadOnlySharedMemory.html’ saved [51104]\n","\n","--2024-01-12 09:56:12--  https://api.python.langchain.com/en/stable/memory/langchain.memory.simple.SimpleMemory.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.simple.SimpleMemory.html’\n","\n","api.python.langchai     [ <=>                ]  49.23K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:12 (40.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.simple.SimpleMemory.html’ saved [50416]\n","\n","--2024-01-12 09:56:12--  https://api.python.langchain.com/en/stable/memory/langchain.memory.summary.ConversationSummaryMemory.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.summary.ConversationSummaryMemory.html’\n","\n","api.python.langchai     [ <=>                ]  64.56K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:56:12 (38.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.summary.ConversationSummaryMemory.html’ saved [66108]\n","\n","--2024-01-12 09:56:12--  https://api.python.langchain.com/en/stable/memory/langchain.memory.summary.SummarizerMixin.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.summary.SummarizerMixin.html’\n","\n","api.python.langchai     [ <=>                ]  46.58K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:12 (41.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.summary.SummarizerMixin.html’ saved [47695]\n","\n","--2024-01-12 09:56:12--  https://api.python.langchain.com/en/stable/memory/langchain.memory.summary_buffer.ConversationSummaryBufferMemory.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.summary_buffer.ConversationSummaryBufferMemory.html’\n","\n","api.python.langchai     [ <=>                ]  66.12K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:13 (47.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.summary_buffer.ConversationSummaryBufferMemory.html’ saved [67711]\n","\n","--2024-01-12 09:56:13--  https://api.python.langchain.com/en/stable/memory/langchain.memory.token_buffer.ConversationTokenBufferMemory.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.token_buffer.ConversationTokenBufferMemory.html’\n","\n","api.python.langchai     [ <=>                ]  58.40K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:56:13 (33.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.token_buffer.ConversationTokenBufferMemory.html’ saved [59798]\n","\n","--2024-01-12 09:56:13--  https://api.python.langchain.com/en/stable/memory/langchain.memory.vectorstore.VectorStoreRetrieverMemory.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.vectorstore.VectorStoreRetrieverMemory.html’\n","\n","api.python.langchai     [ <=>                ]  54.47K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:56:13 (23.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.vectorstore.VectorStoreRetrieverMemory.html’ saved [55776]\n","\n","--2024-01-12 09:56:13--  https://api.python.langchain.com/en/stable/memory/langchain.memory.zep_memory.ZepMemory.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.zep_memory.ZepMemory.html’\n","\n","api.python.langchai     [ <=>                ]  59.21K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:13 (63.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.zep_memory.ZepMemory.html’ saved [60632]\n","\n","--2024-01-12 09:56:13--  https://api.python.langchain.com/en/stable/memory/langchain.memory.utils.get_prompt_input_key.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.utils.get_prompt_input_key.html’\n","\n","api.python.langchai     [ <=>                ]  11.76K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:13 (137 MB/s) - ‘docs/api.python.langchain.com/en/stable/memory/langchain.memory.utils.get_prompt_input_key.html’ saved [12038]\n","\n","--2024-01-12 09:56:13--  https://api.python.langchain.com/en/stable/model_laboratory/langchain.model_laboratory.ModelLaboratory.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/model_laboratory/langchain.model_laboratory.ModelLaboratory.html’\n","\n","api.python.langchai     [ <=>                ]  19.46K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:13 (199 MB/s) - ‘docs/api.python.langchain.com/en/stable/model_laboratory/langchain.model_laboratory.ModelLaboratory.html’ saved [19931]\n","\n","--2024-01-12 09:56:13--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.boolean.BooleanOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.boolean.BooleanOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 124.20K  --.-KB/s    in 0.007s  \n","\n","2024-01-12 09:56:13 (17.7 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.boolean.BooleanOutputParser.html’ saved [127182]\n","\n","--2024-01-12 09:56:13--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.combining.CombiningOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.combining.CombiningOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 124.40K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:13 (41.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.combining.CombiningOutputParser.html’ saved [127387]\n","\n","--2024-01-12 09:56:13--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.datetime.DatetimeOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.datetime.DatetimeOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 124.37K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:14 (315 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.datetime.DatetimeOutputParser.html’ saved [127351]\n","\n","--2024-01-12 09:56:14--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.enum.EnumOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.enum.EnumOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 123.13K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:14 (42.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.enum.EnumOutputParser.html’ saved [126089]\n","\n","--2024-01-12 09:56:14--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.ernie_functions.JsonKeyOutputFunctionsParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.ernie_functions.JsonKeyOutputFunctionsParser.html’\n","\n","api.python.langchai     [ <=>                ] 129.24K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:14 (41.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.ernie_functions.JsonKeyOutputFunctionsParser.html’ saved [132342]\n","\n","--2024-01-12 09:56:14--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.ernie_functions.JsonOutputFunctionsParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.ernie_functions.JsonOutputFunctionsParser.html’\n","\n","api.python.langchai     [ <=>                ] 128.31K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:56:14 (50.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.ernie_functions.JsonOutputFunctionsParser.html’ saved [131388]\n","\n","--2024-01-12 09:56:14--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.ernie_functions.OutputFunctionsParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.ernie_functions.OutputFunctionsParser.html’\n","\n","api.python.langchai     [ <=>                ] 123.06K  --.-KB/s    in 0.006s  \n","\n","2024-01-12 09:56:14 (21.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.ernie_functions.OutputFunctionsParser.html’ saved [126011]\n","\n","--2024-01-12 09:56:14--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.ernie_functions.PydanticAttrOutputFunctionsParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.ernie_functions.PydanticAttrOutputFunctionsParser.html’\n","\n","api.python.langchai     [ <=>                ] 126.59K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:14 (43.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.ernie_functions.PydanticAttrOutputFunctionsParser.html’ saved [129625]\n","\n","--2024-01-12 09:56:14--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.ernie_functions.PydanticOutputFunctionsParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.ernie_functions.PydanticOutputFunctionsParser.html’\n","\n","api.python.langchai     [ <=>                ] 125.35K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:15 (37.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.ernie_functions.PydanticOutputFunctionsParser.html’ saved [128361]\n","\n","--2024-01-12 09:56:15--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.fix.OutputFixingParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.fix.OutputFixingParser.html’\n","\n","api.python.langchai     [ <=>                ] 130.21K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:15 (46.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.fix.OutputFixingParser.html’ saved [133338]\n","\n","--2024-01-12 09:56:15--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_functions.JsonKeyOutputFunctionsParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_functions.JsonKeyOutputFunctionsParser.html’\n","\n","api.python.langchai     [ <=>                ] 129.38K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:56:15 (51.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_functions.JsonKeyOutputFunctionsParser.html’ saved [132480]\n","\n","--2024-01-12 09:56:15--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_functions.JsonOutputFunctionsParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_functions.JsonOutputFunctionsParser.html’\n","\n","api.python.langchai     [ <=>                ] 128.44K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:15 (42.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_functions.JsonOutputFunctionsParser.html’ saved [131522]\n","\n","--2024-01-12 09:56:15--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_functions.OutputFunctionsParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_functions.OutputFunctionsParser.html’\n","\n","api.python.langchai     [ <=>                ] 123.17K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:15 (43.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_functions.OutputFunctionsParser.html’ saved [126131]\n","\n","--2024-01-12 09:56:15--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_functions.PydanticAttrOutputFunctionsParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_functions.PydanticAttrOutputFunctionsParser.html’\n","\n","api.python.langchai     [ <=>                ] 126.71K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:15 (43.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_functions.PydanticAttrOutputFunctionsParser.html’ saved [129752]\n","\n","--2024-01-12 09:56:15--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_functions.PydanticOutputFunctionsParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_functions.PydanticOutputFunctionsParser.html’\n","\n","api.python.langchai     [ <=>                ] 125.47K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:16 (44.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_functions.PydanticOutputFunctionsParser.html’ saved [128486]\n","\n","--2024-01-12 09:56:16--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_tools.JsonOutputKeyToolsParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_tools.JsonOutputKeyToolsParser.html’\n","\n","api.python.langchai     [ <=>                ] 122.82K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:16 (35.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_tools.JsonOutputKeyToolsParser.html’ saved [125766]\n","\n","--2024-01-12 09:56:16--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_tools.JsonOutputToolsParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_tools.JsonOutputToolsParser.html’\n","\n","api.python.langchai     [ <=>                ] 121.90K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:16 (36.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_tools.JsonOutputToolsParser.html’ saved [124827]\n","\n","--2024-01-12 09:56:16--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_tools.PydanticToolsParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_tools.PydanticToolsParser.html’\n","\n","api.python.langchai     [ <=>                ] 122.46K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:16 (40.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.openai_tools.PydanticToolsParser.html’ saved [125395]\n","\n","--2024-01-12 09:56:16--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.pandas_dataframe.PandasDataFrameOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.pandas_dataframe.PandasDataFrameOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 127.66K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:16 (45.7 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.pandas_dataframe.PandasDataFrameOutputParser.html’ saved [130727]\n","\n","--2024-01-12 09:56:16--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.pydantic.PydanticOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.pydantic.PydanticOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 124.90K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:16 (36.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.pydantic.PydanticOutputParser.html’ saved [127901]\n","\n","--2024-01-12 09:56:16--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.rail_parser.GuardrailsOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.rail_parser.GuardrailsOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 135.36K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:16 (44.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.rail_parser.GuardrailsOutputParser.html’ saved [138608]\n","\n","--2024-01-12 09:56:16--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.regex.RegexParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.regex.RegexParser.html’\n","\n","api.python.langchai     [ <=>                ] 124.69K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:16 (190 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.regex.RegexParser.html’ saved [127682]\n","\n","--2024-01-12 09:56:16--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.regex_dict.RegexDictParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.regex_dict.RegexDictParser.html’\n","\n","api.python.langchai     [ <=>                ] 125.06K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:17 (48.7 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.regex_dict.RegexDictParser.html’ saved [128064]\n","\n","--2024-01-12 09:56:17--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.retry.RetryOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.retry.RetryOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 131.44K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:17 (39.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.retry.RetryOutputParser.html’ saved [134596]\n","\n","--2024-01-12 09:56:17--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.retry.RetryWithErrorOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.retry.RetryWithErrorOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 132.76K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:17 (45.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.retry.RetryWithErrorOutputParser.html’ saved [135946]\n","\n","--2024-01-12 09:56:17--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.structured.ResponseSchema.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.structured.ResponseSchema.html’\n","\n","api.python.langchai     [ <=>                ]  40.05K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:17 (86.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.structured.ResponseSchema.html’ saved [41008]\n","\n","--2024-01-12 09:56:17--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.structured.StructuredOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.structured.StructuredOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 128.56K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:17 (37.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.structured.StructuredOutputParser.html’ saved [131646]\n","\n","--2024-01-12 09:56:17--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.yaml.YamlOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.yaml.YamlOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 123.85K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:17 (264 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.yaml.YamlOutputParser.html’ saved [126826]\n","\n","--2024-01-12 09:56:17--  https://api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.loading.load_output_parser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.loading.load_output_parser.html’\n","\n","api.python.langchai     [ <=>                ]  11.22K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:17 (9.71 MB/s) - ‘docs/api.python.langchain.com/en/stable/output_parsers/langchain.output_parsers.loading.load_output_parser.html’ saved [11485]\n","\n","--2024-01-12 09:56:17--  https://api.python.langchain.com/en/stable/prompts/langchain.prompts.example_selector.ngram_overlap.NGramOverlapExampleSelector.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/prompts/langchain.prompts.example_selector.ngram_overlap.NGramOverlapExampleSelector.html’\n","\n","api.python.langchai     [ <=>                ]  45.72K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:18 (63.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/prompts/langchain.prompts.example_selector.ngram_overlap.NGramOverlapExampleSelector.html’ saved [46817]\n","\n","--2024-01-12 09:56:18--  https://api.python.langchain.com/en/stable/prompts/langchain.prompts.example_selector.ngram_overlap.ngram_overlap_score.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/prompts/langchain.prompts.example_selector.ngram_overlap.ngram_overlap_score.html’\n","\n","api.python.langchai     [ <=>                ]  12.03K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:18 (172 MB/s) - ‘docs/api.python.langchain.com/en/stable/prompts/langchain.prompts.example_selector.ngram_overlap.ngram_overlap_score.html’ saved [12320]\n","\n","--2024-01-12 09:56:18--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.contextual_compression.ContextualCompressionRetriever.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.contextual_compression.ContextualCompressionRetriever.html’\n","\n","api.python.langchai     [ <=>                ] 133.61K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:56:18 (36.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.contextual_compression.ContextualCompressionRetriever.html’ saved [136814]\n","\n","--2024-01-12 09:56:18--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.base.BaseDocumentCompressor.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.base.BaseDocumentCompressor.html’\n","\n","api.python.langchai     [ <=>                ]  45.30K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:18 (99.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.base.BaseDocumentCompressor.html’ saved [46389]\n","\n","--2024-01-12 09:56:18--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.base.DocumentCompressorPipeline.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.base.DocumentCompressorPipeline.html’\n","\n","api.python.langchai     [ <=>                ]  47.62K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:18 (40.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.base.DocumentCompressorPipeline.html’ saved [48767]\n","\n","--2024-01-12 09:56:18--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.chain_extract.LLMChainExtractor.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.chain_extract.LLMChainExtractor.html’\n","\n","api.python.langchai     [ <=>                ]  51.85K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:18 (50.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.chain_extract.LLMChainExtractor.html’ saved [53092]\n","\n","--2024-01-12 09:56:18--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.chain_extract.NoOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.chain_extract.NoOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 125.70K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:56:18 (32.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.chain_extract.NoOutputParser.html’ saved [128717]\n","\n","--2024-01-12 09:56:18--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.chain_filter.LLMChainFilter.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.chain_filter.LLMChainFilter.html’\n","\n","api.python.langchai     [ <=>                ]  50.68K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:18 (41.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.chain_filter.LLMChainFilter.html’ saved [51899]\n","\n","--2024-01-12 09:56:18--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.cohere_rerank.CohereRerank.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.cohere_rerank.CohereRerank.html’\n","\n","api.python.langchai     [ <=>                ]  50.05K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:18 (48.7 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.cohere_rerank.CohereRerank.html’ saved [51255]\n","\n","--2024-01-12 09:56:18--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.embeddings_filter.EmbeddingsFilter.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.embeddings_filter.EmbeddingsFilter.html’\n","\n","api.python.langchai     [ <=>                ]  49.59K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:19 (36.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.embeddings_filter.EmbeddingsFilter.html’ saved [50785]\n","\n","--2024-01-12 09:56:19--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.ensemble.EnsembleRetriever.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.ensemble.EnsembleRetriever.html’\n","\n","api.python.langchai     [ <=>                ] 137.89K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:19 (53.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.ensemble.EnsembleRetriever.html’ saved [141197]\n","\n","--2024-01-12 09:56:19--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.merger_retriever.MergerRetriever.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.merger_retriever.MergerRetriever.html’\n","\n","api.python.langchai     [ <=>                ] 134.15K  --.-KB/s    in 0.007s  \n","\n","2024-01-12 09:56:19 (18.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.merger_retriever.MergerRetriever.html’ saved [137374]\n","\n","--2024-01-12 09:56:19--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.multi_query.LineList.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.multi_query.LineList.html’\n","\n","api.python.langchai     [ <=>                ]  38.42K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:56:19 (24.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.multi_query.LineList.html’ saved [39339]\n","\n","--2024-01-12 09:56:19--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.multi_query.LineListOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.multi_query.LineListOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 123.72K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:56:19 (31.7 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.multi_query.LineListOutputParser.html’ saved [126687]\n","\n","--2024-01-12 09:56:19--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.multi_query.MultiQueryRetriever.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.multi_query.MultiQueryRetriever.html’\n","\n","api.python.langchai     [ <=>                ] 149.03K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:56:19 (34.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.multi_query.MultiQueryRetriever.html’ saved [152602]\n","\n","--2024-01-12 09:56:19--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.multi_vector.MultiVectorRetriever.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.multi_vector.MultiVectorRetriever.html’\n","\n","api.python.langchai     [ <=>                ] 135.06K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:56:19 (73.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.multi_vector.MultiVectorRetriever.html’ saved [138298]\n","\n","--2024-01-12 09:56:19--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.multi_vector.SearchType.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.multi_vector.SearchType.html’\n","\n","api.python.langchai     [ <=>                ]  12.97K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:19 (47.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.multi_vector.SearchType.html’ saved [13286]\n","\n","--2024-01-12 09:56:19--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.parent_document_retriever.ParentDocumentRetriever.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.parent_document_retriever.ParentDocumentRetriever.html’\n","\n","api.python.langchai     [ <=>                ] 145.54K  --.-KB/s    in 0.006s  \n","\n","2024-01-12 09:56:19 (23.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.parent_document_retriever.ParentDocumentRetriever.html’ saved [149032]\n","\n","--2024-01-12 09:56:19--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.re_phraser.RePhraseQueryRetriever.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.re_phraser.RePhraseQueryRetriever.html’\n","\n","api.python.langchai     [ <=>                ] 135.03K  --.-KB/s    in 0.004s  \n","\n","2024-01-12 09:56:20 (29.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.re_phraser.RePhraseQueryRetriever.html’ saved [138266]\n","\n","--2024-01-12 09:56:20--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.base.SelfQueryRetriever.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.base.SelfQueryRetriever.html’\n","\n","api.python.langchai     [ <=>                ] 143.83K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:56:20 (71.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.base.SelfQueryRetriever.html’ saved [147282]\n","\n","--2024-01-12 09:56:20--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.chroma.ChromaTranslator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.chroma.ChromaTranslator.html’\n","\n","api.python.langchai     [ <=>                ]  17.37K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:20 (133 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.chroma.ChromaTranslator.html’ saved [17782]\n","\n","--2024-01-12 09:56:20--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.dashvector.DashvectorTranslator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.dashvector.DashvectorTranslator.html’\n","\n","api.python.langchai     [ <=>                ]  17.67K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:20 (47.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.dashvector.DashvectorTranslator.html’ saved [18095]\n","\n","--2024-01-12 09:56:20--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.deeplake.DeepLakeTranslator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.deeplake.DeepLakeTranslator.html’\n","\n","api.python.langchai     [ <=>                ]  17.48K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:20 (197 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.deeplake.DeepLakeTranslator.html’ saved [17902]\n","\n","--2024-01-12 09:56:20--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.elasticsearch.ElasticsearchTranslator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.elasticsearch.ElasticsearchTranslator.html’\n","\n","api.python.langchai     [ <=>                ]  17.78K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:20 (190 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.elasticsearch.ElasticsearchTranslator.html’ saved [18209]\n","\n","--2024-01-12 09:56:20--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.milvus.MilvusTranslator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.milvus.MilvusTranslator.html’\n","\n","api.python.langchai     [ <=>                ]  17.32K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:20 (135 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.milvus.MilvusTranslator.html’ saved [17731]\n","\n","--2024-01-12 09:56:20--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.mongodb_atlas.MongoDBAtlasTranslator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.mongodb_atlas.MongoDBAtlasTranslator.html’\n","\n","api.python.langchai     [ <=>                ]  17.70K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:20 (172 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.mongodb_atlas.MongoDBAtlasTranslator.html’ saved [18120]\n","\n","--2024-01-12 09:56:20--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.myscale.MyScaleTranslator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.myscale.MyScaleTranslator.html’\n","\n","api.python.langchai     [ <=>                ]  18.69K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:21 (64.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.myscale.MyScaleTranslator.html’ saved [19139]\n","\n","--2024-01-12 09:56:21--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.opensearch.OpenSearchTranslator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.opensearch.OpenSearchTranslator.html’\n","\n","api.python.langchai     [ <=>                ]  17.62K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:21 (123 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.opensearch.OpenSearchTranslator.html’ saved [18042]\n","\n","--2024-01-12 09:56:21--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.pinecone.PineconeTranslator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.pinecone.PineconeTranslator.html’\n","\n","api.python.langchai     [ <=>                ]  17.48K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:21 (69.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.pinecone.PineconeTranslator.html’ saved [17904]\n","\n","--2024-01-12 09:56:21--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.qdrant.QdrantTranslator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.qdrant.QdrantTranslator.html’\n","\n","api.python.langchai     [ <=>                ]  18.06K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:21 (33.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.qdrant.QdrantTranslator.html’ saved [18493]\n","\n","--2024-01-12 09:56:21--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.redis.RedisTranslator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.redis.RedisTranslator.html’\n","\n","api.python.langchai     [ <=>                ]  20.67K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:21 (46.1 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.redis.RedisTranslator.html’ saved [21164]\n","\n","--2024-01-12 09:56:21--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.supabase.SupabaseVectorTranslator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.supabase.SupabaseVectorTranslator.html’\n","\n","api.python.langchai     [ <=>                ]  18.02K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:21 (179 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.supabase.SupabaseVectorTranslator.html’ saved [18450]\n","\n","--2024-01-12 09:56:21--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.timescalevector.TimescaleVectorTranslator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.timescalevector.TimescaleVectorTranslator.html’\n","\n","api.python.langchai     [ <=>                ]  18.18K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:21 (55.7 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.timescalevector.TimescaleVectorTranslator.html’ saved [18621]\n","\n","--2024-01-12 09:56:21--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.vectara.VectaraTranslator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.vectara.VectaraTranslator.html’\n","\n","api.python.langchai     [ <=>                ]  17.42K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:21 (244 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.vectara.VectaraTranslator.html’ saved [17841]\n","\n","--2024-01-12 09:56:21--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.weaviate.WeaviateTranslator.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.weaviate.WeaviateTranslator.html’\n","\n","api.python.langchai     [ <=>                ]  17.45K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:21 (62.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.weaviate.WeaviateTranslator.html’ saved [17866]\n","\n","--2024-01-12 09:56:21--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.time_weighted_retriever.TimeWeightedVectorStoreRetriever.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.time_weighted_retriever.TimeWeightedVectorStoreRetriever.html’\n","\n","api.python.langchai     [ <=>                ] 143.64K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:22 (231 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.time_weighted_retriever.TimeWeightedVectorStoreRetriever.html’ saved [147085]\n","\n","--2024-01-12 09:56:22--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.web_research.LineList.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.web_research.LineList.html’\n","\n","api.python.langchai     [ <=>                ]  38.43K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:22 (66.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.web_research.LineList.html’ saved [39354]\n","\n","--2024-01-12 09:56:22--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.web_research.QuestionListOutputParser.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.web_research.QuestionListOutputParser.html’\n","\n","api.python.langchai     [ <=>                ] 124.36K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:56:22 (25.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.web_research.QuestionListOutputParser.html’ saved [127343]\n","\n","--2024-01-12 09:56:22--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.web_research.SearchQueries.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.web_research.SearchQueries.html’\n","\n","api.python.langchai     [ <=>                ]  38.68K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:22 (96.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.web_research.SearchQueries.html’ saved [39610]\n","\n","--2024-01-12 09:56:22--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.web_research.WebResearchRetriever.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.web_research.WebResearchRetriever.html’\n","\n","api.python.langchai     [ <=>                ] 139.95K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:22 (42.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.web_research.WebResearchRetriever.html’ saved [143304]\n","\n","--2024-01-12 09:56:22--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.chain_extract.default_get_input.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.chain_extract.default_get_input.html’\n","\n","api.python.langchai     [ <=>                ]  11.81K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:22 (166 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.chain_extract.default_get_input.html’ saved [12090]\n","\n","--2024-01-12 09:56:22--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.chain_filter.default_get_input.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.chain_filter.default_get_input.html’\n","\n","api.python.langchai     [ <=>                ]  11.80K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:22 (163 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.document_compressors.chain_filter.default_get_input.html’ saved [12079]\n","\n","--2024-01-12 09:56:22--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.deeplake.can_cast_to_float.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.deeplake.can_cast_to_float.html’\n","\n","api.python.langchai     [ <=>                ]  11.05K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:22 (208 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.deeplake.can_cast_to_float.html’ saved [11312]\n","\n","--2024-01-12 09:56:22--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.milvus.process_value.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.milvus.process_value.html’\n","\n","api.python.langchai     [ <=>                ]  11.64K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:23 (208 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.milvus.process_value.html’ saved [11921]\n","\n","--2024-01-12 09:56:23--  https://api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.vectara.process_value.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.vectara.process_value.html’\n","\n","api.python.langchai     [ <=>                ]  11.34K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:23 (173 MB/s) - ‘docs/api.python.langchain.com/en/stable/retrievers/langchain.retrievers.self_query.vectara.process_value.html’ saved [11614]\n","\n","--2024-01-12 09:56:23--  https://api.python.langchain.com/en/stable/runnables/langchain.runnables.hub.HubRunnable.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/runnables/langchain.runnables.hub.HubRunnable.html’\n","\n","api.python.langchai     [ <=>                ] 122.06K  --.-KB/s    in 0.003s  \n","\n","2024-01-12 09:56:23 (39.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/runnables/langchain.runnables.hub.HubRunnable.html’ saved [124991]\n","\n","--2024-01-12 09:56:23--  https://api.python.langchain.com/en/stable/runnables/langchain.runnables.openai_functions.OpenAIFunction.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/runnables/langchain.runnables.openai_functions.OpenAIFunction.html’\n","\n","api.python.langchai     [ <=>                ]  25.56K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:23 (26.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/runnables/langchain.runnables.openai_functions.OpenAIFunction.html’ saved [26178]\n","\n","--2024-01-12 09:56:23--  https://api.python.langchain.com/en/stable/runnables/langchain.runnables.openai_functions.OpenAIFunctionsRouter.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/runnables/langchain.runnables.openai_functions.OpenAIFunctionsRouter.html’\n","\n","api.python.langchai     [ <=>                ] 125.67K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:56:23 (49.7 MB/s) - ‘docs/api.python.langchain.com/en/stable/runnables/langchain.runnables.openai_functions.OpenAIFunctionsRouter.html’ saved [128684]\n","\n","--2024-01-12 09:56:23--  https://api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.runner_utils.arun_on_dataset.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.runner_utils.arun_on_dataset.html’\n","\n","api.python.langchai     [ <=>                ]  26.90K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:23 (91.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.runner_utils.arun_on_dataset.html’ saved [27548]\n","\n","--2024-01-12 09:56:23--  https://api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.runner_utils.run_on_dataset.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.runner_utils.run_on_dataset.html’\n","\n","api.python.langchai     [ <=>                ]  26.79K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:23 (41.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.runner_utils.run_on_dataset.html’ saved [27432]\n","\n","--2024-01-12 09:56:23--  https://api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.config.RunEvalConfig.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.config.RunEvalConfig.html’\n","\n","api.python.langchai     [ <=>                ] 519.12K  --.-KB/s    in 0.01s   \n","\n","2024-01-12 09:56:23 (37.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.config.RunEvalConfig.html’ saved [531582]\n","\n","--2024-01-12 09:56:23--  https://api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.config.EvalConfig.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.config.EvalConfig.html’\n","\n","api.python.langchai     [ <=>                ]  40.79K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:23 (54.7 MB/s) - ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.config.EvalConfig.html’ saved [41771]\n","\n","--2024-01-12 09:56:23--  https://api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.config.SingleKeyEvalConfig.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.config.SingleKeyEvalConfig.html’\n","\n","api.python.langchai     [ <=>                ]  43.39K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:24 (359 MB/s) - ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.config.SingleKeyEvalConfig.html’ saved [44429]\n","\n","--2024-01-12 09:56:24--  https://api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.progress.ProgressBarCallback.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.progress.ProgressBarCallback.html’\n","\n","api.python.langchai     [ <=>                ]  72.14K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:24 (54.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.progress.ProgressBarCallback.html’ saved [73868]\n","\n","--2024-01-12 09:56:24--  https://api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.runner_utils.EvalError.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.runner_utils.EvalError.html’\n","\n","api.python.langchai     [ <=>                ]  25.75K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:24 (36.7 MB/s) - ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.runner_utils.EvalError.html’ saved [26367]\n","\n","--2024-01-12 09:56:24--  https://api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.runner_utils.InputFormatError.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.runner_utils.InputFormatError.html’\n","\n","api.python.langchai     [ <=>                ]  10.68K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:24 (118 MB/s) - ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.runner_utils.InputFormatError.html’ saved [10938]\n","\n","--2024-01-12 09:56:24--  https://api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.runner_utils.TestResult.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.runner_utils.TestResult.html’\n","\n","api.python.langchai     [ <=>                ]  27.44K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:24 (51.7 MB/s) - ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.runner_utils.TestResult.html’ saved [28095]\n","\n","--2024-01-12 09:56:24--  https://api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.string_run_evaluator.ChainStringRunMapper.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.string_run_evaluator.ChainStringRunMapper.html’\n","\n","api.python.langchai     [ <=>                ]  50.48K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:24 (61.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.string_run_evaluator.ChainStringRunMapper.html’ saved [51688]\n","\n","--2024-01-12 09:56:24--  https://api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.string_run_evaluator.LLMStringRunMapper.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.string_run_evaluator.LLMStringRunMapper.html’\n","\n","api.python.langchai     [ <=>                ]  51.50K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:24 (46.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.string_run_evaluator.LLMStringRunMapper.html’ saved [52741]\n","\n","--2024-01-12 09:56:24--  https://api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.string_run_evaluator.StringExampleMapper.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.string_run_evaluator.StringExampleMapper.html’\n","\n","api.python.langchai     [ <=>                ]  50.65K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:24 (110 MB/s) - ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.string_run_evaluator.StringExampleMapper.html’ saved [51864]\n","\n","--2024-01-12 09:56:24--  https://api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.string_run_evaluator.StringRunEvaluatorChain.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.string_run_evaluator.StringRunEvaluatorChain.html’\n","\n","api.python.langchai     [ <=>                ] 174.04K  --.-KB/s    in 0.005s  \n","\n","2024-01-12 09:56:25 (34.3 MB/s) - ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.string_run_evaluator.StringRunEvaluatorChain.html’ saved [178220]\n","\n","--2024-01-12 09:56:25--  https://api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.string_run_evaluator.StringRunMapper.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.string_run_evaluator.StringRunMapper.html’\n","\n","api.python.langchai     [ <=>                ]  48.35K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:56:25 (30.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.string_run_evaluator.StringRunMapper.html’ saved [49506]\n","\n","--2024-01-12 09:56:25--  https://api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.string_run_evaluator.ToolStringRunMapper.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.string_run_evaluator.ToolStringRunMapper.html’\n","\n","api.python.langchai     [ <=>                ]  48.32K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:25 (47.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.string_run_evaluator.ToolStringRunMapper.html’ saved [49476]\n","\n","--2024-01-12 09:56:25--  https://api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.name_generation.random_name.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.name_generation.random_name.html’\n","\n","api.python.langchai     [ <=>                ]  10.78K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:25 (220 MB/s) - ‘docs/api.python.langchain.com/en/stable/smith/langchain.smith.evaluation.name_generation.random_name.html’ saved [11034]\n","\n","--2024-01-12 09:56:25--  https://api.python.langchain.com/en/stable/storage/langchain.storage.encoder_backed.EncoderBackedStore.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/storage/langchain.storage.encoder_backed.EncoderBackedStore.html’\n","\n","api.python.langchai     [ <=>                ]  26.94K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:25 (52.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/storage/langchain.storage.encoder_backed.EncoderBackedStore.html’ saved [27587]\n","\n","--2024-01-12 09:56:25--  https://api.python.langchain.com/en/stable/storage/langchain.storage.file_system.LocalFileStore.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/storage/langchain.storage.file_system.LocalFileStore.html’\n","\n","api.python.langchai     [ <=>                ]  23.67K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:25 (43.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/storage/langchain.storage.file_system.LocalFileStore.html’ saved [24242]\n","\n","--2024-01-12 09:56:25--  https://api.python.langchain.com/en/stable/storage/langchain.storage.in_memory.InMemoryBaseStore.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/storage/langchain.storage.in_memory.InMemoryBaseStore.html’\n","\n","api.python.langchai     [ <=>                ]  22.05K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:25 (40.7 MB/s) - ‘docs/api.python.langchain.com/en/stable/storage/langchain.storage.in_memory.InMemoryBaseStore.html’ saved [22577]\n","\n","--2024-01-12 09:56:25--  https://api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.CharacterTextSplitter.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.CharacterTextSplitter.html’\n","\n","api.python.langchai     [ <=>                ]  40.36K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:25 (34.2 MB/s) - ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.CharacterTextSplitter.html’ saved [41325]\n","\n","--2024-01-12 09:56:25--  https://api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.ElementType.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.ElementType.html’\n","\n","api.python.langchai     [ <=>                ]  12.53K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:25 (33.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.ElementType.html’ saved [12831]\n","\n","--2024-01-12 09:56:25--  https://api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.HTMLHeaderTextSplitter.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.HTMLHeaderTextSplitter.html’\n","\n","api.python.langchai     [ <=>                ]  22.47K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:25 (49.4 MB/s) - ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.HTMLHeaderTextSplitter.html’ saved [23012]\n","\n","--2024-01-12 09:56:25--  https://api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.HeaderType.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.HeaderType.html’\n","\n","api.python.langchai     [ <=>                ]  11.85K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:26 (160 MB/s) - ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.HeaderType.html’ saved [12133]\n","\n","--2024-01-12 09:56:26--  https://api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.Language.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.Language.html’\n","\n","api.python.langchai     [ <=>                ]  21.28K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:26 (279 MB/s) - ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.Language.html’ saved [21789]\n","\n","--2024-01-12 09:56:26--  https://api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.LatexTextSplitter.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.LatexTextSplitter.html’\n","\n","api.python.langchai     [ <=>                ]  31.21K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:26 (89.7 MB/s) - ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.LatexTextSplitter.html’ saved [31960]\n","\n","--2024-01-12 09:56:26--  https://api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.LineType.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.LineType.html’\n","\n","api.python.langchai     [ <=>                ]  11.64K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:26 (87.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.LineType.html’ saved [11915]\n","\n","--2024-01-12 09:56:26--  https://api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.MarkdownHeaderTextSplitter.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.MarkdownHeaderTextSplitter.html’\n","\n","api.python.langchai     [ <=>                ]  18.95K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:26 (71.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.MarkdownHeaderTextSplitter.html’ saved [19406]\n","\n","--2024-01-12 09:56:26--  https://api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.MarkdownTextSplitter.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.MarkdownTextSplitter.html’\n","\n","api.python.langchai     [ <=>                ]  31.37K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:26 (134 MB/s) - ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.MarkdownTextSplitter.html’ saved [32121]\n","\n","--2024-01-12 09:56:26--  https://api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.NLTKTextSplitter.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.NLTKTextSplitter.html’\n","\n","api.python.langchai     [ <=>                ]  29.79K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:26 (64.7 MB/s) - ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.NLTKTextSplitter.html’ saved [30505]\n","\n","--2024-01-12 09:56:26--  https://api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.PythonCodeTextSplitter.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.PythonCodeTextSplitter.html’\n","\n","api.python.langchai     [ <=>                ]  31.46K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:26 (66.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.PythonCodeTextSplitter.html’ saved [32217]\n","\n","--2024-01-12 09:56:26--  https://api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html’\n","\n","api.python.langchai     [ <=>                ]  38.21K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:26 (42.6 MB/s) - ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html’ saved [39124]\n","\n","--2024-01-12 09:56:26--  https://api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.SentenceTransformersTokenTextSplitter.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.SentenceTransformersTokenTextSplitter.html’\n","\n","api.python.langchai     [ <=>                ]  33.25K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:27 (40.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.SentenceTransformersTokenTextSplitter.html’ saved [34045]\n","\n","--2024-01-12 09:56:27--  https://api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.SpacyTextSplitter.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.SpacyTextSplitter.html’\n","\n","api.python.langchai     [ <=>                ]  31.17K  --.-KB/s    in 0.002s  \n","\n","2024-01-12 09:56:27 (13.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.SpacyTextSplitter.html’ saved [31918]\n","\n","--2024-01-12 09:56:27--  https://api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.TextSplitter.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.TextSplitter.html’\n","\n","api.python.langchai     [ <=>                ]  31.71K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:27 (57.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.TextSplitter.html’ saved [32473]\n","\n","--2024-01-12 09:56:27--  https://api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.TokenTextSplitter.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.TokenTextSplitter.html’\n","\n","api.python.langchai     [ <=>                ]  33.74K  --.-KB/s    in 0.001s  \n","\n","2024-01-12 09:56:27 (42.0 MB/s) - ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.TokenTextSplitter.html’ saved [34547]\n","\n","--2024-01-12 09:56:27--  https://api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.Tokenizer.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.Tokenizer.html’\n","\n","api.python.langchai     [ <=>                ]  16.07K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:27 (126 MB/s) - ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.Tokenizer.html’ saved [16455]\n","\n","--2024-01-12 09:56:27--  https://api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.split_text_on_tokens.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.split_text_on_tokens.html’\n","\n","api.python.langchai     [ <=>                ]  11.61K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:27 (59.8 MB/s) - ‘docs/api.python.langchain.com/en/stable/text_splitter/langchain.text_splitter.split_text_on_tokens.html’ saved [11887]\n","\n","--2024-01-12 09:56:27--  https://api.python.langchain.com/en/stable/tools/langchain.tools.retriever.RetrieverInput.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/tools/langchain.tools.retriever.RetrieverInput.html’\n","\n","api.python.langchai     [ <=>                ]  38.25K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:27 (292 MB/s) - ‘docs/api.python.langchain.com/en/stable/tools/langchain.tools.retriever.RetrieverInput.html’ saved [39168]\n","\n","--2024-01-12 09:56:27--  https://api.python.langchain.com/en/stable/tools/langchain.tools.render.render_text_description.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/tools/langchain.tools.render.render_text_description.html’\n","\n","api.python.langchai     [ <=>                ]  11.40K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:27 (171 MB/s) - ‘docs/api.python.langchain.com/en/stable/tools/langchain.tools.render.render_text_description.html’ saved [11675]\n","\n","--2024-01-12 09:56:27--  https://api.python.langchain.com/en/stable/tools/langchain.tools.render.render_text_description_and_args.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/tools/langchain.tools.render.render_text_description_and_args.html’\n","\n","api.python.langchai     [ <=>                ]  11.64K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:27 (71.5 MB/s) - ‘docs/api.python.langchain.com/en/stable/tools/langchain.tools.render.render_text_description_and_args.html’ saved [11920]\n","\n","--2024-01-12 09:56:27--  https://api.python.langchain.com/en/stable/tools/langchain.tools.retriever.create_retriever_tool.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/tools/langchain.tools.retriever.create_retriever_tool.html’\n","\n","api.python.langchai     [ <=>                ]  13.03K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:28 (101 MB/s) - ‘docs/api.python.langchain.com/en/stable/tools/langchain.tools.retriever.create_retriever_tool.html’ saved [13346]\n","\n","--2024-01-12 09:56:28--  https://api.python.langchain.com/en/stable/utils/langchain.utils.ernie_functions.FunctionDescription.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/utils/langchain.utils.ernie_functions.FunctionDescription.html’\n","\n","api.python.langchai     [ <=>                ]  12.15K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:28 (249 MB/s) - ‘docs/api.python.langchain.com/en/stable/utils/langchain.utils.ernie_functions.FunctionDescription.html’ saved [12446]\n","\n","--2024-01-12 09:56:28--  https://api.python.langchain.com/en/stable/utils/langchain.utils.ernie_functions.ToolDescription.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/utils/langchain.utils.ernie_functions.ToolDescription.html’\n","\n","api.python.langchai     [ <=>                ]  11.91K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:28 (24.9 MB/s) - ‘docs/api.python.langchain.com/en/stable/utils/langchain.utils.ernie_functions.ToolDescription.html’ saved [12200]\n","\n","--2024-01-12 09:56:28--  https://api.python.langchain.com/en/stable/utils/langchain.utils.ernie_functions.convert_pydantic_to_ernie_function.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/utils/langchain.utils.ernie_functions.convert_pydantic_to_ernie_function.html’\n","\n","api.python.langchai     [ <=>                ]  12.55K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:28 (224 MB/s) - ‘docs/api.python.langchain.com/en/stable/utils/langchain.utils.ernie_functions.convert_pydantic_to_ernie_function.html’ saved [12855]\n","\n","--2024-01-12 09:56:28--  https://api.python.langchain.com/en/stable/utils/langchain.utils.ernie_functions.convert_pydantic_to_ernie_tool.html\n","Reusing existing connection to api.python.langchain.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘docs/api.python.langchain.com/en/stable/utils/langchain.utils.ernie_functions.convert_pydantic_to_ernie_tool.html’\n","\n","api.python.langchai     [ <=>                ]  12.50K  --.-KB/s    in 0s      \n","\n","2024-01-12 09:56:28 (169 MB/s) - ‘docs/api.python.langchain.com/en/stable/utils/langchain.utils.ernie_functions.convert_pydantic_to_ernie_tool.html’ saved [12795]\n","\n","FINISHED --2024-01-12 09:56:28--\n","Total wall clock time: 56s\n","Downloaded: 424 files, 32M in 0.8s (38.8 MB/s)\n"]}],"source":["!wget -r -l1 -A.html -P docs https://api.python.langchain.com/en/stable/langchain_api_reference.html"],"id":"qiodFLkaLUsF"},{"cell_type":"markdown","metadata":{"id":"5DWa9M6JLb8O"},"source":[" The docs are going to be used as input text for answering questions that a normal language model might not be aware of (LangChain docs is not necessarily part of its training data of Llama2). We can use LangChain itself to process these docs. Use the [ReadTheDocsLoader](https://python.langchain.com/docs/integrations/document_loaders/readthedocs_documentation) to load the docs from the `docs` folder.\n","\n"," At the time of creating this notebook, there  `423` documents were downloaded. However, since the documentation is being updated regularly this number might be different for you."],"id":"5DWa9M6JLb8O"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55240,"status":"ok","timestamp":1705053744299,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"Nd8ufORKLVy0","outputId":"301c8578-a124-4332-e61b-6f03792dcaa5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["423"]},"metadata":{},"execution_count":32}],"source":["from langchain.document_loaders import ReadTheDocsLoader\n","#### your code ####\n","loader = ReadTheDocsLoader('docs')\n","docs = loader.load()\n","#### your code ####\n","len(docs)"],"id":"Nd8ufORKLVy0"},{"cell_type":"markdown","metadata":{"id":"yKs0_OrBMQ7M"},"source":["Let's take a look at one of the documents. You see that LangChain has created a `Document` object. Look at the example below and fill in the cells to print out the text content and URL of the page (the URL of the page should starts with `https://`)."],"id":"yKs0_OrBMQ7M"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":208,"status":"ok","timestamp":1705053488916,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"vcpuudNRLV7k","outputId":"1fb54f69-2092-495b-aaed-e88c897cdb44"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(page_content='langchain_mistralai 0.0.1¶\\nlangchain_mistralai.chat_models¶\\nClasses¶\\nchat_models.ChatMistralAI\\nA chat model that uses the MistralAI API.\\nFunctions¶\\nchat_models.acompletion_with_retry(llm[,\\xa0...])\\nUse tenacity to retry the async completion call.', metadata={'source': 'docs/api.python.langchain.com/en/stable/mistralai_api_reference.html'})"]},"metadata":{},"execution_count":2}],"source":["docs[10]"],"id":"vcpuudNRLV7k"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":214,"status":"ok","timestamp":1705053490566,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"5Q24oWkGM3IP","outputId":"f3a13083-ca01-44d8-d174-51f33327c22e"},"outputs":[{"output_type":"stream","name":"stdout","text":["langchain_mistralai 0.0.1¶\n","langchain_mistralai.chat_models¶\n","Classes¶\n","chat_models.ChatMistralAI\n","A chat model that uses the MistralAI API.\n","Functions¶\n","chat_models.acompletion_with_retry(llm[, ...])\n","Use tenacity to retry the async completion call.\n","https://api.python.langchain.com/en/stable/mistralai_api_reference.html\n"]}],"source":["#### your code ####\n","page_content= docs[10].page_content\n","page_url=docs[10].metadata['source'].replace('docs/', 'https://')\n","#### your code ####\n","print(page_content)\n","print(page_url)"],"id":"5Q24oWkGM3IP"},{"cell_type":"markdown","metadata":{"id":"OfZKdZJsMeyj"},"source":["As you can imagine the documents can be long and if multiple of them are required as context to answer questions, we need to take the document lengths into account.\n","This is due to the fact that language models do not have unlimited context span. In our case, we plan to use Llama2 for this project, where the maximum token limit is 4096. This limit is not only the input but also takes the generated output into account, moreover, you need to leave room for the query and instructions as well. Therefore, it is important to chunk the longer documents into smaller-sized fragments.\n","\n","Based on your use case and how many contexts you plan to feed into the model the length of these fragments will differ.\n","In this case, we choose to assign 2000 tokens to context and choose to generate the answer from 5 context fragments, which leaves us with 400 tokens per context fragment as the maximum chunk size.\n","\n","To count the number of tokens in a chunk, we need to load the correct tokenizer for Llama2. Fill the code cell below to load the correct tokenizer and use it to complete the function that counts the number of tokens per given chunk.\n","\n","**Hint:** you need to use your Hugging Face authentication token to load the tokenizer."],"id":"OfZKdZJsMeyj"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1462,"status":"ok","timestamp":1705053518662,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"WjOHBqXLLWDA","outputId":"61a4f0e5-0816-4609-98cb-402d20370a50"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}],"source":["#If you get an error here during the first import from the `transformers` package, restart the kernel and try again.\n","#### your code ####\n","from transformers import LlamaTokenizer\n","hf_auth = os.environ.get('HF_AUTH')\n","tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\",use_auth_token=hf_auth)\n","#### your code ####"],"id":"WjOHBqXLLWDA"},{"cell_type":"code","execution_count":null,"metadata":{"id":"G5GutpFdLWFu"},"outputs":[],"source":["def token_len(text):\n","  #### your code ####\n","    tokens = tokenizer.encode(text)\n","    return len(tokens)\n","    #### your code ####"],"id":"G5GutpFdLWFu"},{"cell_type":"markdown","metadata":{"id":"2pdG2JeGPfcS"},"source":["Count the number of tokens for all documents and use it to compute minimum, maximum, and average token count statistics across all documents. Depending on how the documentation is updated by the time you run the cell below the numbers might slightly differ."],"id":"2pdG2JeGPfcS"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11725,"status":"ok","timestamp":1705053533363,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"HdAdFPYyLWIM","outputId":"9882d0a8-033f-4f5a-bdc5-53dcd1967849"},"outputs":[{"output_type":"stream","name":"stdout","text":["Min: 48\n","Avg: 2663\n","Max: 36800\n"]}],"source":["#### your code ####\n","token_counts = [token_len(doc.page_content) for doc in docs]\n","min_tokens=min(token_counts)\n","avg_tokens=int(sum(token_counts) / len(token_counts))\n","max_tokens=max(token_counts)\n","#### your code ####\n","print(f\"\"\"Min: {min_tokens}\n","Avg: {avg_tokens}\n","Max: {max_tokens}\"\"\")"],"id":"HdAdFPYyLWIM"},{"cell_type":"markdown","metadata":{"id":"iz9rYYRtMo2N"},"source":["Now we will use LangChain's built-in chunking functionality to split the text into smaller chunks. LangChain offers a variety of text splitters that you can check out [here](https://api.python.langchain.com/en/latest/langchain_api_reference.html#module-langchain.text_splitter).\n","Use the general-purpose splitter that splits text by recursively looking at characters. Use this class to split the text into 400 token-sized chunks, where the length of each chunk is computed based on the `token_len` function. The length is not the only criterion for splitting, if any of these separators `'\\n\\n', '\\n', ' ', ''` is encountered, we will have a new chunk.\n","Since splitting only based on maximum length might result in incoherent chunks for every consecutive chunk, let the chunk overlap by 50 tokens. This way,  we preserve some of the previous context while chunking."],"id":"iz9rYYRtMo2N"},{"cell_type":"code","execution_count":null,"metadata":{"id":"0_RaaGMPPUd9"},"outputs":[],"source":["#### your code ####\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=400,\n","    chunk_overlap=50,\n","    length_function=token_len,\n","    separators=['\\n\\n', '\\n', ' ', '']\n",")\n","#### your code ####"],"id":"0_RaaGMPPUd9"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":486,"status":"ok","timestamp":1705053823846,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"kS6uNIzpOD-I","outputId":"094d7fd9-8651-479f-b5b8-de077e7526a7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":43}],"source":["chunks = text_splitter.split_text(docs[100].page_content)\n","len(chunks)"],"id":"kS6uNIzpOD-I"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":307,"status":"ok","timestamp":1705053825305,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"-r29RUJ7PUg6","outputId":"41bc7177-cb06-41cc-98a9-9c0c90443b09"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(344, 256)"]},"metadata":{},"execution_count":44}],"source":["token_len(chunks[0]))"],"id":"-r29RUJ7PUg6"},{"cell_type":"markdown","metadata":{"id":"SGaeBXo8ONLj"},"source":["The next step is to apply the splitting function to all the documents in our corpus and to save our chunks in a logical way. We also want to assign a unique ID to each chunk so we know which part of the documentation they come from. In the end, the corpus should be transformed into a list of dictionaries of the following format:\n","\n","\n","```\n","[\n","    {\n","        \"id\": \"glossary-0\",\n","        \"text\": \"first chunk of the document glossary\",\n","        \"source\": \"https://langchain.readthedocs.io/en/latest/glossary.html\"\n","    },\n","    {\n","        \"id\": \"glossary-1\",\n","        \"text\": \"second chunk of glossary\",\n","        \"source\": \"https://langchain.readthedocs.io/en/latest/glossary.html\"\n","    }\n","    ...\n","]\n","```\n","\n","Construct the IDs by taking the name of the page before the suffix `.html` and appending a chronological number indicating which chunk it is.\n"],"id":"SGaeBXo8ONLj"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["9060e5a8d91b4d85854cdaf83d370461","52b8052f9cf241edba5aacd0c84af72a","b8e83672b43c4f6eb33798b25d285862","5c2d619411384db2a10be6f29a15fba1","6e4aa47797e84ed493114b4477f760f6","1f57375f46884249bd808810e256cf41","657751b79ce64bdbad5e33b6a82bbd27","f1d4a8a147e543aa802256012dcf4e2e","4426ef37b434471d94894afb77f14d95","9de72bae52ab422d9adbdf891904063f","a83f975227f6499da754c41caeaeb60c"]},"executionInfo":{"elapsed":51226,"status":"ok","timestamp":1705053880587,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"pBsNJBUNPUjC","outputId":"a5a5f1e8-63f4-4a5a-bb6f-95a86cb80e2e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/423 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9060e5a8d91b4d85854cdaf83d370461"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["4065"]},"metadata":{},"execution_count":45}],"source":["from tqdm.auto import tqdm\n","\n","documents = []\n","\n","for doc in tqdm(docs):\n","  #### your code ####\n","    url = doc.metadata['source']\n","    uid =url.split(\"/\")[-1].replace(\".html\",\"\")\n","    chunks = text_splitter.split_text(doc.page_content)\n","    for i, chunk in enumerate(chunks):\n","        documents.append({\n","            'id': f'{uid}-{i}',\n","            'text': chunk,\n","            'source': url\n","        })\n","  #### your code ####\n","len(documents) # once again this value might differ based on how the LangChain documentation is updated"],"id":"pBsNJBUNPUjC"},{"cell_type":"markdown","metadata":{"id":"Hnx_s1bBQFQM"},"source":["For the next steps, we require a `DataFrame`."],"id":"Hnx_s1bBQFQM"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1705053880587,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"V64EmH9bLWNO","outputId":"946d119f-5ec5-4a7b-a694-98a106831d7e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                          id  \\\n","0  langchain_api_reference-0   \n","1  langchain_api_reference-1   \n","2  langchain_api_reference-2   \n","3  langchain_api_reference-3   \n","4  langchain_api_reference-4   \n","\n","                                                text  \\\n","0  langchain 0.1.0¶\\nlangchain.agents¶\\nAgent is ...   \n","1  Base Single Action Agent class.\\nagents.agent....   \n","2  [Deprecated]  Chat Agent.[Deprecated] Chat Age...   \n","3  [Deprecated]  Agent for the MRKL chain.[Deprec...   \n","4  Parses a message into agent action/finish.\\nag...   \n","\n","                                              source  \n","0  docs/api.python.langchain.com/en/latest/langch...  \n","1  docs/api.python.langchain.com/en/latest/langch...  \n","2  docs/api.python.langchain.com/en/latest/langch...  \n","3  docs/api.python.langchain.com/en/latest/langch...  \n","4  docs/api.python.langchain.com/en/latest/langch...  "],"text/html":["\n","  <div id=\"df-2e1c1e69-20be-44ee-8a98-fcac9e6e96fd\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>source</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>langchain_api_reference-0</td>\n","      <td>langchain 0.1.0¶\\nlangchain.agents¶\\nAgent is ...</td>\n","      <td>docs/api.python.langchain.com/en/latest/langch...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>langchain_api_reference-1</td>\n","      <td>Base Single Action Agent class.\\nagents.agent....</td>\n","      <td>docs/api.python.langchain.com/en/latest/langch...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>langchain_api_reference-2</td>\n","      <td>[Deprecated]  Chat Agent.[Deprecated] Chat Age...</td>\n","      <td>docs/api.python.langchain.com/en/latest/langch...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>langchain_api_reference-3</td>\n","      <td>[Deprecated]  Agent for the MRKL chain.[Deprec...</td>\n","      <td>docs/api.python.langchain.com/en/latest/langch...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>langchain_api_reference-4</td>\n","      <td>Parses a message into agent action/finish.\\nag...</td>\n","      <td>docs/api.python.langchain.com/en/latest/langch...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e1c1e69-20be-44ee-8a98-fcac9e6e96fd')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-2e1c1e69-20be-44ee-8a98-fcac9e6e96fd button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-2e1c1e69-20be-44ee-8a98-fcac9e6e96fd');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-916d0281-dd43-49a4-92c0-4ce84154f047\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-916d0281-dd43-49a4-92c0-4ce84154f047')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-916d0281-dd43-49a4-92c0-4ce84154f047 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":46}],"source":["import pandas as pd\n","data = pd.DataFrame(documents)\n","data.head()"],"id":"V64EmH9bLWNO"},{"cell_type":"markdown","metadata":{"id":"5b-NyMbXONhr"},"source":["✅ Point distribution ✅\n","- 0.5 point if the documents are correctly loaded with `ReadTheDocsLoader`.\n","- 0.5 point if the page content and URL is extracted from the `Document` object.\n","- 0.5 point if the tokenizer is correctly initlized.\n","- 0.25 point if the `token_len` function is correct.\n","- 0.75 point to compute the minimum, maximum and average length of all documents.\n","- 1 point for using the correct text splitter and using the correct parameters. For each mistake, deduct 0.25 point.\n","- 1 point for converting the documents into chunks and list of dictionaries.\n","\n"],"id":"5b-NyMbXONhr"},{"cell_type":"markdown","metadata":{"id":"CGbju_bkOQoL"},"source":["#### ${\\color{red}{Comments\\ 1.1}}$\n","\n","${\\color{red}{⚠️Comments\\ begin⚠️}}$\n","\n","\n","```\n","cross-feedback comment section\n","```\n","\n","\n","${\\color{red}{⚠️Comments\\ end⚠️}}$"],"id":"CGbju_bkOQoL"},{"cell_type":"markdown","metadata":{"id":"VTrg2hMrO0vF"},"source":["## Subtask 1.2: Document Embedding Pipeline\n"],"id":"VTrg2hMrO0vF"},{"cell_type":"markdown","metadata":{"id":"q_DSEsWIT9eI"},"source":["In this task, we initialize the embedding pipeline to transform the chunks into vector embeddings using Hugging Face and LangChain. These embeddings are used for similarity search between the query and the chunks to retrieve the most relevant chunks.\n","  We will use the `sentence-transformers/all-MiniLM-L6-v2` model for embedding, which is a rather small model that you can easily run on Colab. Initialize the model using `HuggingFaceEmbeddings` to use Hugging Face via Langchain. The encoding batch size should be 32, and make sure that the model is placed on the correct device, otherwise, this can take a long time."],"id":"q_DSEsWIT9eI"},{"cell_type":"code","execution_count":null,"metadata":{"id":"DYpEw-IKUTqK"},"outputs":[],"source":["from torch import cuda\n","from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n","import os\n","import pinecone\n","from tqdm import tqdm"],"id":"DYpEw-IKUTqK"},{"cell_type":"code","execution_count":null,"metadata":{"id":"HI3CJ5bbUTsT"},"outputs":[],"source":["embedding_model = 'sentence-transformers/all-MiniLM-L6-v2'\n","device = 'cuda:0' # make sure you are on gpu\n","docs = [\n","    \"An example document\",\n","    \"A second document as an example\"\n","]\n","### your code ###\n","embed_model = HuggingFaceEmbeddings(\n","    model_name=embedding_model,\n","    model_kwargs={'device': device},\n","    encode_kwargs={'device': device, 'batch_size': 32}\n",")\n","### your code ###"],"id":"HI3CJ5bbUTsT"},{"cell_type":"markdown","metadata":{"id":"_U73Hj93V0_-"},"source":["Embed the example documents using the model you created and check the output.\n","The output should be a list of lists, containing the embeddings."],"id":"_U73Hj93V0_-"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":779,"status":"ok","timestamp":1705053538944,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"9Qt3SDKoUvKx","outputId":"ce7dcfc9-a144-4f2b-a772-bfece6487c88"},"outputs":[{"output_type":"stream","name":"stdout","text":["number of docs: 2\n","dimension of docs: 384\n"]}],"source":["### your code ###\n","embeddings = embed_model.embed_documents(docs)\n","### your code ###\n","print(\"number of docs:\",len(embeddings))\n","print(\"dimension of docs:\",len(embeddings[0]))"],"id":"9Qt3SDKoUvKx"},{"cell_type":"markdown","metadata":{"id":"2D8sr-g9W8cw"},"source":["Now we use the embedding pipeline created above to store the embeddings in a Pinecone vector index. First, lets setup the Pinecone environment, collect your API key and environment name from the environment variables, and initiate Pinecone with them."],"id":"2D8sr-g9W8cw"},{"cell_type":"code","execution_count":null,"metadata":{"id":"tycv_q6RW8tO"},"outputs":[],"source":["### your code ###\n","pinecone.init(\n","    api_key=os.environ.get('PINECONE_API_KEY') ,\n","    environment=os.environ.get('PINECONE_ENVIRONMENT')\n",")\n","### your code ###"],"id":"tycv_q6RW8tO"},{"cell_type":"markdown","metadata":{"id":"_9tvVVQ7X2pV"},"source":["Initialize the index `rag-assignment` inside Pinecone. Use the cosine similarity as similarity metric. Keep in mind that if you run this multiple times on a free tier, where only one index is allowed, you need to remove the index created to make room for a new one (Pinecone index gets archived automatically after 14 days of inactivity)."],"id":"_9tvVVQ7X2pV"},{"cell_type":"code","execution_count":null,"metadata":{"id":"g_ZA_2J5XyTD"},"outputs":[],"source":["index_name = 'rag-assignment'\n","### your code ###\n","pinecone.create_index(\n","    index_name,\n","    dimension=len(embeddings[0]),\n","    metric='cosine'\n",")\n","### your code ###"],"id":"g_ZA_2J5XyTD"},{"cell_type":"markdown","metadata":{"id":"rxfs9gshYfKx"},"source":["Lets take a look at the index you created. As of now the index should be empty but have the correct embedding dimension."],"id":"rxfs9gshYfKx"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":493,"status":"ok","timestamp":1705053905519,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"gNnN9mEyYWHB","outputId":"62db2f11-e2a2-42e5-920d-11381d117e86"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'dimension': 384,\n"," 'index_fullness': 0.0,\n"," 'namespaces': {},\n"," 'total_vector_count': 0}"]},"metadata":{},"execution_count":49}],"source":["index_name = 'rag-assignment'\n","index = pinecone.Index(index_name)\n","index.describe_index_stats()"],"id":"gNnN9mEyYWHB"},{"cell_type":"markdown","metadata":{"id":"woQKBXfobZMg"},"source":["Process the dataset in batches of `32` and push the vectors to the Pinecone index. Your index should include the IDs and embeddings for each chunk. As metadata, pass the original text as `text` and the URL as `source` (no need to add the `https`). We use this metadata later to retrieve the original text."],"id":"woQKBXfobZMg"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["05a24f7921854e97878dd5f543e541cd","3f77b31375aa4d0f82c51688b5f44c21","85ac3c06d3f943a79499f92033b9ce8c","b05a4d871dac4f5481d7a773bc6befe5","6d6b49f2715943dd9ed50a5f158c118c","354b40407e88486989083d196230d13e","cdeb46dc3021402e80763a568d37c3ae","212f79ee379d4809966dc9458be2322c","f1ff6c6cd8c04329bfff406af53e2e29","6ccfeceacaab4f2e9f889073f00aeddb","2f49a196a4794b87b9d72b562a4bee49"]},"executionInfo":{"elapsed":28407,"status":"ok","timestamp":1705053935100,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"K8vjCSVKa8Y6","outputId":"374e9b4a-44d9-492a-ec67-b10dab29d753"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/128 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05a24f7921854e97878dd5f543e541cd"}},"metadata":{}}],"source":["batch_size = 32\n","\n","for i in tqdm(range(0, len(data), batch_size)):\n","  ### your code ###\n","    i_end = min(len(data), i+batch_size)\n","    batch = data.iloc[i:i_end]\n","    ids = [f\"{x['id']}\" for i, x in batch.iterrows()]\n","    texts = [x['text'] for i, x in batch.iterrows()]\n","    embeds = embed_model.embed_documents(texts)\n","    # get metadata to store in Pinecone\n","    metadata = [\n","        {'text': x['text'],\n","         'source': x['source']} for i, x in batch.iterrows()\n","    ]\n","    index.upsert(vectors=zip(ids, embeds, metadata))\n","    ### your code ###\n"],"id":"K8vjCSVKa8Y6"},{"cell_type":"markdown","metadata":{"id":"kz2UZVj8ff_L"},"source":["Now if we look at the index statistics we should have vectors of dimension `384`."],"id":"kz2UZVj8ff_L"},{"cell_type":"code","execution_count":null,"metadata":{"id":"cH9Zq6azfgJm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705053935100,"user_tz":-60,"elapsed":4,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"}},"outputId":"9983413e-ccb6-4425-a7a2-2c03b141e8f3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'dimension': 384,\n"," 'index_fullness': 0.03948,\n"," 'namespaces': {'': {'vector_count': 3948}},\n"," 'total_vector_count': 3948}"]},"metadata":{},"execution_count":51}],"source":["index.describe_index_stats()"],"id":"cH9Zq6azfgJm"},{"cell_type":"markdown","metadata":{"id":"8RUzTWtbqmGo"},"source":["✅ Point distribution ✅\n","- 0.5 point if `HuggingFaceEmbeddings` is correclty initialized.\n","- 0.5 point if examples are correctly processed with `HuggingFaceEmbeddings`.\n","- 0.5 point if Pinecone is initialized with the enviroment variables.\n","- 0.5 if the Pinecone index is correctly created with the metric specified.\n","- 1 point if the passages are converted to vectors and are pushed into Pinecone.\n"],"id":"8RUzTWtbqmGo"},{"cell_type":"markdown","metadata":{"id":"NK-2Scv8qqYV"},"source":["#### ${\\color{red}{Comments\\ 1.2}}$\n","\n","${\\color{red}{⚠️Comments\\ begin⚠️}}$\n","\n","\n","```\n","cross-feedback comment section\n","```\n","\n","\n","${\\color{red}{⚠️Comments\\ end⚠️}}$"],"id":"NK-2Scv8qqYV"},{"cell_type":"markdown","metadata":{"id":"5bVrpkiJhHAF"},"source":["## Subtask 1.3: Text Generation Pipeline\n"],"id":"5bVrpkiJhHAF"},{"cell_type":"markdown","metadata":{"id":"ZPykfVaHkIkO"},"source":["So far we have our index ready and a way to find the most similar chunks to our query. Now, we need a way to generate the answer from the retrieved chunks. For this purpose, we use the `text-generation` pipeline from Hugging Face (refer to the Hugging Face [tutorial](https://moodle.uni-heidelberg.de/pluginfile.php/1286642/mod_resource/content/1/HuggingFace.ipynb)) and load it into LangChain using a wrapper."],"id":"ZPykfVaHkIkO"},{"cell_type":"code","execution_count":null,"metadata":{"id":"CL_PhWM3lEnj"},"outputs":[],"source":["from torch import cuda, bfloat16\n","import os\n","import transformers\n","model_id = 'meta-llama/Llama-2-13b-chat-hf'"],"id":"CL_PhWM3lEnj"},{"cell_type":"markdown","metadata":{"id":"A3l34T5T3eEN"},"source":["Quantization techniques reduce memory and computational costs by representing weights and activations with lower-precision data types like 8-bit integers (int8). This enables loading larger models you normally wouldn’t be able to fit into memory, and thus speeds up inference.\n","To make the process of model quantization more accessible, Hugging Face has seamlessly integrated with the [Bitsandbytes](https://huggingface.co/docs/accelerate/usage_guides/quantization) library.\n","\n","Define a config from `Bitsandbytes` that enables 4-bit quantization and set the nested quantization to `true`. This changes the datatype from float 32 (default) to normalized float 4 datatype to contain 4 bits of information.\n","Additionally, add a compute type to store weights in 4-bits, but the computation to happen in 16-bit (bfloat16).\n","Moreover, set the `bnb_4bit_use_double_quant` to true, which uses a second quantization after the first one to save an additional 0.4 bits per parameter.\n","Refer to [here](https://huggingface.co/docs/transformers/main_classes/quantization) for more information."],"id":"A3l34T5T3eEN"},{"cell_type":"code","execution_count":null,"metadata":{"id":"mXNTODoOlFjX"},"outputs":[],"source":["  ### your code ###\n","bitsAndBites_config = transformers.BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type='nf4',\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_compute_dtype=bfloat16\n",")\n","  ### your code ###"],"id":"mXNTODoOlFjX"},{"cell_type":"markdown","metadata":{"id":"Wea-kVMF4Kvf"},"source":["Use your Hugging Face token to load the correct model configuration using the `transformers` library."],"id":"Wea-kVMF4Kvf"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":306,"status":"ok","timestamp":1705053949751,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"7U47CyIk4EUz","outputId":"f76a44bd-231f-4f17-a4c2-7a7a9093ceba"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py:1033: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]}],"source":["### your code ###\n","\n","model_config = transformers.AutoConfig.from_pretrained(\n","    model_id,\n","    use_auth_token=os.environ.get('HF_AUTH')\n",")\n","### your code ###\n"],"id":"7U47CyIk4EUz"},{"cell_type":"markdown","metadata":{"id":"V8CNl7G1SsUC"},"source":["Load the model for text generation (pay attention to the model type) using the configuration file you have defined, with the specified quantization, and set the `trust_remote_code` flag to `true`. Another flag that is useful for large mode is  `device_map=\"auto\"`. By setting this flag, Accelerate will determine where to put each layer to maximize the use of GPUs and offload the rest on the CPU, or even the hard drive if you don’t have enough GPU RAM (or CPU RAM).\n","\n","It will take a while for the model to download."],"id":"V8CNl7G1SsUC"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123,"referenced_widgets":["86d4a03a170a4a8599521d15ff60cb3e","4a859df296274912910857911e51f588","7dd281f9077f4e06b0f4a2df645f97ed","8258d606565c49b4b68c242586171217","87aa98d99385407d8ad0f687c71aa6be","19658639e58f415ea88ddafdc614a0f7","ab5aa81d300b46e9b1351cf9e7c50a52","a89ded0152474c92a2a77fa438aaf271","16060e341d1349d2ad344e0950444068","aaf9fcaec7ab4f30b522e9f7f71f70fb","37a8738fd90c42b48f6f991ab486c91e"]},"executionInfo":{"elapsed":133589,"status":"ok","timestamp":1705054099197,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"TUzuGsv61cBA","outputId":"4bf8dc92-af9c-4153-f227-5f6875484ecc"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86d4a03a170a4a8599521d15ff60cb3e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Model loaded \n"]}],"source":["#Loading the model will take some time, (roughly 5 min)\n","### your code ###\n","model = transformers.AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    trust_remote_code=True,\n","    config=model_config,\n","    quantization_config=bitsAndBites_config,\n","    device_map='auto',\n","    token=os.environ.get('HF_AUTH')\n",")\n","### your code ###\n","model.eval()# we only use the model for inference\n","print(f\"Model loaded \")"],"id":"TUzuGsv61cBA"},{"cell_type":"markdown","metadata":{"id":"-brElhysTZVZ"},"source":["You can even check the memory footprint of your model using the `get_memory_footprint` method.\n"],"id":"-brElhysTZVZ"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1705054099456,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"zWDTgyKhlLLQ","outputId":"d59c0103-fae5-4e3c-afaf-eeb515676b9d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["7083970560"]},"metadata":{},"execution_count":57}],"source":["model.get_memory_footprint()"],"id":"zWDTgyKhlLLQ"},{"cell_type":"markdown","metadata":{"id":"ViFoo0vlSFMp"},"source":["The next thing we need to do is initialize a `text-generation` pipeline with Hugging Face that uses the Llama2 model to generate some text, given some input. We will then use this pipeline inside LangChain to build our question-answering system.\n","`text-generation` pipeline generates text from a language model conditioned on a given input. The pipeline is similar to other Hugging Face pipelines and requires two things that we must initialize:\n","\n","1.   A language model, in this case, it will be `meta-llama/Llama-2-13b-chat-hf`.\n","2.   A tokenizer for the language model.\n","\n","LangChain expects the full-text outputs, therefore set the `return_full_text` to true. You can also pass additional generation parameters to the model.\n","Since we want the questions to be answered mainly based on the retrieved chunks, let's set the model temperature to a low value of 0.01 to reduce randomness. Additionally, add a repetition penalty of 1.1 to stop the model from repeating itself and the maximum number of generation tokens to 512."],"id":"ViFoo0vlSFMp"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1aFv-9-lPCJO"},"outputs":[],"source":["### your code ###\n","generate_text = transformers.pipeline(\n","    model=model, tokenizer=tokenizer,\n","    return_full_text=True,\n","    task='text-generation',\n","    # we pass model parameters here too\n","    temperature=0.01,\n","    max_new_tokens=512,  # max number of tokens to generate in the output\n","    repetition_penalty=1.1  # without this output begins repeating\n",")\n","### your code ###"],"id":"1aFv-9-lPCJO"},{"cell_type":"markdown","metadata":{"id":"ZvdSEw9PZ-n9"},"source":["We provide the language model a general question to make sure our pipeline is working correctly."],"id":"ZvdSEw9PZ-n9"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61141,"status":"ok","timestamp":1705054166987,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"FpuCkc77RF53","outputId":"7b7093a0-e5c1-4a42-fd87-7e26302fad32"},"outputs":[{"output_type":"stream","name":"stdout","text":["Explain to me the difference between alligator and crocodile.\n","Alligators and crocodiles are both large, carnivorous reptiles that live in wetlands and rivers, but there are several key differences between them. Here are some of the main differences:\n","\n","1. Appearance: Alligators have a wider, rounder snout compared to crocodiles, which have a longer, thinner snout. Alligators also have a more rounded body shape and shorter legs than crocodiles.\n","2. Habitat: Alligators are found only in freshwater environments such as lakes, rivers, and swamps, while crocodiles can be found in both freshwater and saltwater environments.\n","3. Geographic range: Alligators are only found in the southeastern United States and China, while crocodiles are found in many parts of the world, including Africa, Asia, Australia, and the Americas.\n","4. Behavior: Alligators are generally less aggressive than crocodiles and tend to avoid confrontations with humans. Crocodiles, on the other hand, are known for their aggressive behavior and have been responsible for many human attacks and deaths.\n","5. Nesting habits: Alligators build mounds of vegetation and mud to lay their eggs, while crocodiles dig holes in the sand or mud to lay their eggs.\n","6. Jaw structure: Alligators have a stronger bite force than crocodiles, but crocodiles have a more powerful jaw muscle that allows them to exert more force when biting.\n","7. Diet: Both alligators and crocodiles are carnivores, but alligators tend to eat more fish and smaller animals, while crocodiles prefer larger prey like buffalo and even humans.\n","8. Lifespan: Alligators typically live for 30-50 years in the wild, while crocodiles can live for up to 70 years in captivity.\n","\n","These are just a few of the differences between alligators and crocodiles. While they may look similar at first glance, they are distinct species with unique characteristics and habits.\n"]}],"source":["sample_input=\"Explain to me the difference between alligator and crocodile.\"\n","### your code ###\n","res = generate_text(sample_input)\n","generated_text=res[0][\"generated_text\"]\n","### your code ###\n","print(generated_text)"],"id":"FpuCkc77RF53"},{"cell_type":"markdown","metadata":{"id":"2QtViqTBSsPS"},"source":["Use the LangChain Hugging Face wrapper, as subset of [LLM chain](https://python.langchain.com/docs/modules/chains/foundational/llm_chain) to create an interface for the text generation pipeline."],"id":"2QtViqTBSsPS"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6-spabNmRMM2"},"outputs":[],"source":["### your code ###\n","from langchain.llms import HuggingFacePipeline\n","llm = HuggingFacePipeline(pipeline=generate_text)\n","### your code ###"],"id":"6-spabNmRMM2"},{"cell_type":"markdown","metadata":{"id":"9EFZBGNgTM20"},"source":["To confirm that it works the same way, use the sample input to generate text using the llm chain. The input should be passed as the `prompt` to the language model."],"id":"9EFZBGNgTM20"},{"cell_type":"code","execution_count":null,"metadata":{"id":"rDToa1YOTFEX","colab":{"base_uri":"https://localhost:8080/","height":198},"executionInfo":{"status":"ok","timestamp":1705054230799,"user_tz":-60,"elapsed":63829,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"}},"outputId":"a6e80018-b07d-4ad3-de8f-5cd59e5d51f7"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n","  warn_deprecated(\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\nAlligators and crocodiles are both large, carnivorous reptiles that live in wetlands and rivers, but there are several key differences between them. Here are some of the main differences:\\n\\n1. Appearance: Alligators have a wider, rounder snout compared to crocodiles, which have a longer, thinner snout. Alligators also have a more rounded body shape and shorter legs than crocodiles.\\n2. Habitat: Alligators are found only in freshwater environments such as lakes, rivers, and swamps, while crocodiles can be found in both freshwater and saltwater environments.\\n3. Geographic range: Alligators are only found in the southeastern United States and China, while crocodiles are found in many parts of the world, including Africa, Asia, Australia, and the Americas.\\n4. Behavior: Alligators are generally less aggressive than crocodiles and tend to avoid confrontations with humans. Crocodiles, on the other hand, are known for their aggressive behavior and have been responsible for many human attacks and deaths.\\n5. Nesting habits: Alligators build mounds of vegetation and mud to lay their eggs, while crocodiles dig holes in the sand or mud to lay their eggs.\\n6. Jaw structure: Alligators have a stronger bite force than crocodiles, but crocodiles have a more powerful jaw muscle that allows them to exert more force when biting.\\n7. Diet: Both alligators and crocodiles are carnivores, but alligators tend to eat more fish and smaller animals, while crocodiles prefer larger prey like buffalo and even humans.\\n8. Lifespan: Alligators typically live for 30-50 years in the wild, while crocodiles can live for up to 70 years in captivity.\\n\\nThese are just a few of the differences between alligators and crocodiles. While they may look similar at first glance, they are distinct species with unique characteristics and habits.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":61}],"source":["### your code ###\n","llm(prompt=sample_input)\n","### your code ###"],"id":"rDToa1YOTFEX"},{"cell_type":"markdown","metadata":{"id":"nysI3c2NVfBR"},"source":["✅ Point distribution ✅\n","- 1 point if the parameters for `bitsAndBites_config` are correct.\n","- 0.5 point if the configuration file is correctly initlized.\n","- 1 point if the generation model is correct.\n","- 0.5 point if question is answered using the Hugging Face pipeline.\n","- 0.5 point if the LLM chain containig the Hugging Face pipeline is correct.\n","- 0.5 point if question is answered using LLM chain.\n","\n"],"id":"nysI3c2NVfBR"},{"cell_type":"markdown","metadata":{"id":"mn7-zQxeVhWF"},"source":["#### ${\\color{red}{Comments\\ 1.3}}$\n","\n","${\\color{red}{⚠️Comments\\ begin⚠️}}$\n","\n","\n","```\n","cross-feedback comment section\n","```\n","\n","\n","${\\color{red}{⚠️Comments\\ end⚠️}}$"],"id":"mn7-zQxeVhWF"},{"cell_type":"markdown","metadata":{"id":"VRwTtnQEWq5h"},"source":["## Subtask 1.4: Question Answering Chain\n"],"id":"VRwTtnQEWq5h"},{"cell_type":"markdown","metadata":{"id":"wr9QzQvYXDBS"},"source":["For Retrieval Augmented Generation (RAG) in LangChain, we need to initialize either a `RetrievalQA` or `RetrievalQAWithSourcesChain` object.\n","\n","`RetrievalQA` is a method for question-answering tasks, utilizing an index to retrieve relevant documents or text chunks, it is suitable for straightforward Q&A applications.\n","\n","`RetrievalQAWithSourcesChain` is an extension of RetrievalQA that chains together multiple sources of information, providing context and the source for answers.\n","\n"," For both of these, we need an LLM and a Pinecone index. For LangChain to be able to use the Pinecone index, we need to initialize it through the LangChain vector store.\n","\n"," **Hint**: You need to explicitly tell the vector storage where to find the original text."],"id":"wr9QzQvYXDBS"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":189,"status":"ok","timestamp":1705054811627,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"RQIR_Gb6Wrfs","outputId":"1ed55c1f-eb0e-4299-91ea-b12c4ea13634"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/pinecone.py:73: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n","  warnings.warn(\n"]}],"source":["from langchain.vectorstores import Pinecone\n","### your code ###\n","vectorstore = Pinecone(index, embed_model.embed_query, 'text')\n","### your code ###"],"id":"RQIR_Gb6Wrfs"},{"cell_type":"markdown","metadata":{"id":"mS9d51dyptJd"},"source":["Let's try a query that is specific to the LangChain documentation and see which chunks are relevant. Use the vector storage defined above to find the top-3 chunks related to the given query."],"id":"mS9d51dyptJd"},{"cell_type":"code","execution_count":null,"metadata":{"id":"sQeWNPDqXdJe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705054813676,"user_tz":-60,"elapsed":753,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"}},"outputId":"6bfdcb24-70cc-4b6d-a8c9-baa388acb47a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(page_content='langchain 0.0.353¶\\nlangchain.agents¶\\nAgent is a class that uses an LLM to choose a sequence of actions to take.\\nIn Chains, a sequence of actions is hardcoded. In Agents,\\na language model is used as a reasoning engine to determine which actions\\nto take and in which order.\\nAgents select and use Tools and Toolkits for actions.\\nClass hierarchy:\\nBaseSingleActionAgent --> LLMSingleActionAgent\\n                          OpenAIFunctionsAgent\\n                          XMLAgent\\n                          Agent --> <name>Agent  # Examples: ZeroShotAgent, ChatAgent\\nBaseMultiActionAgent  --> OpenAIMultiFunctionsAgent\\nMain helpers:\\nAgentType, AgentExecutor, AgentOutputParser, AgentExecutorIterator,\\nAgentAction, AgentFinish\\nClasses¶\\nagents.agent.Agent\\nAgent that calls the language model and deciding the action.\\nagents.agent.AgentExecutor\\nAgent that is using tools.\\nagents.agent.AgentOutputParser\\nBase class for parsing agent output into agent action/finish.\\nagents.agent.BaseMultiActionAgent\\nBase Multi Action Agent class.\\nagents.agent.BaseSingleActionAgent\\nBase Single Action Agent class.\\nagents.agent.ExceptionTool\\nTool that just returns the query.\\nagents.agent.LLMSingleActionAgent', metadata={'source': 'docs/api.python.langchain.com/en/stable/langchain_api_reference.html'}),\n"," Document(page_content='langchain.agents.loading.load_agent¶\\nlangchain.agents.loading.load_agent(path: Union[str, Path], **kwargs: Any) → Union[BaseSingleActionAgent, BaseMultiActionAgent][source]¶\\nUnified method for loading an agent from LangChainHub or local fs.\\nParameters\\npath – Path to the agent file.\\n**kwargs – Additional keyword arguments passed to the agent executor.\\nReturns\\nAn agent executor.', metadata={'source': 'docs/api.python.langchain.com/en/stable/agents/langchain.agents.loading.load_agent.html'}),\n"," Document(page_content='langchain.agents.loading.load_agent_from_config¶\\nlangchain.agents.loading.load_agent_from_config(config: dict, llm: Optional[BaseLanguageModel] = None, tools: Optional[List[Tool]] = None, **kwargs: Any) → Union[BaseSingleActionAgent, BaseMultiActionAgent][source]¶\\nLoad agent from Config Dict.\\nParameters\\nconfig – Config dict to load agent from.\\nllm – Language model to use as the agent.\\ntools – List of tools this agent has access to.\\n**kwargs – Additional keyword arguments passed to the agent executor.\\nReturns\\nAn agent executor.', metadata={'source': 'docs/api.python.langchain.com/en/stable/agents/langchain.agents.loading.load_agent_from_config.html'})]"]},"metadata":{},"execution_count":63}],"source":["query = 'what is a LangChain Agent?'\n","### your code ###\n","vectorstore.similarity_search(\n","    query,  # the search query\n","    k=3  # returns top 3 most relevant chunks of text\n",")\n","### your code ###"],"id":"sQeWNPDqXdJe"},{"cell_type":"markdown","metadata":{"id":"gEMooqALXqOt"},"source":["Now use the `vectorstore` and `llm` to initialize the `RetrievalQA` object, which showcases question answering over an index. `RetrievalQA` is a document chain, these are useful for summarizing documents, answering questions about documents, extracting information from documents, and more. All such chains operate with 4 different chain types:\n","\n","\n","1.   `stuff`: it takes a list of documents, inserts them all into a prompt, and passes that prompt to an LLM.\n","2.   `refine`: it constructs a response by looping over the input documents and iteratively updating its answer. It is well-suited for tasks that require analyzing more documents than can fit in the model’s context.\n","3. `map_reduce`:  it first applies an LLM chain to each document individually (the Map step), treating the chain output as a new document. It then passes all the new documents to a separate combined documents chain to get a single output (the Reduce step).\n","4. `map_re_rank`: it runs an initial prompt on each document that not only tries to complete a task but also gives a score for how certain it is in its answer. The highest-scoring response is returned.\n","\n","For this assignment, we focus only on the first type. Make sure to set the `verbose` to `true`, so we can see the different stages of processing that happens while answering a question (you might need to set this parameter more than once). As mentioned before, we want our retrieve to input top-5 most similiar chunks to the query to generate an answer."],"id":"gEMooqALXqOt"},{"cell_type":"code","execution_count":null,"metadata":{"id":"lYnUrFnaXqU0"},"outputs":[],"source":["from langchain.chains import RetrievalQA\n","### your code ###\n","\n","rag_pipeline = RetrievalQA.from_chain_type(\n","    llm=llm,\n","    chain_type=\"stuff\",\n","    verbose=True,\n","    retriever=vectorstore.as_retriever(search_kwargs={\"k\":5}),\n","    chain_type_kwargs={\n","        \"verbose\": True },\n","\n",")\n","\n","### your code ###\n","query='what is a LangChain Agent?'"],"id":"lYnUrFnaXqU0"},{"cell_type":"markdown","metadata":{"id":"O4Mo0JNMqSC7"},"source":["First, we try to answer the question only using Llama2. As you see the answer is not convincing as it does not have access to the LangChain documentation."],"id":"O4Mo0JNMqSC7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"jGk2ZsUVYiJ0","colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"status":"ok","timestamp":1705054870420,"user_tz":-60,"elapsed":51024,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"}},"outputId":"69326ff7-51bb-4640-f467-e0a237647919"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n\\nA LangChain Agent is an intelligent agent that uses natural language processing (NLP) and machine learning (ML) techniques to assist users in finding relevant information on the web. It is designed to help users navigate the vast amount of information available online by providing personalized recommendations and answers to their questions.\\n\\nThe name \"LangChain\" refers to the idea of linking together different languages and knowledge sources to create a comprehensive and coherent view of the world. The agent is able to understand and respond to user queries in multiple languages, and it can draw upon a wide range of sources, including text, images, videos, and other forms of media, to provide accurate and relevant results.\\n\\nSome of the key features of a LangChain Agent include:\\n\\n1. Natural Language Processing (NLP): The agent is able to understand and interpret natural language queries, allowing users to ask questions in everyday language.\\n2. Machine Learning (ML): The agent uses ML algorithms to learn from user interactions and improve its performance over time.\\n3. Multi-lingual support: The agent can understand and respond to user queries in multiple languages, making it accessible to a global audience.\\n4. Knowledge graph integration: The agent can draw upon a knowledge graph to provide more detailed and accurate information about specific topics.\\n5. Personalization: The agent can provide personalized recommendations based on user preferences and interests.\\n\\nOverall, a LangChain Agent is designed to be a powerful tool for helping users find relevant information on the web, and it has the potential to revolutionize the way we interact with technology.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":65}],"source":["llm(query)"],"id":"jGk2ZsUVYiJ0"},{"cell_type":"markdown","metadata":{"id":"W8U5Vce0qdch"},"source":["Now use the Pipeline from above and see how the answer changes."],"id":"W8U5Vce0qdch"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29374,"status":"ok","timestamp":1705054899791,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"1KaQyXKQYn57","outputId":"37c584a2-aa9c-4791-cb48-d07723463a07"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n","  warn_deprecated(\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n","\n","langchain 0.0.353¶\n","langchain.agents¶\n","Agent is a class that uses an LLM to choose a sequence of actions to take.\n","In Chains, a sequence of actions is hardcoded. In Agents,\n","a language model is used as a reasoning engine to determine which actions\n","to take and in which order.\n","Agents select and use Tools and Toolkits for actions.\n","Class hierarchy:\n","BaseSingleActionAgent --> LLMSingleActionAgent\n","                          OpenAIFunctionsAgent\n","                          XMLAgent\n","                          Agent --> <name>Agent  # Examples: ZeroShotAgent, ChatAgent\n","BaseMultiActionAgent  --> OpenAIMultiFunctionsAgent\n","Main helpers:\n","AgentType, AgentExecutor, AgentOutputParser, AgentExecutorIterator,\n","AgentAction, AgentFinish\n","Classes¶\n","agents.agent.Agent\n","Agent that calls the language model and deciding the action.\n","agents.agent.AgentExecutor\n","Agent that is using tools.\n","agents.agent.AgentOutputParser\n","Base class for parsing agent output into agent action/finish.\n","agents.agent.BaseMultiActionAgent\n","Base Multi Action Agent class.\n","agents.agent.BaseSingleActionAgent\n","Base Single Action Agent class.\n","agents.agent.ExceptionTool\n","Tool that just returns the query.\n","agents.agent.LLMSingleActionAgent\n","\n","langchain.agents.loading.load_agent¶\n","langchain.agents.loading.load_agent(path: Union[str, Path], **kwargs: Any) → Union[BaseSingleActionAgent, BaseMultiActionAgent][source]¶\n","Unified method for loading an agent from LangChainHub or local fs.\n","Parameters\n","path – Path to the agent file.\n","**kwargs – Additional keyword arguments passed to the agent executor.\n","Returns\n","An agent executor.\n","\n","langchain.agents.loading.load_agent_from_config¶\n","langchain.agents.loading.load_agent_from_config(config: dict, llm: Optional[BaseLanguageModel] = None, tools: Optional[List[Tool]] = None, **kwargs: Any) → Union[BaseSingleActionAgent, BaseMultiActionAgent][source]¶\n","Load agent from Config Dict.\n","Parameters\n","config – Config dict to load agent from.\n","llm – Language model to use as the agent.\n","tools – List of tools this agent has access to.\n","**kwargs – Additional keyword arguments passed to the agent executor.\n","Returns\n","An agent executor.\n","\n","langchain.agents.agent.Agent¶\n","class langchain.agents.agent.Agent[source]¶\n","Bases: BaseSingleActionAgent\n","Agent that calls the language model and deciding the action.\n","This is driven by an LLMChain. The prompt in the LLMChain MUST include\n","a variable called “agent_scratchpad” where the agent can put its\n","intermediary work.\n","Create a new model by parsing and validating input data from keyword arguments.\n","Raises ValidationError if the input data cannot be parsed to form a valid model.\n","param allowed_tools: Optional[List[str]] = None¶\n","param llm_chain: langchain.chains.llm.LLMChain [Required]¶\n","param output_parser: langchain.agents.agent.AgentOutputParser [Required]¶\n","async aplan(intermediate_steps: List[Tuple[AgentAction, str]], callbacks: Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]] = None, **kwargs: Any) → Union[AgentAction, AgentFinish][source]¶\n","Given input, decided what to do.\n","Parameters\n","intermediate_steps – Steps the LLM has taken to date,\n","along with observations\n","callbacks – Callbacks to run.\n","**kwargs – User inputs.\n","Returns\n","Action specifying what tool to use.\n","classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model¶\n","\n","langchain.agents.xml.base.create_xml_agent¶\n","langchain.agents.xml.base.create_xml_agent(llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: BasePromptTemplate) → Runnable[source]¶\n","Create an agent that uses XML to format its logic.\n","Examples:\n","from langchain import hub\n","from langchain.chat_models import ChatAnthropic\n","from langchain.agents import AgentExecutor, create_xml_agent\n","prompt = hub.pull(\"hwchase17/xml-agent-convo\")\n","model = ChatAnthropic()\n","tools = ...\n","agent = create_xml_agent(model, tools, prompt)\n","agent_executor = AgentExecutor(agent=agent, tools=tools)\n","agent_executor.invoke({\"input\": \"hi\"})\n","# Use with chat history\n","from langchain_core.messages import AIMessage, HumanMessage\n","agent_executor.invoke(\n","    {\n","        \"input\": \"what's my name?\",\n","        # Notice that chat_history is a string\n","        # since this prompt is aimed at LLMs, not chat models\n","        \"chat_history\": \"Human: My name is Bob\n","AI: Hello Bob!”,\n","}\n",")\n","Args:llm: LLM to use as the agent.\n","tools: Tools this agent has access to.\n","\n","Question: what is a LangChain Agent?\n","Helpful Answer:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["{'query': 'what is a LangChain Agent?',\n"," 'result': ' A LangChain Agent is a piece of software that uses a chain of language models (LLMs) to perform tasks. It takes in user input, passes it through a series of LLMs, and then uses the output of the final LLM to determine what action to take. The agent can also use tools and toolkits to help it perform its tasks.\\n\\nPlease let me know if you need any further information or clarification.'}"]},"metadata":{},"execution_count":66}],"source":["### your code ###\n","rag_pipeline(query)\n","### your code ###\n"],"id":"1KaQyXKQYn57"},{"cell_type":"markdown","metadata":{"id":"OY8fbwGhaoc_"},"source":["✅ Point distribution ✅\n","- 0.5 point to correctly initialize a vector store.\n","- 1 point for performing the top-3 similiarity search.\n","- 1 point for correct initialization of the `rag_pipeline` with all the parameters. If the verbose is not set correctly remove only 0.25.\n","- 0.5 for correct use of the pipeline to answer the question.\n"],"id":"OY8fbwGhaoc_"},{"cell_type":"markdown","metadata":{"id":"pJ2JxOL4aqM2"},"source":["#### ${\\color{red}{Comments\\ 1.4}}$\n","\n","${\\color{red}{⚠️Comments\\ begin⚠️}}$\n","\n","\n","```\n","cross-feedback comment section\n","```\n","\n","\n","${\\color{red}{⚠️Comments\\ end⚠️}}$"],"id":"pJ2JxOL4aqM2"},{"cell_type":"markdown","metadata":{"id":"inIFkOziTce3"},"source":["## Subtask 1.5: Conversational Retrieval Chain\n","\n","\n"],"id":"inIFkOziTce3"},{"cell_type":"markdown","metadata":{"id":"btxcJlEITiDt"},"source":["We can also extend our retrieval chain to be able to remember the previous questions and answer the current question by looking at the previous context.\n","The important part of a conversational model is conversation memory, which transforms the stateless language model to be able to remember previous interactions, e.g., similiar to ChatGPT. In this subtask, we will use LangChain to create a conversational memory.\n"],"id":"btxcJlEITiDt"},{"cell_type":"markdown","metadata":{"id":"DYHPVo-9TiGU"},"source":["To implement the memory we use `ConversationalRetrievalChain`.\n","This chain takes in chat history (a list of messages) and new questions and then returns an answer to that question. The algorithm for this chain consists of three parts:\n","\n","1. Use the chat history and the new question to create a new question that contains the information from the previous context.\n","\n","2. This new question is passed to the retriever and relevant documents are returned.\n","\n","3. The retrieved documents are passed to an LLM to generate a final response."],"id":"DYHPVo-9TiGU"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37580,"status":"ok","timestamp":1704290819454,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"GLv_d-RYyNYL","outputId":"4ad0ab9d-5458-4f17-c763-d76cc381c1cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n","\n","Lazily import lmformatenforcer.\n","llms.rellm_decoder.import_rellm()\n","Lazily import rellm.\n","langchain_experimental.open_clip¶\n","Classes¶\n","open_clip.open_clip.OpenCLIPEmbeddings\n","Create a new model by parsing and validating input data from keyword arguments.\n","langchain_experimental.pal_chain¶\n","Implements Program-Aided Language Models.\n","As in https://arxiv.org/pdf/2211.10435.pdf.\n","This is vulnerable to arbitrary code execution:\n","https://github.com/langchain-ai/langchain/issues/5872\n","Classes¶\n","pal_chain.base.PALChain\n","Implements Program-Aided Language Models (PAL).\n","pal_chain.base.PALValidation([...])\n","Initialize a PALValidation instance.\n","langchain_experimental.plan_and_execute¶\n","Classes¶\n","plan_and_execute.agent_executor.PlanAndExecute\n","Plan and execute a chain of steps.\n","plan_and_execute.executors.base.BaseExecutor\n","Base executor.\n","plan_and_execute.executors.base.ChainExecutor\n","Chain executor.\n","plan_and_execute.planners.base.BasePlanner\n","Base planner.\n","\n","Convert a LangChain message to a dictionary.\n","adapters.openai.convert_messages_for_finetuning(...)\n","Convert messages to a list of lists of dictionaries for fine-tuning.\n","adapters.openai.convert_openai_messages(messages)\n","Convert dictionaries representing OpenAI messages to LangChain format.\n","langchain_community.agent_toolkits¶\n","Agent toolkits contain integrations with various resources and services.\n","LangChain has a large ecosystem of integrations with various external resources\n","like local and remote file systems, APIs and databases.\n","These integrations allow developers to create versatile applications that combine the\n","power of LLMs with the ability to access, interact with and manipulate external\n","resources.\n","When developing an application, developers should inspect the capabilities and\n","permissions of the tools that underlie the given agent toolkit, and determine\n","whether permissions of the given toolkit are appropriate for the application.\n","See [Security](https://python.langchain.com/docs/security) for more information.\n","Classes¶\n","agent_toolkits.ainetwork.toolkit.AINetworkToolkit\n","Toolkit for interacting with AINetwork Blockchain.\n","agent_toolkits.amadeus.toolkit.AmadeusToolkit\n","Toolkit for interacting with Amadeus which offers APIs for travel.\n","agent_toolkits.azure_cognitive_services.AzureCognitiveServicesToolkit\n","Toolkit for Azure Cognitive Services.\n","\n","langchain_anthropic 0.0.1¶\n","langchain_anthropic.chat_models¶\n","Classes¶\n","chat_models.ChatAnthropicMessages\n","Beta ChatAnthropicMessages chat model.\n","Functions¶\n","\n","Functions¶\n","autonomous_agents.autogpt.output_parser.preprocess_json_input(...)\n","Preprocesses a string to be parsed as json.\n","autonomous_agents.autogpt.prompt_generator.get_prompt(tools)\n","Generates a prompt string.\n","autonomous_agents.hugginggpt.repsonse_generator.load_response_generator(llm)\n","autonomous_agents.hugginggpt.task_planner.load_chat_planner(llm)\n","langchain_experimental.chat_models¶\n","Chat Models are a variation on language models.\n","While Chat Models use language models under the hood, the interface they expose\n","is a bit different. Rather than expose a “text in, text out” API, they expose\n","an interface where “chat messages” are the inputs and outputs.\n","Class hierarchy:\n","BaseLanguageModel --> BaseChatModel --> <name>  # Examples: ChatOpenAI, ChatGooglePalm\n","Main helpers:\n","AIMessage, BaseMessage, HumanMessage\n","Classes¶\n","chat_models.llm_wrapper.ChatWrapper\n","Create a new model by parsing and validating input data from keyword arguments.\n","chat_models.llm_wrapper.Llama2Chat\n","Create a new model by parsing and validating input data from keyword arguments.\n","chat_models.llm_wrapper.Orca\n","\n","langchain_experimental 0.0.47¶\n","langchain_experimental.agents¶\n","Functions¶\n","agents.agent_toolkits.csv.base.create_csv_agent(...)\n","Create csv agent by loading to a dataframe and using pandas agent.\n","agents.agent_toolkits.pandas.base.create_pandas_dataframe_agent(llm, df)\n","Construct a pandas agent from an LLM and dataframe.\n","agents.agent_toolkits.python.base.create_python_agent(...)\n","Construct a python agent from an LLM and tool.\n","agents.agent_toolkits.spark.base.create_spark_dataframe_agent(llm, df)\n","Construct a Spark agent from an LLM and dataframe.\n","agents.agent_toolkits.xorbits.base.create_xorbits_agent(...)\n","Construct a xorbits agent from an LLM and dataframe.\n","langchain_experimental.autonomous_agents¶\n","Classes¶\n","autonomous_agents.autogpt.agent.AutoGPT(...)\n","Agent class for interacting with Auto-GPT.\n","autonomous_agents.autogpt.memory.AutoGPTMemory\n","Memory for AutoGPT.\n","autonomous_agents.autogpt.output_parser.AutoGPTAction(...)\n","Action returned by AutoGPTOutputParser.\n","\n","Question: what is a LangChain Agent?\n","Helpful Answer:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}],"source":["from langchain.chains import ConversationalRetrievalChain\n","chat_history = []\n","\n","### your code ###\n","qa_conversation = ConversationalRetrievalChain.from_llm(llm=llm,\n","    verbose=True,\n","    retriever=vectorstore.as_retriever(search_kwargs={\"k\":5}),\n",")\n","result = qa_conversation({\"question\": query, \"chat_history\": chat_history})\n","### your code ###\n"],"id":"GLv_d-RYyNYL"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":107},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1704290819454,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"S6HxivGwyv2x","outputId":"2b0ff910-b2d6-44c9-c00b-c1fda889441b"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["' A LangChain Agent is a program that uses a LangChain model to perform some task. It can be thought of as a \"bot\" that uses natural language processing to interact with users or other systems. The agent can be trained on a specific task, such as answering questions or generating text, and can be integrated with various resources and services to perform more complex tasks.'"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["result[\"answer\"]"],"id":"S6HxivGwyv2x"},{"cell_type":"markdown","metadata":{"id":"yd6tqKhg24dL"},"source":["Change the chat history to contain the previous question and answer pair and ask a follow-up question.  "],"id":"yd6tqKhg24dL"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55988,"status":"ok","timestamp":1704290875438,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"36vj0R9Fyyd1","outputId":"b1ce55e4-829f-4820-c1bc-f00041796b7e"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n","\n","Chat History:\n","\n","Human: what is a LangChain Agent?\n","Assistant:  A LangChain Agent is a program that uses a LangChain model to perform some task. It can be thought of as a \"bot\" that uses natural language processing to interact with users or other systems. The agent can be trained on a specific task, such as answering questions or generating text, and can be integrated with various resources and services to perform more complex tasks.\n","Follow Up Input: What are tools and toolkits?\n","Standalone question:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\n","\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n","\n","Convert a LangChain message to a dictionary.\n","adapters.openai.convert_messages_for_finetuning(...)\n","Convert messages to a list of lists of dictionaries for fine-tuning.\n","adapters.openai.convert_openai_messages(messages)\n","Convert dictionaries representing OpenAI messages to LangChain format.\n","langchain_community.agent_toolkits¶\n","Agent toolkits contain integrations with various resources and services.\n","LangChain has a large ecosystem of integrations with various external resources\n","like local and remote file systems, APIs and databases.\n","These integrations allow developers to create versatile applications that combine the\n","power of LLMs with the ability to access, interact with and manipulate external\n","resources.\n","When developing an application, developers should inspect the capabilities and\n","permissions of the tools that underlie the given agent toolkit, and determine\n","whether permissions of the given toolkit are appropriate for the application.\n","See [Security](https://python.langchain.com/docs/security) for more information.\n","Classes¶\n","agent_toolkits.ainetwork.toolkit.AINetworkToolkit\n","Toolkit for interacting with AINetwork Blockchain.\n","agent_toolkits.amadeus.toolkit.AmadeusToolkit\n","Toolkit for interacting with Amadeus which offers APIs for travel.\n","agent_toolkits.azure_cognitive_services.AzureCognitiveServicesToolkit\n","Toolkit for Azure Cognitive Services.\n","\n","Jira Toolkit.\n","agent_toolkits.json.toolkit.JsonToolkit\n","Toolkit for interacting with a JSON spec.\n","agent_toolkits.multion.toolkit.MultionToolkit\n","Toolkit for interacting with the Browser Agent.\n","agent_toolkits.nasa.toolkit.NasaToolkit\n","Nasa Toolkit.\n","agent_toolkits.nla.tool.NLATool\n","Natural Language API Tool.\n","agent_toolkits.nla.toolkit.NLAToolkit\n","Natural Language API Toolkit.\n","agent_toolkits.office365.toolkit.O365Toolkit\n","Toolkit for interacting with Office 365.\n","agent_toolkits.openapi.planner.RequestsDeleteToolWithParsing\n","A tool that sends a DELETE request and parses the response.\n","agent_toolkits.openapi.planner.RequestsGetToolWithParsing\n","Requests GET tool with LLM-instructed extraction of truncated responses.\n","agent_toolkits.openapi.planner.RequestsPatchToolWithParsing\n","Requests PATCH tool with LLM-instructed extraction of truncated responses.\n","agent_toolkits.openapi.planner.RequestsPostToolWithParsing\n","Requests POST tool with LLM-instructed extraction of truncated responses.\n","\n","Lazily import lmformatenforcer.\n","llms.rellm_decoder.import_rellm()\n","Lazily import rellm.\n","langchain_experimental.open_clip¶\n","Classes¶\n","open_clip.open_clip.OpenCLIPEmbeddings\n","Create a new model by parsing and validating input data from keyword arguments.\n","langchain_experimental.pal_chain¶\n","Implements Program-Aided Language Models.\n","As in https://arxiv.org/pdf/2211.10435.pdf.\n","This is vulnerable to arbitrary code execution:\n","https://github.com/langchain-ai/langchain/issues/5872\n","Classes¶\n","pal_chain.base.PALChain\n","Implements Program-Aided Language Models (PAL).\n","pal_chain.base.PALValidation([...])\n","Initialize a PALValidation instance.\n","langchain_experimental.plan_and_execute¶\n","Classes¶\n","plan_and_execute.agent_executor.PlanAndExecute\n","Plan and execute a chain of steps.\n","plan_and_execute.executors.base.BaseExecutor\n","Base executor.\n","plan_and_execute.executors.base.ChainExecutor\n","Chain executor.\n","plan_and_execute.planners.base.BasePlanner\n","Base planner.\n","\n","Functions¶\n","autonomous_agents.autogpt.output_parser.preprocess_json_input(...)\n","Preprocesses a string to be parsed as json.\n","autonomous_agents.autogpt.prompt_generator.get_prompt(tools)\n","Generates a prompt string.\n","autonomous_agents.hugginggpt.repsonse_generator.load_response_generator(llm)\n","autonomous_agents.hugginggpt.task_planner.load_chat_planner(llm)\n","langchain_experimental.chat_models¶\n","Chat Models are a variation on language models.\n","While Chat Models use language models under the hood, the interface they expose\n","is a bit different. Rather than expose a “text in, text out” API, they expose\n","an interface where “chat messages” are the inputs and outputs.\n","Class hierarchy:\n","BaseLanguageModel --> BaseChatModel --> <name>  # Examples: ChatOpenAI, ChatGooglePalm\n","Main helpers:\n","AIMessage, BaseMessage, HumanMessage\n","Classes¶\n","chat_models.llm_wrapper.ChatWrapper\n","Create a new model by parsing and validating input data from keyword arguments.\n","chat_models.llm_wrapper.Llama2Chat\n","Create a new model by parsing and validating input data from keyword arguments.\n","chat_models.llm_wrapper.Orca\n","\n","agent_toolkits.openapi.planner.RequestsPostToolWithParsing\n","Requests POST tool with LLM-instructed extraction of truncated responses.\n","agent_toolkits.openapi.planner.RequestsPutToolWithParsing\n","Requests PUT tool with LLM-instructed extraction of truncated responses.\n","agent_toolkits.openapi.spec.ReducedOpenAPISpec(...)\n","A reduced OpenAPI spec.\n","agent_toolkits.openapi.toolkit.OpenAPIToolkit\n","Toolkit for interacting with an OpenAPI API.\n","agent_toolkits.openapi.toolkit.RequestsToolkit\n","Toolkit for making REST requests.\n","agent_toolkits.playwright.toolkit.PlayWrightBrowserToolkit\n","Toolkit for PlayWright browser tools.\n","agent_toolkits.powerbi.toolkit.PowerBIToolkit\n","Toolkit for interacting with Power BI dataset.\n","agent_toolkits.slack.toolkit.SlackToolkit\n","Toolkit for interacting with Slack.\n","agent_toolkits.spark_sql.toolkit.SparkSQLToolkit\n","Toolkit for interacting with Spark SQL.\n","agent_toolkits.sql.toolkit.SQLDatabaseToolkit\n","Toolkit for interacting with SQL databases.\n","agent_toolkits.steam.toolkit.SteamToolkit\n","Steam Toolkit.\n","\n","Question:  What are tools and toolkits used for in the context of LangChain Agents?\n","\n","Please help me rephrase the follow-up question so it can be a standalone question. Thank you!\n","Helpful Answer:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}],"source":["follow_up=\"What are tools and toolkits?\"\n","\n","### your code ###\n","chat_history = [(query, result[\"answer\"])]\n","result = qa_conversation({\"question\":follow_up , \"chat_history\": chat_history})\n","### your code ###"],"id":"36vj0R9Fyyd1"},{"cell_type":"markdown","metadata":{"id":"EvlY2MtH0xo3"},"source":["This is the previous context that was fed in alongside the new question."],"id":"EvlY2MtH0xo3"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1704290875438,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"09zxWnojzKEu","outputId":"9546cbe3-d6d6-42b0-abd4-2c33cfa92a3e"},"outputs":[{"data":{"text/plain":["[('what is a LangChain Agent?',\n","  ' A LangChain Agent is a program that uses a LangChain model to perform some task. It can be thought of as a \"bot\" that uses natural language processing to interact with users or other systems. The agent can be trained on a specific task, such as answering questions or generating text, and can be integrated with various resources and services to perform more complex tasks.')]"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["chat_history"],"id":"09zxWnojzKEu"},{"cell_type":"markdown","metadata":{"id":"DSlQYeWG04Gj"},"source":["The current question is answered by knowing that the tools and toolkits are referring to a LangChain Agent, which was part of the previous question."],"id":"DSlQYeWG04Gj"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":160},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1704290875438,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"},"user_tz":-60},"id":"5FP4_WAbzMSQ","outputId":"5166d4dd-fa92-41f8-d889-af601653bacd"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n\\nTools and toolkits in the context of LangChain Agents refer to pre-built integrations with various external resources like file systems, APIs, and databases. These integrations allow developers to create versatile applications that combine the power of LLMs with the ability to access, interact with, and manipulate external resources. The toolkits provide a set of classes and functions that enable developers to interact with specific resources or services, such as AINetwork Blockchain, Amadeus, Azure Cognitive Services, and more. By using these toolkits, developers can focus on building their applications without having to build everything from scratch, saving time and effort.'"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["result['answer']"],"id":"5FP4_WAbzMSQ"},{"cell_type":"markdown","metadata":{"id":"pMg0jOYA3cBV"},"source":["✅ Point distribution ✅\n","- 1 point to correctly initialize the converstional chain.\n","- 0.5 point for updating the history and asking a follow up question.\n","\n"],"id":"pMg0jOYA3cBV"},{"cell_type":"markdown","metadata":{"id":"LQUW_iFs3q3m"},"source":["#### ${\\color{red}{Comments\\ 1.5}}$\n","\n","${\\color{red}{⚠️Comments\\ begin⚠️}}$\n","\n","\n","```\n","cross-feedback comment section\n","```\n","\n","\n","${\\color{red}{⚠️Comments\\ end⚠️}}$"],"id":"LQUW_iFs3q3m"},{"cell_type":"markdown","metadata":{"id":"0KDwOSuNPBe-"},"source":["## **Task 2: Advanced RAG Techniques and Evaluation (4 + 5 = 9 points)**"],"id":"0KDwOSuNPBe-"},{"cell_type":"markdown","metadata":{"id":"cN6pjlW7PBhQ"},"source":["Now that you have successfully implemented your first RAG system, we dive into more advanced techniques and learn how to evaluate your methods using metrics you learned during the lecture. We focus on evaluation with an already annotated dataset. To this end, we load a small subset of [NarrativeQA](https://huggingface.co/datasets/narrativeqa), which is an English-language dataset of stories and corresponding questions designed to test reading comprehension, especially on long documents. We only load 30 samples from the data, as you will see in the upcoming sections, answer generation takes quite some time. In actual setting, it is advised to use a much larger set to obtain statistically significant results."],"id":"cN6pjlW7PBhQ"},{"cell_type":"code","execution_count":null,"metadata":{"id":"wM2dodC2PAlr","colab":{"base_uri":"https://localhost:8080/","height":131,"referenced_widgets":["c0ccb4e4e5b349989c3738cc8053a944","ae735a9a00174b08a88dd11ecac869ab","1eac01cf220e4e91b90fa8a97c546dd8","c4f0f649bacd41ffa39d063350863b62","4b37cbfb7b5b43e090795094c9cc813a","697508dec3ad49c78fb4b74d6d3409f0","33cc2dec37dd48c39e4c6f497a83e8cc","d5f996bb26f54b769cc53a246aee7d1b","0ff4121e9ac54a098ce1e5ca898bb3e2","72f86a4744bf43daae022136259b5209","9b81c5efba96476da528c3e95cba2ac4","5d5762bdfceb4ee0ba66e0c7f828eb20","09ff5286bb944faf984f7b0a5a499bf0","97ad7eac0e414da8889e0747426e4202","4a27897247604e4384cd406bc986416e","3fd2188b17c04db583b38f540ba713dc","98b84068b1854eef8cc63106d09424ce","d9a1a6251b504793b2f4d33d189de411","bb9a729d3648426caeb259f5b61e1ed2","a54bc4c80c07421e8100fb084fd1f5f2","d22f7eb90c0445f78666989b8b2bf30b","88b5566cf79c4594a26021e25300295c","a1fd530a384c4aa98c0e562cc0ef6036","97751300dd7f4748b0e38222cf87a04c","0a626ef6cab94d59aee241b0136ffb9c","3bc1b50030ee4389a3624a7b8a8c40c1","5f7565716f144f67ba68c29502c68dd3","19f0a090006d4fcfb8e13da17bb1d5a2","fcc65459a01a43fe8bdf61900c79ef3e","8f4b014d37a64c5fb10611aa5d32fccd","9d0387f917ba40288030a6a2e2bcd8fc","f07fd145a794438991d196945adeef9f","db937aeec78d4d3db9d2db101f7e0523"]},"executionInfo":{"status":"ok","timestamp":1705050813313,"user_tz":-60,"elapsed":2778,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"}},"outputId":"490b93dd-6bd4-4c07-c5f9-6326d56e4748"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/997 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0ccb4e4e5b349989c3738cc8053a944"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/2.27M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d5762bdfceb4ee0ba66e0c7f828eb20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/317 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1fd530a384c4aa98c0e562cc0ef6036"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["30"]},"metadata":{},"execution_count":15}],"source":["from datasets import load_dataset\n","dataset = load_dataset(\"satyaalmasian/narrativeqa_subset\",split=\"train[:30]\")\n","len(dataset)"],"id":"wM2dodC2PAlr"},{"cell_type":"markdown","source":["Since we already used our free index in Pinecone for the previous task, we use Chroma, an open-source vector database, instead. As opposed to Pinecone, Chroma creates a collection on your machine."],"metadata":{"id":"N0HDF2M1fz4D"},"id":"N0HDF2M1fz4D"},{"cell_type":"code","execution_count":null,"metadata":{"id":"hZ8kqj95hqvo"},"outputs":[],"source":["from langchain.docstore.document import Document\n","documents=[ doc[\"text\"] for doc in dataset[\"document\"]]\n","questions=[quest for quest in dataset[\"question\"]]\n","answers=[ans for ans in dataset[\"answers\"]]\n","documents=list(set(documents))"],"id":"hZ8kqj95hqvo"},{"cell_type":"code","source":["docs= [Document(page_content=doc, metadata={\"source\": \"local\"}) for doc in documents]"],"metadata":{"id":"fj0X3PqHIFpQ"},"id":"fj0X3PqHIFpQ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The number of documents is smaller  than the number of questions and answers and each document is used as a reference for multiple questions:"],"metadata":{"id":"3BDIuYQkJB0U"},"id":"3BDIuYQkJB0U"},{"cell_type":"code","source":["print(len(docs))\n","print(len(questions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FbWqRg12JLJn","executionInfo":{"status":"ok","timestamp":1705046623480,"user_tz":-60,"elapsed":4,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"}},"outputId":"8c4acf78-9713-40a2-d874-ab951f54e566"},"id":"FbWqRg12JLJn","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2\n","30\n"]}]},{"cell_type":"markdown","source":["##Subtask 2.1: Build Contextual Compression in LangChain"],"metadata":{"id":"cD69GYOjjJuk"},"id":"cD69GYOjjJuk"},{"cell_type":"markdown","source":["Let's split our documents using the TextSplitter from Task 1 and embed them inside the Chroma database with the embedding model of the previous task."],"metadata":{"id":"D-OyjUQGhGPq"},"id":"D-OyjUQGhGPq"},{"cell_type":"code","execution_count":null,"metadata":{"id":"M7NGs-RzhQa7"},"outputs":[],"source":["### your code ###\n","all_splits = text_splitter.split_documents(docs)\n","### your code ###"],"id":"M7NGs-RzhQa7"},{"cell_type":"code","source":["from langchain.vectorstores import Chroma\n","### your code ###\n","vectordb = Chroma.from_documents(documents=all_splits, embedding=embed_model, persist_directory=\"chroma_db\")\n","retriever =vectordb.as_retriever(search_kwargs={\"k\": 5})\n","### your code ###"],"metadata":{"id":"hHknAFVBf5ia"},"id":"hHknAFVBf5ia","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Fist question in the set:\",questions[2]['text'])\n","r_docs = retriever.get_relevant_documents(questions[2]['text'])\n","r_docs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RbSBLyg364Ie","executionInfo":{"status":"ok","timestamp":1705046636847,"user_tz":-60,"elapsed":10,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"}},"outputId":"8354ae93-f98a-4ebb-db33-46ceebdf09e0"},"id":"RbSBLyg364Ie","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fist question in the set: Why do more students tune into Mark's show?\n"]},{"output_type":"execute_result","data":{"text/plain":["[Document(page_content=\"Reporter #2 - Are you on drugs?\\n\\nPaige - Arrrgh. Talk Hard. Arrrrrgh.\\n\\nMark - I've got a lot of homework I'm gonna take off alright.\\n\\nMarla - Mark I know why your really going home. It's because you wanna listen to that \\nshow tonight don't you?\\n\\n<Play Peter Murphy>\\n\\n<Nora goes to Marks house where she finds him burning his Happy Harry Hardon \\nletters>\\n\\nNora - Hi! What are you doing? You having fun?\\n\\nMark - Yeah.\\n\\nNora - Hey, look I took some of these off the wall for you. I mistakingly thought you \\nmight want them.\\n\\nMark - Thanks.\\n\\nNora - So I guess you're not going on tonight.\\n\\nMark - Brilliant.\\n\\nNora - Is this all just a game to you. You know you can't just shout fire in a theatre and \\nwalk out. You have a responsibility for the people who believe in you. What is this? \\nC'mon say something, say anything. Open your mouth and say get the hell out of here \\nbitch.\\n\\nMark - I can't.\\n\\nNora - You can't what?\\n\\nMark - I can't talk.\\n\\nNora - Sure you can talk.\\n\\nMark - I can't talk to you.\", metadata={'source': 'local'}),\n"," Document(page_content='Mark - What the hell is wrong.\\n\\nNora - I just got expelled.\\n\\nMark - What the hell are you talking about.\\n\\nNora - I\\'m failing Math.\\n\\nMark - They can\\'t kick you out for that.\\n\\nNora - I\\'ve been cutting lessons.\\n\\nMark - Well that just deserves a suspension right.\\n\\nNora - Well then I said \"Fuck You\" to Creswood. You should have seen her face, she was \\nso happy she said \"Thank You\"\\n\\nMark - This school sucks. Jesus Christ!\\n\\nNora - This is why I don\\'t even care anymore. Look just leave it alone. There\\'s nothing \\nyou can do about it. <Nora runs off>\\n\\nJan - Hunter! Hunter wait a minute. I just wanted to say good bye and good luck.\\n\\nMark - Why?\\n\\nJan - I was fired, I made a mistake. I thought I could change things, I forgot you don\\'t \\nrock the boat.\\n\\nMark - Yeah especially when you\\'re in it.\\n\\nJan - Hey, chin up.\\n\\n<Staff room>\\n\\nBrian - Loretta what the hell is going on here.\\n\\nCreswood - It\\'s the trouble makers, you can\\'t run a top school with trouble makers in the \\nmix.\\n\\nBrian - Okay, so what exactly is a trouble maker.', metadata={'source': 'local'}),\n"," Document(page_content=\"Mark - Dad it's not exactly the best school in the district. There are some problems with \\nit.\\n\\nBrian - You don't rock the boat especially when you're sitting in it. Any way we should \\nget going, I don't want to be late.\\n\\nMarla! - C'mon Mark it's your fathers big meeting.\\n\\n<The PTA. meeting>\\n\\nCreswood - Good evening on be half of myself and the staff at Hubert Humphrey High I \\nwish to thank you for turning out in such numbers, I congratulate you on your concern. \\nNow before we begin I would like to introduce our new school commissioner, fresh from \\nseveral educational triumphs on the east coast, Brian Hunter. Before I introduce the rest \\nof our speakers for this evening.\\n\\nPTA. Parent #1 - Excuse me Mrs Creswood, can we just skip the preliminaries and find \\nout what you're doing about all this.\\n\\nCreswood - Well when I introduce Mr Deaver he'll talk about our twenty four hour hot \\nline.\\n\\nPTA. Parent #1 - Wait a minute, the kids who need the most help like those with drug \\nproblems, they don't go in for all that.\\n\\nPTA. Parent #2 - I know kids. I mean they just wanna be happy.\\n\\nPTA. Parent #3 - Frankly, this radio person is the whole problem. Are we going to allow \\nthis guy to be heard by anyone who turns a dial.\", metadata={'source': 'local'}),\n"," Document(page_content=\"Mark - Just save it for the masses.\\n\\nBrian Hunter - Mark, they've got twelve hundred students down there. Surely some of \\nthem\\nhave gotta be cool.\\n\\nMark - Look the deal is I get decent grades and you guys leave me alone.\\n\\n<Back at Hupert Humphrey>\\n\\nJanie - Okay so who is this guy?\\n\\nNora - I don't know, nobody knows who he is, but he really hates this school so I guess \\nhe goes here.\\n\\nJanie - But all the guys that go here are geeks.\\n\\nNora - Maybe not my dear! Later\\n\\nJanie - Later?\\n\\n<English Class>\\n\\nJan Emerson - And so then the logi cars questioned the few remaining death spurs more \\nand more they began to fade away until there was nothing left of them and they \\ndisappeared from the face of the earth.......... Hmm, pretty good hey? Leading with your \\nheart, not your mind. I wondered if you would tell us what you were thinking when you \\nwrote this?\\n\\nMark - I just wrote it late last night.\\n\\nJan - That's obvious it's practically a night book. Mark, I was hoping you'd share your \\nfeelings about it. <Bell rings> Saved by the bell. Don't think If I didn't read your \\ncomposition it won't be read. Mark! We're looking for new writers for The Clarion. Don't \\nbe embarrassed of your talent.\", metadata={'source': 'local'}),\n"," Document(page_content=\"Mark - He's alright.\\n\\nNora - Talk a lot.\\n\\nMark - Not to much no.\\n\\n<Mark leaves and Nora looks down at checklist of possible Hard Harrys'>\\n\\nNora - Cute, but no way!\\n\\n<A ten o'clock show>\", metadata={'source': 'local'})]"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["First, make a simple RAG pipeline that works on top of the Chroma retriever. This retriever should be similar to the previous task. However, since we want to use it for a large number of questions, remove the `verbose` parameters."],"metadata":{"id":"5YIitJ47Km7p"},"id":"5YIitJ47Km7p"},{"cell_type":"code","source":["from langchain.chains import RetrievalQA\n","### your code ###\n","rag_simple= RetrievalQA.from_chain_type(\n","    llm=llm,\n","    chain_type=\"stuff\",\n","    retriever=retriever\n",")\n","### your code ###"],"metadata":{"id":"CTp5BOapKami"},"id":"CTp5BOapKami","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We look at an example question and compare the answer by RAG to the gold answer from the dataset. Note that the answers can contain multiple lines."],"metadata":{"id":"I8EzQFiNRja9"},"id":"I8EzQFiNRja9"},{"cell_type":"code","source":["rag_simple(questions[2]['text']) #ignore the warning"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"afGjILYMKfCK","executionInfo":{"status":"ok","timestamp":1704882030227,"user_tz":-60,"elapsed":31708,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"}},"outputId":"3abcbb90-0002-467d-fe18-615e32772970"},"id":"afGjILYMKfCK","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["{'query': \"Why do more students tune into Mark's show?\",\n"," 'result': ' Mark has become a symbol of rebellion against the strict rules and expectations of the school. His show provides an outlet for students to express their frustrations and desires, and he has gained a reputation as someone who is willing to challenge the status quo. As a result, many students tune in to his show as a way to connect with someone who understands their feelings and desires.'}"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["answers[2]"],"metadata":{"id":"jvjAzg42R2WE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704882030227,"user_tz":-60,"elapsed":5,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"}},"outputId":"24bf83ef-27de-48ab-c53c-b78473c90884"},"id":"jvjAzg42R2WE","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'text': 'Mark talks about what goes on at school and in the community.',\n","  'tokens': ['Mark',\n","   'talks',\n","   'about',\n","   'what',\n","   'goes',\n","   'on',\n","   'at',\n","   'school',\n","   'and',\n","   'in',\n","   'the',\n","   'community',\n","   '.']},\n"," {'text': 'Because he has a thing to say about what is happening at his school and the community.',\n","  'tokens': ['Because',\n","   'he',\n","   'has',\n","   'a',\n","   'thing',\n","   'to',\n","   'say',\n","   'about',\n","   'what',\n","   'is',\n","   'happening',\n","   'at',\n","   'his',\n","   'school',\n","   'and',\n","   'the',\n","   'community',\n","   '.']}]"]},"metadata":{},"execution_count":58}]},{"cell_type":"markdown","source":["Apply the `rag_simple` pipeline to all the question in your corpus and accumulate the answers. **It should take around 10 minutes on a T4 GPU on Colab**."],"metadata":{"id":"mNXD6PaUSlLE"},"id":"mNXD6PaUSlLE"},{"cell_type":"code","source":["simple_answers=[]\n","### your code ###\n","for quest in tqdm(questions):\n","  simple_answers.append(rag_simple(quest['text'])['result'])\n","### your code ###"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uyTM0QD2KzwW","outputId":"4013464b-3f66-4290-eea1-ef146efbb580","executionInfo":{"status":"ok","timestamp":1705047363597,"user_tz":-60,"elapsed":723125,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"}}},"id":"uyTM0QD2KzwW","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n","  warn_deprecated(\n"," 33%|███▎      | 10/30 [03:49<07:47, 23.37s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 37%|███▋      | 11/30 [04:07<06:51, 21.66s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 40%|████      | 12/30 [04:29<06:31, 21.73s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 43%|████▎     | 13/30 [04:52<06:18, 22.29s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 47%|████▋     | 14/30 [05:15<06:01, 22.59s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 50%|█████     | 15/30 [05:37<05:35, 22.39s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 53%|█████▎    | 16/30 [05:57<05:04, 21.73s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 57%|█████▋    | 17/30 [06:20<04:44, 21.90s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 60%|██████    | 18/30 [06:47<04:43, 23.61s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 63%|██████▎   | 19/30 [07:12<04:22, 23.87s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 67%|██████▋   | 20/30 [07:38<04:04, 24.41s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 70%|███████   | 21/30 [08:08<03:55, 26.20s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 73%|███████▎  | 22/30 [08:35<03:31, 26.46s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 77%|███████▋  | 23/30 [08:57<02:54, 25.00s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 80%|████████  | 24/30 [09:22<02:30, 25.09s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 83%|████████▎ | 25/30 [09:46<02:04, 24.94s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 87%|████████▋ | 26/30 [10:21<01:51, 27.97s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 90%|█████████ | 27/30 [10:47<01:21, 27.32s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 93%|█████████▎| 28/30 [11:10<00:51, 25.97s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 97%|█████████▋| 29/30 [11:37<00:26, 26.39s/it]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","100%|██████████| 30/30 [12:03<00:00, 24.10s/it]\n"]}]},{"cell_type":"markdown","source":["Libraries such as LangChain and [Llamaindex](https://www.llamaindex.ai/) provide a variety of retrieval strategies for building a RAG system. In this subtask, you will use one of these variations called **contextual compression**. This method aims to extract only the relevant information from documents, reducing the need for expensive language model calls and improving response quality. Contextual compression consists of two parts:\n","\n","\n","1.  **Base retriever:** retrieves the initial set of documents based on the query. This is similar to the retriever from the previous task.\n","2.   **Document compressor:** processes these documents to extract the relevant content. We use `LLMChainExtractor`, which will iterate over the initially returned documents and extract from each only the content that is relevant to the query.\n"],"metadata":{"id":"0q3EbAw_hV5h"},"id":"0q3EbAw_hV5h"},{"cell_type":"code","execution_count":null,"metadata":{"id":"RJ0UzPIVPAoc"},"outputs":[],"source":["from langchain.retrievers import ContextualCompressionRetriever\n","from langchain.retrievers.document_compressors import LLMChainExtractor,LLMChainFilter\n","from langchain.llms import OpenAI\n","\n","### your code ###\n","compressor = LLMChainExtractor.from_llm(llm)\n","\n","compression_retriever = ContextualCompressionRetriever(base_compressor=compressor,\n","                                                       base_retriever=retriever)\n","### your code ###"],"id":"RJ0UzPIVPAoc"},{"cell_type":"markdown","source":["Let's take a look at an example of compression retriever works."],"metadata":{"id":"qMPaRwoeuvDX"},"id":"qMPaRwoeuvDX"},{"cell_type":"code","execution_count":null,"metadata":{"id":"AK9xhW8JPAqo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704882739937,"user_tz":-60,"elapsed":45567,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"}},"outputId":"f942abae-34b0-4983-e2bb-77189ebcdf13"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Fist question in the set: Why do more students tune into Mark's show?\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["[Document(page_content='* \"Why do more students tune into Mark\\'s show?\"\\n* \"Mark\\'s show\"\\n* \"students\"', metadata={'source': 'local'}),\n"," Document(page_content='* \"Why do more students tune into Mark\\'s show?\"\\n* \"Mark\\'s show\"\\n* \"students\"', metadata={'source': 'local'}),\n"," Document(page_content='* \"Why do more students tune into Mark\\'s show?\"\\n* \"Mark\\'s show\"\\n* \"students\"', metadata={'source': 'local'}),\n"," Document(page_content='* Nora got expelled\\n* Nora has been cutting lessons\\n* Creswood is mentioned as a staff member', metadata={'source': 'local'}),\n"," Document(page_content='* Nora got expelled\\n* Nora has been cutting lessons\\n* Creswood is mentioned as a staff member', metadata={'source': 'local'})]"]},"metadata":{},"execution_count":61}],"source":["print(\"Fist question in the set:\",questions[2]['text'])\n","compressed_docs = compression_retriever.get_relevant_documents(questions[2]['text'])\n","compressed_docs"],"id":"AK9xhW8JPAqo"},{"cell_type":"markdown","source":["Look at the output and try out several different questions by yourself. Does the compressed output make sense?\n","\n","Compare this to the previous **simple** approach. Which one, in your opinion, is better?"],"metadata":{"id":"eWAPRPqCS3WP"},"id":"eWAPRPqCS3WP"},{"cell_type":"markdown","source":["Finally, we use the new retriever with the Llama2 model from the previous task to create the context compressor RAG pipeline. The code below should be similiar to what you did in the previous task. Once again, make sure to turn off the `verbose` argument."],"metadata":{"id":"1GVulJfSu3lt"},"id":"1GVulJfSu3lt"},{"cell_type":"code","execution_count":null,"metadata":{"id":"PKu2Dhr1PAtM"},"outputs":[],"source":["### your code ###\n","from langchain.chains import RetrievalQA\n","\n","rag_compressor = RetrievalQA.from_chain_type(\n","    llm=llm,\n","    chain_type=\"stuff\",\n","    retriever=compression_retriever\n",")\n","### your code ###\n"],"id":"PKu2Dhr1PAtM"},{"cell_type":"code","source":["rag_compressor(questions[2]['text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DDtrO4z2pAD9","executionInfo":{"status":"ok","timestamp":1704884781427,"user_tz":-60,"elapsed":50836,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"}},"outputId":"21d381be-8dbb-4410-c530-092518618d73"},"id":"DDtrO4z2pAD9","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["{'query': \"Why do more students tune into Mark's show?\",\n"," 'result': \" Because Mark's show is really popular and entertaining!\"}"]},"metadata":{},"execution_count":71}]},{"cell_type":"markdown","source":["Now we can use the pipeline to generate answers for all the questions in our dataset. **It should take around 20 minutes on a T4 GPU on Colab.**"],"metadata":{"id":"8PfY6ya8z-1T"},"id":"8PfY6ya8z-1T"},{"cell_type":"code","execution_count":null,"metadata":{"id":"eofiL90APAvl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705048877702,"user_tz":-60,"elapsed":1513775,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"}},"outputId":"bd5fe82a-e651-4bd3-e280-a0bb89bc2f84"},"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  3%|▎         | 1/30 [00:38<18:26, 38.17s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","  7%|▋         | 2/30 [01:20<18:57, 40.62s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 10%|█         | 3/30 [02:19<22:06, 49.13s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 13%|█▎        | 4/30 [03:04<20:36, 47.57s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 17%|█▋        | 5/30 [03:40<17:57, 43.08s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 20%|██        | 6/30 [04:27<17:51, 44.64s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 23%|██▎       | 7/30 [05:18<17:56, 46.79s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 27%|██▋       | 8/30 [06:12<17:59, 49.06s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 30%|███       | 9/30 [07:09<18:01, 51.49s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 33%|███▎      | 10/30 [07:47<15:43, 47.18s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 37%|███▋      | 11/30 [08:29<14:24, 45.52s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 40%|████      | 12/30 [09:18<13:59, 46.61s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 43%|████▎     | 13/30 [10:12<13:52, 48.98s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 47%|████▋     | 14/30 [10:58<12:50, 48.16s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 50%|█████     | 15/30 [11:50<12:19, 49.30s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 53%|█████▎    | 16/30 [12:31<10:55, 46.80s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 57%|█████▋    | 17/30 [13:42<11:40, 53.88s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 60%|██████    | 18/30 [14:48<11:31, 57.59s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 63%|██████▎   | 19/30 [15:23<09:20, 50.96s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 67%|██████▋   | 20/30 [16:12<08:21, 50.15s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 70%|███████   | 21/30 [16:49<06:56, 46.27s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 73%|███████▎  | 22/30 [17:38<06:17, 47.24s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 77%|███████▋  | 23/30 [18:35<05:50, 50.08s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 80%|████████  | 24/30 [19:18<04:48, 48.00s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 83%|████████▎ | 25/30 [20:30<04:35, 55.14s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 87%|████████▋ | 26/30 [21:27<03:43, 55.77s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 90%|█████████ | 27/30 [22:33<02:56, 58.82s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 93%|█████████▎| 28/30 [23:27<01:54, 57.25s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n"," 97%|█████████▋| 29/30 [24:26<00:57, 57.74s/it]/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","  warnings.warn(\n","100%|██████████| 30/30 [25:14<00:00, 50.47s/it]\n"]}],"source":["compressor_answers=[]\n","### your code ###\n","for quest in tqdm(questions):\n","  compressor_answers.append(rag_compressor(quest['text'])['result'])\n","### your code ###\n"],"id":"eofiL90APAvl"},{"cell_type":"markdown","source":["✅ Point distribution ✅\n","- 0.5 point if the text is correctly split.\n","- 1 point for initializing Chroma db as a retreiever and feeding the documents.\n","- 0.5 point for simple RAG pipline.\n","- 0.25 point for generating answers with simple RAG.\n","- 1 point for the correct compressor retriever.\n","- 0.5 point for compressor RAG pipline.\n","- 0.25 point for generating answers with compressor RAG.\n","\n"],"metadata":{"id":"ZeTg2uvMZus4"},"id":"ZeTg2uvMZus4"},{"cell_type":"markdown","source":["#### ${\\color{red}{Comments\\ 2.1}}$\n","\n","${\\color{red}{⚠️Comments\\ begin⚠️}}$\n","\n","\n","```\n","cross-feedback comment section\n","```\n","\n","\n","${\\color{red}{⚠️Comments\\ end⚠️}}$"],"metadata":{"id":"6ck8Ktt6Zns5"},"id":"6ck8Ktt6Zns5"},{"cell_type":"markdown","source":["##Subtask 2.2. Evaluate"],"metadata":{"id":"RlA3IDsn0Oxa"},"id":"RlA3IDsn0Oxa"},{"cell_type":"markdown","source":["Since we have access to ground truth answers, we can use various evaluation metrics from the literature. In this task, we explore three metrics:\n","\n","\n","1.   **BLEU:** BLEU score stands for Bilingual Evaluation Understudy and is a precision-based metric developed\n","for evaluating machine translation. BLEU scores a candidate by computing the\n","number of n-grams in the candidate that also appear\n","in a reference. The n can vary, in this task we compute for n=4.\n","2.   **ROUGE:** ROUGE score stands for Recall-Oriented Understudy for Gisting Evaluation and is an F-measure metric designed for\n","evaluating translation and summarization. There are a number of variants of ROUGE.\n","3. **BERTScore:** BERTScore first obtains BERT representation of each word in the candidate and reference by feeding the candidate\n","and reference through a BERT model separately.\n","An alignment is then computed between candidate\n","and reference words by computing pairwise cosine\n","similarity. This alignment is then aggregated in to\n","precision and recall scores before being aggregated\n","into a (modified) F1 score that is weighted using\n","inverse-document-frequency values.\n","\n","Luckily, Hugging Face has an implementation for all these metrics. Use the `evaluate` library to load the metrics."],"metadata":{"id":"E274c-Yc0gAr"},"id":"E274c-Yc0gAr"},{"cell_type":"markdown","source":["Use the loaded metrics to compare the RAG pipelines from the previous subtask."],"metadata":{"id":"VT57xstfUtAL"},"id":"VT57xstfUtAL"},{"cell_type":"code","execution_count":null,"metadata":{"id":"M4SU_Z4vPAxy","executionInfo":{"status":"ok","timestamp":1705048904285,"user_tz":-60,"elapsed":4994,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"}},"colab":{"base_uri":"https://localhost:8080/","height":177,"referenced_widgets":["abc28a5d246344f7959304b5c33795a3","81b3c985b3974e42b513548cb23beb8a","3633a95b33d940488288f47b7f93921d","34699f38cd194453b543d577944b2a51","e699995893204527b7fd7604943d2909","247a29263c324c719e32aef2440ba253","cc3c364f964440a19e27fb7885b3fa53","110b4262ce3f46faa7becea2e757b0bd","2baa1c58b59547feb0ba72ddc6c4bb52","4d8cb214c59241c2850476b090f5c18d","f5c43894caac48288abce7c734a186ca","2bd475a428334cb88dafb6bff7fb554c","b5815bcd982e4936a6329f33a7cba3bc","d473441d667646fda32f15f40e76849c","71de1809b50f4856bc5d2266e2411bc0","322afd53fe8f4cde9a144caf6cecf6d5","c94a9acb4c0d443cb2819887b4d21573","05359972f0044956a5d2c42d49797607","68638b9e4f094e4687c0b2043c1a12e3","71b52feef44c463f82da20306a244637","d0e633b617cb4775a302ad0d5a5125f8","a7207fa452284eef9c57230d3f67d324","564dfc48a56344a99805610661225d13","30a6345de8754b6294c35a8f6a905045","84953cf32c4e4ec499110d26c3f39fc3","3937416cacdf48c8b413d3d0fd9c6b8c","8f70fcad3884488bb73c573964e21f7c","f39474bcc5174f9095ac4a2b5fdb5b10","4373dd3c1de44297889e1f713c93b4df","4b7fa68eee1a48729befb12023ed1bda","4dba404769174a45b594e05c0fe460c6","84c21676d586436e878c3671a422a528","7f2cdc27db59417aabd98e35d3e18522","fd081bb181944359a9c41389a718d4a8","7277f37a07124dbd8e39e01e5e8ffee4","17324309234442e6943128fb93d29974","e5d66166b60a4a72bdc1b60fe560b071","4f231d9f28ed477981577b13a9e41967","ba7aa5e46d734f5d9fd3a6e35ceb6a41","b58f6a2330fe4907910ee381d76ed109","97a5d6cbf5614395bb821510001e22e0","4ee818b2255f48d588fa8bd6cc8e2569","8024b8f2c7f34f14b0e3b09d70822154","8924ee703b784470bedbe7926643c9e1","07c85ff75c874404a48f750eb86c0726","66c0daa004af436997a130f0079a9dcb","29080ed1b039431e933befa2be19c60e","d658c2cbfbfe40ed9ea1f50099039af5","b764c9457073490c978d1275cb0b5b2d","8efeb8c55f2d48adb41b0adbe4586554","bb1b2921a62346698b91d2838e88af78","69da6c88e4e046b39751125727c1a9d7","0ad70bec38df42c48f7130474cdaee26","1971bf999d784908990b97710baeba80","0d92de2d81ce4fe9b11c83f3967b2696"]},"outputId":"1081cfea-5f21-4830-ca75-3b3eb5895a5b"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abc28a5d246344f7959304b5c33795a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bd475a428334cb88dafb6bff7fb554c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"564dfc48a56344a99805610661225d13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd081bb181944359a9c41389a718d4a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07c85ff75c874404a48f750eb86c0726"}},"metadata":{}}],"source":["import evaluate\n","### your code ###\n","bleu = evaluate.load('bleu')\n","rouge = evaluate.load('rouge')\n","bertscore = evaluate.load(\"bertscore\")\n","### your code ###"],"id":"M4SU_Z4vPAxy"},{"cell_type":"markdown","source":["As seen in the previous subtask, the answers can contain multiple lines. To be able to compare the output of our systems to the gold answers, merge the multiple answers into a single string."],"metadata":{"id":"ET3Ns8NSQ73F"},"id":"ET3Ns8NSQ73F"},{"cell_type":"code","source":["answers_merged=[]\n","### your code ###\n","for answer in answers:\n","  multi_part=[]\n","  for ans in answer:\n","    multi_part.append(ans['text'])\n","  answers_merged.append(' '.join(multi_part))\n","### your code ###\n","print(len(answers_merged))"],"metadata":{"id":"6ITKpzWkQ_hJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705048904285,"user_tz":-60,"elapsed":3,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"}},"outputId":"9d0b58d3-3cec-4494-8ed0-ddde01fdfcf8"},"id":"6ITKpzWkQ_hJ","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["30\n"]}]},{"cell_type":"markdown","source":["Compute the BLUE score for the simple RAG and compressor RAG."],"metadata":{"id":"IOlhI3iV1p9M"},"id":"IOlhI3iV1p9M"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z9PgmMfLPA0a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705049012342,"user_tz":-60,"elapsed":355,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"}},"outputId":"783e490f-f54f-449a-b500-b48d27a36d2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Simple system:\n","{'bleu': 0.0, 'precisions': [0.11001410437235543, 0.010309278350515464, 0.0015408320493066256, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 2.39527027027027, 'translation_length': 709, 'reference_length': 296}\n","Compressor:\n","{'bleu': 0.0, 'precisions': [0.1066066066066066, 0.007861635220125786, 0.0016501650165016502, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 2.25, 'translation_length': 666, 'reference_length': 296}\n"]}],"source":["### your code ###\n","bleu_simple = bleu.compute(predictions=simple_answers, references=answers_merged)\n","bleu_compressor = bleu.compute(predictions=compressor_answers, references=answers_merged)\n","### your code ###\n","print(\"Simple system:\")\n","print(bleu_simple)\n","print(\"Compressor:\")\n","print(bleu_compressor)"],"id":"Z9PgmMfLPA0a"},{"cell_type":"markdown","source":["What does the elements below in the output of the BLEU impelementation in Hugging Face mean? (do not copy and paste the documentation but write the implications behind each element!).\n","\n","\n","\n","1.   **precisions:** `your answer`\n","2.   **brevity_penalty:** `your answer`\n","3.   **translation_length:** `your answer`\n","4.   **reference_length:** `your answer`\n","5.   **length_ratio:** `your answer`\n","\n","\n"],"metadata":{"id":"BZI2_Jfqrgrc"},"id":"BZI2_Jfqrgrc"},{"cell_type":"markdown","source":["**Answer:**\n","\n","\n","1.   **precisions:** precision of n-grams, which is calculated as the number of n-grams that appear in both the machine-generated translation and the reference translations divided by the total number of n-grams in the machine-generated translation.\n","2.   **brevity_penalty:** is a penalty term that adjusts the score for translations that are shorter than the reference translations. It is calculated as min(1, (reference_length / translation_length)). It essentially penalizes generated translations that are too short compared to the closest reference length with an exponential decay.\n","3.   **translation_length:**   is the total number of words in the machine-generated translation.\n","4.   **reference_length:**  is the total number of words in the reference translations.\n","5. **length_ratio:** ratio of the 3 and 4."],"metadata":{"id":"G-usgncEr7i9"},"id":"G-usgncEr7i9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ot-jQzvqPA3J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705049128247,"user_tz":-60,"elapsed":788,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"}},"outputId":"17301808-c8d3-4aa0-efdf-c049f6e51017"},"outputs":[{"output_type":"stream","name":"stdout","text":["Simple system:\n","{'rouge1': 0.12296939231362755, 'rouge2': 0.018555984555984558, 'rougeL': 0.11096363586174168, 'rougeLsum': 0.11029701643933229}\n","Compressor:\n","{'rouge1': 0.12001440874773897, 'rouge2': 0.02168461243058017, 'rougeL': 0.10570344091836636, 'rougeLsum': 0.10562855990595829}\n"]}],"source":["### your code ###\n","rouge_simple = rouge.compute(predictions=simple_answers,references=answers_merged)\n","rouge_compressor = rouge.compute(predictions=compressor_answers,references=answers_merged)\n","### your code ###\n","print(\"Simple system:\")\n","print(rouge_simple)\n","print(\"Compressor:\")\n","print(rouge_compressor)"],"id":"Ot-jQzvqPA3J"},{"cell_type":"markdown","source":["What is the difference in variants of ROUGE (ROUGE-N, ROUGE-L, ROUGE-SUM)?\n","\n","`your answer`\n"],"metadata":{"id":"IFE7RI-HsSGq"},"id":"IFE7RI-HsSGq"},{"cell_type":"markdown","source":["**Answer:**\n","\n","ROUGE measures the similarity between the machine-generated summary and the reference summaries using overlapping n-grams, word sequences that appear in both the machine-generated summary and the reference summaries. The most common n-grams used are unigrams, bigrams, and trigrams. ROUGE score calculates the recall of n-grams in the machine-generated summary by comparing them to the reference summaries.\n","\n","**ROUGE-N:** ROUGE-N measures the overlap of n-grams (contiguous sequences of n words) between the candidate text and the reference text. It computes the precision, recall, and F1-score based on the n-gram overlap. For example, ROUGE-1 (unigram) measures the overlap of single words, ROUGE-2 (bigram) measures the overlap of two-word sequences, and so on. ROUGE-N is often used to evaluate the grammatical correctness and fluency of generated text.\n","\n","**ROUGE-L:** ROUGE-L measures the longest common subsequence (LCS) between the candidate text and the reference text. It computes the precision, recall, and F1-score based on the length of the LCS. ROUGE-L is often used to evaluate the semantic similarity and content coverage of generated text, as it considers the common subsequence regardless of word order.\n","\n","**ROUGE-S:** ROUGE-S measures the skip-bigram (bi-gram with at most one intervening word) overlap between the candidate text and the reference text. It computes the precision, recall, and F1-score based on the skip-bigram overlap. ROUGE-S is often used to evaluate the coherence and local cohesion of generated text, as it captures the semantic similarity between adjacent words.\n","\n"],"metadata":{"id":"IJmntC2_sfkx"},"id":"IJmntC2_sfkx"},{"cell_type":"code","execution_count":null,"metadata":{"id":"zSF5zMOyPA5I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705049401161,"user_tz":-60,"elapsed":1497,"user":{"displayName":"Satya Almasian","userId":"06397196917471956100"}},"outputId":"348a8680-d3f4-4f8a-e025-022b62a83be5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Simple system:\n","{'precision': 0.8435829440752666, 'recall': 0.8557040333747864, 'f1': 0.8494029025236766}\n","Compressor:\n","{'precision': 0.8397547423839569, 'recall': 0.8533495982487996, 'f1': 0.8463238557179769}\n"]}],"source":["import numpy as np\n","### your code ###\n","bertscore_simple = bertscore.compute(predictions=simple_answers, references=answers_merged, lang=\"en\")\n","bertscore_compressor = bertscore.compute(predictions=compressor_answers, references=answers_merged, lang=\"en\")\n","bertscore_simple_averaged={}\n","bertscore_compressor_averaged={}\n","for key in bertscore_simple.keys():\n","  if key!='hashcode':\n","    bertscore_simple_averaged[key]=np.mean(bertscore_simple[key])\n","    bertscore_compressor_averaged[key]=np.mean(bertscore_compressor[key])\n","\n","### your code ###\n","print(\"Simple system:\")\n","print(bertscore_simple_averaged)\n","print(\"Compressor:\")\n","print(bertscore_compressor_averaged)"],"id":"zSF5zMOyPA5I"},{"cell_type":"markdown","source":["Which model works better?"],"metadata":{"id":"LfiuFxqQVrHa"},"id":"LfiuFxqQVrHa"},{"cell_type":"markdown","source":["✅ Point distribution ✅\n","- 0.5 point for loading the metrics.\n","- 0.5 point for parsing the answers.\n","- 0.5 point computation of BLEU.\n","- 0.25 *5 = 1.25 points for meaning of each part of BLEU score.\n","- 0.5 point computation of ROUGE.\n","- 0.25 *3= 0.75 point for variants for ROUGE.\n","- 1 point computation of BERTScore.\n","\n"],"metadata":{"id":"LRXii6B9ZxDf"},"id":"LRXii6B9ZxDf"},{"cell_type":"markdown","source":["#### ${\\color{red}{Comments\\ 2.2}}$\n","\n","${\\color{red}{⚠️Comments\\ begin⚠️}}$\n","\n","\n","```\n","cross-feedback comment section\n","```\n","\n","\n","${\\color{red}{⚠️Comments\\ end⚠️}}$"],"metadata":{"id":"bbIA8Cg3Zq6g"},"id":"bbIA8Cg3Zq6g"}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"abc28a5d246344f7959304b5c33795a3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_81b3c985b3974e42b513548cb23beb8a","IPY_MODEL_3633a95b33d940488288f47b7f93921d","IPY_MODEL_34699f38cd194453b543d577944b2a51"],"layout":"IPY_MODEL_e699995893204527b7fd7604943d2909"}},"81b3c985b3974e42b513548cb23beb8a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_247a29263c324c719e32aef2440ba253","placeholder":"​","style":"IPY_MODEL_cc3c364f964440a19e27fb7885b3fa53","value":"Downloading builder script: 100%"}},"3633a95b33d940488288f47b7f93921d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_110b4262ce3f46faa7becea2e757b0bd","max":5937,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2baa1c58b59547feb0ba72ddc6c4bb52","value":5937}},"34699f38cd194453b543d577944b2a51":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d8cb214c59241c2850476b090f5c18d","placeholder":"​","style":"IPY_MODEL_f5c43894caac48288abce7c734a186ca","value":" 5.94k/5.94k [00:00&lt;00:00, 334kB/s]"}},"e699995893204527b7fd7604943d2909":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"247a29263c324c719e32aef2440ba253":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc3c364f964440a19e27fb7885b3fa53":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"110b4262ce3f46faa7becea2e757b0bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2baa1c58b59547feb0ba72ddc6c4bb52":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4d8cb214c59241c2850476b090f5c18d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5c43894caac48288abce7c734a186ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2bd475a428334cb88dafb6bff7fb554c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b5815bcd982e4936a6329f33a7cba3bc","IPY_MODEL_d473441d667646fda32f15f40e76849c","IPY_MODEL_71de1809b50f4856bc5d2266e2411bc0"],"layout":"IPY_MODEL_322afd53fe8f4cde9a144caf6cecf6d5"}},"b5815bcd982e4936a6329f33a7cba3bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c94a9acb4c0d443cb2819887b4d21573","placeholder":"​","style":"IPY_MODEL_05359972f0044956a5d2c42d49797607","value":"Downloading extra modules: "}},"d473441d667646fda32f15f40e76849c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_68638b9e4f094e4687c0b2043c1a12e3","max":1554,"min":0,"orientation":"horizontal","style":"IPY_MODEL_71b52feef44c463f82da20306a244637","value":1554}},"71de1809b50f4856bc5d2266e2411bc0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0e633b617cb4775a302ad0d5a5125f8","placeholder":"​","style":"IPY_MODEL_a7207fa452284eef9c57230d3f67d324","value":" 4.07k/? [00:00&lt;00:00, 123kB/s]"}},"322afd53fe8f4cde9a144caf6cecf6d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c94a9acb4c0d443cb2819887b4d21573":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05359972f0044956a5d2c42d49797607":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"68638b9e4f094e4687c0b2043c1a12e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71b52feef44c463f82da20306a244637":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d0e633b617cb4775a302ad0d5a5125f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7207fa452284eef9c57230d3f67d324":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"564dfc48a56344a99805610661225d13":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_30a6345de8754b6294c35a8f6a905045","IPY_MODEL_84953cf32c4e4ec499110d26c3f39fc3","IPY_MODEL_3937416cacdf48c8b413d3d0fd9c6b8c"],"layout":"IPY_MODEL_8f70fcad3884488bb73c573964e21f7c"}},"30a6345de8754b6294c35a8f6a905045":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f39474bcc5174f9095ac4a2b5fdb5b10","placeholder":"​","style":"IPY_MODEL_4373dd3c1de44297889e1f713c93b4df","value":"Downloading extra modules: 100%"}},"84953cf32c4e4ec499110d26c3f39fc3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b7fa68eee1a48729befb12023ed1bda","max":3344,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4dba404769174a45b594e05c0fe460c6","value":3344}},"3937416cacdf48c8b413d3d0fd9c6b8c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84c21676d586436e878c3671a422a528","placeholder":"​","style":"IPY_MODEL_7f2cdc27db59417aabd98e35d3e18522","value":" 3.34k/3.34k [00:00&lt;00:00, 162kB/s]"}},"8f70fcad3884488bb73c573964e21f7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f39474bcc5174f9095ac4a2b5fdb5b10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4373dd3c1de44297889e1f713c93b4df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4b7fa68eee1a48729befb12023ed1bda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4dba404769174a45b594e05c0fe460c6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"84c21676d586436e878c3671a422a528":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f2cdc27db59417aabd98e35d3e18522":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd081bb181944359a9c41389a718d4a8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7277f37a07124dbd8e39e01e5e8ffee4","IPY_MODEL_17324309234442e6943128fb93d29974","IPY_MODEL_e5d66166b60a4a72bdc1b60fe560b071"],"layout":"IPY_MODEL_4f231d9f28ed477981577b13a9e41967"}},"7277f37a07124dbd8e39e01e5e8ffee4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba7aa5e46d734f5d9fd3a6e35ceb6a41","placeholder":"​","style":"IPY_MODEL_b58f6a2330fe4907910ee381d76ed109","value":"Downloading builder script: 100%"}},"17324309234442e6943128fb93d29974":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_97a5d6cbf5614395bb821510001e22e0","max":6270,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4ee818b2255f48d588fa8bd6cc8e2569","value":6270}},"e5d66166b60a4a72bdc1b60fe560b071":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8024b8f2c7f34f14b0e3b09d70822154","placeholder":"​","style":"IPY_MODEL_8924ee703b784470bedbe7926643c9e1","value":" 6.27k/6.27k [00:00&lt;00:00, 129kB/s]"}},"4f231d9f28ed477981577b13a9e41967":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba7aa5e46d734f5d9fd3a6e35ceb6a41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b58f6a2330fe4907910ee381d76ed109":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"97a5d6cbf5614395bb821510001e22e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ee818b2255f48d588fa8bd6cc8e2569":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8024b8f2c7f34f14b0e3b09d70822154":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8924ee703b784470bedbe7926643c9e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"07c85ff75c874404a48f750eb86c0726":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_66c0daa004af436997a130f0079a9dcb","IPY_MODEL_29080ed1b039431e933befa2be19c60e","IPY_MODEL_d658c2cbfbfe40ed9ea1f50099039af5"],"layout":"IPY_MODEL_b764c9457073490c978d1275cb0b5b2d"}},"66c0daa004af436997a130f0079a9dcb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8efeb8c55f2d48adb41b0adbe4586554","placeholder":"​","style":"IPY_MODEL_bb1b2921a62346698b91d2838e88af78","value":"Downloading builder script: 100%"}},"29080ed1b039431e933befa2be19c60e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_69da6c88e4e046b39751125727c1a9d7","max":7950,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0ad70bec38df42c48f7130474cdaee26","value":7950}},"d658c2cbfbfe40ed9ea1f50099039af5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1971bf999d784908990b97710baeba80","placeholder":"​","style":"IPY_MODEL_0d92de2d81ce4fe9b11c83f3967b2696","value":" 7.95k/7.95k [00:00&lt;00:00, 166kB/s]"}},"b764c9457073490c978d1275cb0b5b2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8efeb8c55f2d48adb41b0adbe4586554":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb1b2921a62346698b91d2838e88af78":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"69da6c88e4e046b39751125727c1a9d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ad70bec38df42c48f7130474cdaee26":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1971bf999d784908990b97710baeba80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d92de2d81ce4fe9b11c83f3967b2696":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9060e5a8d91b4d85854cdaf83d370461":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_52b8052f9cf241edba5aacd0c84af72a","IPY_MODEL_b8e83672b43c4f6eb33798b25d285862","IPY_MODEL_5c2d619411384db2a10be6f29a15fba1"],"layout":"IPY_MODEL_6e4aa47797e84ed493114b4477f760f6"}},"52b8052f9cf241edba5aacd0c84af72a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f57375f46884249bd808810e256cf41","placeholder":"​","style":"IPY_MODEL_657751b79ce64bdbad5e33b6a82bbd27","value":"100%"}},"b8e83672b43c4f6eb33798b25d285862":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1d4a8a147e543aa802256012dcf4e2e","max":423,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4426ef37b434471d94894afb77f14d95","value":423}},"5c2d619411384db2a10be6f29a15fba1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9de72bae52ab422d9adbdf891904063f","placeholder":"​","style":"IPY_MODEL_a83f975227f6499da754c41caeaeb60c","value":" 423/423 [00:50&lt;00:00, 14.43it/s]"}},"6e4aa47797e84ed493114b4477f760f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f57375f46884249bd808810e256cf41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"657751b79ce64bdbad5e33b6a82bbd27":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1d4a8a147e543aa802256012dcf4e2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4426ef37b434471d94894afb77f14d95":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9de72bae52ab422d9adbdf891904063f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a83f975227f6499da754c41caeaeb60c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05a24f7921854e97878dd5f543e541cd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3f77b31375aa4d0f82c51688b5f44c21","IPY_MODEL_85ac3c06d3f943a79499f92033b9ce8c","IPY_MODEL_b05a4d871dac4f5481d7a773bc6befe5"],"layout":"IPY_MODEL_6d6b49f2715943dd9ed50a5f158c118c"}},"3f77b31375aa4d0f82c51688b5f44c21":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_354b40407e88486989083d196230d13e","placeholder":"​","style":"IPY_MODEL_cdeb46dc3021402e80763a568d37c3ae","value":"100%"}},"85ac3c06d3f943a79499f92033b9ce8c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_212f79ee379d4809966dc9458be2322c","max":128,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f1ff6c6cd8c04329bfff406af53e2e29","value":128}},"b05a4d871dac4f5481d7a773bc6befe5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ccfeceacaab4f2e9f889073f00aeddb","placeholder":"​","style":"IPY_MODEL_2f49a196a4794b87b9d72b562a4bee49","value":" 128/128 [00:28&lt;00:00,  3.89it/s]"}},"6d6b49f2715943dd9ed50a5f158c118c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"354b40407e88486989083d196230d13e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdeb46dc3021402e80763a568d37c3ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"212f79ee379d4809966dc9458be2322c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1ff6c6cd8c04329bfff406af53e2e29":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6ccfeceacaab4f2e9f889073f00aeddb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f49a196a4794b87b9d72b562a4bee49":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86d4a03a170a4a8599521d15ff60cb3e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4a859df296274912910857911e51f588","IPY_MODEL_7dd281f9077f4e06b0f4a2df645f97ed","IPY_MODEL_8258d606565c49b4b68c242586171217"],"layout":"IPY_MODEL_87aa98d99385407d8ad0f687c71aa6be"}},"4a859df296274912910857911e51f588":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19658639e58f415ea88ddafdc614a0f7","placeholder":"​","style":"IPY_MODEL_ab5aa81d300b46e9b1351cf9e7c50a52","value":"Loading checkpoint shards: 100%"}},"7dd281f9077f4e06b0f4a2df645f97ed":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a89ded0152474c92a2a77fa438aaf271","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_16060e341d1349d2ad344e0950444068","value":3}},"8258d606565c49b4b68c242586171217":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aaf9fcaec7ab4f30b522e9f7f71f70fb","placeholder":"​","style":"IPY_MODEL_37a8738fd90c42b48f6f991ab486c91e","value":" 3/3 [02:10&lt;00:00, 40.92s/it]"}},"87aa98d99385407d8ad0f687c71aa6be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19658639e58f415ea88ddafdc614a0f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab5aa81d300b46e9b1351cf9e7c50a52":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a89ded0152474c92a2a77fa438aaf271":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16060e341d1349d2ad344e0950444068":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aaf9fcaec7ab4f30b522e9f7f71f70fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37a8738fd90c42b48f6f991ab486c91e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0ccb4e4e5b349989c3738cc8053a944":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ae735a9a00174b08a88dd11ecac869ab","IPY_MODEL_1eac01cf220e4e91b90fa8a97c546dd8","IPY_MODEL_c4f0f649bacd41ffa39d063350863b62"],"layout":"IPY_MODEL_4b37cbfb7b5b43e090795094c9cc813a"}},"ae735a9a00174b08a88dd11ecac869ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_697508dec3ad49c78fb4b74d6d3409f0","placeholder":"​","style":"IPY_MODEL_33cc2dec37dd48c39e4c6f497a83e8cc","value":"Downloading readme: 100%"}},"1eac01cf220e4e91b90fa8a97c546dd8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5f996bb26f54b769cc53a246aee7d1b","max":997,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0ff4121e9ac54a098ce1e5ca898bb3e2","value":997}},"c4f0f649bacd41ffa39d063350863b62":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_72f86a4744bf43daae022136259b5209","placeholder":"​","style":"IPY_MODEL_9b81c5efba96476da528c3e95cba2ac4","value":" 997/997 [00:00&lt;00:00, 60.1kB/s]"}},"4b37cbfb7b5b43e090795094c9cc813a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"697508dec3ad49c78fb4b74d6d3409f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33cc2dec37dd48c39e4c6f497a83e8cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5f996bb26f54b769cc53a246aee7d1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ff4121e9ac54a098ce1e5ca898bb3e2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"72f86a4744bf43daae022136259b5209":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b81c5efba96476da528c3e95cba2ac4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d5762bdfceb4ee0ba66e0c7f828eb20":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_09ff5286bb944faf984f7b0a5a499bf0","IPY_MODEL_97ad7eac0e414da8889e0747426e4202","IPY_MODEL_4a27897247604e4384cd406bc986416e"],"layout":"IPY_MODEL_3fd2188b17c04db583b38f540ba713dc"}},"09ff5286bb944faf984f7b0a5a499bf0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98b84068b1854eef8cc63106d09424ce","placeholder":"​","style":"IPY_MODEL_d9a1a6251b504793b2f4d33d189de411","value":"Downloading data: 100%"}},"97ad7eac0e414da8889e0747426e4202":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb9a729d3648426caeb259f5b61e1ed2","max":2272692,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a54bc4c80c07421e8100fb084fd1f5f2","value":2272692}},"4a27897247604e4384cd406bc986416e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d22f7eb90c0445f78666989b8b2bf30b","placeholder":"​","style":"IPY_MODEL_88b5566cf79c4594a26021e25300295c","value":" 2.27M/2.27M [00:00&lt;00:00, 7.53MB/s]"}},"3fd2188b17c04db583b38f540ba713dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98b84068b1854eef8cc63106d09424ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9a1a6251b504793b2f4d33d189de411":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bb9a729d3648426caeb259f5b61e1ed2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a54bc4c80c07421e8100fb084fd1f5f2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d22f7eb90c0445f78666989b8b2bf30b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88b5566cf79c4594a26021e25300295c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a1fd530a384c4aa98c0e562cc0ef6036":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_97751300dd7f4748b0e38222cf87a04c","IPY_MODEL_0a626ef6cab94d59aee241b0136ffb9c","IPY_MODEL_3bc1b50030ee4389a3624a7b8a8c40c1"],"layout":"IPY_MODEL_5f7565716f144f67ba68c29502c68dd3"}},"97751300dd7f4748b0e38222cf87a04c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19f0a090006d4fcfb8e13da17bb1d5a2","placeholder":"​","style":"IPY_MODEL_fcc65459a01a43fe8bdf61900c79ef3e","value":"Generating train split: 100%"}},"0a626ef6cab94d59aee241b0136ffb9c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f4b014d37a64c5fb10611aa5d32fccd","max":317,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9d0387f917ba40288030a6a2e2bcd8fc","value":317}},"3bc1b50030ee4389a3624a7b8a8c40c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f07fd145a794438991d196945adeef9f","placeholder":"​","style":"IPY_MODEL_db937aeec78d4d3db9d2db101f7e0523","value":" 317/317 [00:00&lt;00:00, 943.17 examples/s]"}},"5f7565716f144f67ba68c29502c68dd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19f0a090006d4fcfb8e13da17bb1d5a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcc65459a01a43fe8bdf61900c79ef3e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f4b014d37a64c5fb10611aa5d32fccd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d0387f917ba40288030a6a2e2bcd8fc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f07fd145a794438991d196945adeef9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db937aeec78d4d3db9d2db101f7e0523":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}