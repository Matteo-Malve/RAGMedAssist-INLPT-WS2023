{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, retstart, retmax):\n",
    "    Entrez.email = \"sandra_friebolin@proton.me\"\n",
    "    handle = Entrez.esearch(db='pubmed', \n",
    "                            retstart=retstart, \n",
    "                            retmax=retmax, \n",
    "                            retmode='xml', \n",
    "                            term=query)\n",
    "    results = Entrez.read(handle)\n",
    "    return results\n",
    "\n",
    "\n",
    "def fetch_details(id_list):\n",
    "    ids = ','.join(id_list)\n",
    "    Entrez.email = \"sandra_friebolin@proton.me\"\n",
    "    handle = Entrez.efetch(db='pubmed',\n",
    "                            retmode='xml',\n",
    "                            id=ids)\n",
    "    results = Entrez.read(handle)\n",
    "    return results\n",
    "\n",
    "\n",
    "# def get_pub_date(article, medline_citation):\n",
    "#     # Check various fields for publication date\n",
    "#     if 'ArticleDate' in article and article['ArticleDate']:\n",
    "#         return format_date(article['ArticleDate'])\n",
    "#     elif 'PubDate' in article and article['PubDate']:\n",
    "#         return format_date(article['PubDate'])\n",
    "#     elif 'DateCompleted' in medline_citation and medline_citation['DateCompleted']:\n",
    "#         return format_date(medline_citation['DateCompleted'])\n",
    "#     elif 'DateRevised' in medline_citation and medline_citation['DateRevised']:\n",
    "#         return format_date(medline_citation['DateRevised'])\n",
    "#     else:\n",
    "#         # Log or print when a date is not found\n",
    "#         print(f\"Date not found for PMID: {article.get('PMID')}\")\n",
    "#         return \"\"\n",
    "    \n",
    "\n",
    "# def format_date(date_field):\n",
    "#     if isinstance(date_field, list):\n",
    "#         date_field = date_field[0] if date_field else {}\n",
    "\n",
    "#     year = date_field.get('Year', '')\n",
    "#     month = date_field.get('Month', '')\n",
    "#     day = date_field.get('Day', '')\n",
    "\n",
    "#     date_parts = [part for part in [year, month, day] if part]\n",
    "#     formatted_date = '-'.join(date_parts)\n",
    "\n",
    "#     return formatted_date if formatted_date else \"Unknown\"\n",
    "\n",
    "def get_pub_date(paper):\n",
    "    # Fetching the nested PubDate field\n",
    "    journal_issue = paper.get('MedlineCitation', {}).get('Article', {}).get('Journal', {}).get('JournalIssue', {})\n",
    "    pub_date = journal_issue.get('PubDate', {})\n",
    "\n",
    "    year = pub_date.get('Year', '')\n",
    "    month = pub_date.get('Month', '')\n",
    "    day = pub_date.get('Day', '')\n",
    "\n",
    "    # Formatting the date\n",
    "    if year:\n",
    "        formatted_date = year\n",
    "        if month:\n",
    "            formatted_date += f\"-{month}\"\n",
    "            if day:\n",
    "                formatted_date += f\"-{day}\"\n",
    "        return formatted_date\n",
    "    else:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_lists(papers, pmid_list, title_list, abstract_list, author_list, date_list, doi_list):\n",
    "    for paper in papers['PubmedArticle']:\n",
    "        article = paper['MedlineCitation']['Article']\n",
    "        medline_citation = paper['MedlineCitation']\n",
    "\n",
    "        # Check if Abstract is present\n",
    "        if article.get(\"Abstract\") is not None and paper['MedlineCitation']['PMID'] not in pmid_list:\n",
    "            abstract_texts = article['Abstract']['AbstractText']\n",
    "            full_abstract = ' '.join([str(text) for text in abstract_texts])\n",
    "\n",
    "            title_list.append(article['ArticleTitle'])\n",
    "            pmid_list.append(paper['MedlineCitation']['PMID'])\n",
    "\n",
    "            # Fetch authors\n",
    "            if 'AuthorList' in article:\n",
    "                authors = article['AuthorList']\n",
    "                author_names = [author.get('ForeName') + \" \" + author.get('LastName') \\\n",
    "                                    if author.get('ForeName') else author.get('LastName') \\\n",
    "                                for author in authors if 'LastName' in author]\n",
    "                author_list.append(\"; \".join(author_names))\n",
    "            else:\n",
    "                author_list.append(\"\")\n",
    "\n",
    "            # # Fetch Publication Date\n",
    "            # medline_citation = paper.get('MedlineCitation', {})\n",
    "            # article = medline_citation.get('Article', {})\n",
    "            # pub_date = None\n",
    "\n",
    "            # # Check various fields for publication date\n",
    "            # if 'ArticleDate' in article:\n",
    "            #     pub_date = article['ArticleDate']\n",
    "            # elif 'PubDate' in article:\n",
    "            #     pub_date = article['PubDate']\n",
    "            # elif 'DateCompleted' in medline_citation:\n",
    "            #     pub_date = medline_citation['DateCompleted']\n",
    "            # elif 'DateRevised' in medline_citation:\n",
    "            #     pub_date = medline_citation['DateRevised']\n",
    "\n",
    "            # # Format the publication date\n",
    "            # if pub_date:\n",
    "            #     date_str = f\"{pub_date[0]['Year']}-{pub_date[0].get('Month', '01')}-{pub_date[0].get('Day', '01')}\"\n",
    "            # else:\n",
    "            #     date_str = \"\"\n",
    "\n",
    "            # date_list.append(date_str)\n",
    "            \n",
    "            # Fetch and format Publication Date\n",
    "            # pub_date = get_pub_date(article, medline_citation)\n",
    "            pub_date = get_pub_date(paper)\n",
    "            date_list.append(pub_date)\n",
    "\n",
    "            # Fetch DOI\n",
    "            article_id_list = paper.get('PubmedData', {}).get('ArticleIdList', [])\n",
    "            doi = next((id_ for id_ in article_id_list if id_.attributes.get('IdType') == 'doi'), None)\n",
    "            doi_list.append(doi if doi is not None else \"\")\n",
    "\n",
    "            # Append Abstract\n",
    "            abstract_list.append(full_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "642 data for 1/2013-3/2013\n",
      "Newly saved data: 625; Total saved data: 625\n",
      "448 data for 4/2013-6/2013\n",
      "Newly saved data: 349; Total saved data: 974\n",
      "513 data for 7/2013-9/2013\n",
      "Newly saved data: 361; Total saved data: 1335\n",
      "513 data for 10/2013-12/2013\n",
      "Newly saved data: 285; Total saved data: 1620\n",
      "789 data for 1/2014-3/2014\n",
      "Newly saved data: 599; Total saved data: 2219\n",
      "469 data for 4/2014-6/2014\n",
      "Newly saved data: 295; Total saved data: 2514\n",
      "484 data for 7/2014-9/2014\n",
      "Newly saved data: 302; Total saved data: 2816\n",
      "512 data for 10/2014-12/2014\n",
      "Newly saved data: 307; Total saved data: 3123\n",
      "857 data for 1/2015-3/2015\n",
      "Newly saved data: 658; Total saved data: 3781\n",
      "539 data for 4/2015-6/2015\n",
      "Newly saved data: 314; Total saved data: 4095\n",
      "546 data for 7/2015-9/2015\n",
      "Newly saved data: 323; Total saved data: 4418\n",
      "524 data for 10/2015-12/2015\n",
      "Newly saved data: 320; Total saved data: 4738\n",
      "827 data for 1/2016-3/2016\n",
      "Newly saved data: 620; Total saved data: 5358\n",
      "544 data for 4/2016-6/2016\n",
      "Newly saved data: 319; Total saved data: 5677\n",
      "571 data for 7/2016-9/2016\n",
      "Newly saved data: 375; Total saved data: 6052\n",
      "616 data for 10/2016-12/2016\n",
      "Newly saved data: 329; Total saved data: 6381\n",
      "897 data for 1/2017-3/2017\n",
      "Newly saved data: 680; Total saved data: 7061\n",
      "634 data for 4/2017-6/2017\n",
      "Newly saved data: 396; Total saved data: 7457\n",
      "645 data for 7/2017-9/2017\n",
      "Newly saved data: 389; Total saved data: 7846\n",
      "714 data for 10/2017-12/2017\n",
      "Newly saved data: 460; Total saved data: 8306\n",
      "1232 data for 1/2018-3/2018\n",
      "Newly saved data: 929; Total saved data: 9235\n",
      "881 data for 4/2018-6/2018\n",
      "Newly saved data: 529; Total saved data: 9764\n",
      "966 data for 7/2018-9/2018\n",
      "Newly saved data: 575; Total saved data: 10339\n",
      "1076 data for 10/2018-12/2018\n",
      "Newly saved data: 671; Total saved data: 11010\n",
      "1712 data for 1/2019-3/2019\n",
      "Newly saved data: 1287; Total saved data: 12297\n",
      "1325 data for 4/2019-6/2019\n",
      "Newly saved data: 781; Total saved data: 13078\n",
      "1512 data for 7/2019-9/2019\n",
      "Newly saved data: 931; Total saved data: 14009\n",
      "1588 data for 10/2019-12/2019\n",
      "Newly saved data: 989; Total saved data: 14998\n",
      "2669 data for 1/2020-3/2020\n",
      "Newly saved data: 2007; Total saved data: 17005\n",
      "2090 data for 4/2020-6/2020\n",
      "Newly saved data: 1376; Total saved data: 18381\n",
      "2452 data for 7/2020-9/2020\n",
      "Newly saved data: 1551; Total saved data: 19932\n",
      "2621 data for 10/2020-12/2020\n",
      "Newly saved data: 1628; Total saved data: 21560\n",
      "4534 data for 1/2021-3/2021\n",
      "Newly saved data: 3560; Total saved data: 25120\n",
      "3022 data for 4/2021-6/2021\n",
      "Newly saved data: 1870; Total saved data: 26990\n",
      "3281 data for 7/2021-9/2021\n",
      "Newly saved data: 2010; Total saved data: 29000\n",
      "3498 data for 10/2021-12/2021\n",
      "Newly saved data: 2129; Total saved data: 31129\n",
      "6737 data for 1/2022-3/2022\n",
      "Newly saved data: 5348; Total saved data: 36477\n",
      "4069 data for 4/2022-6/2022\n",
      "Newly saved data: 2314; Total saved data: 38791\n",
      "4288 data for 7/2022-9/2022\n",
      "Newly saved data: 2418; Total saved data: 41209\n",
      "4376 data for 10/2022-12/2022\n",
      "Newly saved data: 2647; Total saved data: 43856\n",
      "6755 data for 1/2023-3/2023\n",
      "Newly saved data: 5100; Total saved data: 48956\n",
      "4967 data for 4/2023-6/2023\n",
      "Newly saved data: 3119; Total saved data: 52075\n",
      "5601 data for 7/2023-9/2023\n",
      "Newly saved data: 3463; Total saved data: 55538\n",
      "5611 data for 10/2023-12/2023\n",
      "Newly saved data: 3312; Total saved data: 58850\n"
     ]
    }
   ],
   "source": [
    "pmid_list = []\n",
    "title_list = []\n",
    "abstract_list =[]\n",
    "author_list = []\n",
    "date_list = []\n",
    "doi_list = []\n",
    "saved_data_cnt = 0\n",
    "total_to_fetch = 1000\n",
    "\n",
    "for year in range(2013, 2024):\n",
    "    # if len(pmid_list) >= total_to_fetch:\n",
    "    #     break  # Stop if we have fetched enough records\n",
    "    \n",
    "    for quartal in range(4):\n",
    "        # if len(pmid_list) >= total_to_fetch:\n",
    "        #     break       \n",
    "\n",
    "        month_start, month_end = (quartal) * 3 + 1, ((quartal)) * 3 + 3\n",
    "        query = f\"intelligence[Title/Abstract] AND (\\\"{year}/{month_start}\\\"[Date - Publication] : \\\"{year}/{month_end}\\\"[Date - Publication])\"\n",
    "        handle = Entrez.esearch(db='pubmed', retmax=10000, retmode='xml', term=query)\n",
    "        studies = Entrez.read(handle)\n",
    "\n",
    "        print(f\"{studies['Count']} data for {month_start}/{year}-{month_end}/{year}\")\n",
    "\n",
    "        studiesIdList = studies['IdList']\n",
    "        papers = fetch_details(studiesIdList)\n",
    "        save_data_to_lists(papers, pmid_list, title_list, abstract_list, author_list, date_list, doi_list)\n",
    "\n",
    "        print(f\"Newly saved data: {len(pmid_list)-saved_data_cnt}; Total saved data: {len(pmid_list)}\")\n",
    "        \n",
    "        saved_data_cnt = len(pmid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'PMID': pmid_list,\n",
    "    'Title': title_list,\n",
    "    'Abstract': abstract_list,\n",
    "    'Authors': author_list,\n",
    "    'Publication Date': date_list,\n",
    "    'DOI': doi_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Publication Date</th>\n",
       "      <th>DOI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58845</th>\n",
       "      <td>35235526</td>\n",
       "      <td>Efficient Architecture Search for Continual Learning.</td>\n",
       "      <td>Continual learning with neural networks, which aims to learn a sequence of tasks, is an important learning framework in artificial intelligence (AI). However, it often confronts three challenges: 1) overcome the catastrophic forgetting problem; 2) adapt the current network to new tasks; and 3) control its model complexity. To reach these goals, we propose a novel approach named continual learning with efficient architecture search (CLEAS). CLEAS works closely with neural architecture search (NAS), which leverages reinforcement learning techniques to search for the best neural architecture that fits a new task. In particular, we design a neuron-level NAS controller that decides which old neurons from previous tasks should be reused (knowledge transfer) and which new neurons should be added (to learn new knowledge). Such a fine-grained controller allows finding a very concise architecture that can fit each new task well. Meanwhile, since we do not alter the weights of the reused neurons, we perfectly memorize the knowledge learned from the previous tasks. We evaluate CLEAS on numerous sequential classification tasks, and the results demonstrate that CLEAS outperforms other state-of-the-art alternative methods, achieving higher classification accuracy while using simpler neural architectures.</td>\n",
       "      <td>Qiang Gao; Zhipeng Luo; Diego Klabjan; Fengli Zhang</td>\n",
       "      <td>2023-Nov</td>\n",
       "      <td>10.1109/TNNLS.2022.3151511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58846</th>\n",
       "      <td>35230953</td>\n",
       "      <td>QTT-DLSTM: A Cloud-Edge-Aided Distributed LSTM for Cyber-Physical-Social Big Data.</td>\n",
       "      <td>Cyber-physical-social systems (CPSS), an emerging cross-disciplinary research area, combines cyber-physical systems (CPS) with social networking for the purpose of providing personalized services for humans. CPSS big data, recording various aspects of human lives, should be processed to mine valuable information for CPSS services. To efficiently deal with CPSS big data, artificial intelligence (AI), an increasingly important technology, is used for CPSS data processing and analysis. Meanwhile, the rapid development of edge devices with fast processors and large memories allows local edge computing to be a powerful real-time complement to global cloud computing. Therefore, to facilitate the processing and analysis of CPSS big data from the perspective of multi-attributes, a cloud-edge-aided quantized tensor-train distributed long short-term memory (QTT-DLSTM) method is presented in this article. First, a tensor is used to represent the multi-attributes CPSS big data, which will be decomposed into the QTT form to facilitate distributed training and computing. Second, a distributed cloud-edge computing model is used to systematically process the CPSS data, including global large-scale data processing in the cloud, and local small-scale data processed at the edge. Third, a distributed computing strategy is used to improve the efficiency of training via partitioning the weight matrix and large amounts of input data in the QTT form. Finally, the performance of the proposed QTT-DLSTM method is evaluated using experiments on a public discrete manufacturing process dataset, the Li-ion battery dataset, and a public social dataset.</td>\n",
       "      <td>Xiaokang Wang; Lei Ren; Ruixue Yuan; Laurence T Yang; M Jamal Deen</td>\n",
       "      <td>2023-Oct</td>\n",
       "      <td>10.1109/TNNLS.2022.3140238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58847</th>\n",
       "      <td>35130174</td>\n",
       "      <td>A Review of Recurrent Neural Network-Based Methods in Computational Physiology.</td>\n",
       "      <td>Artificial intelligence and machine learning techniques have progressed dramatically and become powerful tools required to solve complicated tasks, such as computer vision, speech recognition, and natural language processing. Since these techniques have provided promising and evident results in these fields, they emerged as valuable methods for applications in human physiology and healthcare. General physiological recordings are time-related expressions of bodily processes associated with health or morbidity. Sequence classification, anomaly detection, decision making, and future status prediction drive the learning algorithms to focus on the temporal pattern and model the nonstationary dynamics of the human body. These practical requirements give birth to the use of recurrent neural networks (RNNs), which offer a tractable solution in dealing with physiological time series and provide a way to understand complex time variations and dependencies. The primary objective of this article is to provide an overview of current applications of RNNs in the area of human physiology for automated prediction and diagnosis within different fields. Finally, we highlight some pathways of future RNN developments for human physiology.</td>\n",
       "      <td>Shitong Mao; Ervin Sejdic</td>\n",
       "      <td>2023-Oct</td>\n",
       "      <td>10.1109/TNNLS.2022.3145365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58848</th>\n",
       "      <td>35108210</td>\n",
       "      <td>Online Intention Recognition With Incomplete Information Based on a Weighted Contrastive Predictive Coding Model in Wargame.</td>\n",
       "      <td>The incomplete and imperfect essence of the battlefield situation results in a challenge to the efficiency, stability, and reliability of traditional intention recognition methods. For this problem, we propose a deep learning architecture that consists of a contrastive predictive coding (CPC) model, a variable-length long short-term memory network (LSTM) model, and an attention weight allocator for online intention recognition with incomplete information in wargame (W-CPCLSTM). First, based on the typical characteristics of intelligence data, a CPC model is designed to capture more global structures from limited battlefield information. Then, a variable-length LSTM model is employed to classify the learned representations into predefined intention categories. Next, a weighted approach to the training attention of CPC and LSTM is introduced to allow for the stability of the model. Finally, performance evaluation and application analysis of the proposed model for the online intention recognition task were carried out based on four different degrees of detection information and a perfect situation of ideal conditions in a wargame. Besides, we explored the effect of different lengths of intelligence data on recognition performance and gave application examples of the proposed model to a wargame platform. The simulation results demonstrate that our method not only contributes to the growth of recognition stability, but it also improves recognition accuracy by 7%-11%, 3%-7%, 3%-13%, and 3%-7%, the recognition speed by 6- 32× , 4- 18× , 13-* × , and 1- 6× compared with the traditional LSTM, classical FCN, OctConv, and OctFCN models, respectively, which characterizes it as a promising reference tool for command decision-making.</td>\n",
       "      <td>Li Chen; Xingxing Liang; Yanghe Feng; Longfei Zhang; Jing Yang; Zhong Liu</td>\n",
       "      <td>2023-Oct</td>\n",
       "      <td>10.1109/TNNLS.2022.3144171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58849</th>\n",
       "      <td>34670834</td>\n",
       "      <td>A Nietzschean critique of liberal eugenics.</td>\n",
       "      <td>Ethical debates about liberal eugenics frequently focus on the supposed unnaturalness of its means and possible harm to autonomy. I present a Nietzsche-inspired critique focusing on intention rather than means and harm to abilities rather than to autonomy. I first critique subjective eugenics, the selection of extrinsically valuable traits, drawing on Nietzsche's notion of 'slavish' values reducible to the negation of another's good. Subjective eugenics slavishly evaluates traits relative to a negatively evaluated norm (eg, above-average intelligence), disguising a harmful intention to diminish the relative value of that norm. I then argue there is no objective form of eugenics on the Nietzschean ground that abilities are not valuable intrinsically; they are valuable only if one possesses the relative power to exercise them. Abilities frustrated by conflict with other abilities or environment are harmful, while disabilities that empower one's other abilities are beneficial. Consequently, all forms of eugenics are subject to the prior ethical critique of subjective eugenics.</td>\n",
       "      <td>Donovan Tateshi Miyasaki</td>\n",
       "      <td>2023-Dec-14</td>\n",
       "      <td>10.1136/medethics-2021-107414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           PMID  \\\n",
       "58845  35235526   \n",
       "58846  35230953   \n",
       "58847  35130174   \n",
       "58848  35108210   \n",
       "58849  34670834   \n",
       "\n",
       "                                                                                                                              Title  \\\n",
       "58845                                                                         Efficient Architecture Search for Continual Learning.   \n",
       "58846                                            QTT-DLSTM: A Cloud-Edge-Aided Distributed LSTM for Cyber-Physical-Social Big Data.   \n",
       "58847                                               A Review of Recurrent Neural Network-Based Methods in Computational Physiology.   \n",
       "58848  Online Intention Recognition With Incomplete Information Based on a Weighted Contrastive Predictive Coding Model in Wargame.   \n",
       "58849                                                                                   A Nietzschean critique of liberal eugenics.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Abstract  \\\n",
       "58845                                                                                                                                                                                                                                                                                                                                                                                                                                                         Continual learning with neural networks, which aims to learn a sequence of tasks, is an important learning framework in artificial intelligence (AI). However, it often confronts three challenges: 1) overcome the catastrophic forgetting problem; 2) adapt the current network to new tasks; and 3) control its model complexity. To reach these goals, we propose a novel approach named continual learning with efficient architecture search (CLEAS). CLEAS works closely with neural architecture search (NAS), which leverages reinforcement learning techniques to search for the best neural architecture that fits a new task. In particular, we design a neuron-level NAS controller that decides which old neurons from previous tasks should be reused (knowledge transfer) and which new neurons should be added (to learn new knowledge). Such a fine-grained controller allows finding a very concise architecture that can fit each new task well. Meanwhile, since we do not alter the weights of the reused neurons, we perfectly memorize the knowledge learned from the previous tasks. We evaluate CLEAS on numerous sequential classification tasks, and the results demonstrate that CLEAS outperforms other state-of-the-art alternative methods, achieving higher classification accuracy while using simpler neural architectures.   \n",
       "58846                                                                                                       Cyber-physical-social systems (CPSS), an emerging cross-disciplinary research area, combines cyber-physical systems (CPS) with social networking for the purpose of providing personalized services for humans. CPSS big data, recording various aspects of human lives, should be processed to mine valuable information for CPSS services. To efficiently deal with CPSS big data, artificial intelligence (AI), an increasingly important technology, is used for CPSS data processing and analysis. Meanwhile, the rapid development of edge devices with fast processors and large memories allows local edge computing to be a powerful real-time complement to global cloud computing. Therefore, to facilitate the processing and analysis of CPSS big data from the perspective of multi-attributes, a cloud-edge-aided quantized tensor-train distributed long short-term memory (QTT-DLSTM) method is presented in this article. First, a tensor is used to represent the multi-attributes CPSS big data, which will be decomposed into the QTT form to facilitate distributed training and computing. Second, a distributed cloud-edge computing model is used to systematically process the CPSS data, including global large-scale data processing in the cloud, and local small-scale data processed at the edge. Third, a distributed computing strategy is used to improve the efficiency of training via partitioning the weight matrix and large amounts of input data in the QTT form. Finally, the performance of the proposed QTT-DLSTM method is evaluated using experiments on a public discrete manufacturing process dataset, the Li-ion battery dataset, and a public social dataset.   \n",
       "58847                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Artificial intelligence and machine learning techniques have progressed dramatically and become powerful tools required to solve complicated tasks, such as computer vision, speech recognition, and natural language processing. Since these techniques have provided promising and evident results in these fields, they emerged as valuable methods for applications in human physiology and healthcare. General physiological recordings are time-related expressions of bodily processes associated with health or morbidity. Sequence classification, anomaly detection, decision making, and future status prediction drive the learning algorithms to focus on the temporal pattern and model the nonstationary dynamics of the human body. These practical requirements give birth to the use of recurrent neural networks (RNNs), which offer a tractable solution in dealing with physiological time series and provide a way to understand complex time variations and dependencies. The primary objective of this article is to provide an overview of current applications of RNNs in the area of human physiology for automated prediction and diagnosis within different fields. Finally, we highlight some pathways of future RNN developments for human physiology.   \n",
       "58848  The incomplete and imperfect essence of the battlefield situation results in a challenge to the efficiency, stability, and reliability of traditional intention recognition methods. For this problem, we propose a deep learning architecture that consists of a contrastive predictive coding (CPC) model, a variable-length long short-term memory network (LSTM) model, and an attention weight allocator for online intention recognition with incomplete information in wargame (W-CPCLSTM). First, based on the typical characteristics of intelligence data, a CPC model is designed to capture more global structures from limited battlefield information. Then, a variable-length LSTM model is employed to classify the learned representations into predefined intention categories. Next, a weighted approach to the training attention of CPC and LSTM is introduced to allow for the stability of the model. Finally, performance evaluation and application analysis of the proposed model for the online intention recognition task were carried out based on four different degrees of detection information and a perfect situation of ideal conditions in a wargame. Besides, we explored the effect of different lengths of intelligence data on recognition performance and gave application examples of the proposed model to a wargame platform. The simulation results demonstrate that our method not only contributes to the growth of recognition stability, but it also improves recognition accuracy by 7%-11%, 3%-7%, 3%-13%, and 3%-7%, the recognition speed by 6- 32× , 4- 18× , 13-* × , and 1- 6× compared with the traditional LSTM, classical FCN, OctConv, and OctFCN models, respectively, which characterizes it as a promising reference tool for command decision-making.   \n",
       "58849                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Ethical debates about liberal eugenics frequently focus on the supposed unnaturalness of its means and possible harm to autonomy. I present a Nietzsche-inspired critique focusing on intention rather than means and harm to abilities rather than to autonomy. I first critique subjective eugenics, the selection of extrinsically valuable traits, drawing on Nietzsche's notion of 'slavish' values reducible to the negation of another's good. Subjective eugenics slavishly evaluates traits relative to a negatively evaluated norm (eg, above-average intelligence), disguising a harmful intention to diminish the relative value of that norm. I then argue there is no objective form of eugenics on the Nietzschean ground that abilities are not valuable intrinsically; they are valuable only if one possesses the relative power to exercise them. Abilities frustrated by conflict with other abilities or environment are harmful, while disabilities that empower one's other abilities are beneficial. Consequently, all forms of eugenics are subject to the prior ethical critique of subjective eugenics.   \n",
       "\n",
       "                                                                         Authors  \\\n",
       "58845                        Qiang Gao; Zhipeng Luo; Diego Klabjan; Fengli Zhang   \n",
       "58846         Xiaokang Wang; Lei Ren; Ruixue Yuan; Laurence T Yang; M Jamal Deen   \n",
       "58847                                                  Shitong Mao; Ervin Sejdic   \n",
       "58848  Li Chen; Xingxing Liang; Yanghe Feng; Longfei Zhang; Jing Yang; Zhong Liu   \n",
       "58849                                                   Donovan Tateshi Miyasaki   \n",
       "\n",
       "      Publication Date                            DOI  \n",
       "58845         2023-Nov     10.1109/TNNLS.2022.3151511  \n",
       "58846         2023-Oct     10.1109/TNNLS.2022.3140238  \n",
       "58847         2023-Oct     10.1109/TNNLS.2022.3145365  \n",
       "58848         2023-Oct     10.1109/TNNLS.2022.3144171  \n",
       "58849      2023-Dec-14  10.1136/medethics-2021-107414  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to a CSV file\n",
    "# the sizes becomes more than 100 MB which can't be pushed\n",
    "# df.to_csv('pubmed_data.csv', index=False)\n",
    "\n",
    "split_index = df.shape[0] // 2\n",
    "\n",
    "# Split the DataFrame into two parts\n",
    "df_part1 = df.iloc[:split_index]\n",
    "df_part2 = df.iloc[split_index:]\n",
    "\n",
    "# Save each part to a CSV file\n",
    "df_part1.to_csv('pubmed_data_part1.csv', index=False)\n",
    "df_part2.to_csv('pubmed_data_part2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pubmed_data_part1.csv', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Replace unusual line terminators with standard newline character\n",
    "content = content.replace('\\u2028', '\\n').replace('\\u2029', '\\n')\n",
    "\n",
    "# Write the corrected content back to the file\n",
    "with open('pubmed_data_part1.csv', 'w', encoding='utf-8') as file:\n",
    "    file.write(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 58854 entries, 0 to 58853\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   PMID              58854 non-null  object\n",
      " 1   Title             58854 non-null  object\n",
      " 2   Abstract          58850 non-null  object\n",
      " 3   Authors           58766 non-null  object\n",
      " 4   Publication Date  58850 non-null  object\n",
      " 5   DOI               57986 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Read the two CSV files\n",
    "df_part1 = pd.read_csv('pubmed_data_part1.csv')\n",
    "df_part2 = pd.read_csv('pubmed_data_part2.csv')\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "df = pd.concat([df_part1, df_part2], ignore_index=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
