{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "# default extractor\n",
    "from ragas.testset.extractor import KeyphraseExtractor\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "# default DocumentStore\n",
    "from ragas.testset.docstore import InMemoryDocumentStore\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.embeddings import Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class BaseLanguageModel with abstract methods agenerate_prompt, apredict, apredict_messages, generate_prompt, invoke, predict, predict_messages",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# define llm and embeddings\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m langchain_llm \u001b[38;5;241m=\u001b[39m \u001b[43mBaseLanguageModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmistralai/Mistral-7B-Instruct-v0.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# any langchain LLM instance\u001b[39;00m\n\u001b[1;32m      3\u001b[0m langchain_embeddings \u001b[38;5;241m=\u001b[39m Embeddings(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthenlper/gte-base\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# any langchain Embeddings instance\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# wrap them with wrappers\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class BaseLanguageModel with abstract methods agenerate_prompt, apredict, apredict_messages, generate_prompt, invoke, predict, predict_messages"
     ]
    }
   ],
   "source": [
    "# define llm and embeddings\n",
    "langchain_llm = BaseLanguageModel(model=\"mistralai/Mistral-7B-Instruct-v0.1\") # any langchain LLM instance\n",
    "langchain_embeddings = Embeddings(model=\"thenlper/gte-base\") # any langchain Embeddings instance\n",
    "\n",
    "# wrap them with wrappers\n",
    "langchain_llm = LangchainLLMWrapper(langchain_llm)\n",
    "langchain_embeddings = LangchainEmbeddingsWrapper(langchain_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in preprocessed files\n",
    "df1 = pd.read_csv(\"../../data/processed_data_part1.csv\")\n",
    "df2 = pd.read_csv(\"../../data/processed_data_part2.csv\")\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "df.drop(\"Abstract_Length\", axis=1, inplace=True)\n",
    "\n",
    "loader = DataFrameLoader(df, page_content_column=\"Abstract\")\n",
    "docs_all = loader.load()\n",
    "\n",
    "documents = docs_all[:2]\n",
    "\n",
    "for document in documents:\n",
    "    document.metadata['file_name'] = document.metadata['PMID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f6cb96f08744eb84b450a64e999415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/ragas/executor.py\", line 75, in run\n",
      "    results = self.loop.run_until_complete(self._aresults())\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/ragas/executor.py\", line 63, in _aresults\n",
      "    raise e\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/ragas/executor.py\", line 58, in _aresults\n",
      "    r = await future\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/ragas/executor.py\", line 91, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/ragas/testset/extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/ragas/llms/base.py\", line 92, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/tenacity/_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/tenacity/_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/tenacity/_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/ragas/llms/base.py\", line 169, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 553, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 513, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 616, in _agenerate_with_cache\n",
      "    return await self._agenerate(\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 526, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 1291, in create\n",
      "    return await self._post(\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/openai/_base_client.py\", line 1578, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/openai/_base_client.py\", line 1339, in request\n",
      "    return await self._request(\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/openai/_base_client.py\", line 1414, in _request\n",
      "    return await self._retry_request(\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/openai/_base_client.py\", line 1460, in _retry_request\n",
      "    return await self._request(\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/openai/_base_client.py\", line 1414, in _request\n",
      "    return await self._retry_request(\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/openai/_base_client.py\", line 1460, in _retry_request\n",
      "    return await self._request(\n",
      "  File \"/Users/sandrafriebolin/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/openai/_base_client.py\", line 1429, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    },
    {
     "ename": "ExceptionInRunner",
     "evalue": "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExceptionInRunner\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Change resulting question type distribution\u001b[39;00m\n\u001b[1;32m      5\u001b[0m distributions \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     simple: \u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m      7\u001b[0m     multi_context: \u001b[38;5;241m0.4\u001b[39m,\n\u001b[1;32m      8\u001b[0m     reasoning: \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m      9\u001b[0m }\n\u001b[0;32m---> 12\u001b[0m testset \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_langchain_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistributions\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     14\u001b[0m test_df \u001b[38;5;241m=\u001b[39m testset\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m     15\u001b[0m test_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/ragas/testset/generator.py:150\u001b[0m, in \u001b[0;36mTestsetGenerator.generate_with_langchain_docs\u001b[0;34m(self, documents, test_size, distributions, with_debugging_logs, is_async, raise_exceptions, run_config)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_with_langchain_docs\u001b[39m(\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    141\u001b[0m     documents: t\u001b[38;5;241m.\u001b[39mSequence[LCDocument],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m ):\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# chunk documents and add to docstore\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mDocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_langchain_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    155\u001b[0m         test_size\u001b[38;5;241m=\u001b[39mtest_size,\n\u001b[1;32m    156\u001b[0m         distributions\u001b[38;5;241m=\u001b[39mdistributions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[1;32m    161\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/ragas/testset/docstore.py:210\u001b[0m, in \u001b[0;36mInMemoryDocumentStore.add_documents\u001b[0;34m(self, docs, show_progress)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# split documents with self.splitter into smaller nodes\u001b[39;00m\n\u001b[1;32m    205\u001b[0m nodes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    206\u001b[0m     Node\u001b[38;5;241m.\u001b[39mfrom_langchain_document(d)\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39mtransform_documents(docs)\n\u001b[1;32m    208\u001b[0m ]\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp-transformers/lib/python3.10/site-packages/ragas/testset/docstore.py:250\u001b[0m, in \u001b[0;36mInMemoryDocumentStore.add_nodes\u001b[0;34m(self, nodes, show_progress, desc)\u001b[0m\n\u001b[1;32m    248\u001b[0m results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mresults()\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;241m==\u001b[39m []:\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(nodes):\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m nodes_to_embed\u001b[38;5;241m.\u001b[39mkeys():\n",
      "\u001b[0;31mExceptionInRunner\u001b[0m: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91b3ae99b9a474d9b0daf6579868a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/118762 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generator with openai models\n",
    "generator = TestsetGenerator.with_openai()\n",
    "\n",
    "# Change resulting question type distribution\n",
    "distributions = {\n",
    "    simple: 0.5,\n",
    "    multi_context: 0.4,\n",
    "    reasoning: 0.1\n",
    "}\n",
    "\n",
    "\n",
    "testset = generator.generate_with_langchain_docs(documents, 2, distributions) \n",
    "\n",
    "test_df = testset.to_pandas()\n",
    "test_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
