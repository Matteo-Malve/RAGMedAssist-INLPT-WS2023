{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be9f7653"
      },
      "source": [
        "**Heidelberg University**\n",
        "\n",
        "**Data Science  Group**\n",
        "    \n",
        "Prof. Dr. Michael Gertz  \n",
        "\n",
        "Ashish Chouhan, Satya Almasian, John Ziegler, Jayson Salazar, Nicolas Reuter\n",
        "    \n",
        "October 30, 2023\n",
        "    \n",
        "Natural Language Processing with Transformers\n",
        "\n",
        "Winter Semster 2023/2024     \n",
        "***"
      ],
      "id": "be9f7653"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "258e9648"
      },
      "source": [
        "# **Assignment 1: “Word Embeddings and Probabilistic Language Models”**\n",
        "**Due**: Monday, November 13, 2pm, via [Moodle](https://moodle.uni-heidelberg.de/course/view.php?id=19251)\n",
        "\n"
      ],
      "id": "258e9648"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc27ad9e"
      },
      "source": [
        "### **Submission Guidelines**\n",
        "\n",
        "- Solutions need to be uploaded as a **single** Jupyter notebook. You will find several pre-filled code segments in the notebook, your task is to fill in the missing cells.\n",
        "- For the written solution, use LaTeX in markdown inside the same notebook. Do **not** hand in a separate file for it.\n",
        "- Download the .zip file containing the dataset but do **not** upload it with your solution.\n",
        "- It is sufficient if one person per group uploads the solution to Moodle, but make sure that the complete names of all team members are given in the notebook.\n",
        "\n",
        "***"
      ],
      "id": "fc27ad9e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e322e8b0"
      },
      "source": [
        "## **Task 1: F.R.I.E.N.D.S and  Word2Vec (Points (3 + 3 + 2) = 8)**"
      ],
      "id": "e322e8b0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4ca26ac"
      },
      "source": [
        "[Friends](https://en.wikipedia.org/wiki/Friends) is an American television sitcom, created by David Crane and Marta Kauffman. In this assignment we will use the transcripts from the show to train a Word2Vec model using the [Gensim](https://radimrehurek.com/gensim/) library."
      ],
      "id": "b4ca26ac"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc29cb37"
      },
      "source": [
        "### Subtask 1: Pre-processing\n",
        "We start by loading and cleaning the data. Download the dataset for this assignment and load the `friends_quotes.csv` using pandas. The dataset is from Kaggle (https://www.kaggle.com/ryanstonebraker/friends-transcript) and is created for building a classifier that  determines which friend from the Friend's TV Show would be most likely to say a quote. The column `quote` contains a line from the movie and `author` is the one who said it. Since these are the only two columns we need, we remove the rest and only keep these two columns."
      ],
      "id": "fc29cb37"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "787e2059"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import spacy\n",
        "import logging  # Setting up the loggings to monitor gensim\n",
        "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
      ],
      "id": "787e2059"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EALqYnmgipa"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.5 points when file is loaded and 'author' and 'quote' columns are filtered. If columns other than 'author' and 'quote' are filtered then zero points or if any of the two columns are missing then zero points."
      ],
      "id": "9EALqYnmgipa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "7e7e8c7d",
        "outputId": "4217868b-03d9-4e20-d793-dff12bfb8d9b",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     author                                              quote\n",
              "0    Monica  There's nothing to tell! He's just some guy I ...\n",
              "1      Joey  C'mon, you're going out with the guy! There's ...\n",
              "2  Chandler  All right Joey, be nice. So does he have a hum...\n",
              "3    Phoebe                           Wait, does he eat chalk?\n",
              "4    Phoebe  Just, 'cause, I don't want her to go through w...\n",
              "5    Monica  Okay, everybody relax. This is not even a date...\n",
              "6  Chandler                          Sounds like a date to me.\n",
              "7  Chandler  Alright, so I'm back in high school, I'm stand...\n",
              "8       All                          Oh, yeah. Had that dream.\n",
              "9  Chandler  Then I look down, and I realize there's a phon..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a7f0c22a-837c-4a55-97ef-697b48202b51\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>quote</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Monica</td>\n",
              "      <td>There's nothing to tell! He's just some guy I ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Joey</td>\n",
              "      <td>C'mon, you're going out with the guy! There's ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Chandler</td>\n",
              "      <td>All right Joey, be nice. So does he have a hum...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Phoebe</td>\n",
              "      <td>Wait, does he eat chalk?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Phoebe</td>\n",
              "      <td>Just, 'cause, I don't want her to go through w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Monica</td>\n",
              "      <td>Okay, everybody relax. This is not even a date...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Chandler</td>\n",
              "      <td>Sounds like a date to me.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Chandler</td>\n",
              "      <td>Alright, so I'm back in high school, I'm stand...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>All</td>\n",
              "      <td>Oh, yeah. Had that dream.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Chandler</td>\n",
              "      <td>Then I look down, and I realize there's a phon...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7f0c22a-837c-4a55-97ef-697b48202b51')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a7f0c22a-837c-4a55-97ef-697b48202b51 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a7f0c22a-837c-4a55-97ef-697b48202b51');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e558475c-74eb-46a8-a85c-4c045debefac\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e558475c-74eb-46a8-a85c-4c045debefac')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e558475c-74eb-46a8-a85c-4c045debefac button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "df = pd.read_csv('./friends_quotes.csv')\n",
        "df = df[['author','quote']]\n",
        "df.head(10)"
      ],
      "id": "7e7e8c7d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be5fc72f"
      },
      "source": [
        "Fortunately, there is no missing data, so we do not need to worry about that."
      ],
      "id": "be5fc72f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc3aea10",
        "outputId": "942db00c-5d26-437c-fdcb-76c36867bb2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "author    0\n",
              "quote     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df.isnull().sum() # check for missing data"
      ],
      "id": "dc3aea10"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26a249b1"
      },
      "source": [
        "Use SpaCy to preprocess the text. For this, perform the following steps:\n",
        "- lowercase the words\n",
        "- remove the stopwords and single characters\n",
        "- use regex to remove non-alphabetic characters (anything that is not a number or alphabet including punctuations), in other words only keep \"a\" to \"z\" and digits.\n",
        "- remove lines that have less than 4 words, since they cannot contribute much to the training process.\n",
        "\n",
        "Please do not add any additional steps on your own or additional cleaning as we want to achieve comparable results."
      ],
      "id": "26a249b1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyMcK0oMgG-v"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.5 points when all 4 pre-processing steps are performed if missed any one then zero points."
      ],
      "id": "uyMcK0oMgG-v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5491d9f0",
        "outputId": "5c9910da-6dd0-4787-f147-1fb7f9e19e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\",\"ner\"])\n",
        "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "df[\"quote\"] = df[\"quote\"].apply(lambda x:re.sub(\"r[^A-Za-z\\d']+\", ' ', str(x)).lower())\n",
        "quotes =[]\n",
        "for script in df.quote.tolist():\n",
        "    new_text=' '.join([token.text for token in nlp(script) if token.text not in stopwords and len(token.text)>1 ])\n",
        "    if  len(new_text.split(' '))>3:\n",
        "        quotes.append(new_text)"
      ],
      "id": "5491d9f0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aaa2190"
      },
      "source": [
        "The next step is to build the vocabulary of the words and word combinations we want to learn representations from. We choose a subset of the most frequent words and bigrams to represent our corpus.\n",
        "- Use the Gensim Phrases package to automatically detect common phrases (bigrams) from a list of lines from the previous step (`min_count=10`). Now words like New_York will be considered as one entity and character names like joey_tribbiani will be recognized.\n",
        "- Create a list of words/bigrams with their frequencies and choose the top 15.000 words for the vocabulary, in order to keep the computation time-limited and to choose the most important words."
      ],
      "id": "6aaa2190"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Im6719zeyNt"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.5 points when common phrases (bi-grams) are correctly identified otherwise zero points."
      ],
      "id": "6Im6719zeyNt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3059fd0",
        "outputId": "e4aa77d2-3512-4b0b-998b-4b076e07db10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"c'm\", 'going', 'guy', 'got_ta', 'wrong']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from gensim.models.phrases import Phrases\n",
        "words = [row.split(' ') for row in quotes]\n",
        "phrases = Phrases(words, min_count=10, progress_per=10000)\n",
        "new_lines = phrases[words]\n",
        "new_lines[0]"
      ],
      "id": "a3059fd0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBlu7V5kf3ar"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.5 points when top 50 words/bigrams exactly matches the provided words/bigrams if less than 50 words/bigrams matched then zero points."
      ],
      "id": "NBlu7V5kf3ar"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a9ca7541"
      },
      "outputs": [],
      "source": [
        "word_freq = defaultdict(int)\n",
        "for sent in new_lines:\n",
        "    for i in sent:\n",
        "        word_freq[i] += 1\n",
        "vocab= sorted(word_freq, key=word_freq.get, reverse=True)[:15000]"
      ],
      "id": "a9ca7541"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYyn1UAAdujW"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "\n",
        "- 0.5 points: `word_freq['central_perk']` value is `36` for any other value zero points.\n",
        "- 0.5 points: `word_freq['joey']` is 1951 for any other value zero points."
      ],
      "id": "sYyn1UAAdujW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9d08031d",
        "outputId": "3edce048-78a3-4c51-dfbd-18bfeeafacac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_freq['central_perk']"
      ],
      "id": "9d08031d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0882dc8c",
        "outputId": "281b899b-0314-4640-b305-ab8ea50efd23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1879"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_freq['joey']"
      ],
      "id": "0882dc8c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####${\\color{red}{Comments\\ 1.1}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ],
      "metadata": {
        "id": "9LzEpHQMwtdB"
      },
      "id": "9LzEpHQMwtdB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15dfce56"
      },
      "source": [
        "### Subtask 2: Training the Model\n"
      ],
      "id": "15dfce56"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b59c94e0"
      },
      "source": [
        "Use the Gensim implementation of Word2Vec to train a model on the scripts. The training can be divided into 3 stages:\n",
        "\n",
        "\n",
        "1) Set up your model with parameters; define your parameters in such a way that the following conditions are satisfied:\n",
        " - ignore all words that have a total frequency of less than 2.\n",
        " - dimensions of the embeddings: 100\n",
        " - initial learning rate (step size) of 0.03\n",
        " - 20 negative samples\n",
        " - window size 3\n",
        " - the learning rate in the training will decrease as you apply more and more updates. Most of the time when starting with gradient descent the initial steps can be larger, and as we get close to the local minima it is best to use smaller steps to avoid jumping over the local minima. This adjustment is done internally using a learning rate scheduler. Make sure that the smallest learning rate does not go below 0.0001.\n",
        " - set the threshold for configuring which higher-frequency words are randomly down-sampled to 6e-5. This parameter forces the sampling to choose the very frequent words less often in the sampling.\n",
        " - set the hashfunction of the word2vec to the given function.\n",
        " - train on a single worker to make sure you get the same result as ours."
      ],
      "id": "b59c94e0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dfbce0a2"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "def hash(astring):\n",
        "    return ord(astring[0])"
      ],
      "id": "dfbce0a2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcWvYgK9edgb"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- 1.5 points when all the 8 hyper-parameters mentioned in the question are covered. If missed 1 to 4 hyper-parameter then 0.5 points and more than 4 hyper-parameter missed then zero points."
      ],
      "id": "wcWvYgK9edgb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7c12a7a6"
      },
      "outputs": [],
      "source": [
        "w2v = Word2Vec(min_count=2,\n",
        "                     window=3,\n",
        "                     vector_size=100,\n",
        "                     sample=6e-5,\n",
        "                     alpha=0.03,\n",
        "                     min_alpha=0.0001,\n",
        "                     negative=20,\n",
        "                     workers=1, hashfxn=hash)"
      ],
      "id": "7c12a7a6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03eba781"
      },
      "source": [
        "2) Before training, Word2Vec requires us to build the vocabulary table by filtering out the unique words and doing some basic counts on them.\n",
        "Use the `build_vocab` function to process the data. If you look at the logs you can see the effect of `min_count` and `sample` on the word corpus."
      ],
      "id": "03eba781"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWYx5OIDftP1"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.5 points if vocabulary table is built correctly with `new_lines` otherwise zero points."
      ],
      "id": "rWYx5OIDftP1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d9795441",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "w2v.build_vocab(new_lines, progress_per=10000)"
      ],
      "id": "d9795441"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aefdc06f"
      },
      "source": [
        "3) Finally, we  train the model. Train the model for 100 epochs. This will take a while. As we do not plan to train the model any further, we call `init_sims()`, which will make the model much more memory-efficient by precomputing L2-norms of word weight vectors for further analysis."
      ],
      "id": "aefdc06f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ08k5FRf9Y0"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- 1 point when training of word2vec model is carried out otherwise if training is not successful or error then zero points."
      ],
      "id": "fQ08k5FRf9Y0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0febaedc",
        "outputId": "63f5c4ca-a029-4fa6-f76a-c45b95cfbf06"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(12064977, 26754600)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v.train(new_lines, total_examples=w2v.corpus_count, epochs=100, report_delay=1)\n"
      ],
      "id": "0febaedc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####${\\color{red}{Comments\\ 1.2}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ],
      "metadata": {
        "id": "rhYonUHbwquk"
      },
      "id": "rhYonUHbwquk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0c86f82"
      },
      "source": [
        "### Subtask 3: Exploring the Model\n"
      ],
      "id": "a0c86f82"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "377e1f70"
      },
      "source": [
        "As mentioned in the lecture, word embeddings are suited for similarity and analogy tasks. Let's explore some of that with our dataset:\n",
        "\n",
        "We look for most similar words to the name of the famous coffee shop where most of the episodes took place, namely `central_perk` and also one of the characters `joey`. If you have followed the exercise correctly until now, you should see that words like `laying` are similar to `central_perk` and the other main characters are also considered similar to `joey`.\n",
        "\n"
      ],
      "id": "377e1f70"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "690gmHq7hfBx"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.25 points if the answer matches the displayed result."
      ],
      "id": "690gmHq7hfBx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b7074ffa",
        "scrolled": true,
        "outputId": "81994736-657a-48e5-ce4c-bfcef141688c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('conan', 0.6685703992843628),\n",
              " ('sitting_couch', 0.5100923776626587),\n",
              " ('intrigued', 0.44448766112327576),\n",
              " ('continued', 0.43079474568367004),\n",
              " ('aaah', 0.4189146161079407),\n",
              " ('uhhh', 0.416584312915802),\n",
              " ('rehearsing', 0.4084688723087311),\n",
              " ('hippity', 0.406421959400177),\n",
              " ('crossword', 0.40370339155197144),\n",
              " ('mitzi', 0.4036918878555298)]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v.wv.most_similar(positive=[\"central_perk\"])"
      ],
      "id": "b7074ffa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZZxx3VChndu"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.25 points if the answer matches the displayed result."
      ],
      "id": "MZZxx3VChndu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2e66e435",
        "scrolled": true,
        "outputId": "8c3537cc-e2a0-46e1-f86c-c7b7c74cebb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('chandle', 0.7459915280342102),\n",
              " ('ross', 0.7339990735054016),\n",
              " ('rachel', 0.6974701881408691),\n",
              " ('monica', 0.6890754103660583),\n",
              " ('phoebe', 0.6026953458786011),\n",
              " ('ove', 0.5965407490730286),\n",
              " ('hey', 0.5781211256980896),\n",
              " ('right', 0.55622798204422),\n",
              " ('goes', 0.5538894534111023),\n",
              " ('looks', 0.5196943879127502)]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v.wv.most_similar(positive=[\"joey\"])"
      ],
      "id": "2e66e435"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3591274"
      },
      "source": [
        "Look at the similarity of `mrs_green` to `rachel` (her mom) and `ross`  and `spaceship` (urelated). The first one should have a high and the second a low score. Finally, look at the similarity of `smelly_cat` ( a song from pheobe) and `song` the similarity should be high."
      ],
      "id": "e3591274"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--2zvagThpx_"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.25 points if the answer matches the displayed result."
      ],
      "id": "--2zvagThpx_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c5de6309",
        "outputId": "4f7818c9-4bd8-4e83-fd72-00b44c63bacf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.4352267"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v.wv.similarity('rachel', 'mrs_green')"
      ],
      "id": "c5de6309"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wo_IJmShsIB"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.25 points if the answer matches the displayed result."
      ],
      "id": "7Wo_IJmShsIB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "322d0605",
        "outputId": "c2864e4e-545d-4bc4-f758-e62213560648"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.16437878"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v.wv.similarity('ross', 'spaceship')"
      ],
      "id": "322d0605"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ixXnhfohtHi"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.25 points if the answer matches the displayed result."
      ],
      "id": "_ixXnhfohtHi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f7a0a1f5",
        "scrolled": true,
        "outputId": "0d7805ca-a1c9-4ab1-ef9f-42a548b4ddfd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.52747095"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v.wv.similarity('smelly_cat', 'song')"
      ],
      "id": "f7a0a1f5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7343365d"
      },
      "source": [
        "We can also ask our model to give us the word that does not belong to a list of words. Let's see from the list of all 5 characters which one is the most dissimilar?\n"
      ],
      "id": "7343365d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "37bff805"
      },
      "outputs": [],
      "source": [
        "character_names= ['joey', 'rachel', 'phoebe','monica','chandler']"
      ],
      "id": "37bff805"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quDZzVFzhvT5"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.25 points if the answer matches the displayed result."
      ],
      "id": "quDZzVFzhvT5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9020cd09",
        "scrolled": true,
        "outputId": "40952e3d-b9bc-4047-c67f-d5ddadca06be"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'chandler'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v.wv.doesnt_match(character_names)"
      ],
      "id": "9020cd09"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28abdaec"
      },
      "source": [
        "Based on the analogies, which word is to `monica` as `man` is to `women`? (print the top 3 words); you should get `chandler`  among the answers."
      ],
      "id": "28abdaec"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QpYaIeWhxyf"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.5 points if the answer matches the displayed result."
      ],
      "id": "5QpYaIeWhxyf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4a270c9b",
        "outputId": "bd9ce1f4-d213-4d09-f9f1-a354994bbfad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('chandle', 0.6718476414680481),\n",
              " ('ross', 0.5510697960853577),\n",
              " ('joey', 0.5449243187904358)]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2v.wv.most_similar(positive=[\"man\", \"monica\"], negative=[\"women\"], topn=3)"
      ],
      "id": "4a270c9b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c97ae83"
      },
      "source": [
        "Finally, lets use [t-SNE](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) to look at the distribution of our embeddings in the vector space for the character `joey`. Follow the instructions and fill in the blank in the `tsneplot` function."
      ],
      "id": "1c97ae83"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0b8930e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import sys\n",
        "from sklearn.manifold import TSNE"
      ],
      "id": "d0b8930e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faa51d9e"
      },
      "outputs": [],
      "source": [
        "def tsneplot(model, word):\n",
        "    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction for the top 10 most similar and dissimiliar words\n",
        "    \"\"\"\n",
        "    embs = np.empty((0, 100), dtype='f')# to save all the embeddings\n",
        "    word_labels = [word]\n",
        "    color_list  = ['green']\n",
        "\n",
        "    # adds the vector of the query word\n",
        "    embs = np.append(embs, model.wv.__getitem__([word]), axis=0)\n",
        "\n",
        "    # gets list of most similar words\n",
        "    close_words = model.wv.most_similar([word])\n",
        "    all_sims = model.wv.most_similar([word], topn=sys.maxsize)\n",
        "    far_words = list(reversed(all_sims[-10:]))\n",
        "\n",
        "    # adds the vector for each of the closest words to the array\n",
        "    for wrd_score in close_words:\n",
        "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
        "        word_labels.append(wrd_score[0])\n",
        "        color_list.append('blue')\n",
        "        embs = np.append(embs, wrd_vector, axis=0)\n",
        "\n",
        "    # adds the vector for each of the furthest words to the array\n",
        "    for wrd_score in far_words:\n",
        "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
        "        word_labels.append(wrd_score[0])\n",
        "        color_list.append('red')\n",
        "        embs = np.append(embs, wrd_vector, axis=0)\n",
        "\n",
        "    np.set_printoptions(suppress=True)\n",
        "    Y = TSNE(n_components=2, random_state=110, perplexity=15).fit_transform(embs)\n",
        "\n",
        "    # sets everything up to plot\n",
        "    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n",
        "                       'y': [y for y in Y[:, 1]],\n",
        "                       'words': word_labels,\n",
        "                       'color': color_list})\n",
        "\n",
        "    fig, _ = plt.subplots()\n",
        "    fig.set_size_inches(10, 10)\n",
        "\n",
        "    # basic plot\n",
        "    p1 = sns.regplot(data=df,\n",
        "                     x=\"x\",\n",
        "                     y=\"y\",\n",
        "                     fit_reg=False,\n",
        "                     marker=\"o\",\n",
        "                     scatter_kws={'s': 40,\n",
        "                                  'facecolors': df['color']\n",
        "                                 }\n",
        "                    )\n",
        "\n",
        "    # adds annotations one by one with a loop\n",
        "    for line in range(0, df.shape[0]):\n",
        "         p1.text(df[\"x\"][line],\n",
        "                 df['y'][line],\n",
        "                 '  ' + df[\"words\"][line].title(),\n",
        "                 horizontalalignment='left',\n",
        "                 verticalalignment='bottom', size='medium',\n",
        "                 color=df['color'][line],\n",
        "                 weight='normal'\n",
        "                ).set_size(15)\n",
        "\n",
        "\n",
        "    plt.xlim(Y[:, 0].min()-1, Y[:, 0].max()+1)\n",
        "    plt.ylim(Y[:, 1].min()-1, Y[:, 1].max()+1)\n",
        "\n",
        "    plt.title('t-SNE visualization for {}'.format(word.title()))\n"
      ],
      "id": "faa51d9e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "3880bad8",
        "outputId": "e33f57f3-ee4b-411d-c79c-ec17ed423335",
        "scrolled": false
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tsneplot(w2v, 'joey')"
      ],
      "id": "3880bad8"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CrVXY7ZMwRQh"
      },
      "id": "CrVXY7ZMwRQh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####${\\color{red}{Comments\\ 1.3}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ],
      "metadata": {
        "id": "cp-afHMMu0vx"
      },
      "id": "cp-afHMMu0vx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ce2c270"
      },
      "source": [
        "## **Task 2: Questions on the Conceptual Level (non-programming) (Points (2 + 1 + 1 + 4) = 8)**"
      ],
      "id": "1ce2c270"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3fa409f"
      },
      "source": [
        "Please answer the following questions in the notebook cells using markdown. Be percise and short."
      ],
      "id": "c3fa409f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ec614b0"
      },
      "source": [
        "### Subtask 1: For gradient descent, what advantage has a decaying learning rate?"
      ],
      "id": "1ec614b0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhUo3JYUj4Q4"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- 2 points if answer comprises of both the highlighted text or text having interpretation similar to highligted text. If only one highlighted text or its interpretation is present then 1 point otherwise zero points."
      ],
      "id": "NhUo3JYUj4Q4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bec059fb"
      },
      "source": [
        "Decaying learning has the advantage that at the beginning of training where the model weights are usually\n",
        "chosen at random, **the algorithm can fastly converge towards a minimum**. After more and more steps, it can be\n",
        "assumed that the **solution is near such a minimum, and only smaller steps are taken**, such that the algorithm can\n",
        "now \"fine-tune\" model parameters, instead of altering weights massively, potentially stepping over the optimal\n",
        "the solution again and again."
      ],
      "id": "bec059fb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####${\\color{red}{Comments\\ 2.1}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ],
      "metadata": {
        "id": "1HmkYBIlw0My"
      },
      "id": "1HmkYBIlw0My"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0af2b619"
      },
      "source": [
        "### Subtask 2: Why is it easier to maximize the log likelihood instead of the \"normal\" likelihood?"
      ],
      "id": "0af2b619"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn9lrWV0kmqS"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- 1 point if answer comprises of the highlighted text otherwise zero points."
      ],
      "id": "Jn9lrWV0kmqS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deb75406"
      },
      "source": [
        "The advantage of using the log likelihood is that **the derivative is a sum instead of a product**. On one hand\n",
        "this makes math easier for us. On the other hand, this means that the individual terms are independent from\n",
        "each other. Additionally, calculating sums instead of products is numerically more stable, hence, making it more\n",
        "suitable for computer applications."
      ],
      "id": "deb75406"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####${\\color{red}{Comments\\ 2.2}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ],
      "metadata": {
        "id": "0nrapp91w3dn"
      },
      "id": "0nrapp91w3dn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9828a2f"
      },
      "source": [
        "### Subtask 3: Name one advantage that fastText has over Word2Vec?"
      ],
      "id": "a9828a2f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR5Zn6ZGkxR8"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- 1 point if answer comprises of the highlighted text or interpretation of highlighted text, otherwise zero points."
      ],
      "id": "IR5Zn6ZGkxR8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c7aeb3b"
      },
      "source": [
        "FastText considers **subwords (character n-grams), such that a word embedding does not consider only the word itself but also a set of substrings of the word**. This has the advantage that also previously unseen words\n",
        "(or rare words in general) can have a meaningful vector representation if the substrings are known to the model."
      ],
      "id": "6c7aeb3b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####${\\color{red}{Comments\\ 2.3}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ],
      "metadata": {
        "id": "Emh95Wbdw6yg"
      },
      "id": "Emh95Wbdw6yg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07634ed8"
      },
      "source": [
        "### Subtask 4: Compute the partial derivate of softmax loss for word2vec with respect to the center word vector.\n",
        "$$ \\frac{\\partial J}{\\partial v_c} =\\frac{\\partial}{\\partial v_c}\\left[ -log \\left( \\frac{exp(u^T_o v_c)}{\\Sigma_{w \\in Vocab} exp(u^T_o v_c)} \\right)\\right] $$\n",
        "use $U$ to denote the matrix of all embeddings and $y$ for a one-hot vector with a 1 for the true outside word $o$, and $\\hat{y}$ for the predicted distribution $P(w|c)$."
      ],
      "id": "07634ed8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtou_gU8lI9g"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- alloted points for the step is mentioned below the step (step-marking)"
      ],
      "id": "Wtou_gU8lI9g"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baba8a44"
      },
      "source": [
        "$$ \\frac{\\partial J}{\\partial v_c} =\\frac{\\partial}{\\partial v_c}\\left[-log \\left(\\frac{exp(u^T_o v_c)}{\\Sigma_{w \\in Vocab} exp(u^T_w v_c)}\\right)\\right]= $$\n",
        "\n",
        "$$ \\frac{\\partial}{\\partial v_c}\\left[- \\left(log (exp(u^T_o v_c)) - log(\\Sigma_{w \\in Vocab} exp(u^T_w v_c)) \\right)\\right]= $$\n",
        "**0.5 points**\n",
        "\n",
        "$$ \\frac{\\partial}{\\partial v_c}\\left[ log \\left( \\Sigma_{w \\in Vocab} exp(u^T_w v_c))\\right) - u^T_o v_c)\\right]$$\n",
        "\n",
        "$$ \\Sigma_{w \\in Vocab} \\frac{ exp(u^T_w v_c u_w)}{\\Sigma_{w \\in Vocab} exp(u^T_o v_c )} - u_o $$\n",
        "**0.5 points**\n",
        "\n",
        "$$ \\Sigma_{w \\in Vocab} \\frac{ exp(u^T_w v_c)}{\\Sigma_{w \\in Vocab} exp(u^T_o v_c )}u_w - u_o $$\n",
        "$$ \\Sigma_{w \\in Vocab}P(w|c)u_w - u_o $$\n",
        "**1.25 points**\n",
        "\n",
        "$$  \\Sigma_{w \\in Vocab} \\hat{y_w}u_w - u_o $$\n",
        "$$  U \\hat{y_w} - u_o $$\n",
        "**1.25 points**\n",
        "$$  U \\hat{y_w} - Uy $$\n",
        "$$  U (\\hat{y_w} - y) $$\n",
        "**0.5 points**"
      ],
      "id": "baba8a44"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####${\\color{red}{Comments\\ 2.4}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ],
      "metadata": {
        "id": "Sl645_p7KBs9"
      },
      "id": "Sl645_p7KBs9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8364e05b"
      },
      "source": [
        "## **Task 3: Auto-Complete Feature (Points (2 + 6 + 4) = 12)**\n",
        "\n",
        "Let's get even more practical! In this problem set, you will build your own auto-completion system that you see every day while using search engines.\n",
        "\n",
        "[google]: https://www.thedad.com/wp-content/uploads/2018/05/screen-shot-2018-05-12-at-2-01-56-pm.png \"google auto complete\"\n",
        "\n",
        "![google]\n",
        "By the end of this assignment, you will develop a simple prototype of such a system using n-gram language model. At the heart of the system is a language model that assigns the probability to a sequence of words. We take advantage of this probability calculation to predict the next word.\n",
        "\n",
        "The problem set contains 3 main parts:\n",
        "\n",
        "1. Load and preprocess data (tokenize and split into train and test)\n",
        "2. Develop n-gram based language model by estimating the conditional probability of the next word.\n",
        "3. Evaluate the model by computing the perplexity score.\n"
      ],
      "id": "8364e05b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9d5f84e"
      },
      "source": [
        "### Subtask 1: Load and Preprocess Data\n",
        "We use a subset of English tweets to train our model. Run the cell below to load the data and observe a few lines of it. Notice that tweets are saved in a text file, where the individual tweets are separated by `\\n`"
      ],
      "id": "c9d5f84e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "48fc8e51",
        "outputId": "807a03c0-9aa4-4577-cd46-acb44f00b238"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 500 characters of the data:\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A LOT D; Ughh Going To Sleep Like In 5 Minutes ;)\\nWords from a complete stranger! Made my birthday even better :)\\nFirst Cubs game ever! Wrigley field is gorgeous. This is perfect. Go Cubs Go!\\ni no! i ge\""
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\",\"ner\"])\n",
        "\n",
        "\n",
        "with open(\"./twitter.txt\", \"r\") as f:\n",
        "    data = f.read()\n",
        "print(\"First 500 characters of the data:\")\n",
        "display(data[0:500])\n",
        "print(\"-------\")"
      ],
      "id": "48fc8e51"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa007e1d"
      },
      "source": [
        "Now we need to separate the tweets and split them into train and test set. Apply the following pre-processing steps:\n",
        "\n",
        "1. Split data into sentences using \"\\n\" as the delimiter and remove the leading and trailing spaces (drop empty sentences)\n",
        "2. Tokenize the sentences into words using SpaCy and lowercase them. (notice that we do not remove stop words or punctuations.)\n",
        "3. Divide the sentences into 80 percent training and 20 percent test set. No validation set is required. Although in a real-world application it is best to set aside part of the data for hyperparameter tuning.\n",
        "4. To limit the vocabulary and remove potential spelling mistakes, make a vocabulary of the words that appear at least 2 times. The rest of the words will be replaced by the `<unk>` symbol. This is a crucial step since if your model encounters a word that it never saw during training, it won't have an input word that helps determining the next word for suggestion. We use the `<unk>` word for **out of Vocabulary (OOV)** words. Keep in mind that we built the vocabulary on the training data only."
      ],
      "id": "aa007e1d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SypiYRemnRP"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.25 points if data is splitted into sentences using \"\\n\" **AND** trailing and leading spaces are removed otherwise zero points."
      ],
      "id": "7SypiYRemnRP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16d5b640"
      },
      "outputs": [],
      "source": [
        "sentences = data.split('\\n')\n",
        "sentences = [s.strip() for s in sentences if len(s.strip()) > 0]"
      ],
      "id": "16d5b640"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsn6Neivm6Fs"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.25 points if sentences are tokenized into words using spacy **AND** lowercase them.\n",
        "- zero points if sentences are not tokenized using spacy and lowercased or removed stop words or punctuations"
      ],
      "id": "tsn6Neivm6Fs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2faabb7a",
        "outputId": "e036c8cf-9886-4327-92b8-25c0c61989b6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ],
      "source": [
        "tokenized_corpus = []# list of list of the tokens in a sentence\n",
        "for sentence in sentences:\n",
        "    tokenized_corpus.append([token.text.lower() for token in nlp(sentence)])"
      ],
      "id": "2faabb7a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYfa4lMjngaE"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.5 points if 80% training and 20% testing split is performed otherwise zero points."
      ],
      "id": "AYfa4lMjngaE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "235d1b53"
      },
      "outputs": [],
      "source": [
        "from random import Random\n",
        "Random(4).shuffle(tokenized_corpus)\n",
        "\n",
        "train_size = int(len(tokenized_corpus) * 0.8)\n",
        "train = tokenized_corpus[0:train_size]\n",
        "test = tokenized_corpus[train_size:]"
      ],
      "id": "235d1b53"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lvKOwd9nxBB"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- 1 point when count value of `print(len(vocab))` matched the displayed result **AND** `<unk>` is tackled.\n",
        "- 0.5 points:\n",
        "  - if count value does not match but `<unk>` is tackled.\n",
        "  - if `<unk>` is not tackled but count value matches"
      ],
      "id": "7lvKOwd9nxBB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2526e4b1",
        "outputId": "0b02add6-0c21-4bdc-c462-386a923d378d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14861\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "flatten_corpus = [token for sentence in train for token in sentence]\n",
        "word_counts = Counter(flatten_corpus)\n",
        "vocab = []\n",
        "for word, cnt in word_counts.items():\n",
        "    if cnt >= 2:\n",
        "        vocab.append(word)\n",
        "vocab = set(vocab)\n",
        "train_replaced = []\n",
        "test_replaced = []\n",
        "for sentence in train:\n",
        "    train_replaced.append([token if token in vocab else '<unk>' for token in sentence ])\n",
        "for sentence in test:\n",
        "    test_replaced.append([token if token in vocab else '<unk>' for token in sentence ])\n",
        "print(len(vocab))"
      ],
      "id": "2526e4b1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####${\\color{red}{Comments\\ 3.1}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ],
      "metadata": {
        "id": "PmhVqwNxxGs3"
      },
      "id": "PmhVqwNxxGs3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f447294a"
      },
      "source": [
        "### Subtask 2: N-gram Based Language Model:\n",
        "In this section, you will develop an n-grams language model [**1. Large Language Models (LLMs), slide 1-24**]. We assume that the probability of the next word depends only on the previous n-gram or previous n words. We compute this probability by counting the occurrences in the corpus.\n",
        "The conditional probability for the word at position 't' in the sentence, given that the words preceding it are $w_{t-1}, w_{t-2} \\cdots w_{t-n}$ can be estimated as follows:\n",
        "\n",
        "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_t)}{C(w_{t-1}\\dots w_{t-n})}  $$\n",
        "\n",
        "The numerator is the number of times word '$w_t$' appears after the n-gram, and the denominator is the number of times the n-gram occurs in the corpus, where $C(\\cdots)$ is a count function. Later, we add k-smoothing to avoid errors when any counts are zero."
      ],
      "id": "f447294a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a590b34"
      },
      "source": [
        "To tackle the problem of probability estimation we divide the problem into 3 parts. In the following you will:\n",
        "1. Implement a function that computes the counts of n-grams for an arbitrary number n.\n",
        "2. Estimate the probability of a word given the prior n-words using the n-gram counts.\n",
        "3. Calculate probabilities for all possible words.\n",
        "The steps are detailed below."
      ],
      "id": "7a590b34"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8da09885"
      },
      "source": [
        "Let's start by implementing a function that computes the counts of n-grams for an arbitrary number n.\n",
        "- Prepend necessary starting markers `<s>` to indicate the beginning of the sentence. In the case of a bi-gram model, you need to prepend two start tokens `<s><s>` to be able to predict the first word. \"hello world\"-> \"`<s><s>`hello world\".\n",
        "- Append an end token `<e>` so that the model can predict when to finish a sentence.\n",
        "- Create a dictionary to store all the n_gram counts."
      ],
      "id": "8da09885"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "benfkuHWpouy"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- 2 points when function implementation is correct\n",
        "- 1 point:\n",
        "  - if n-gram count implementation is correct but start and end token is not added.\n",
        "- 0.5 points:\n",
        "  - if n-gram count implementation is wrong but start and end token is added."
      ],
      "id": "benfkuHWpouy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8a98cee"
      },
      "outputs": [],
      "source": [
        "def n_grams_counts(corpus, n ):\n",
        "    \"\"\"\n",
        "    Count all n-grams in the corpus given the parameter n\n",
        "\n",
        "    data: List of lists of words (your tokenized corpus)\n",
        "    n: n in the n-gram\n",
        "\n",
        "    Returns: A dictionary that maps a tuple of n words to its frequency\n",
        "    \"\"\"\n",
        "    start_token='<s>'\n",
        "    end_token = '<e>'\n",
        "    n_grams = defaultdict(lambda:1)\n",
        "    for sentence in corpus:\n",
        "        sentence = [start_token] * n+ sentence + [end_token]\n",
        "        # convert list to tuple so it can be used a the key in the dictionary\n",
        "        sentence = tuple(sentence)\n",
        "        m = len(sentence)+1-n\n",
        "        for i in range(m):\n",
        "            # Get the n-gram from i to i+n\n",
        "            n_gram = sentence[i:i+n]\n",
        "            n_grams[n_gram] += 1\n",
        "    return n_grams"
      ],
      "id": "e8a98cee"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba75f530"
      },
      "source": [
        "The next step is to estimate the probability of a word given the prior n words using the n-gram counts, based on the formula given at the beginning of this task. To deal with the problem of zero division we add k-smoothing. K-smoothing adds a positive constant $k$ to each numerator and $k \\times |vocabulary size|$ in the denominator. Below we will define a function that takes in a dictionary `n_gram_cnt`, where the key is the n-gram, and the value is the count of that n-gram, plus a dictionary for `plus_current_gram_cnt`, which you'll use to find the count for the previous n-gram plus the current word. Notice that these dictionaries are computed using the previous function `n_grams_counts`.\n"
      ],
      "id": "ba75f530"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBRxNTyIqMRn"
      },
      "source": [
        "✅ Point distribution ✅\n",
        "- 2 points when function implementation is correct with all the variables populated otherwise zero points."
      ],
      "id": "yBRxNTyIqMRn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2c6898d"
      },
      "outputs": [],
      "source": [
        "def probability(word, prev_n_gram,\n",
        "                         n_gram_cnts, plus_current_gram_cnts, vocab_size):\n",
        "    \"\"\"\n",
        "    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n",
        "    word: next word\n",
        "    prev_n_gram: previous n gram\n",
        "    n_gram_cnts: dictionary of counts of n-grams\n",
        "    plus_current_gram_cnts: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
        "    vocab_size: number of words in the vocabulary\n",
        "\n",
        "    Returns: A probability\n",
        "    \"\"\"\n",
        "    k=1.0\n",
        "    prev_n_gram = tuple(prev_n_gram)\n",
        "\n",
        "    prev_n_gram_cnt = n_gram_cnts.get(prev_n_gram,0) # get the previous n-gram count from\n",
        "    #the dictionary\n",
        "    denominator = prev_n_gram_cnt + k * vocab_size # denominator with the previous n-gram count and k-smoothing\n",
        "    plus_current_gram = prev_n_gram + (word,) # add the current word to the n-gram\n",
        "    plus_current_gram_cnt = plus_current_gram_cnts.get(plus_current_gram,0)  # get the current n-gram\n",
        "    # count using the dictionary\n",
        "    numerator = plus_current_gram_cnt + k #calculate the numerator with k-smoothing\n",
        "    prob = numerator / denominator\n",
        "\n",
        "    return prob"
      ],
      "id": "a2c6898d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7803eb7c"
      },
      "source": [
        "Let's use the functions we have defined to calculate probabilities for all possible words.\n"
      ],
      "id": "7803eb7c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7157c4cb"
      },
      "outputs": [],
      "source": [
        "def probabilities(prev_n_gram, n_gram_cnts, plus_current_gram_cnts, vocab):\n",
        "    \"\"\"\n",
        "    Estimate the probabilities for all the words in the vocabulary\n",
        "    prev_n_gram: previous n gram\n",
        "    n_gram_cnts: dictionary of counts of n-grams\n",
        "    plus_current_gram_cnts: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
        "    vocab: List of words\n",
        "\n",
        "    Returns: A dictionary mapping from next words to the probability.\n",
        "    \"\"\"\n",
        "    prev_n_gram = tuple(prev_n_gram)\n",
        "\n",
        "    vocab = vocab + [\"<e>\", \"<unk>\"] # add <e> <unk> to the vocabulary\n",
        "    vocabulary_size = len(vocab)\n",
        "    probabilities = {}\n",
        "    for word in vocab:\n",
        "        probabilities[word] = probability(word, prev_n_gram, n_gram_cnts, plus_current_gram_cnts,vocabulary_size)\n",
        "    return probabilities"
      ],
      "id": "7157c4cb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Point distribution ✅\n",
        "- 2 points when \"moon\" is having the highest probability `0.25`.\n",
        "- 1 point when \"moon\" is having the highest probability but value other than `0.25`.\n",
        "- zero points if \"moon\" is not having highest probability."
      ],
      "metadata": {
        "id": "hM03bFnXfu9m"
      },
      "id": "hM03bFnXfu9m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88fea049",
        "outputId": "52e9a13c-c537-41f1-df74-187c020d9e75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The word 'moon' should have the highest probability, if it is not the case, re-visit your previous functions.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'and': 0.05,\n",
              " 'tonight': 0.05,\n",
              " 'mars': 0.05,\n",
              " 'stars': 0.05,\n",
              " 'moon': 0.25,\n",
              " 'shining': 0.05,\n",
              " 'is': 0.05,\n",
              " 'a': 0.05,\n",
              " 'bright': 0.05,\n",
              " 'plant': 0.05,\n",
              " 'shinnig': 0.05,\n",
              " 'are': 0.05,\n",
              " 'plants': 0.05,\n",
              " 'the': 0.05,\n",
              " '<e>': 0.05,\n",
              " '<unk>': 0.05}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Predict the probability of the all possible words after the unigram \"the\"\n",
        "sentences = [['the', 'moon', 'and', 'stars', 'are','shining','bright'],\n",
        "             ['the', 'moon', 'is', 'shinnig','tonight'],\n",
        "             ['mars','and' ,'moon', 'are', 'plants'],\n",
        "             ['the' ,'moon', 'is','a', 'plant']]\n",
        "unique_words = list(set(sentences[0] + sentences[1] + sentences[2]+ sentences[3]))\n",
        "unigram_counts = n_grams_counts(sentences, 1)\n",
        "bigram_counts = n_grams_counts(sentences, 2)\n",
        "print(\"The word 'moon' should have the highest probability, if it is not the case, re-visit your previous functions.\")\n",
        "probabilities([\"the\"], unigram_counts, bigram_counts, unique_words)"
      ],
      "id": "88fea049"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####${\\color{red}{Comments\\ 3.2}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ],
      "metadata": {
        "id": "SZ9h4n25xJKV"
      },
      "id": "SZ9h4n25xJKV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "014a385f"
      },
      "source": [
        "### Subtask 3: Evaluation"
      ],
      "id": "014a385f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90a4345f"
      },
      "source": [
        "In this part, we use the perplexity score to evaluate your model on the test set. The perplexity score of the test set on an n-gram model is defined as follows:\n",
        "\n",
        "$$ PP(W) =\\sqrt[N]{ \\prod_{t=n}^{N-1} \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } $$\n",
        "- where $N$ is the length of the sentence. ($N-1$ is used because in the code we start from the index 0).\n",
        "- $n$ is the number of words in the n-gram.\n",
        "- $W$ is the n-gram\n",
        "\n",
        "Notice that we have already computed this probability.\n",
        "\n",
        "The higher the probabilities are, the lower the perplexity will be."
      ],
      "id": "90a4345f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Point distribution ✅\n",
        "- 1 point when all 7 code placeholder are matching the solution.\n",
        "- 0.5 points when code in the for loop are matching the solution\n",
        "- 0 points when code inside the loop are not matching the solution"
      ],
      "metadata": {
        "id": "0oGmEuhlhbWR"
      },
      "id": "0oGmEuhlhbWR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9644c33f"
      },
      "outputs": [],
      "source": [
        "def perplexity(sentence, n_gram_cnts, plus_current_gram_cnts, vocab_size):\n",
        "    \"\"\"\n",
        "    Calculate perplexity for a sentence\n",
        "    sentence: List of strings\n",
        "    n_gram_cnts: dictionary of counts of n-grams\n",
        "    plus_current_gram_cnts: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
        "    vocab_size: number of unique words in the vocabulary\n",
        "\n",
        "    Returns: Perplexity score for a single sentence\n",
        "    \"\"\"\n",
        "\n",
        "    n = len(list(n_gram_cnts.keys())[0]) # get the number 'n' in  n-gram  from n_gram_cnts\n",
        "\n",
        "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"] # prepend <s> and append <e>\n",
        "    sentence = tuple(sentence)\n",
        "    N = len(sentence)# length of sentence\n",
        "\n",
        "\n",
        "    product_pi = 1.0\n",
        "\n",
        "    ### Compute the product of probabilites ###\n",
        "\n",
        "    for t in range(n, N):\n",
        "        n_gram = sentence[t-n:t]# get the n-gram before the predicted word (n-gram before t )\n",
        "        word = sentence[t] # get the word to be predicted (position t)\n",
        "        prob = probability(word,n_gram, n_gram_cnts, plus_current_gram_cnts, vocab_size)\n",
        "        product_pi *= 1 / prob # Update the product of the probabilities\n",
        "\n",
        "    perplexity = product_pi**(1/float(N)) # Take the Nth root of the product\n",
        "    return perplexity"
      ],
      "id": "9644c33f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8745e1bc"
      },
      "source": [
        "Use `perplexity` function to find the perplexity of a bi-gram model on the first training sample and on the first test sample (first element of the set)."
      ],
      "id": "8745e1bc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Point distribution ✅\n",
        "- Perplexity for first train sample: 125.3628 (0.5 points)\n",
        "- Perplexity for test sample: 2389.7532 (0.5 points)\n"
      ],
      "metadata": {
        "id": "zrt0ThyOiA6Y"
      },
      "id": "zrt0ThyOiA6Y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d857043a",
        "outputId": "1d676284-bb1d-49ba-cdfc-9765d4b839b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity for first train sample: 125.3628\n",
            "Perplexity for test sample: 2389.7532\n"
          ]
        }
      ],
      "source": [
        "bigram_counts = n_grams_counts(train_replaced, 2)\n",
        "trigram_counts = n_grams_counts(train_replaced, 3)\n",
        "\n",
        "perplexity_train = perplexity(train_replaced[0],bigram_counts, trigram_counts,len(vocab))\n",
        "print(f\"Perplexity for first train sample: {perplexity_train:.4f}\")\n",
        "\n",
        "perplexity_test = perplexity(test_replaced[0],bigram_counts, trigram_counts,len(vocab))\n",
        "print(f\"Perplexity for test sample: {perplexity_test:.4f}\")# the preprexity for the train sample should be much lower"
      ],
      "id": "d857043a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c697faa"
      },
      "source": [
        "Finally, let's use the model we created to generate an auto-complete system that makes suggestions."
      ],
      "id": "7c697faa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6d6163b"
      },
      "outputs": [],
      "source": [
        "def suggest_a_word(up_to_here, n_gram_cnts, plus_current_gram_cnts, vocab , start_with=None):\n",
        "    \"\"\"\n",
        "    Get suggestion for the next word\n",
        "    up_to_here: the sentence so far, must have length => n\n",
        "    n_gram_cnts: dictionary of counts of n-grams\n",
        "    plus_current_gram_cnts: dictionary of counts of (n+1)-grams -> current word+ n-gram = (n+1)-gram\n",
        "    vocab: List of words\n",
        "    start_with: If not None, specifies the first few letters of the next word\n",
        "\n",
        "    Returns: (most likely next word,  probability)\n",
        "    \"\"\"\n",
        "    n = len(list(n_gram_cnts.keys())[0]) # get the number 'n' in  n-gram  from n_gram_cnts\n",
        "    up_to_here = up_to_here[-n:] # get the last 'n' words as the previous n-gram from the input sentence\n",
        "\n",
        "    # Estimate the probabilities for each word in the vocabulary\n",
        "    probabs = probabilities(up_to_here,n_gram_cnts, plus_current_gram_cnts,vocab)\n",
        "\n",
        "    probabs = {k: v for k, v in sorted(probabs.items(), key=lambda item: item[1], reverse=True)}\n",
        "    if start_with is None:\n",
        "        next_item= next(iter(probabs))\n",
        "        return (next_item,probabs[next_item])\n",
        "    else:\n",
        "        for word, prob in probabs.items():\n",
        "            if word.startswith(start_with):\n",
        "                return (word, prob)\n",
        "    return(None,0)\n"
      ],
      "id": "b6d6163b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f88f9abc"
      },
      "source": [
        "Test your model based on the bi-gram model created on the training corpus."
      ],
      "id": "f88f9abc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.5 points when 'baseball' is answer otherwise zero points\n"
      ],
      "metadata": {
        "id": "EaqtT9v7itWd"
      },
      "id": "EaqtT9v7itWd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "460dff09",
        "outputId": "aea79940-08f2-4d1d-97e3-2197a679a826"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('baseball', 0.0001998534408100726)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "previous_tokens=['i','like']\n",
        "start_with='b'\n",
        "suggestion = suggest_a_word(previous_tokens, bigram_counts,trigram_counts, list(vocab),start_with=start_with)\n",
        "suggestion"
      ],
      "id": "460dff09"
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.5 points when 'see' is answer otherwise zero points"
      ],
      "metadata": {
        "id": "kv8WM-M4i7vJ"
      },
      "id": "kv8WM-M4i7vJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e68f730c",
        "outputId": "cd0c0f65-6e34-445d-f177-49ee1f07379d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('see', 0.0006685385746757588)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "previous_tokens=['i','like','to']\n",
        "start_with=None\n",
        "suggestion = suggest_a_word(previous_tokens, bigram_counts,trigram_counts, list(vocab),start_with=start_with)\n",
        "suggestion"
      ],
      "id": "e68f730c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.5 points when `<unk>` is answer otherwise zero points"
      ],
      "metadata": {
        "id": "GxCz-lFAi9ji"
      },
      "id": "GxCz-lFAi9ji"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "001a7ff4",
        "outputId": "2a1d58e2-a8a2-4bd0-abd4-ca190c42871d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('<unk>', 0.0004702089071001545)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "previous_tokens=[\"hello\", \"my\", \"name\", \"is\"]\n",
        "start_with=None\n",
        "suggestion = suggest_a_word(previous_tokens, bigram_counts,trigram_counts, list(vocab),start_with=start_with)\n",
        "suggestion"
      ],
      "id": "001a7ff4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.5 points when 'allison' is answer otherwise zero points"
      ],
      "metadata": {
        "id": "8Hd0--cTjBsj"
      },
      "id": "8Hd0--cTjBsj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c556e16f",
        "outputId": "9fcff49f-2472-4657-91a6-a2f59d347a66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('allison', 0.00020151810304292335)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "previous_tokens=[\"hello\", \"my\", \"name\", \"is\"]\n",
        "start_with='a'\n",
        "suggestion = suggest_a_word(previous_tokens, bigram_counts,trigram_counts, list(vocab),start_with=start_with)\n",
        "suggestion"
      ],
      "id": "c556e16f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####${\\color{red}{Comments\\ 3.3}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ],
      "metadata": {
        "id": "w8udKLdixERs"
      },
      "id": "w8udKLdixERs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4a15ac8"
      },
      "source": [
        "## **Task 4: Understanding GloVe (Points (2 + 4.25 + 4 + 1.75) = 12)**"
      ],
      "id": "b4a15ac8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15e586ca"
      },
      "source": [
        "In this part, you will implement the [GloVe](https://nlp.stanford.edu/projects/glove/) model and train your own word vectors with gradient\n",
        "descent and numpy. GloVe stands for Global Vectors for word representation, which was developed by researchers at Stanford University to generate word embeddings from corpus statistics.\n",
        "The statistics of the corpus are represented by a co-occurrence matrix, indicating how often a particular word pair occurs together.\n",
        "GloVe is based on ratios of probabilities from this co-occurrence matrix, combining the intuitions of count-based models while also being similar to neural models like word2vec.\n",
        "From this matrix, one can compute the co-occurrence probabilities. We motivate this with an example:\n",
        "\n",
        "$P_{ik} = P(i,j)$ -> co-occurrence probability or joint probability of words $i$ and $k$\n",
        "\n",
        "$P_{jk}$ ->co-occurrence probability of words $j$ and $k$\n",
        "\n",
        "$\\frac{P_{ik}}{P_{jk}}$-> corelation between $i$ and $j$ with the prob word $k$\n",
        "\n",
        "This ratio gives us some insight into the co-relation of the probe word $k$ with the words $i$ and $j$.\n",
        "An example is shown below for different prob words $k$, where $i$ and $j$ are `ice` and `steam`, respectively."
      ],
      "id": "15e586ca"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "076234c5"
      },
      "source": [
        "\n",
        "![glove](https://nlp.stanford.edu/projects/glove/images/table.png)\n"
      ],
      "id": "076234c5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b933587"
      },
      "source": [
        "Image taken from Stanford NLP."
      ],
      "id": "3b933587"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76e8f6f0"
      },
      "source": [
        "As shown in the example above, compared to the raw probabilities, the ratio of probabilities is better able to distinguish relevant words from irrelevant words. Consider the raw probabilities (the first two rows of the table), the values are close to one another and not indicative of the relationships. However, the ratios (bottom row) have more distinct values.\n",
        "\n",
        "This ratio can be small, large, or equal to 1 depending on the prob word and its co-relation. In the example above, the ratio between `ice` and `steam` for `k=solid` is large and for `k=gas` is small, indicating that `solid` is related to ice but `gas` is irrelevant. On the other hand, `water` is not a discriminating element between `ice` and `steam`, and therefore the ratio is close to one. The same applies to an irrelevant word like `fashion`, where the ratio is once again close to one.\n",
        "\n",
        "The GloVe model is built on the idea that the \"ratio of conditional probabilities represents the word meanings\", and a neural model is trained to estimate this conditional probability.\n",
        "The GloVe model primarily aims to estimate the following function $F$:\n",
        "\n",
        "$F(w_i,w_j,\\tilde{w_k})=\\frac{P_{ik}}{P_{jk}}$ -> the right-hand side is computed from the corpus statistics, $w$ is the word vector and $\\tilde{w_k}$ is the context vector.\n",
        "\n",
        "The GloVe model embeds the words in a vector space and claims that the difference between these vectors (distinguishing factor) is hidden in the ratio of probabilities. In vector space, the best way to encode this is by vector differences.\n",
        "In other words, GloVe trains a neural model such that the difference between the word vectors encodes the ratio of probabilities.\n",
        "\n",
        "$F((w_i-w_j),\\tilde{w_k})=\\frac{P_{ik}}{P_{jk}}$\n",
        "\n",
        "At this point, the left side is a vector and the right-hand side is a scalar showing the similarity of $i$ and $j$ in relation to the context word $k$.\n",
        "For both sides to become scalar values, the left-hand side becomes a dot product. This is convenient since similarity in vector space is often encoded via a dot product.\n",
        "\n",
        "$F((w_i-w_j)^{T}.\\tilde{w_k})=\\frac{P_{ik}}{P_{jk}}=\\frac{F(w_i.\\tilde{w_k})}{F(w_j.\\tilde{w_k})} $\n",
        "\n",
        "The nominator and the denominator are probabilities that can be computed using the co-occurrence matrix.\n",
        "\n",
        "$F(w_i.\\tilde{w_k})=\\frac{X_{ik}}{X_{i}}$ -> where $X$ is the co-occurrence matrix\n",
        "\n",
        "There are multiple choices for the function $F$. To satisfy a symmetrical relationship (a.k.a. relation(a, b) = relation(b, a)), $F$ is chosen to be an exponential function, $F(x)=exp(x)$. As a result:\n",
        "\n",
        "$w_i.\\tilde{w_k} = log(P_{ik} ) = log(X_{ik} ) − log(X_{i})$\n",
        "\n",
        "$log(X_{i})$ is independent of $k$ and can be absorbed into a bias term (a constant number).\n",
        "\n",
        "$w_i.\\tilde{w_k} = log(P_{ik} ) + b_i +\\tilde{b_k} = log(X_{ik} )$\n",
        "\n",
        "After some weighting and alterations the final cost function, based on the weighted least squares regression model is as follows:\n",
        "\n",
        "$J= \\Sigma^{V}_{i,j=1} f(X_{ij})(w_i.w_j+ b_i +b_j)-log(X_{ik})$\n",
        "\n",
        "For a detailed overview refer to the original paper: https://nlp.stanford.edu/pubs/glove.pdf\n",
        "You need to read and understand the GloVe model in order to solve this exercise.\n"
      ],
      "id": "76e8f6f0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d71fbc8"
      },
      "source": [
        "### Subtask 1: Cost function"
      ],
      "id": "0d71fbc8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "174acc1f"
      },
      "source": [
        "Read the paper and describe the following, in your own words:\n",
        "\n",
        "1. The intuition behind the weighting schema in the cost function. # 1 point\n",
        "\n",
        "2. How does the objective function of GloVe relate to the objective function of the (word2vec) skip-gram model? # 1 point"
      ],
      "id": "174acc1f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Point distribution ✅\n",
        "- 1 point each for two questions, i.e., 2 points if answer comprises of the highlighted text or text having interpretation similar to highligted text\n",
        "- Otherwise zero points"
      ],
      "metadata": {
        "id": "RSa41fc_lOGV"
      },
      "id": "RSa41fc_lOGV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dac9d93"
      },
      "source": [
        "**Answer :**\n",
        "\n",
        "1) A main drawback to this model is that it **weighs all co-occurrences equally, even those that happen rarely or never**. Such rare cooccurrences are noisy and carry less information\n",
        "than the more frequent ones.\n",
        "$f$ is a weighting function that acts to reduce the impact of frequent co-occurring terms which is a drawback discussed above.\n",
        "When the co-occurrence count is higher or equal to a threshold, say 100, the weight will be 1. Otherwise, the weight will be smaller, subject to the co-occurrence count."
      ],
      "id": "9dac9d93"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1e3887c"
      },
      "source": [
        "2) The Word2vec model attempts to maximize the log probability as a context window scans over the corpus. The global objective function is:\n",
        "\n",
        "$J= - \\Sigma_{i \\in corpus, j \\in context(i)} log \\frac{w_i^T.\\tilde{w_j}}{\\Sigma^{V}_{k=1} w_i^T.\\tilde{w_k}}=- \\Sigma_{i \\in corpus, j \\in context(i)} log Q_{ij}$\n",
        "\n",
        "The denominator is a sum over the vocabulary and is very costly, therefore, word2vec comes up with an approximation for this with negative sampling.\n",
        "However, the **fraction can be efficiently estimated if we first group together those terms that have the same values for $i$ and $j$. We can get the co-occurrences from the co-occurrence matrix**.\n",
        "\n",
        "$J= - \\Sigma_{i}^{V}\\Sigma_{j}^{V}X_{ij}Q_{ij}$\n",
        "\n",
        "Moreover, the number of occurrences of $i$ is given by a row in $X$ as $X_i=\\Sigma_k X_{ik}$ and $P_{ij}= \\frac{X_{ij}}{X_i}$. Then we can re-write $J$ as:\n",
        "\n",
        "$J= - \\Sigma_{i}^{V} X_i\\Sigma_{j}^{V}P_{ij}Q_{ij} = \\Sigma_{i}^{V} X_i H(P_i,Q_i) $\n",
        "\n",
        "Where $H$ is the cross entropy of $P$ and $Q$. The whole equation is a weighted cross entropy, which bears some formal resemblance to the weighted least squares objective of GloVe."
      ],
      "id": "d1e3887c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####${\\color{red}{Comments\\ 4.1}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ],
      "metadata": {
        "id": "GKTQL0EsxSUU"
      },
      "id": "GKTQL0EsxSUU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "988ae19d"
      },
      "source": [
        "### Subtask 2: Build Co-occurence matrix\n"
      ],
      "id": "988ae19d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edca833e"
      },
      "source": [
        "We use the same dataset as in the first task and use the `quotes` as the corpus to build the co-occurrence matrix. Similar to the first task, we use the `phrases` to transform our input and extract the vocabulary that also includes bi-grams."
      ],
      "id": "edca833e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bca0b788"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from math import log\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "from tqdm import tqdm"
      ],
      "id": "bca0b788"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dacc60e8"
      },
      "source": [
        "Complete the function to create dictionaries used for mapping ids to words and words to ids, and a dictionary that counts the number of occurrences of each word. The first two dictionaries are used to map indices in vector space to words and back, the third dictionary contains the counts of the corpus statistics."
      ],
      "id": "dacc60e8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09dc3179"
      },
      "outputs": [],
      "source": [
        "def create_vocab(corpus): # 1 point\n",
        "  \"\"\"\n",
        "  Build a vocabulary containing the frequencies\n",
        "  corpus: the list of tokenized lines form the corpus\n",
        "\n",
        "  Returns  dictionaries `word` -> (index or unique identified), frequency)`\n",
        "  and `word` -> (index or unique identified)\n",
        "  and index or unique identified -> `word`\n",
        "  and length of the vocab\n",
        "  \"\"\"\n",
        "\n",
        "  word_count_dict = {}\n",
        "  id_to_word={}\n",
        "  word_to_id={}\n",
        "  count = Counter(chain(*corpus))\n",
        "  for index,word in enumerate(count.keys()):\n",
        "      id_to_word[index]= word\n",
        "      word_to_id[word]=index\n",
        "      word_count_dict[index]= count[word]\n",
        "\n",
        "\n",
        "  return word_count_dict,id_to_word,word_to_id, len(word_count_dict)"
      ],
      "id": "09dc3179"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fb4ca54"
      },
      "outputs": [],
      "source": [
        "word_count_dict, id_to_word,word_to_id, vocab_size=create_vocab(new_lines)"
      ],
      "id": "9fb4ca54"
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.5 points is the total tokens are correct and 0.5 points if the number of occurrance are correct"
      ],
      "metadata": {
        "id": "qQ6mSeBcvdZs"
      },
      "id": "qQ6mSeBcvdZs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlX4ugx88_v7"
      },
      "source": [
        "If you have done the exercise correctly, you have `15333` tokens in the vocabulary, and the number of occurrences for `joey` is `1951` and for `central_perk` is `36`"
      ],
      "id": "GlX4ugx88_v7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8b31695",
        "outputId": "10bf3c54-c044-4bdc-b318-c9b7bbca24a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of occurances for joey: 1879\n",
            "number of occurances for central perk: 36\n",
            "vocab size is: 17234\n"
          ]
        }
      ],
      "source": [
        "print(\"number of occurances for joey:\",word_count_dict[word_to_id['joey']])\n",
        "print(\"number of occurances for central perk:\",word_count_dict[word_to_id['central_perk']])\n",
        "print(\"vocab size is:\",vocab_size)"
      ],
      "id": "c8b31695"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b027280a"
      },
      "outputs": [],
      "source": [
        "def calculate_weight(cooccurrences, context_word_ids, center_word_id, side):  # 1.25 point\n",
        "    \"\"\"\n",
        "    Calculate the weight in the co-occurrence matrix based on the distance of a word\n",
        "    to the center word\n",
        "    sentence = [I, went, to , the, bank]\n",
        "    Let center word be \"to\" and window size =2\n",
        "    left_context =[I,went]\n",
        "    right_context = [the,bank]\n",
        "\n",
        "    Weights:\n",
        "    1/distance-> `went` and `the` have weight of 1 and `I` and `bank` have weight of 1/2\n",
        "    \"\"\"\n",
        "\n",
        "    if side == \"right_context\":\n",
        "        context_word_ids.reverse()\n",
        "\n",
        "    #len of context_word_ids will be used while calculating distance\n",
        "    number_of_context_words = len(context_word_ids)\n",
        "\n",
        "    for i in range(number_of_context_words):\n",
        "        distance = number_of_context_words - i\n",
        "        weight = 1 / float(distance)\n",
        "\n",
        "        #center word will act as the row and the context word is the column\n",
        "        cooccurrences[center_word_id, context_word_ids[i]] += weight\n",
        "\n",
        "    return cooccurrences\n",
        "\n"
      ],
      "id": "b027280a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUF8zXO59P3b"
      },
      "source": [
        "The weight of first to second element on the example matrix should be `1.0`."
      ],
      "id": "MUF8zXO59P3b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7c32c0e",
        "outputId": "49bf793d-34d6-46f0-9959-bfe63cfb72ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight of id=0 to id=1 : 1.0\n"
          ]
        }
      ],
      "source": [
        "cooccurrences = sparse.lil_matrix((10, 10),dtype=np.float64)\n",
        "calculate_weight(cooccurrences, [1,3,5], 0, side=\"right_context\")\n",
        "print(\"weight of id=0 to id=1 :\",cooccurrences[0,1])"
      ],
      "id": "d7c32c0e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.25 points if weight of id=0 to id=1 : 1.0.\n",
        "- 0.5 points if the weights are 1 / float(distance)\n",
        "- 0.5 points if the position of the context and center word are correct in the matrix"
      ],
      "metadata": {
        "id": "-chcb4KOvrJV"
      },
      "id": "-chcb4KOvrJV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ede37e8"
      },
      "source": [
        "We build co-occurrence as a sparse matrix to speed up the computation. The original matrix is a square matrix in the size of the vocabulary. However, many words do not co-occur with one another and we do not need to store those elements."
      ],
      "id": "0ede37e8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d63f39e7"
      },
      "outputs": [],
      "source": [
        "def build_cooccur(corpus, window_size=3, min_count=5): # 2 points\n",
        "    \"\"\"\n",
        "    Create a coocurrance matrix given a corpus\n",
        "    corpus: the list of tokenized lines form the corpus\n",
        "    window_size: how many words to right and left to consider\n",
        "\n",
        "    Returns the co-oocurrance sparse matrix\n",
        "    \"\"\"\n",
        "    vocab, id_to_word,word_to_id, vocab_size= create_vocab(corpus)\n",
        "\n",
        "    #sparse lil_matrix is optimized to operate on matrix which mostly has zeros.\n",
        "    cooccurrences = sparse.lil_matrix((vocab_size, vocab_size),dtype=np.float64)\n",
        "\n",
        "    for i, line in enumerate(corpus):\n",
        "\n",
        "        #Get the ID of words from vocab dictionary\n",
        "        word_ids = [word_to_id[word] for word in line]\n",
        "\n",
        "        for i, center_word_id in enumerate(word_ids):\n",
        "            #left side context\n",
        "            left_context_word_ids  = word_ids[max(0, i-window_size):i]\n",
        "\n",
        "            #right side context\n",
        "            right_context_word_ids = word_ids[i+1: i+window_size]\n",
        "\n",
        "            #update the matrix based on the distance weights on both sides\n",
        "            cooccurrences = calculate_weight(cooccurrences, left_context_word_ids, center_word_id,\"left_context\")\n",
        "            cooccurrences = calculate_weight(cooccurrences, right_context_word_ids, center_word_id,\"right_context\")\n",
        "\n",
        "    # go into the LiL-matrix to quickly iterate through all nonzero cells and filter out the ones with minimum count\n",
        "    cooccurrences_tuples = []\n",
        "    for i, (row, data) in enumerate(zip(cooccurrences.rows,cooccurrences.data)):\n",
        "        if min_count is not None and vocab[i] < min_count:\n",
        "            continue\n",
        "\n",
        "        for data_idx, j in enumerate(row):\n",
        "            if min_count is not None and vocab[j] < min_count:\n",
        "                continue\n",
        "\n",
        "            cooccurrences_tuples.append((i, j, float(data[data_idx])))\n",
        "    return cooccurrences_tuples\n"
      ],
      "id": "d63f39e7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3aba5bd"
      },
      "source": [
        "Build a matrix with a window_size of 3 words, and the minimum number of times a word has to occur to be part of the matrix is 10."
      ],
      "id": "f3aba5bd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69c198e0",
        "outputId": "bc632ec5-8fba-459d-a291-a1deaee3a94b",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 998, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "matrix=build_cooccur(new_lines, window_size=3, min_count=10)\n",
        "matrix[103]"
      ],
      "id": "69c198e0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.25 points if the output of matrix[103] is (0, 1136, 1.0)\n",
        "- 0.75 points if the left and right side context are choosen correctly from `word_ids`\n",
        "- 0.5 points for update of the matrix\n",
        "- 0.5 points for skipping words less than `min_count` and creating the correct format for the `cooccurrences_tuples`"
      ],
      "metadata": {
        "id": "H_tm9-v1wHZ3"
      },
      "id": "H_tm9-v1wHZ3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####${\\color{red}{Comments\\ 4.2}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ],
      "metadata": {
        "id": "6FZTPwd3xUyd"
      },
      "id": "6FZTPwd3xUyd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82cc1256"
      },
      "source": [
        "### Subtask 3: Modelling and Training\n"
      ],
      "id": "82cc1256"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91764831"
      },
      "source": [
        "We initialize the weights for the context and center words and learn the vectors through backprop, using the GloVe cost function.\n",
        "Make sure you use the correct weighting schema."
      ],
      "id": "91764831"
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.5 points for correct shape of the weight vectors\n",
        "- 0.5 points for correct shape of the biases"
      ],
      "metadata": {
        "id": "jHKSBxeBwwiZ"
      },
      "id": "jHKSBxeBwwiZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05251525"
      },
      "outputs": [],
      "source": [
        "# Random normal weights intialization\n",
        "np.random.seed(77)# we set a seed to have similiar results\n",
        "def init_weights(vocab_size, hidden):\n",
        "     #Each word has a center word vector and a context vector.\n",
        "    W_center = np.random.randn(vocab_size, hidden)\n",
        "    b_center = np.random.randn(vocab_size, 1)\n",
        "    W_context = np.random.randn(vocab_size, hidden)\n",
        "    b_context = np.random.randn(vocab_size, 1)\n",
        "    return W_center, b_center, W_context, b_context"
      ],
      "id": "05251525"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-mcYRis9wet"
      },
      "source": [
        "keep track of `W_center[0,1]` as it should change based on backprop in the next cells."
      ],
      "id": "y-mcYRis9wet"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2f31560",
        "outputId": "5dfaa724-8362-4a69-d6f8-a545656f64b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "check the shapes to make sure the matrices have correct sizes:\n",
            "(100, 32)\n",
            "(100, 1)\n",
            "(100, 32)\n",
            "(100, 1)\n",
            "Look at the value of this element and how it changes with back prob:\n",
            "0.6615314728168009\n"
          ]
        }
      ],
      "source": [
        "W_center, b_center, W_context, b_context=init_weights(100, 32)\n",
        "print(\"check the shapes to make sure the matrices have correct sizes:\")\n",
        "print(W_center.shape)\n",
        "print(b_center.shape)\n",
        "print(W_context.shape)\n",
        "print(b_context.shape)\n",
        "print(\"Look at the value of this element and how it changes with back prob:\")\n",
        "print(W_center[0,1])"
      ],
      "id": "b2f31560"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gae189dl-ZZo"
      },
      "source": [
        "Implement the backpropagation for the GloVe model, given the embedding of center and context words.\n",
        "Each step should go over the entire co-occurance matrix."
      ],
      "id": "gae189dl-ZZo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.25 points if the weight for W_center[0,1] is `0.6621378355253088`\n",
        "- 0.25 points if the cost is `0.20551776`\n",
        "- 0.5 points if the cost computation is correct\n",
        "- 1 point if the gradient computations are correct\n",
        "- 0.5 points if the update of the weight vectors are correct"
      ],
      "metadata": {
        "id": "7dq3uqPqxLWB"
      },
      "id": "7dq3uqPqxLWB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8669b4fb"
      },
      "outputs": [],
      "source": [
        "# Back Propagation\n",
        "def back_prop(W_center, b_center, W_context, b_context, matrix, x_max,  vocab_size, learning_rate,alpha=2):\n",
        "    \"\"\"\n",
        "    W_center, b_center: weight and bias of the center word\n",
        "    W_context, b_context: weight and bias of the context word\n",
        "    vocab_size: vocabulary size\n",
        "    x_max: define our weighting function when computing the cost for two word pairs; see the GloVe paper for more\n",
        "    details.\n",
        "    matrix: coocurrance matrix\n",
        "    alpha: the power of x_max function\n",
        "    learning_rate: learning rate for gradient descent\n",
        "    \"\"\"\n",
        "\n",
        "    global_cost = 0\n",
        "    for i, j, cooccurrence in matrix:\n",
        "            weight = ((cooccurrence/x_max)**alpha if(cooccurrence < x_max) else 1)\n",
        "            # Compute inner component of cost function  J = w_i^Tw_j + b_i + b_j - log(X_{ij})\n",
        "            cost_inner = np.dot(W_center[i], W_context[j]) + b_center[i] + b_context[j] - (np.log(cooccurrence) if(cooccurrence != 0) else 0)\n",
        "\n",
        "\n",
        "            # Compute cost J = f(X_{ij}) (J')^2\n",
        "            cost = weight*(cost_inner**2)\n",
        "            global_cost = global_cost + cost\n",
        "\n",
        "             # Compute gradients for word vectors\n",
        "            dW_center = weight*cost_inner*W_context[j]\n",
        "            dW_context = weight*cost_inner*W_center[i]\n",
        "            # Compute gradients for bias terms\n",
        "            db_center = weight*cost_inner\n",
        "            db_context = weight*cost_inner\n",
        "\n",
        "            #update the weights\n",
        "            W_center[i] = W_center[i] - learning_rate*dW_center\n",
        "            W_context[j] = W_context[j] - learning_rate*dW_context\n",
        "            b_center[i] = b_center[i] - learning_rate*db_center\n",
        "            b_context[j] = b_context[j] - learning_rate*db_context\n",
        "\n",
        "    return W_center, b_center, W_context, b_context, global_cost"
      ],
      "id": "8669b4fb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV9uZIgX95tZ"
      },
      "source": [
        "Based on the random seed, the value of `W_center[0,1]` should have changed due to backpropagation.\n"
      ],
      "id": "sV9uZIgX95tZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84a467a0",
        "outputId": "e65de9a8-f7ae-44f0-c290-e0506e800b8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost: [0.20551776]\n",
            "changed value:\n",
            "0.6621378355253088\n"
          ]
        }
      ],
      "source": [
        "test_matrix=[(0,1,1),(0,2,0.4),(0,3,0.9),(0,4,0.4)]\n",
        "W_center, b_center, W_context, b_context, global_cost  =back_prop(W_center, b_center, W_context, b_context, test_matrix, x_max=10,  vocab_size=100, learning_rate=0.01)\n",
        "print(\"cost:\",global_cost)\n",
        "print(\"changed value:\")\n",
        "print(W_center[0,1])"
      ],
      "id": "84a467a0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULLmdXvC-QpX"
      },
      "source": [
        "Write a training script for the GloVe model that goes over the entire co-occurrence matrix given a number of epochs."
      ],
      "id": "ULLmdXvC-QpX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.25 points if the correct parameters are passed\n"
      ],
      "metadata": {
        "id": "wE6tzKvXx2sh"
      },
      "id": "wE6tzKvXx2sh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12dd535b"
      },
      "outputs": [],
      "source": [
        "def train_GloVe(matrix, vocab_size, epochs = 10, learning_rate = 0.0001, x_max = 10, hidden_dim=100):\n",
        "    \"\"\"\n",
        "    Train the glove model based the co-ocurrance matrix for a number of epochs\n",
        "    matrix: co-occcurance matrix\n",
        "    vocab_size: number of words in vocab\n",
        "    epochs: number of passes through the data\n",
        "    learning_rate: learning rate for back prop\n",
        "    x_max: parameter of the weighting function\n",
        "    hidden_dim: dimension of the vectors\n",
        "    \"\"\"\n",
        "    W_center, b_center, W_context, b_context = init_weights(vocab_size, hidden_dim)\n",
        "    for i in tqdm(range(epochs)):\n",
        "        W_center, b_center, W_context, b_context, global_cost = back_prop(W_center, b_center, W_context, b_context, matrix, x_max, vocab_size, learning_rate)\n",
        "        print(global_cost)\n",
        "    return W_center, W_context"
      ],
      "id": "12dd535b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0116c62"
      },
      "source": [
        "Train the model with hidden dimension of `100` and learning rate of `0.001` for a `100` epochs."
      ],
      "id": "e0116c62"
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.25 points if the correct parameters are passed"
      ],
      "metadata": {
        "id": "igNl9tpux916"
      },
      "id": "igNl9tpux916"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2355c76",
        "outputId": "775ff5cc-b5ac-4d42-dcf8-c1e7f9afed26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 1/100 [00:13<22:18, 13.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1515265.06361148]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 2/100 [00:26<22:01, 13.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[994326.86037132]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 3/100 [00:40<21:57, 13.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[754064.98571399]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 4/100 [00:54<21:58, 13.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[616206.66300626]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 5/100 [01:09<22:07, 13.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[527039.12543357]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 6/100 [01:23<21:54, 13.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[464651.90234449]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 7/100 [01:36<21:27, 13.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[418472.58560133]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 8/100 [01:50<21:07, 13.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[382803.9135298]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 9/100 [02:03<20:47, 13.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[354321.66068234]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 10/100 [02:17<20:38, 13.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[330964.77900272]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 11/100 [02:31<20:17, 13.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[311392.05346204]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 12/100 [02:44<20:03, 13.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[294695.27899088]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 13/100 [02:58<19:58, 13.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[280238.5244389]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 14/100 [03:12<19:42, 13.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[267563.54980622]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 15/100 [03:26<19:25, 13.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[256331.83155835]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 16/100 [03:39<19:03, 13.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[246287.76799558]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 17/100 [03:53<18:50, 13.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[237234.61121584]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 18/100 [04:06<18:32, 13.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[229018.3003776]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▉        | 19/100 [04:20<18:18, 13.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[221516.34304338]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 20/100 [04:33<18:05, 13.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[214630.00466661]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██        | 21/100 [04:49<18:35, 14.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[208278.71587877]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 22/100 [05:03<18:17, 14.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[202395.99748114]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 23/100 [05:16<17:53, 13.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[196926.44365613]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 24/100 [05:30<17:45, 14.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[191823.45576652]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 25/100 [05:44<17:30, 14.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[187047.51699375]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 26/100 [06:01<18:15, 14.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[182564.86238977]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 27/100 [06:25<21:13, 17.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[178346.44194115]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 28/100 [06:38<19:36, 16.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[174367.10349637]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▉       | 29/100 [06:52<18:23, 15.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[170604.94259963]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 30/100 [07:06<17:27, 14.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[167040.78041095]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███       | 31/100 [07:19<16:46, 14.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[163657.74092168]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 32/100 [07:33<16:11, 14.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[160440.90587688]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 33/100 [07:48<16:02, 14.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[157377.03105033]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▍      | 34/100 [08:02<15:58, 14.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[154454.31136247]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 35/100 [08:17<15:47, 14.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[151662.18518579]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 36/100 [08:31<15:22, 14.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[148991.17032047]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 37/100 [08:45<14:49, 14.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[146432.72574125]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 38/100 [08:58<14:24, 13.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[143979.13445062]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39%|███▉      | 39/100 [09:12<14:10, 13.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[141623.40372305]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 40/100 [09:26<13:57, 13.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[139359.17976116]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 41%|████      | 41/100 [09:40<13:40, 13.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[137180.67435955]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 42/100 [09:53<13:19, 13.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[135082.60162439]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 43/100 [10:07<13:01, 13.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[133060.12315493]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 44/100 [10:20<12:41, 13.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[131108.80037799]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 45/100 [10:34<12:33, 13.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[129224.55295581]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 46/100 [10:48<12:21, 13.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[127403.62237114]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 47/100 [11:02<12:08, 13.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[125642.53994356]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 48/100 [11:16<11:59, 13.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[123938.0986523]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 49%|████▉     | 49/100 [11:30<11:49, 13.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[122287.32824064]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 50/100 [11:44<11:36, 13.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[120687.47315886]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 51%|█████     | 51/100 [11:58<11:22, 13.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[119135.97297053]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 52/100 [12:12<11:04, 13.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[117630.44490309]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 53/100 [12:25<10:48, 13.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[116168.6682704]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▍    | 54/100 [12:39<10:29, 13.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[114748.57053435]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 55/100 [12:52<10:15, 13.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[113368.21480523]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 56/100 [13:06<09:59, 13.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[112025.78860858]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 57/100 [13:19<09:42, 13.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[110719.59376936]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 58/100 [13:33<09:30, 13.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[109448.03728446]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 59%|█████▉    | 59/100 [13:46<09:16, 13.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[108209.62307133]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 60/100 [14:00<09:04, 13.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[107002.94449497]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61%|██████    | 61/100 [14:14<08:50, 13.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[105826.67758798]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 62/100 [14:27<08:37, 13.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[104679.57488891]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63%|██████▎   | 63/100 [14:41<08:21, 13.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[103560.4598334]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 64/100 [14:54<08:09, 13.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[102468.22164023]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 65/100 [15:09<08:00, 13.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101401.81064182]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▌   | 66/100 [15:23<07:52, 13.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100360.23401382]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 67/100 [15:37<07:44, 14.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[99342.55186442]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 68/100 [15:51<07:27, 13.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[98347.87364787]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 69%|██████▉   | 69/100 [16:05<07:09, 13.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[97375.35487108]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 70/100 [16:18<06:53, 13.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[96424.19406527]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████   | 71/100 [16:32<06:38, 13.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[95493.62999788]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 72/100 [16:46<06:25, 13.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[94582.9391025]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 73/100 [16:59<06:10, 13.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[93691.433107]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▍  | 74/100 [17:13<05:55, 13.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[92818.45684194]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 75/100 [17:26<05:40, 13.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[91963.38621337]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 76/100 [17:40<05:28, 13.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[91125.62632561]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 77%|███████▋  | 77/100 [17:54<05:13, 13.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[90304.60974099]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 78/100 [18:07<04:59, 13.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[89499.79486505]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 79%|███████▉  | 79/100 [18:21<04:45, 13.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[88710.66444646]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 80/100 [18:34<04:31, 13.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[87936.72418226]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81%|████████  | 81/100 [18:48<04:18, 13.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[87177.50141975]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 82/100 [19:02<04:04, 13.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[86432.5439472]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 83/100 [19:16<03:52, 13.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[85701.41886633]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 84/100 [19:30<03:40, 13.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[84983.71153998]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 85/100 [19:44<03:27, 13.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[84279.02460926]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 86/100 [19:58<03:14, 13.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[83586.9770747]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 87/100 [20:11<03:00, 13.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[82907.20343658]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 88/100 [20:25<02:45, 13.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[82239.35289]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89%|████████▉ | 89/100 [20:38<02:30, 13.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[81583.08857055]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 90/100 [20:52<02:16, 13.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[80938.0868469]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91%|█████████ | 91/100 [21:05<02:02, 13.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[80304.03665691]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 92/100 [21:19<01:48, 13.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[79680.63888412]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 93/100 [21:33<01:35, 13.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[79067.60577166]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 94/100 [21:46<01:21, 13.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[78464.6603711]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 95/100 [22:00<01:07, 13.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[77871.5360237]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 96/100 [22:13<00:54, 13.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[77287.97587182]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|█████████▋| 97/100 [22:27<00:40, 13.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[76713.73239854]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|█████████▊| 98/100 [22:40<00:27, 13.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[76148.56699342]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 99%|█████████▉| 99/100 [22:54<00:13, 13.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[75592.24954286]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [23:07<00:00, 13.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[75044.5580432]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "W_center, W_context = train_GloVe(matrix, vocab_size, 100, 0.001) #takes about 15 minutes"
      ],
      "id": "b2355c76"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "770701f4"
      },
      "source": [
        "As you can see by looking at the loss, the model still needs more time to converge to a minimum.\n",
        "However, we keep the training short and keeping in mind that the vectors can improve we look at some examples.\n",
        "Take the average, transpose, and normalize the matrix."
      ],
      "id": "770701f4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b272ab8a"
      },
      "outputs": [],
      "source": [
        "from numpy.linalg import norm\n",
        "# take the average of the learned vector as the final vector\n",
        "W = np.add(W_center, W_context)/2\n",
        "W = W.T\n",
        "W = W/norm(W)"
      ],
      "id": "b272ab8a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "334aff38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76375bba-381c-46b0-ef01-3cdcf901ec6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 17234)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "W.shape"
      ],
      "id": "334aff38"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d106c990"
      },
      "source": [
        "Lets create a dictionary that points from a word to its vector."
      ],
      "id": "d106c990"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db81b66e"
      },
      "outputs": [],
      "source": [
        "# Generates word to word embedding dictionary\n",
        "word_to_vector = {}\n",
        "for word in word_to_id.keys():\n",
        "    word_to_vector[word] = W[:, word_to_id[word]]"
      ],
      "id": "db81b66e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####${\\color{red}{Comments\\ 4.3}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ],
      "metadata": {
        "id": "guKG_LCyxYwb"
      },
      "id": "guKG_LCyxYwb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11c86eeb"
      },
      "source": [
        "### Subtask 4: Comparison with Skip-gram\n"
      ],
      "id": "11c86eeb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8fd8763"
      },
      "source": [
        "Let's compute the similarities for the same words in Task 1 to compare the results with word2vec. This time you need to implement the similarity function, based on the dot product. To get to the topk you need to sort the elements based on their similarity.\n",
        "\n"
      ],
      "id": "e8fd8763"
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.5 points if the dot product is used and some kind of argmax\n",
        "- 0.25 points if `grub` or `speech` or `follow` or `paleonthologist` is one of the most similiar one to `central_perk`\n",
        "- 0.25 points if `approvingly` or `sleeperson` or `perceived` or `lottery`  is one of the most similiar one to `joey`"
      ],
      "metadata": {
        "id": "SsaAHvIqyAcm"
      },
      "id": "SsaAHvIqyAcm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2f3f722"
      },
      "outputs": [],
      "source": [
        "from numpy import dot\n",
        "\n",
        "def most_similar(word_vector,all_vectors,id_to_word, topk): # 1 point\n",
        "    \"\"\"\n",
        "    function to find the topk most similar words to a word vector\n",
        "    word_vector: vector of the search word\n",
        "    all_vectors: all word vectors\n",
        "    id_to_word: dictionary from id to words\n",
        "    topk: number of elements to return\n",
        "    \"\"\"\n",
        "    sim= dot(word_vector, all_vectors) # vectors are normalized so the dot product is equal to cosine similarity\n",
        "    words_ids=np.argpartition(sim,-topk)[-topk:]\n",
        "    np.argmax(sim)\n",
        "    word =[(id_to_word[i],sim[i]) for i in words_ids]\n",
        "    word.reverse()\n",
        "    return word"
      ],
      "id": "a2f3f722"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53c1a84a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a8ded51-dc13-4c7b-93e0-a37ac2f90145"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('central_perk', 5.977919192387161e-05),\n",
              " ('grub', 2.4675323918191684e-05),\n",
              " ('follow', 2.2840512822683808e-05),\n",
              " ('speech', 2.2740853591069154e-05),\n",
              " ('paleonthologist', 2.202024243400331e-05)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "most_similar(word_to_vector[\"central_perk\"],W,id_to_word,5)"
      ],
      "id": "53c1a84a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c8e8519",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8963b98f-2dc3-4739-9f3d-312351ea013e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('approvingly', 2.2121175411037674e-06),\n",
              " ('sleeperson', 2.2287440098693597e-06),\n",
              " ('perceived', 2.200112058361667e-06),\n",
              " ('lottery', 2.175875925285404e-06),\n",
              " ('deluxe', 2.164907674345203e-06)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "most_similar(word_to_vector[\"joey\"],W,id_to_word,5)"
      ],
      "id": "2c8e8519"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bdcd5f5"
      },
      "source": [
        "Compute the similarity between the `('rachel', 'mrs_green')`, `('smelly_cat', 'song')` and `('ross', 'spaceship')`."
      ],
      "id": "6bdcd5f5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Point distribution ✅\n",
        "- 0.5 points if they use dot product for similiarity\n",
        "- 0.25 points if the absolute value of  `ross` to `spaceship` is lower than all others"
      ],
      "metadata": {
        "id": "nQTTfmZG6N34"
      },
      "id": "nQTTfmZG6N34"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e77f01a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9db21378-f5aa-4185-d607-3e4b8d7a6cd2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1.0040189369103668e-06"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "word_to_vector['rachel'].dot(word_to_vector['mrs_green']) #0.25 point"
      ],
      "id": "e77f01a5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1def847",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69053af1-24ee-41b9-eb98-2300dab1d19f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.871731423755601e-06"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "word_to_vector['smelly_cat'].dot(word_to_vector['song']) #0.25 point"
      ],
      "id": "d1def847"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "933ca3ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eefb452f-55a3-47e4-ccd3-e6b051322fa0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.156815602953935e-07"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "word_to_vector['ross'].dot(word_to_vector['spaceship']) #0.25 point"
      ],
      "id": "933ca3ac"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecace919"
      },
      "source": [
        "If you see your results are not as meaningful as the gensim model, do not be discouraged. With better optimization and longer training the results should improve. If you have time play around a bit more with your model and see if you can generate more meaningful vectors."
      ],
      "id": "ecace919"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####${\\color{red}{Comments\\ 4.4}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ],
      "metadata": {
        "id": "O0RktPntxbco"
      },
      "id": "O0RktPntxbco"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X661TkR7xcRu"
      },
      "id": "X661TkR7xcRu",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}