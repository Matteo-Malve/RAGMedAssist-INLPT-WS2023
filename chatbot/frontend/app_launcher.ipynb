{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Locally (VScode)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0m\n","\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n","\u001b[0m\n","\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n","\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://192.168.0.223:8501\u001b[0m\n","\u001b[0m\n","/Users/matteom/miniconda3/envs/torch-test/lib/python3.11/site-packages/langchain/__init__.py:29: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n","  warnings.warn(\n","/Users/matteom/miniconda3/envs/torch-test/lib/python3.11/site-packages/langchain/retrievers/__init__.py:46: LangChainDeprecationWarning: Importing this retriever from langchain is deprecated. Importing it from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n","\n","`from langchain_community.retrievers import BM25Retriever`.\n","\n","To install langchain-community run `pip install -U langchain-community`.\n","  warnings.warn(\n","cuda is available: False\n","mps is available: True\n","Loading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]\n","2024-03-03 19:02:41.569 Uncaught app exception\n","Traceback (most recent call last):\n","  File \"/Users/matteom/miniconda3/envs/torch-test/lib/python3.11/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 535, in _run_script\n","    exec(code, module.__dict__)\n","  File \"/Users/matteom/shared-folder/nlpt_group/chatbot/frontend/app.py\", line 16, in <module>\n","    chatbot = MedicalChatbot(cfg)\n","              ^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/matteom/shared-folder/nlpt_group/chatbot/frontend/../app/custom_chatbot.py\", line 70, in __init__\n","    self.qa_chain = self.init_qa_chain()\n","                    ^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/matteom/shared-folder/nlpt_group/chatbot/frontend/../app/custom_chatbot.py\", line 214, in init_qa_chain\n","    llm = self.get_llm()\n","          ^^^^^^^^^^^^^^\n","  File \"/Users/matteom/shared-folder/nlpt_group/chatbot/frontend/../app/custom_chatbot.py\", line 160, in get_llm\n","    self.llm = self.init_llm_pipeline()\n","               ^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/matteom/shared-folder/nlpt_group/chatbot/frontend/../app/custom_chatbot.py\", line 203, in init_llm_pipeline\n","    model=self.init_model(),\n","          ^^^^^^^^^^^^^^^^^\n","  File \"/Users/matteom/shared-folder/nlpt_group/chatbot/frontend/../app/custom_chatbot.py\", line 179, in init_model\n","    return AutoModelForCausalLM.from_pretrained(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/matteom/miniconda3/envs/torch-test/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 561, in from_pretrained\n","    return model_class.from_pretrained(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/matteom/miniconda3/envs/torch-test/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3502, in from_pretrained\n","    ) = cls._load_pretrained_model(\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/matteom/miniconda3/envs/torch-test/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3926, in _load_pretrained_model\n","    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n","                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/matteom/miniconda3/envs/torch-test/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 805, in _load_state_dict_into_meta_model\n","    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)\n","  File \"/Users/matteom/miniconda3/envs/torch-test/lib/python3.11/site-packages/accelerate/utils/modeling.py\", line 384, in set_module_tensor_to_device\n","    new_value = value.to(device)\n","                ^^^^^^^^^^^^^^^^\n","TypeError: BFloat16 is not supported on MPS\n","^C\n","\u001b[34m  Stopping...\u001b[0m\n"]}],"source":["!streamlit run app.py"]},{"cell_type":"markdown","metadata":{},"source":["### Colab:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C9-Tvndw6QLk"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%cd <path_to_your__/chatbot/frontend__folder>\n","# ex. /content/drive/MyDrive/TEST-NLP/nlpt_group/chatbot/frontend"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install -r ../../requirements/requirements-cuda.txt"]},{"cell_type":"markdown","metadata":{},"source":["The password to the tunnel is the ip address printed by the next cell"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":419518,"status":"ok","timestamp":1708007453820,"user":{"displayName":"Seed 19","userId":"12712001482791270809"},"user_tz":-60},"id":"rjmmkXGm5nc8","outputId":"ec528bd8-0dda-42ce-f060-c7e1b0ca8c4f"},"outputs":[],"source":["!wget -q -O - ipv4.icanhazip.com\n","!streamlit run app.py & npx localtunnel --port 8501"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNEPMt/JiOkBBFnHAMU0nxR","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
