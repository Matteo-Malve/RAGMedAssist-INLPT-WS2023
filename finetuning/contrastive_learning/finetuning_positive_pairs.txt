["To summarize key research published by the author's group, dealing with skin photoaging and its photoprotection. Two methodologies (standard photographs and selfies imaging) resulting from referential skin ageing atlases were applied on 2487 subjects of different ancestries, ages and gender. These aimed at first to best assess and grade the variable severities of some facial signs (texture/wrinkles, pigmentary spots, sagging and vascular disorders) that occur progressively during the photoaging process. Second, such methodologies were used to record the benefits brought by a photoprotective regimen (671 women and men). In all studied ancestries, each facial sign show an increase severity along decades, at its own pace, some showing a linear like progression, whereas some plateau at early ages. These changes differed according to skin innate colour (phototype) and with individual behaviours vis-a-vis sun exposures, more so among European women than North-East Asian women. An effect of gender (less severe alterations) was observed on Chinese men, as compared to Chinese women. Pigmentary disorders were found hallmarks of photo-aged North-East Asian women. Globally, photoaging impact the apparent age of the different subjects. The counteracting effects of strong topical Photoprotective regimens were evidenced along a 6-month seasonality in Chinese and French women or 1-year period in Brazilian women with phototypes II to VI. Photoprotection led to a more even skin complexion among Indian subjects with pigment irregularities. Two factors clearly hamper a better assessment of the efficiency of photoprotection of the sun-induced cutaneous ageing: short durations (months) of the studies with regard to the global fate of photoaging by decades and the number of studied subjects, limited to hundreds for practical reasons. The methodology based on the automatic analysis of facial signs through selfies images could theoretically bypass both factors, allowing thousands of subjects to be studied along years.", "The author's group published research on skin photoaging and its photoprotection. 2487 subjects of different ages and genders were photographed using the two methodologies created by referential skin aging Atlases. The goal of these is to assess and grade the variable severities of some facialsigns that occur in the photoaging process. The benefits of a photoprotective regimen were recorded using such methodologies. In all studied ancestries, facial signs show an increase severity at their own pace, some showing a linear progression and others plateauing at early ages. European women tend to have higher sun exposure than North East Asian women. There was a gender difference between Chinese men and Chinese women. North-East Asian women have a lot of wrinkling in their skin. The age of the different subjects is affected by photoaging. During the Chinese and French winter months and in Brazil during the Brazilian summer, it was seen that strong Photoprotective remedies counteracted their effects. Protection from the sun lead to a more even complexion among Indian subjects. There are two factors that can affect the efficiency of photoprotection of the sun- induced cutaneous ageing: short duration and limited number of studied subjects, which means that the assessment of the global fate of photoaging by decades can't be made. Thousands of people can be studied over many years using the automatic analysis of facial signs using selfies."]

['Deep learning is a branch of artificial intelligence where networks of simple interconnected units are used to extract patterns from data in order to solve complex problems. Deep-learning algorithms have shown groundbreaking performance in a variety of sophisticated tasks, especially those related to images. They have often matched or exceeded human performance. Since the medical field of radiology mainly relies on extracting useful information from images, it is a very natural application area for deep learning, and research in this area has rapidly grown in recent years. In this article, we discuss the general context of radiology and opportunities for application of deep-learning algorithms. We also introduce basic concepts of deep learning, including convolutional neural networks. Then, we present a survey of the research in deep learning applied to radiology. We organize the studies by the types of specific tasks that they attempt to solve and review a broad range of deep-learning algorithms being utilized. Finally, we briefly discuss opportunities and challenges for incorporating deep learning in the radiology practice of the future. Level of Evidence: 3 Technical Efficacy: Stage 1 J. Magn. Reson. Imaging 2019;49:939-954.', 'Deep learning is an artificial intelligence method that uses small networks of connected units to solve complex problems. Performance in a variety of tasks have been shown by deep-learning programs. They have exceeded human performance. In recent years, research in the area of radiology has grown faster than any other area. We discuss the general context of Radiology in this article. Basic concepts of deep learning are introduced by us. We surveyed the research that was done with deep learning in the field of Radiology. It is a requirement that the studies are organized by the type of particular tasks that they attempt to solve and the wide range of deep- learning technology being utilized. There are opportunities and challenges when it comes to incorporating deep learning in the future of radiology practices. Stage 1 J has a level of evidence of 3 technical Efficacy. Magn is a person. Reson.']

['The application of artificial intelligence and machine learning (ML) technologies in healthcare have immense potential to improve the care of patients. While there are some emerging practices surrounding responsible ML as well as regulatory frameworks, the traditional role of research ethics oversight has been relatively unexplored regarding its relevance for clinical ML. In this paper, we provide a comprehensive research ethics framework that can apply to the systematic inquiry of ML research across its development cycle. The pathway consists of three stages: (1) exploratory, hypothesis-generating data access; (2) silent period evaluation; (3) prospective clinical evaluation. We connect each stage to its literature and ethical justification and suggest adaptations to traditional paradigms to suit ML while maintaining ethical rigor and the protection of individuals. This pathway can accommodate a multitude of research designs from observational to controlled trials, and the stages can apply individually to a variety of ML applications.', "The use of artificial intelligence and machine learning has great potential in healthcare to improve patients' care. The traditional role of research ethics oversight has been largely overlooked regarding it's relevance for clinical ML. The framework we give in this paper is designed to apply to the systematic inquiry of the research across the entire development cycle. The pathway consists of three stages. We suggest to the stage to adapt to traditional models in order to suit them while maintaining ethical rigor. The stages can apply to a variety of applications and the pathway can accommodate many different research designs."]

['Allergic diseases are increasing around the world with unprecedented complexity and severity. One of the reasons is that genetically modified crops produce new potentially allergenic proteins. From this starting point, many researchers have paid attention to the development of tools to predict the allergenicity of new proteins. In this study, a novel approach is introduced for the prediction of food allergens based on Artificial Intelligence techniques: a pairwise sequence alignment with the FASTA program for feature extraction and the use of the Deep Learning technique known as Restricted Boltzmann Machines in combination with the Decision Tree method for the prediction process. The developed tool, called ALLERDET (publicly available at http://allerdet.frangam.com), overcomes the state-of-the-art methods. The performance of our method is: 98.46% sensitivity, 94.37% specificity and 97.26% accuracy), on a data set built from several publicly available sources.', 'Allergic diseases are becoming more severe. New potentially allergenic Proteins are produced by genetically modified crops. From this beginning, researchers have been looking at the development of tools to predict theallergenicity of new drugs. The prediction of food allergens using Artificial Intelligence techniques such as a pairwise sequence alignment, Deep Learning techniques and a Decision Tree method was introduced in this study. ALLERDET is publically available at http://allerdet. Frangam. The methods are state-of-the-art. Our method has a sensitivity of 97.41%, a specificity of 94.31% and a accuracy of 97.26%.']

['Conductive hydrogels have attracted increasing attention because of their important application in flexible pressure sensors. However, designing hydrogels with a combination of excellent mechanical properties, high sensitivity, and good biocompatibility is still a profound challenge. Here we report a conductive and biocompatible PVA-Gelatin-nHAP hydrogel (PGHAP gel) by connecting a double network with inorganic nano-particles <i>via</i> ionic bonds. The as-prepared gel achieves excellent elasticity and good fatigue resistance even after 50 cycles of compression. Then a hydrogel pressure sensor was obtained using the as-prepared gel, exhibiting high pressure sensitivity almost linearly responding up to 1.5 kPa and adequate stability of the capacitance-pressure over 4 cycles. These results demonstrate the great potential applications of the hydrogel in biomedical devices, including artificial intelligence, human motion detection, and wearable devices.', "The important application of conductive hydrogels in flexible pressure sensors has attracted increased attention. It's not easy to design a product with the combination of mechanical, biocompatibility and sensitivity. The double network with the ion bonds were reported in the report. Even though the gel has 50 cycles of compression, it still has good elasticity and fatigue resistance. high pressure sensitivity almost linearly responds up to 1.5 kPa and adequate stability was achieved through the use of a gel that was prepared prior to use. The results show the potential applications of hydrogel in devices such as artificial intelligence, human motion detection, and Wearable devices."]

['There is a tendency toward nonoperative management of appendicitis resulting in an increasing need for preoperative diagnosis and classification. For medical purposes, simple conceptual decision-making models that can learn are widely used. Decision trees are reliable and effective techniques which provide high classification accuracy. We tested if we could detect appendicitis and differentiate uncomplicated from complicated cases using machine learning algorithms. We analyzed all cases admitted between 2010 and 2016 that fell into the following categories: healthy controls (Group 1); sham controls (Group 2); sham disease (Group 3), and acute abdomen (Group 4). The latter group was further divided into four groups: false laparotomy; uncomplicated appendicitis; complicated appendicitis without abscess, and complicated appendicitis with abscess. Patients with comorbidities and whose complete blood count and/or pathology results were lacking were excluded. Data were collected for demographics, preoperative blood analysis, and postoperative diagnosis. Various machine learning algorithms were applied to detect appendicitis patients. There were 7244 patients with a mean age of 6.84 ± 5.31 years, of whom 82.3% (5960/7244) were male. Most algorithms tested, especially linear methods, provided similar performance measures. We preferred the decision tree model due to its easier interpretability. With this algorithm, we detected appendicitis patients with 93.97% area under the curve (AUC), 94.69% accuracy, 93.55% sensitivity, and 96.55% specificity, and uncomplicated appendicitis with 79.47% AUC, 70.83% accuracy, 66.81% sensitivity, and 81.88% specificity. Machine learning is a novel approach to prevent unnecessary operations and decrease the burden of appendicitis both for patients and health systems. III.', 'The need for preoperative diagnosis and classification is going up because of the tendency to nonoperative management of appendicitis. Simple models that can learn are often used. Decision trees offer high classification accuracy. We tried to detect and differentiate from complicated cases using machine learning. There were healthy controls, sham controls, sham disease, and acute abdomen in the cases we analyzed. The groups include false laparotomy, uncomplicated appendicitis, complicatedAppendicitis without abscess, and complicatedAppendicitis Patients with comorbidities and their pathology results were excluded. There was data for demographics, blood analysis, and diagnosis. Machine learning was used to help detect patients with appendicitis. There were 7241 patients with a mean age of 6.84 years, of which 86 percent were male. The performance measures provided by most algorithm tested were similar. The decision tree model is more interpretable than the decision tree model. We detected patients in appendicitis who had 93.97% AUC, 94.6% accuracy, 96.55% specificity, and 70% accuracy. The use of Machine learning can reduce the burden of appendicitis for patients and health system. I.']

["The CellMax (CMx®) platform was developed to enrich for epithelial circulating tumor cells (CTCs) in the whole blood. This report provides assay performance data, including accuracy, linearity, limit of blank, limit of detection (LOD), specificity, and precision of enumeration of cancer cell line cells (CLCs) spiked in cell culture medium or healthy donor blood samples. Additionally, assay specificity was demonstrated in 32 young healthy donors and clinical feasibility was demonstrated in a cohort of 47 subjects consisting of healthy donors and patients who were colonoscopy verified to have colorectal cancer, adenomas, or a negative result. The CMx platform demonstrated high accuracy, linearity, and sensitivity for the enumeration of all CLC concentrations tested, including the extremely low range of 1 to 10 cells in 2 mL of blood, which is most relevant for early cancer detection. Theoretically, the assay LOD is 0.71 CTCs in 2 mL of blood. The analytical specificity was 100% demonstrated using 32 young healthy donor samples. We also demonstrated precision across multiple days and multiple operators, with good reproducibility of recovery efficiency. In a clinical feasibility study, the CMx platform identified 8 of 10 diseased subjects as positive (80% clinical sensitivity) and 4 of 5 controls as negative (80% clinical specificity). We also compared processing time and transportation effects for similar blood samples from two different sites and assessed an artificial intelligence-based counting method. Finally, unlike other platforms for which captured CTCs are retained on ferromagnetic beads or tethered to the slide surface, the CMx platform's unique airfoam-enabled release of CTCs allows captured cells to be transferred from a microfluidic chip to an Eppendorf tube, enabling a seamless transition to downstream applications such as genetic analyses and live cell manipulations.", 'The CellMax platform was developed to enrich for the cells that make up the tumor. The report has the performance information for the cell culture medium or healthy donor blood samples used to enumeration cancer cell lines. In a group of 47 subjects composed of 32 healthy donors and the patients who were colonoscopy verified to have colorectal cancer or a negative result, a clinical feasibility demonstration was done. The CMx platform demonstrated high sensitivity and high accuracy for the enumeration of all CLC concentrations, which can be found in a very small amount of blood. The LOD is determined by the quantity of blood taken. The analytical specificity was demonstrated by a sample of healthy donors. Multiple operators and precision across multiple days were demonstrated by us. A CMx platform study showed that 8 of 10 healthy subjects are positive and the rest are negative. We assessed an artificial intelligence based counting method for comparing the processing time and transportation effects for two different blood samples. The CMx platform has an airfoam technology that allows cells to be transferred from a chip to a tube in a seamless transition.']

['Deficient air quality in industrial environments creates a number of problems that affect both the staff and the ecosystems of a particular area. To address this, periodic measurements must be taken to monitor the pollutant substances discharged into the atmosphere. However, the deployed system should also be adapted to the specific requirements of the industry. This paper presents a complete air quality monitoring infrastructure based on the IoT paradigm that is fully integrable into current industrial systems. It includes the development of two highly precise compact devices to facilitate real-time monitoring of particulate matter concentrations and polluting gases in the air. These devices are able to collect other information of interest, such as the temperature and humidity of the environment or the Global Positioning System (GPS) location of the device. Furthermore, machine learning techniques have been applied to the Big Data collected by this system. The results identify that the Gaussian Process Regression is the technique with the highest accuracy among the air quality data sets gathered by the devices. This provides our solution with, for instance, the intelligence to predict when safety levels might be surpassed.', 'Air quality in industrial environments can result in a number of problems for the staff and the environment. Monitoring pollutant substances discharged into the atmosphere must be done on a periodic basis. The deployed system should be adapted to the requirements of the industry. The paper shows how the Internet of Things can be used to create a complete air quality monitoring infrastructure. Two highly precise tiny devices were developed to allow real-time monitoring of particulate matter concentrations and harmful gases in the air. The temperature and humidity of the environment, as well as the location of the device, can be collected. Machine learning techniques have been applied to the data gathered by this system. The technique known as the Gaussian Process Regression has the highest accuracy among the air quality data sets gathered by the devices, according to the results. Intelligence to predict when safety levels might be surpassed is provided by this.']

['Mild traumatic brain injury (mTBI) results in variable clinical outcomes, which may be influenced by genetic variation. A single-nucleotide polymorphism in catechol-o-methyltransferase (COMT), an enzyme which degrades catecholamine neurotransmitters, may influence cognitive deficits following moderate and/or severe head trauma. However, this has been disputed, and its role in mTBI has not been studied. Here, we utilize the Transforming Research and Clinical Knowledge in Traumatic Brain Injury Pilot (TRACK-TBI Pilot) study to investigate whether the COMT Val (158) Met polymorphism influences outcome on a cognitive battery 6 months following mTBI--Wechsler Adult Intelligence Test Processing Speed Index Composite Score (WAIS-PSI), Trail Making Test (TMT) Trail B minus Trail A time, and California Verbal Learning Test, Second Edition Trial 1-5 Standard Score (CVLT-II). All patients had an emergency department Glasgow Coma Scale (GCS) of 13-15, no acute intracranial pathology on head CT, and no polytrauma as defined by an Abbreviated Injury Scale (AIS) score of ≥3 in any extracranial region. Results in 100 subjects aged 40.9 (SD 15.2) years (COMT Met (158) /Met (158) 29 %, Met (158) /Val (158) 47 %, Val (158) /Val (158) 24 %) show that the COMT Met (158) allele (mean 101.6 ± SE 2.1) associates with higher nonverbal processing speed on the WAIS-PSI when compared to Val (158) /Val (158) homozygotes (93.8 ± SE 3.0) after controlling for demographics and injury severity (mean increase 7.9 points, 95 % CI [1.4 to 14.3], p = 0.017). The COMT Val (158) Met polymorphism did not associate with mental flexibility on the TMT or with verbal learning on the CVLT-II. Hence, COMT Val (158) Met may preferentially modulate nonverbal cognition following uncomplicated mTBI.Registry: ClinicalTrials.gov Identifier NCT01565551.', "Mild traumatic brain injury results in variable clinical outcomes which may be influenced by genetics. It is possible that a single-nucleotide polymorphism in catechol-o-methyltransferase can affect cognitive deficits after head trauma. This has been disputed, and its role in m TBRI has not been studied. The Transforming Research and Clinical Knowledge in Traumatic Brain Injury Pilot study is a part of this project. The Glasgow Coma Scale was 13-15 for patients and there were no acute intracranial pathology, or polytrauma, as a result of their visit to the emergency department. The COMT Met (158) allele is found in 29 percent of subjects and in 47 percent of subjects. The COMT Val didn't associate mental flexibility on the TMT or with verbal learning. COMT Val may preferentially modify communication after uncomplicated mTBI. Clinical trials are in the registry."]

["Community pharmacists contribute substantially to public health and person-centred care. Emotional intelligence (EI) may help health professionals better engage with patients, handle stress in challenging situations and, presumably, better introduce and implement new services. The study's aims were to compare the EI and perceived stress (PS) levels of community pharmacists who provided a new service to patients with diabetes with their controls who provided standard pharmaceutical services and to test the correlations between the two constructs. This study used a survey methodology. Well-validated instruments were distributed electronically to all participating pharmacists. To compare the continuous EI and PS data between the two study groups, the paired-samples t test was used. Pearson and Spearman's correlations were used to test the associations between EI and PS and their respective subdomains. A total of 86 pharmacists participated in the study (n = 43 in each group). The study groups did not differ by any characteristic except gender. Their mean EI and PS levels were 120.95 ± 11.53 and 17.45 ± 4.55, respectively, with no difference between the groups. In both study groups, inverse correlations were found between PS and EI levels, with statistical significance in the control group and in the overall study population (r = - 0.611 and r = - 0.370, respectively). Our results suggest that the introduction of the EI agenda into certification programmes for new community pharmacy services should be considered. The results also suggest that higher EI may have protective effects against PS. Additional research would clarify the need to invest more in such programmes.", 'Community pharmacists help with public health and person-centred care. It is possible that emotional intelligence may help health professionals better handle stressors, better engage with patients, and introduce and implement new services. The aim was to compare the perceived stress of community pharmacists who provided a new service to patients with diabetes with the levels of their controls who provided more standard pharmaceutical services. The study uses a survey method. All of the participating pharmacy shops received well-validated instruments electronically. The test was used to find out if the continuous data between the two study groups was similar. The correlations were used to test the relationships between the two. A total of 86 drugstores took part in the study. The study groups were 888-609- 888-609- 888-609- 888-609- 888-609- The levels for the groups were the same as those for the group with no difference. inverse correlations between PS andEI levels were found in both study groups The results show that the introduction of theei agenda should be considered. The results suggest that higher Epigenetics may have protective effects against PS. There is need to invest more in these programmes.']

['Ultrasound is a fundamental and indispensable diagnostic method in the field of emergency medicine [...].', "In emergency medicine, the use of x-rays is very important. They did a great job It's not true that it's not true that it's not true that it's not true that it's not true that it's not ."]

["Artificial intelligence (AI), which includes machine learning and deep learning has been introduced to nursing care in recent years. The present study reviews the following topics: the concepts of AI, machine learning, and deep learning; examples of AI-based nursing research; the necessity of education on AI in nursing schools; and the areas of nursing care where AI is useful. AI refers to an intelligent system consisting not of a human, but a machine. Machine learning refers to computers' ability to learn without being explicitly programmed. Deep learning is a subset of machine learning that uses artificial neural networks consisting of multiple hidden layers. It is suggested that the educational curriculum should include big data, the concept of AI, algorithms and models of machine learning, the model of deep learning, and coding practice. The standard curriculum should be organized by the nursing society. An example of an area of nursing care where AI is useful is prenatal nursing interventions based on pregnant women's nursing records and AI-based prediction of the risk of delivery according to pregnant women's age. Nurses should be able to cope with the rapidly developing environment of nursing care influenced by AI and should understand how to apply AI in their field. It is time for Korean nurses to take steps to become familiar with AI in their research, education, and practice.", 'Artificial intelligence has been introduced into nursing care. The topic of machine learning and deep learning has been discussed in the present study. Artificial intelligence is an intelligent system composed of a machine. Computers can learn without being explicitlyprogrammed. Artificial Neural networks with additional hidden layers are used in deep learning. Big data, machine learning, model of deep learning, and coding practice should be incorporated in the educational curriculum. The nursing society should organize the standard curriculum. Pregnant nursing interventions are based on the nursing records of women and use artificial intelligence to predict the risk of birth. Nurses should be able to cope with rapidly evolving nursing care influenced by Artificial Intelligence and should understand how to apply it in their field. It is time for Korean nurses to become familiar with artificial intelligence.']

["<b>Objectives:</b> To examine differences in cognition parameters by birth weight categories and to analyze whether the relationships between birth weight and cognitive functions are mediated by cardiorespiratory fitness (CRF) in schoolchildren. <b>Methods:</b> A cross-sectional study using a sample of 664 school children from the MOVI-Kids study. Variables: i) cognitive function measured by the Battery of General and Differential Aptitudes (BADyG); ii) birth weight, reported by parents; and iii) CRF (20-m shuttle run test). ANCOVA models were estimated to assess differences in cognitive function categories across birth weight and CRF categories. Mediation analysis was conducted with Hayes' PROCESS macro. <b>Results:</b> CRF is a full mediator of the association between birth weight with the verbal and numerical factors, and general intelligence; and is a partial mediator when logical reasoning and the spatial factor were the dependent variables. The available data suggest that, in schoolchildren, the influence of birth weight on cognitive function is mediated by CRF. <b>Conclusions:</b> These findings highlight that children with lower birth weight values and lower fitness levels should be target subgroups to improve children's cognition, in which long-life physical activity interventions at early ages are a priority.", 'To examine differences in cognitive parameters between birth weight categories and whether the relationship is related to cardiovascular fitness. The cross-sectional study used a sample of school children from the study. The factors include: cognitive function measured by the Bracelet of General and Differential Aptitudes; birth weight reported by parents; and CRF (20-m shuttle run test). The models estimated how cognitive function differed across birth weight and CRF categories. The PROCESS macro was used in the mediation analysis. There is an association between birth weight, verbal and numerical factors, and general intelligence when logical reasoning and the spatial factor are dependent on the other variables. The data shows that the influence of birth weight on cognitive function is mediated. According to this findings, children with lower birth weight values and low fitness levels should be prioritised for targeted physical activity interventions at the early ages.']

['A sustainable territorial development process represents a society characterized by its uniqueness to the territory of implementation. This notion of territory constitutes a fact of human nature, the development of which integrates collective and individual wellbeing through an intersectoral and multidisciplinary process. When carrying out joint projects, the partnership dynamic can transform interpersonal relationships and encourage the development of trust at the very core of the sustainable local development process. This trust enables the creation of a collective territorial intelligence, based on individual and sectoral knowledge and on know-how that form a shared, intangible heritage for each of the stakeholders in the sustainable territorial development process; however, as a fact of human nature, this trust remains fragile.', 'A sustainable territorial development process is characterized by its uniqueness. The notion of territories is a truth of human nature and integrates collective and individual wellbeing through an intersectoral and multidisciplinary process. The partnership dynamic can cause a change in the relationships between people and encourage trust at the core of sustainable local development. This trust leads to the creation of a collective territorial intelligence based on individual and sectoral knowledge, which in turn leads to a shared intangible heritage for each of the stakeholders in the sustainable territorial development process.']

['Haploinsufficiency of HDAC4 gene has been reported to result in brachydactyly-"mental retardation" syndrome (BDMR), a condition with significant intellectual impairment, brachydactyly type E, and typical facial features. Presented here are three individuals with haploinsufficiency of HDAC4 who have brachydactyly type E, non-dysmorphic facial features, and normal intelligence. This is in contradistinction to previous reports that haploinsufficiency of HDAC4 is sufficient to cause BDMR.', 'brachydactyly-mental retardation syndrome has been reported when the HDAC4 gene is not present. Three people with haploinsufficiency of HDAC4 who have certain facial features are presented. It is contrary to reports that the haploinsufficiency of HDAC4 is necessary.']

['Autism spectrum disorders (ASD) are lifelong neurodevelopmental disorders. It is not clear whether working memory (WM) deficits are commonly experienced by individuals with ASD. To determine whether individuals with ASD experience significant impairments in WM and whether there are specific domains of working memory that are impaired. We conducted a meta-analysis using four electronic databases EMBASE (OVID), MEDLINE (OVID), PsychINFO (EBSCOHOST), and Web of Science, to examine the literature to investigate whether people with ASD experience impairments related to WM. Meta-analyses were conducted separately for phonological and visuospatial domains of WM. Subgroup analyses investigated age and intelligence quotient as potential moderators. A total of 29 papers containing 34 studies measuring phonological and visuospatial domains of WM met the inclusion criteria. WM scores were significantly lower for individuals with ASD compared to typically developed (TD) controls, in both the visuospatial domain when investigating accuracy (d: -0.73, 95% CI -1.04 to -0.42, p < 0.05) and error rates (d: 0.56, 95% CI 0.25 to 0.88, p<0.05), and the phonological domain when investigating accuracy (d:-0.67, 95% CI -1.10 to -0.24, p>0.05) and error rate (d: 1.45, 95% CI -0.07 to 2.96, p = 0.06). Age and IQ did not explain the differences in WM in ASD. The findings of this meta-analysis indicate that across the lifespan, individuals with ASD demonstrate large impairments in WM across both phonological and visuospatial WM domains when compared to healthy individuals.', "There is a lifelong neurological disorders such as the Asperger's spectrum disorders. It is not clear whether working memory deficits are experienced by individuals with disabilities. To determine the degree of impairment of working memory in individuals with ASD We conducted a meta-analysis to look at the literature to see if people with ASD experiences impairments related to WM. There were two meta- analyses conducted for the visuospatial and phonological domains. The subgroup analyzed age and intelligence quotient. Thirty-four papers measuring the pheologic and visuospatial domains of Wm met the criteria for inclusion. In the visuospatial domain, the score for individuals with ASD was lower than the score for controls. Age and IQ weren't the reason for the difference in the way we communicate. This meta-analysis shows that individuals with ASD have large impairments in WM across both visual and sensory areas."]

['The purpose of this study was to build and evaluate a high-performance algorithm to detect and characterize the presence of a meniscus tear on magnetic resonance imaging examination (MRI) of the knee. An algorithm was trained on a dataset of 1123 MR images of the knee. We separated the main task into three sub-tasks: first to detect the position of both horns, second to detect the presence of a tear, and last to determine the orientation of the tear. An algorithm based on fast-region convolutional neural network (CNN) and faster-region CNN, was developed to classify the tasks. The algorithm was thus used on a test dataset composed of 700 images for external validation. The performance metric was based on area under the curve (AUC) analysis for each task and a final weighted AUC encompassing the three tasks was calculated. The use of our algorithm yielded an AUC of 0.92 for the detection of the position of the two meniscal horns, of 0.94 for the presence of a meniscal tear and of 083 for determining the orientation of the tear, resulting in a final weighted AUC of 0.90. We demonstrate that our algorithm based on fast-region CNN is able to detect meniscal tears and is a first step towards developing more end-to-end artificial intelligence-powered diagnostic tools.', "The purpose for the study was to build a high-performance, high-efficiency tool to detect a tear in the knee with a magnetic resonance image. The data of 1123 MR images of the knee were used to train an algorithms. We separated the main task into three separate parts, first to detect the position of both horns, second to detect the presence of a tear, and the final to determine the orientation of the tear. An artificial intelligence that uses fast- region CNN and faster- region CNN has been created to classify certain tasks. 700 images were used as a test dataset. Each task's AUC analysis as well as a final weighted URC of the three tasks was used to derive the performance metric. For the detection of the position of the two meniscal horns and for the orientation of the meniscal tear, our method yielded an adjusted volume of 0.99."]

['This study reports the outcome of the first evaluation of the APAS® Independence for automated reading and preliminary interpretation of urine cultures in the routine clinical microbiology laboratory. In a 2-stage evaluation involving 3000 urine samples, two objectives were assessed; 1) the sensitivity and specificity of the APAS® Independence compared to microbiologists using colony enumeration as the primary determinant, and 2) the variability between microbiologists in enumerating bacterial cultures using traditional culture reading techniques, performed independently to APAS® Independence interpretation. Routine urine samples received into the laboratory were processed and culture plates were interpreted by standard methodology and with the APAS® Independence. Results were compared using typical discrepant result resolution and with a composite reference standard, which provided an alternative assessment of performance. The significant growth sensitivity of the APAS® Independence was determined to be 0.919 with a 95% confidence interval of (0.879, 0.948), and the growth specificity was 0.877 with a 95% confidence interval of (0.827, 0.916). Variability between microbiologists was demonstrated with microbiologist bi-plate enumerations in agreement with the consensus 88.6% of the time. The APAS® Independence appears to offer microbiology laboratories a mechanism to standardise the processing and assessment of urine cultures whilst augmenting the skills of specialist microbiology staff.', 'The first evaluation of the APAS Independence for automated reading and preliminary interpretation of urine cultures was reported in the study. The sensitivity and specificity of the APAS Independence was compared to microbiologists using colony enumerated as the primary determinant, and the variability between microbiologists in their sampling was compared. Culture plates are used to interpret urine samples in the laboratory, they had to be used with a standard methodology and with the APAS Independence. The results were compared with a similar standard, which offered an alternative assessment of performance. It was found that the significant growth sensitivity of the APAS® Independence was jetted out with a 95% confidence interval, and that the specificity was jetted out with a 95% confidence interval. Microbiologists could agree with the 88.6% consensus on their bi-plate enumerations. The Independence of the APAS appears to offer a way for laboratories to standardize the processing and assessment of urine cultures.']

['A persistent vegetative state (PVS) can be caused by traumatic or non-traumatic brain injury. PVS is a complex clinical condition with numerous complications. Nursing care, medical treatment, and comprehensive rehabilitation are necessary to improve the outcomes of PVS. However, the prognosis remains unsatisfactory. Acupuncture therapy has been used as a rehabilitation strategy to treat patients with PVS in China, showing better results in the recovery of consciousness, intellectual capability, and motor function. We present the case of a 4-month-long PVS after herpes simplex virus encephalitis (HSVE) in a 3.5-year-old boy who underwent Tongdu Xingshen acupuncture integrated with Western medicine and rehabilitation. The patient regained consciousness post-treatment. His intelligence and motor function gradually recovered after seven treatment sessions. Tongdu Xingshen acupuncture is a potential complementary therapy to optimize clinical outcomes in PVS.', "A persistent vegetative state can be the result of an injury to the brain. There are a number of factors that make up the clinical condition of photovoltaics. Comprehensive rehabilitation and medical treatment are needed to improve the outcomes. The outcome remains not satisfactory. In China, patients with Parkinson's disease (PvS) are receiving a rehabilitation strategy that includes acupuncture therapy. The case of a 4-month-long PVS after a 3.5 year-old boy had an inflammatory brain injury is presented. The patient regained consciousness after being treated. His intelligence and motor function returned to normal after seven sessions of 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 It is possible to improve the outcomes of clinical research with the use of tongsu xingshen."]

['For years, hepatologists have been seeking non-invasive methods able to detect significant liver fibrosis. However, no previous algorithm using routine blood markers has proven to be clinically appropriate in primary care. We present a novel approach based on artificial intelligence, able to predict significant liver fibrosis in low-prevalence populations using routinely available patient data. We built six ensemble learning models (LiverAID) with different complexities using a prospective screening cohort of 3352 asymptomatic subjects. 463 patients were at a significant risk that justified performing a liver biopsy. Using an unseen hold-out dataset, we conducted a head-to-head comparison with conventional methods: standard blood-based indices (FIB-4, Forns and APRI) and transient elastography (TE). LiverAID models appropriately identified patients with significant liver stiffness (> 8 kPa) (AUC of 0.86, 0.89, 0.91, 0.92, 0.92 and 0.94, and NPV ≥ 0.98), and had a significantly superior discriminative ability (p < 0.01) than conventional blood-based indices (AUC = 0.60-0.76). Compared to TE, LiverAID models showed a good ability to rule out significant biopsy-assessed fibrosis stages. Given the ready availability of the required data and the relatively high performance, our artificial intelligence-based models are valuable screening tools that could be used clinically for early identification of patients with asymptomatic chronic liver diseases in primary care.', 'For years, doctors have been looking for non-invasive methods to make sure their patients are healthy. Routine blood markers are not clinically appropriate for primary care. We present a new approach based on Artificial Intelligence that can predict seriousliver fibrosis in low-prevalence populations using routinely available patient data. The models were built with a screening cohort of 3352 subjects in mind. 465 patients were considered to have a significant risk of death. Standard blood-based indicators (FIB-4, Forns and APRI) and Transient elastography (TE28) were compared with conventional methods using an unseen hold-out dataset. The models were better at identifying patients with significant liver stiffness than conventional models. The models gave a good ability to rule out significant stages. Our models are useful for screening patients with asymptomatic chronic liver diseases because they are very easy to use and the data is available.']

['<b>Objective:</b> To assess the performance of a novel deep learning (DL)-based artificial intelligence (AI) system in classifying computed tomography (CT) scans of pneumonia patients into different groups, as well as to present an effective clinically relevant machine learning (ML) system based on medical image identification and clinical feature interpretation to assist radiologists in triage and diagnosis. <b>Methods:</b> The 3,463 CT images of pneumonia used in this multi-center retrospective study were divided into four categories: bacterial pneumonia (<i>n</i> = 507), fungal pneumonia (<i>n</i> = 126), common viral pneumonia (<i>n</i> = 777), and COVID-19 (<i>n</i> = 2,053). We used DL methods based on images to distinguish pulmonary infections. A machine learning (ML) model for risk interpretation was developed using key imaging (learned from the DL methods) and clinical features. The algorithms were evaluated using the areas under the receiver operating characteristic curves (AUCs). <b>Results:</b> The median AUC of DL models for differentiating pulmonary infection was 99.5% (COVID-19), 98.6% (viral pneumonia), 98.4% (bacterial pneumonia), 99.1% (fungal pneumonia), respectively. By combining chest CT results and clinical symptoms, the ML model performed well, with an AUC of 99.7% for SARS-CoV-2, 99.4% for common virus, 98.9% for bacteria, and 99.6% for fungus. Regarding clinical features interpreting, the model revealed distinctive CT characteristics associated with specific pneumonia: in COVID-19, ground-glass opacity (GGO) [92.5%; odds ratio (OR), 1.76; 95% confidence interval (CI): 1.71-1.86]; larger lesions in the right upper lung (75.0%; OR, 1.12; 95% CI: 1.03-1.25) with viral pneumonia; older age (57.0 years ± 14.2, OR, 1.84; 95% CI: 1.73-1.99) with bacterial pneumonia; and consolidation (95.8%, OR, 1.29; 95% CI: 1.05-1.40) with fungal pneumonia. <b>Conclusion:</b> For classifying common types of pneumonia and assessing the influential factors for triage, our AI system has shown promising results. Our ultimate goal is to assist clinicians in making quick and accurate diagnoses, resulting in the potential for early therapeutic intervention.', 'The aim is to assess the effectiveness of a machine learning system for classification of certain computed tomography scans of people with bronchitis. b>Outcome: i>n/i> 507), i>n/i> t-boned (i>n/i>), t-foiled (i>n/i>), and Diagnostic methods based on images were used. A machine learning model for risk interpretation is built using clinical features and key neuroimaging. The areas under the receiver operating characteristic curves were used as a evaluation area. The median AUC was 99.4% for the models for differentiating pneumonia and respiratory infections. The AUC was 99.7% for the common Viruses, 8.4% for the bugs, and 8.8% for the fungus. There were distinct lung characteristics associated with specific pneumonia, such as ground glass opacity (92%), and large lung tumors (18.4%), in the model. The promising results of the system for assessing the influence of the factors for triage are shown. Our goal is to assist clinicians with quick diagnoses, which results in the potential for intervention early.']

['Recent bold, eye-catching headline predictions made by nonradiologists, e.g., "in a few years, radiology will disappear" and "stop training radiologists now," are not only far from reality but also irresponsible and a disservice to the appropriate implementation and adoption of artificial intelligence (AI) technology to health care. It is highly likely and foreseeable that AI will enhance the quality and efficiency of the current clinical practice across many specialties and even render some activities in clinical practice obsolete.', 'Recent bold, eye-catching headline predictions made by nonradiologists. g It\'s not unrealistic to say that in a few years, "radiation will disappear" and that "stop training radiologist now" are not only wrong but also irresponsible and disservice to the appropriate implementation and adoption of artificial intelligence technology to health care. Artificial intelligence will likely enhance the quality and inefficiency of the current clinical practice, render some activities obsolete, and even add to it.']

['Pancreas cancer is an aggressive and fatal disease that will become one of the leading causes of cancer mortality by 2030. An all-out effort is underway to better understand the basic biologic mechanisms of this disease ranging from early development to metastatic disease. In order to change the course of this disease, diagnostic radiology imaging may play a vital role in providing a precise, noninvasive method for early diagnosis and assessment of treatment response. Recent progress in combining medical imaging, advanced image analysis and artificial intelligence, termed radiomics, can offer an innovate approach in detecting the earliest changes of tumor development as well as a rapid method for the detection of response. In this chapter, we introduce the principles of radiomics and demonstrate how it can provide additional information into tumor biology, early detection, and response assessments advancing the goals of precision imaging to deliver the right treatment to the right person at the right time.', 'An all out effort is underway to understand the mechanisms of the aggressive and fatal PancreasCancer will become one the leading causes of cancer mortality by 2030. Diagnostic X-rays may be able to change the course of the disease by helping to detect it early and provide an assessment of the response. Recent progress in combining medical imaging, advanced image analysis and artificial intelligence, termed radiomics, can offer an innovative approach in detecting the earliest changes of tumors and a fast method for detection of response. In this chapter, we show how Radiomics can give additional information and help advance the goals of precision medicine by delivering the correct treatment to the correct person at the right time.']

["A systematic evaluation of the efficacy of hormones in combination with antiepileptic drugs (AEDs) compared to AEDs alone in the treatment of children with encephalopathy related to status epilepticus during slow sleep (ESES). This study provides an evidence-based approach to the treatment of children with ESES. To find all relevant studies published before March 2022, we searched PubMed, Embase, Web of Science, Clinical Trials, Cochrane Library, CNKI, and Wanfang databases. We explore the difference between AEDs combined with hormones and AEDs alone for ESES treatment. All outcome data, including Wechsler Intelligence Scale for Children, the effective rate, EEG discharges, and adverse effects rate (AER), were compared using Review Manager 5.3. There were 805 patients in this study's seven investigations, including 403 in the experimental group and 402 in the control group. Meta-analysis showed that after treatment, compared with the AEDs alone group, the hormone combined with AEDs. The difference in clinical improvement rate [RR = 1.25, 95% CI (1.15, 1.36), <i>p</i> < 0.00001], electroencephalographic (EEG) discharge improvement rate [RR = 1.31, 95% CI (1.22, 1.41), <i>p</i> < 0.00001], and cognitive intelligence score [SMD = 1.02, 95% CI (0.76, 1.28), <i>p</i> < 0.00001] was statistically significant. The differences were statistically significant in terms of 0.00001; the incidence of adverse reactions was higher in the hormone combined with AEDs group than in the AEDs group alone, and the differences were statistically significant [RR = 4.13, 95% CI (1.06, 16.13), <i>p</i> < 0.01], and all adverse reactions improved or disappeared after discontinuation of the drug. The combination of hormones with AEDs for the treatment of epileptic electrical continuity in sleep has advantages over AEDs alone in terms of controlling seizures, improving EEG abnormalities, and improving cognition. The combination of hormones with AEDs has advantages over AEDs alone in controlling seizures, improving EEG abnormalities, and improving cognition and is relatively safe.", 'A systematic analysis of efficacy of hormones in comparison to antiepileptic drugs in the treatment of children with encephalopathy related to status epilepticus during slow sleep. The study provides an evidence-based approach to treatment of children with ESES. We used several databases to find studies that have been published before March 2022, the new date for which is 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 800-211-2519 We explored the difference between hormones and the drugs alone for ESES treatment. There were 905 patients in the study, including 405 in the experimental group and 402 in the control group. The hormone and the AEDs combined were shown to be better after treatment than alone. The discharge improvement rate is lower than the clinical improvement rate. The differences were statistically significant in terms of number of adverse reactions, with the hormone combined with the rest of the group showing the highest incidence of adverse reactions compared to the rest of the group alone. The combination of hormones and theanti-epileptic drugs can help with the treatment of epileptic electrical continuity in sleep, which improves brain activity and reduces seizures. The combination of hormones with AEDs has advantages over the single agent drugs in controlling seizures, improving brain function, and improving cognitive function.']

['Recently, the morbidity and mortality of pancreatic cancer have been increasing year by year. Because of its deep anatomical location and because most presented patients often suffer from abdominal pain or jaundice, it is difficult to diagnose pancreatic cancer at an early stage, leading to late clinical stage and poor prognosis. integrated positron emission tomography/magnetic resonance imaging (PET/MRI) fusion imaging not only has the characteristics of high resolution and multi-parameter imaging of MRI, but also combines the high sensitivity and the semi-quantitative characteristics of PET. In addition, the continuous development of novel MRI imaging and PET imaging biomarkers provide a unique and precise research direction for future pancreatic cancer research. This review summarizes the value of PET/MRI in the diagnosis, staging, efficacy monitoring, and prognosis evaluation of pancreatic cancer, and prognosis for developing emerging imaging agents and artificial intelligence radiomics in pancreatic cancer.', 'The morbidity and mortality of pancreatic cancer have gone up recently. It is hard to diagnose  pancreatic cancer due to its deep location and the fact that many patients have abdominal pain or jaundice and this can lead to late clinical stage and poor progess. High resolution and multi-Parameter neuroimaging of MR are some of the benefits of integrated Positron emission tomography/magnetic resonance (PET/MRI) fusion. In addition, the continuous development of novel Pancreatic cancer research is giving a distinct and precise direction for future research. It summarizes the value of PET for the diagnosis, staging, efficacy monitoring, and prognosis evaluation of breast, colorectal, and ovarian Cancer.']

["Studies have been initiated to investigate the potential impact of connected and automated vehicles (CAVs) on transportation infrastructure. However, most existing research only focuses on the wandering patterns of CAVs. To bridge this gap, an apple-to-apple comparison is first performed to systematically reveal the behavioural differences between the human-driven vehicle (HDV) and CAV trajectory patterns for the first time, with the data collected from the camera-based next generation simulation dataset and autonomous driving co-simulation platform, CARLA and SUMO, respectively. A gradient boosting-based ensemble learning model for pavement performance (i.e. international roughness index) prediction is then developed with the input features including three driving pattern features, namely, lateral wandering deviation, longitudinal car-following distance and driving speed, plus 20 other context variables. A total of 1707 observations is extracted from the long-term pavement performance database for model training purposes. The result indicates that the trained model can accurately predict pavement deterioration and that CAV deteriorates pavement faster than HDV by 8.1% on average. According to the sensitivity analysis, CAV deployment will create a greater impact on the younger pavements, and the rate of pavement deterioration is found to be stable under light traffic, whereas it will increase under congested traffic. This article is part of the theme issue 'Artificial intelligence in failure analysis of transportation infrastructure and materials'.", "The impact of connected and automated vehicles on the transportation infrastructure has been looked into. Research focuses on the wandering patterns of CAVs. The data collected from the next generation simulation and the human driven vehicle's trajectory data will be used to perform an apple toapple comparison. A learning model for pavement performance. There is an e The input features include three driving patterns and 20 other variables. A total of 1707 observations are from the long-term pavement performance database for model training purposes. The results show that the trained model can accurately predict pavement trends, and that CAV degrades pavement much faster than HDV. According to the analysis, a greater impact will be made on the younger pavement and the rate of pavement decline will increase as a result of CAV deployment. This article is related to the theme of the issue."]

['In recent years, the growing impact of climate change on surface water bodies has made the analysis and forecasting of streamflow rates essential for proper planning and management of water resources. This study proposes a novel ensemble (or hybrid) model, based on the combination of a Deep Learning algorithm, the Nonlinear AutoRegressive network with eXogenous inputs, and two Machine Learning algorithms, Multilayer Perceptron and Random Forest, for the short-term streamflow forecasting, considering precipitation as the only exogenous input and a forecast horizon up to 7 days. A large regional study was performed, considering 18 watercourses throughout the United Kingdom, characterized by different catchment areas and flow regimes. In particular, the predictions obtained with the ensemble Machine Learning-Deep Learning model were compared with the ones achieved with simpler models based on an ensemble of both Machine Learning algorithms and on the only Deep Learning algorithm. The hybrid Machine Learning-Deep Learning model outperformed the simpler models, with values of R<sup>2</sup> above 0.9 for several watercourses, with the greatest discrepancies for small basins, where high and non-uniform rainfall throughout the year makes the streamflow rate forecasting a challenging task. Furthermore, the hybrid Machine Learning-Deep Learning model has been shown to be less affected by reductions in performance as the forecasting horizon increases compared to the simpler models, leading to reliable predictions even for 7-day forecasts.', 'The analysis and forecasting of streamflow rates has become an essential part of proper planning and management in the face of climate change. This study proposes a novel ensemble based on the combination of a deep learning method, a network with edo characteristics and a machine learning method, called Multilayer Perceptron and Random Forest, for short-term streamflow forecasting. A study was done regarding 18 watercourses throughout the United Kingdom. The predictions obtained with the ensemble Machine Learning-Deep Learning model were compared to the ones achieved with simpler models based on both Machine Learning and Deep Learning. The model with values of 2/sup> and above was the hybrid machine learning-Deep Learning model, compared to the simpler models which had values of 8/sup> and less. It has been shown that a hybrid model such as MachineLearning-Deep Learning is less affected by reductions in performance when forecasting than simpler models, which leads to reliable predictions even for 7-day forecasts.']

['To explore the influencing factors of the occurrence of propagated sensation effects at Zusanli (ST 36) based on artificial intelligence technology. A total of 272 undergraduates and postgraduates of Guangzhou University of TCM were recruited. The basic information, including TCM constitution, body weight index (BMI), pulse, tongue and coating, face complexion, as well as the occurrence of propagated sensation effects at Zusanli (ST 36) with different acupuncture needles and techniques, were recorded. The repeated incremental pruning to produce error reduction (RIPPER) algorithm was applied to predict the results and summarize the rules of relevant factors. ① The propagated sensation effects were often observed in the subject with <i>yin</i>-deficiency constitution (55 cases in total, of them 1 case was misclassified). ② The propagated sensation effects were observed in the subject with balanced <i>yin</i>-<i>yang</i> constitution (39 cases in total, of them 0 case was misclassified). ③ The propagated sensation effects were observed in the subject with phlegm-damp constitution combined with thin tongue and white coating (31 cases in total, of them 8 cases were misclassified). ④ The propagated sensation effects were observed in the subject with phlegm-damp constitution combined with light tongue coating (7 cases in total, of them 2 cases were misclassified). ⑤ The propagated sensation effects were observed in all the subject with special constitution (6 cases in total, of them 0 cases were misclassified). ⑥ The propagated sensation effects were not observed in the subject with other constitution or the combination of constitution and symptoms (134 cases in total, of them 4 cases were misclassified). The accuracy rate of classification was 94.49%. The difference of acupuncture needles and techniques was not significant in propagated sensation effects (<i>P</i>>0.05). The TCM constitution may be the most essential factor in the occurrence of propagated sensation effects.', 'The impact of artificial intelligence on sensation effects at Zusanli is explored. A total of 272 people were recruited. The basic information that was recorded was the body weight index, TCM constitution, pulse, tongue and coating, face complexion. TheRIPPER algorithm was applied to Predict theResults and summarize the rules of relevant factors. In the subject with a deficiency constitution, the observed sensation effects were often seen and that 1 case was misclassified. The subject had39 cases in total with balanced i>Yani/i> constitution which was misclassified. The subject had a phlegm-damp constitution combined with thin tongue and white coating which resulted in a bunch of sensation effects. Light tongue coating and a phlegm-damp constitution resulted in 7 cases being miscoded. In all the subjects with special constitution, the effects of sensation were observed. 136 cases were misclassified based on the fact that the sensation effects were not observed with the other constitution or with the combination of constitution and symptoms. The accuracy rate was close to 100%. The difference of needles was not important in the propagation of sensation. The most important factor about the occurrence of sensation effects is the TCM constitution.']

['Digital dentistry has become an integral part of our practice today, with artificial intelligence (AI) playing the predominant role. The present systematic review was intended to detect the accuracy of landmarks identified cephalometrically using machine learning and artificial intelligence and compare the same with the manual tracing (MT) group. According to the PRISMA-DTA guidelines, a scoping evaluation of the articles was performed. Electronic databases like Doaj, PubMed, Scopus, Google Scholar, and Embase from January 2001 to November 2022 were searched. Inclusion and exclusion criteria were applied, and 13 articles were studied in detail. Six full-text articles were further excluded (three articles did not provide a comparison between manual tracing and AI for cephalometric landmark detection, and three full-text articles were systematic reviews and meta-analyses). Finally, seven articles were found appropriate to be included in this review. The outcome of this systematic review has led to the conclusion that AI, when employed for cephalometric landmark detection, has shown extremely positive and promising results as compared to manual tracing.', 'Digital dentistry is a significant part of our practice today. The aim of the present systematic review was to see how accurate landmarks are using machine learning and artificial intelligence, and compare it with the manual tracing group. The article was evaluated according to the guidelines. There were various electronic databases which were searched from 2001 to 2011. 13 articles were studied in detail after the inclusion and exclusion criteria were applied. Three full- text articles did not contain a comparison between manual tracing andArtificial Intelligence for Cephalometric landmark detection, as well as two full-text articles that did contain systematic reviews and meta-analyses. The seven articles were considered appropriate for the review. The outcome of the systematic review has led to the conclusion that the results of Artificial Intelligence are very promising compared to manual tracing.']

['Correlation Plenoptic Imaging (CPI) is a novel volumetric imaging technique that uses two sensors and the spatio-temporal correlations of light to detect both the spatial distribution and the direction of light. This novel approach to plenoptic imaging enables refocusing and 3D imaging with significant enhancement of both resolution and depth of field. However, CPI is generally slower than conventional approaches due to the need to acquire sufficient statistics for measuring correlations with an acceptable signal-to-noise ratio (SNR). We address this issue by implementing a Deep Learning application to improve image quality with undersampled frame statistics. We employ a set of experimental images reconstructed by a standard CPI architecture, at three different sampling ratios, and use it to feed a CNN model pre-trained through the transfer learning paradigm U-Net architecture with VGG-19 net for the encoding part. We find that our model reaches a Structural Similarity (SSIM) index value close to 1 both for the test sample (SSIM = [Formula: see text]) and in 5-fold cross validation (SSIM = [Formula: see text]); the results are also shown to outperform classic denoising methods, in particular for images with lower SNR. The proposed work represents the first application of Artificial Intelligence in the field of CPI and demonstrates its high potential: speeding-up the acquisition by a factor 20 over the fastest CPI so far demonstrated, enabling recording potentially 200 volumetric images per second. The presented results open the way to scanning-free real-time volumetric imaging at video rate, which is expected to achieve a substantial influence in various applications scenarios, from monitoring neuronal activity to machine vision and security.', 'The Correlation  lntroMeteography technique uses two sensors and correlations of light to detect the direction and spatial distribution of light This approach to plenoptics has significant enhancement of both resolution and depth of field. Conventional approaches tend to take less time since they need more statistics to measure correlations with an acceptable signal-to-noise ratio. We use a Deep Learning application to improve image quality We use a set of experimental images reconstructed by a standard CPI architecture at three different sampling ratios, and use it to train a CNN model pre-trained through the transfer learning paradigm U-Net architecture with VGG-19 net. We found that our model reached a Structural Similarity index valued close to 1 both for the test sample and in 5-fold cross validation. The high potential, speed and accuracy associated with Artificial Intelligence is demonstrated by the proposed work, which would allow for the recording of up to 200 volumetric images per second. The presented results show a way to conduct real-time volumetric mapping at video rate and it is expected to have a considerable impact in various applications.']

["This timely captivating topic is organized and presented in this special issue of the journal of Seminar in diagnostic pathology. This special issue will be dedicated to the utilization of machine learning within the digital pathology and laboratory medicine fields. Special thanks to all the authors whose contributions to this review series has not only enhanced our overall understanding of this exciting new field but will also enrich the reader's understanding of this important discipline.", 'This topic is presented in a special issue of the journal of Seminar in diagnostic pathology. This issue will be devoted to machine learning and the use of it in the digital pathology field. Thanks to the authors who contributed to this review series, we can now gain a better sense of this exciting field and gain a deeper understanding of it for the reader.']

['The ability to compose new skills from a preacquired behavior repertoire is a hallmark of biological intelligence. Although artificial agents extract reusable skills from past experience and recombine them in a hierarchical manner, whether the brain similarly composes a novel behavior is largely unknown. In the present study, I show that deep reinforcement learning agents learn to solve a novel composite task by additively combining representations of prelearned action values of constituent subtasks. Learning efficacy in the composite task was further augmented by the introduction of stochasticity in behavior during pretraining. These theoretical predictions were empirically tested in mice, where subtask pretraining enhanced learning of the composite task. Cortex-wide, two-photon calcium imaging revealed analogous neural representations of combined action values, with improved learning when the behavior variability was amplified. Together, these results suggest that the brain composes a novel behavior with a simple arithmetic operation of preacquired action-value representations with stochastic policies.', 'The ability to change behavior from an old one is a hallmark of biological intelligence. Although artificial agents can take skills from past experiences and combine them into a new way, it is not certain if the brain can do the same. I show in the present study that deep reinforcement learning agents can learn to solve a novel, novel task by augmenting representations of pre-learned actions values. Learning efficacy was further enhanced by the use of stochasticity during pretraining. In mice, these theoretical predictions were shown to work when subtask training enhanced learning of the task. The neural representations of action values were revealed through the use of two-photon calcium. The results suggest that the brain composes a novel behavior with a simple arithmetic operation.']

["Individuals with dissociative identity disorder (DID) have complex symptoms consistent with severe traumatic reactions. Clinicians and forensic assessors are challenged by distinguishing symptom exaggeration and feigning from genuine symptoms among these individuals. This task may be aided by administering validity measures. This study aimed to document how individuals with DID score on the Structured Inventory of Malingered Symptomatology (SIMS). The second objective was to compare coached DID simulators and healthy controls to DID patients on the SIMS's total score and subscales. The third objective was to examine the utility rates of the SIMS in distinguishing simulated DID from clinically diagnosed DID. We compared SIMS data gathered from participants from two Dutch sites, one Swiss site and one U.S. site. Sixty-three DID patients were compared to 77 coached DID simulators and 64 healthy controls on the SIMS. A multivariate analysis compared the groups on the SIMS total scores and subscales, and post-hoc Games Howell tests and univariate ANOVAs examined differences between the groups. Utility statistics assessed the accuracy of the SIMS in distinguishing clinical from simulated DID. DID simulators scored significantly higher than DID individuals and healthy controls on every SIMS subscale as well as the total score. The majority (85.7%) of the individuals with DID scored above the cut-off, which is typically interpreted as indicative of possible symptom exaggeration. DID individuals scored higher than the healthy controls on every subscale except Low Intelligence, even after controlling for dissociation. The subscales and items most frequently endorsed by the DID group are consistent with symptoms associated with complex trauma exposure and dissociative reactions. The SIMS total score had a sensitivity of 96% but an unacceptably low specificity of 14%. The findings indicate that the instrument is not accurate in assessing potential symptom exaggeration or feigning in DID.", 'People withDID have an array of symptoms that are consistent with traumatic events. Symptom exaggerating and pretending to be real symptoms are challenges for forensic assessors and clinicians. This task can be aided by validity measures. The study wanted to know how individuals with DID score on the Structured Inventory of Malingered Symptomatology. The second objective was to compare the DID sims and healthy controls to DID patients. The third objective was to see if there was any correlation between SIMS utility rates and DID. We compared the data from three different sites S. The site. Sixty-three DID patients were compared against 77 coached DID simulator and 64 healthy controls on the SIMS. The groups were examined in an analysis of their total scores and subscales, along with what was found in a post-hoc games coliseum test. Utility statistics compared the accuracy of the SIMS in distinguishing between clinical and simulation DID. The DID individuals and healthy controls scored lower than the DIDSims, while the total score was higher. The majority of individuals with DID scored above the cut-off which can be seen as an indication of possible symptom exaggeration. The people scoring higher than the healthy controls were after controlling for dissociation. The items endorsed most frequently by the DID group are consistent with symptoms of trauma exposure. The total score was 98% sensitive with a low specificity. According to the findings, the instrument does not do an accurate assessment of potential symptom exaggerating.']

['The overall goal of the present study was to illustrate the potential of artificial intelligence (AI) techniques in sports on the example of weight training. The research focused in particular on the implementation of pattern recognition methods for the evaluation of performed exercises on training machines. The data acquisition was carried out using way and cable force sensors attached to various weight machines, thereby enabling the measurement of essential displacement and force determinants during training. On the basis of the gathered data, it was consequently possible to deduce other significant characteristics like time periods or movement velocities. These parameters were applied for the development of intelligent methods adapted from conventional machine learning concepts, allowing an automatic assessment of the exercise technique and providing individuals with appropriate feedback. In practice, the implementation of such techniques could be crucial for the investigation of the quality of the execution, the assistance of athletes but also coaches, the training optimization and for prevention purposes. For the current study, the data was based on measurements from 15 rather inexperienced participants, performing 3-5 sets of 10-12 repetitions on a leg press machine. The initially preprocessed data was used for the extraction of significant features, on which supervised modeling methods were applied. Professional trainers were involved in the assessment and classification processes by analyzing the video recorded executions. The so far obtained modeling results showed good performance and prediction outcomes, indicating the feasibility and potency of AI techniques in assessing performances on weight training equipment automatically and providing sportsmen with prompt advice. Key pointsArtificial intelligence is a promising field for sport-related analysis.Implementations integrating pattern recognition techniques enable the automatic evaluation of data measurements.Artificial neural networks applied for the analysis of weight training data show good performance and high classification rates.', 'The goal of the study was to demonstrate the usefulness of artificial intelligence techniques in the example of weight training. pattern recognition methods for the evaluation of exercises performed on training machines are the focus of the research. The measurement of essential displacement and force determinants is now possible due to way and cable force sensors that are attached to weight machines. The data gathered can be used to deduce other significant characteristics like time periods or movement velocities. These parameters were used to create the intelligent methods which are able to provide individuals with appropriate feedback. The implementation of these techniques can be crucial for the investigation of the quality of the execution and assistance of athletes but also coaches. The current study was based on measurements from a group of 15 inexperienced participants and they did 4-5 sets of 14-16 reps on a leg press machine. The data that was preprocessed was used for the modeling of features. The assessment and classification processes were being conducted by trainers. The modeling results obtained so far showed a good performance and prediction outcome, indicating the feasibility and potency of some artificial intelligence techniques. Artificial intelligence is an interesting field to investigate sport related issues. Automatic evaluation of data measurements is possible by implementing pattern recognition techniques. Neural networks used to analyse weight training data show good performance.']

['Multirotor drones have been one of the most important technological advances of the last decade. Their mechanics are simple compared to other types of drones and their possibilities in flight are greater. For example, they can take-off vertically. Their capabilities have therefore brought progress to many professional activities. Moreover, advances in computing and telecommunications have also broadened the range of activities in which drones may be used. Currently, artificial intelligence and information analysis are the main areas of research in the field of computing. The case study presented in this article employed artificial intelligence techniques in the analysis of information captured by drones. More specifically, the camera installed in the drone took images which were later analyzed using Convolutional Neural Networks (CNNs) to identify the objects captured in the images. In this research, a CNN was trained to detect cattle, however the same training process could be followed to develop a CNN for the detection of any other object. This article describes the design of the platform for real-time analysis of information and its performance in the detection of cattle.', "One of the most important technological advances of the last decade was multirotor drones. Their mechanics are more simple than other types of drones. They're able to take-off vertically. Their accomplishments gave rise to many professional activities. There are more activities in which drones may be used thanks to advances in computing and telecommunication. Artificial intelligence and information analysis is the main research areas in the field of computing. Artificial intelligence was used in the case study to analyse the captured data. The objects captured in the images were later analyzed using Convolutional Neural Networks, in order to identify them. A CNN can be trained to detect cattle, but it could also be trained to detect any other objects. The design of the platform for analysis of data and its performance were described in this article."]

["With the continuous development of computer technology, big data acquisition and imaging methods, the application of artificial intelligence (AI) in medical fields is expanding. The use of machine learning and deep learning in the diagnosis and treatment of ophthalmic diseases is becoming more widespread. As one of the main causes of visual impairment, myopia has a high global prevalence. Early screening or diagnosis of myopia, combined with other effective therapeutic interventions, is very important to maintain a patient's visual function and quality of life. Through the training of fundus photography, optical coherence tomography, and slit lamp images and through platforms provided by telemedicine, AI shows great application potential in the detection, diagnosis, progression prediction and treatment of myopia. In addition, AI models and wearable devices based on other forms of data also perform well in the behavioral intervention of myopia patients. Admittedly, there are still some challenges in the practical application of AI in myopia, such as the standardization of datasets; acceptance attitudes of users; and ethical, legal and regulatory issues. This paper reviews the clinical application status, potential challenges and future directions of AI in myopia and proposes that the establishment of an AI-integrated telemedicine platform will be a new direction for myopia management in the post-COVID-19 period.", "Artificial intelligence applications in medical fields is expanding with the continuous development of computer technology. Machine learning and deep learning is being used in the treatment of diseases of the eye. One of the primary reasons for visual impairment is the high globalPrevalence. It is important to maintain the patient's visual function and good quality of life before a diagnosis of myopia is made. Artificial intelligence shows great application potential for the detection, diagnosis, progression prediction and treatment of myopia through the training of fundus photography, optical coherence tomography, and slit lamp images. Wearable devices based on other sources of data performed well in the behavioral intervention of patients with myasthenia brevis. There are still some challenges that are hard to overcome when using artificial intelligence, such as the standardization of datasets, user attitudes, and ethical, legal, and regulatory issues. The paper reviews the clinical application status, potential challenges and future directions of artificial intelligence and suggests a new direction for the management of myoretio."]

['Given the swift advancements in artificial intelligence (AI), the utilisation of AI-based clinical decision support systems (AI-CDSSs) has become increasingly prevalent in the medical domain, particularly in the management of cerebrovascular disease. To describe the design, rationale and methods of a cluster-randomised multifaceted intervention trial aimed at investigating the effect of cerebrovascular disease AI-CDSS on the clinical outcomes of patients who had a stroke and on stroke care quality. The GOLDEN BRIDGE II trial is a multicentre, open-label, cluster-randomised multifaceted intervention study. A total of 80 hospitals in China were randomly assigned to the AI-CDSS intervention group or the control group. For eligible participants with acute ischaemic stroke in the AI-CDSS intervention group, cerebrovascular disease AI-CDSS will provide AI-assisted imaging analysis, auxiliary stroke aetiology and pathogenesis analysis, and guideline-based treatment recommendations. In the control group, patients will receive the usual care. The primary outcome is the occurrence of new vascular events (composite of ischaemic stroke, haemorrhagic stroke, myocardial infarction or vascular death) at 3 months after stroke onset. The sample size was estimated to be 21 689 with a 26% relative reduction in the incidence of new composite vascular events at 3 months by using multiple quality-improving interventions provided by AI-CDSS. All analyses will be performed according to the intention-to-treat principle and accounted for clustering using generalised estimating equations. Once the effectiveness is verified, the cerebrovascular disease AI-CDSS could improve stroke care and outcomes in China. NCT04524624.', 'Artificial intelligence-based clinical decision support systems have gained popularity in the management of cerebrovascular disease due to the rapid advancement of this technology. The purpose of the trial is to investigate the effect of cerebrovascular disease on patients who have a stroke. The Golden Bridge trial was a multicentre, openlabel, cluster-randomised intervention study. There are 80 hospitals in China assigned to the intervention group or control group. If you are in the intervention group and have acute ischaemic stroke, the AI-CDSS can give you a variety of services. Patients in the control group will receive the usual healthcare. Incidence of new stroke events at 3 months post stroke is the primary outcome. The sample size was 21 695, and there was a 26% reduction in the incidence of new clot-forming events when using multiple quality- improving interventions. The analyses will include clustering using generalised estimating equations. The effectiveness of a cerebrovascular disease could increase stroke care and outcomes.']

['The digitization of a company necessitates not only the effort of the company but also state backing of network infrastructure. In this study, we applied the difference-in-differences method to examine the impact of the Broadband China Strategy on corporate digitalization and its heterogeneity using the data from Chinese listed firms from 2010 to 2020. The results show that the improvement in network infrastructure plays a vital role in promoting company digitization; this improvement is extremely varied due to variances in market demand and endowments. Non-state-owned firms, businesses in the eastern area, and technology-intensive businesses have profited the most. Among the five types of digitization, artificial intelligence and cloud computing are top priorities for enterprises. Our findings add to the literature on the spillover effects of broadband construction and the factors affecting enterprise digitalization.', 'State backing of network infrastructure is needed for the digitizing of a company. The broadband china strategy helped promote company digitization as a result of improvement in the network infrastructure,Results show that the improvement in network infrastructure plays a vital role in promoting company digitization Technology-intensive businesses, and non-state-owned firms, have been the ones to have made the most. Artificial intelligence, cloud computing and a few other types are top priorities for enterprises. We found evidence that the effects of broadband construction and the factors affecting enterprise digitization are spillovers into the literature.']

["Panoramic radiographs can assist dentist to quickly evaluate patients' overall oral health status. The accurate detection and localization of tooth tissue on panoramic radiographs is the first step to identify pathology, and also plays a key role in an automatic diagnosis system. However, the evaluation of panoramic radiographs depends on the clinical experience and knowledge of dentist, while the interpretation of panoramic radiographs might lead misdiagnosis. Therefore, it is of great significance to use artificial intelligence to segment teeth on panoramic radiographs. In this study, SWin-Unet, the transformer-based Ushaped encoder-decoder architecture with skip-connections, is introduced to perform panoramic radiograph segmentation. To well evaluate the tooth segmentation performance of SWin-Unet, the PLAGH-BH dataset is introduced for the research purpose. The performance is evaluated by F1 score, mean intersection and Union (IoU) and Acc, Compared with U-Net, Link-Net and FPN baselines, SWin-Unet performs much better in PLAGH-BH tooth segmentation dataset. These results indicate that SWin-Unet is more feasible on panoramic radiograph segmentation, and is valuable for the potential clinical application.", "dentist can quickly evaluate patients' oral health status The automatic diagnosis system plays a major role in the detection and Localization of tooth tissue on panoramic radiologists. The evaluation of panoramic radiographs is dependent on the clinical experience and knowledge of the dentist. Artificial intelligence is of great significance to use on panoramic images. SWin-Unet, a transformer-based U-shaped Encoder-decoder architecture with skip-connections, is used in this study. To evaluate the performance of SWin-Unet, the PLAGH–BH dataset is introduced. SWin-Unet has a better performance compared with other internet companies. SWin-Unet is more feasible on radiograph segmenting, and is useful for the clinical application."]

['Advances in imaging with optical coherence tomography (OCT) and optical coherence tomography angiography (OCTA) technology, including the development of swept source OCT/OCTA, widefield or ultra-widefield systems, have greatly improved the understanding, diagnosis, and treatment of myopia and myopia-related complications. Anterior segment OCT is useful for imaging the anterior segment of myopes, providing the basis for implantable collamer lens optimization, or detecting intraocular lens decentration in high myopic patients. OCT has enhanced imaging of vitreous properties, and measurement of choroidal thickness in myopic eyes. Widefield OCT systems have greatly improved the visualization of peripheral retinal lesions and have enabled the evaluation of wide staphyloma and ocular curvature. Based on OCT imaging, a new classification system and guidelines for the management of myopic traction maculopathy have been proposed; different dome-shaped macula morphologies have been described; and myopia-related abnormalities in the optic nerve and peripapillary region have been demonstrated. OCTA can quantitatively evaluate the retinal microvasculature and choriocapillaris, which is useful for the early detection of myopic choroidal neovascularization and the evaluation of anti-vascular endothelial growth factor therapy in these patients. In addition, the application of artificial intelligence in OCT/OCTA imaging in myopia has achieved promising results.', 'The development of the wideFIELD and ultra-widefield systems have led to improved understanding of the causes and treatment of the many syndromes that can be linked to myopia. It is possible to image the anterior segment of myopes with an eye exam using an eye exam. choroidal thickness is measured in the eyes with the help of OCT. Wide fieldOCT systems have improved the visualization of peripheral retina, and have aided the evaluation of wide staphyloma. A new classification system with guidelines for the management of MIP is proposed, as well as a description of a dome-shaped macula in the area of the optic nerve and peripapillary region. The evaluation of choriocapillaris and the detection of neovascularization can be done in patients with OCTA results. artificial intelligence has achieved promising results in a project.']

['Throughout the history of biological/medicine sciences, there has been opposing strategies to find solutions to complex human disease problems. Both empirical and deductive approaches have led to major insights and concepts that have led to practical preventive and therapeutic benefits for the human population. The classic definitions of "science" (to know) has been paired with the classic definition of technology (to do). One knew more as the technology developed, and that development was often based on science. In other words, one could do more if science could improve the technology. In turn, this made possible to know more science with improved technology. However, with the development of new technologies of today in biology and medicine, major advances have been made, such as the information from the Human Genome Project, genetic engineering techniques and the use of bioinformatic uses of sophisticated computer analyses. This has led to the renewed idea that Precision Medicine, while raising some serious ethical concerns, also raises the expectation of improved potential of risk predictions for prevention and treatment of various genetically and environmentally influenced human diseases. This new field Artificial Intelligence, as a major handmaiden to Precision Medicine, is significantly altering the fundamental means of biological discovery. However, can today\'s fundamental premise of "Artificial Intelligence", based on identifying DNA, as the primary nexus of human health and disease, provide the practical solutions to complex human diseases that involve the interaction of those genes with the broad spectrum of "environmental factors"? Will it be "precise" enough to provide practical solutions for prevention and treatments of diseases? In this "Commentary", with the example of human carcinogenesis, it will be challenged that, without the integration of mechanistic and hypothesis-driven approaches with the "unbiased" empirical analyses of large numbers of data, the Artificial Intelligence approach with fall short.', 'There have been multiple strategies to find solutions to complex human disease problems in the history of biological/medicine sciences. The benefits of preventative and therapeutic treatment have arisen from both the empirical and deductive approaches. The traditional definitions of science and technology have been reconciled. The technology was often based on science as it got developed. Science could possibly improve the technology. Thanks to this, more science can be learned with better technology. Major advances have been made with the development of new technologies, such as the human genome project, genetic engineering techniques and the use of bioinformatic uses of sophisticated computer analyses. The renewed idea that Precision Medicine raises some serious ethical concerns also raises the expectation of improved possibility of risk predictions for prevention and treatment of various genetically and environment influenced human diseases Artificial Intelligence is changing the fundamental means of biological discovery. Can Artificial Intelligence based on identifying genes and their interaction with environmental factors give practical solutions to complex Human diseases that involve the interaction of those genes with a broad spectrum of environmental factors?']

['Surgical data science (SDS) aims to improve the quality of interventional healthcare and its value through the capture, organization, analysis, and modeling of procedural data. As data capture has increased and artificial intelligence (AI) has advanced, SDS can help to unlock augmented and automated coaching, feedback, assessment, and decision support in surgery. We review major concepts in SDS and AI as applied to surgical education and surgical oncology.', 'Surgical data science is an area that aims to improve the quality of healthcare through the analysis of procedural data. As data capture has increased and artificial intelligence has advanced, it can help to unlocks augmented and automated coaching and feedback in surgery. Major concepts in AI and SDS can be applied to surgical education.']

["To review the current status of artificial intelligence systems in ophthalmology and highlight the steps required for clinical translation of artificial intelligence into personalized health care (PHC) in retinal disease. Artificial intelligence systems for ophthalmological application have made rapid advances, but are yet to attain a state of technical maturity that allows their adoption into real-world settings. There remains an 'artificial intelligence chasm' in the spheres of validation, regulation, safe implementation, and demonstration of clinical impact that needs to be bridged before the full potential of artificial intelligence to deliver PHC can be realized. Ophthalmology is currently in a stage between the demonstration of the potential of artificial intelligence and widespread deployment. Next stages include aggregating and curating datasets, training and validating artificial intelligence systems, establishing the regulatory framework, implementation and adoption with ongoing evaluation and model adjustment, and finally, meaningful human-artificial intelligence interaction with clinically validated tools that have demonstrated measurable impact on patient and healthcare system outcomes. Ophthalmologists should leverage the ability of artificial intelligence systems to glean insights from large volumes of multivariate data, and to interpret artificial intelligence recommendations in a clinical context. In doing so, the field will be well positioned to lead the transformation of health care in a personalized direction. VIDEO ABSTRACT: http://links.lww.com/COOP/A35.", 'There are steps required for clinical translation of artificial intelligence into personalized health care. Artificial intelligence systems are still in a state of technical maturity, which is needed to allow their adoption into real-world settings. Artificial intelligence has the potential to deliver PHC, however it still needs to be bridged before the full potential of it can be realized. There is a demonstration of the potential of artificial intelligence and widespread deployment. Next stages include establishing the regulatory framework, implementation and adoption, meaningful human-artificial intelligence interaction with clinically validated tools that have demonstrated measurable impact on patient and healthcare system outcomes, and more. Artificial intelligence systems have the ability to get insights from large numbers of data and to make recommendations in a clinical setting. The field will be well positioned to lead the way in the transformation of health care. There are links to the video. It was a long time ago.']

['Digitization is comparatively underdeveloped in the German healthcare system. Many digital applications are potentially capable of improving patient safety. This potential cannot be exploited in the long term with regional and time-limited projects. The present article identifies care areas where digital solutions are possible and necessary for safe patient care. In order for digital solutions and applications to be available to policyholders in the first healthcare market, producer of eHealth solutions will have to consider the goals of patient safety from the very beginning. In addition, processes and structures for proof of benefit must be implemented. (As supplied by publisher).', "German healthcare is not very advanced in digitization. Digital applications are capable of improving patient safety. Regional and time-limited projects can't exploit this potential, in the long term. Digital solutions are needed for safe patient care. In order for the first healthcare market of the future to include digital solutions and applications, the producer needs to think about the goals of patient safe from the beginning. The proof of benefit needs processes and structures implemented. As is provided by the publisher."]

["<b>Introduction</b>: At present, cancer imaging examination relies mainly on manual reading of doctors, which requests a high standard of doctors' professional skills, clinical experience, and concentration. However, the increasing amount of medical imaging data has brought more and more challenges to radiologists. The detection of digestive system cancer (DSC) based on artificial intelligence (AI) can provide a solution for automatic analysis of medical images and assist doctors to achieve high-precision intelligent diagnosis of cancers. <b>Areas covered</b>: The main goal of this paper is to introduce the main research methods of the AI based detection of DSC, and provide relevant reference for researchers. Meantime, it summarizes the main problems existing in these methods, and provides better guidance for future research. <b>Expert commentary</b>: The automatic classification, recognition, and segmentation of DSC can be better realized through the methods of machine learning and deep learning, which minimize the internal information of images that are difficult for humans to discover. In the diagnosis of DSC, the use of AI to assist imaging surgeons can achieve cancer detection rapidly and effectively and save doctors' diagnosis time. These can lay the foundation for better clinical diagnosis, treatment planning and accurate quantitative evaluation of DSC.", "The current paradigm of examining cancer is to rely on manual reading of doctors and requires a high standard of doctors' professional skills, clinical experience, and concentration. The amount of medical images has increased, bringing more challenges for the radiologists. Artificial intelligence can be used for detection of the digestive system cancer in order for it to be diagnosed with higher precision. The main goal of the paper is to give some basic information on the main research methods of the AI based detection of DSC. The main problems that exist in these methods are summarized in this report. The methods of machine learning and deep learning can be used to minimize the internal information of images that are difficult for humans to discover. Artificial intelligence can be used to help doctors find cancer faster and more effectively, and to save doctors' diagnoses time. These can be used to give better clinical diagnosis, treatment planning and accurate quantitative evaluation of DSC."]

['Clinical decision support systems are a growing class of tools with the potential to impact healthcare. This study investigates the construction of a decision support system through which clinicians can efficiently identify which previously approved historical treatment plans are achievable for a new patient to aid in selection of therapy. Treatment data were collected for early-stage lung and postoperative oropharyngeal cancers treated using photon (lung and head and neck) and proton (head and neck) radiotherapy. Machine-learning classifiers were constructed using patient-specific feature-sets and a library of historical plans. Model accuracy was analyzed using learning curves, and historical treatment plan matching was investigated. Learning curves demonstrate that for these datasets, approximately 45, 60, and 30 patients are needed for a sufficiently accurate classification model for radiotherapy for early-stage lung, postoperative oropharyngeal photon, and postoperative oropharyngeal proton, respectively. The resulting classification model provides a database of previously approved treatment plans that are achievable for a new patient. An exemplary case, highlighting tradeoffs between the heart and chest wall dose while holding target dose constant in two historical plans is provided. We report on the first artificial-intelligence based clinical decision support system that connects patients to past discrete treatment plans in radiation oncology and demonstrate for the first time how this tool can enable clinicians to use past decisions to help inform current assessments. Clinicians can be informed of dose tradeoffs between critical structures early in the treatment process, enabling more time spent on finding the optimal course of treatment for individual patients.', 'A growing class of tools is clinical decision support systems. The aim of this study is to build a decision support system that helps clinicians to identify which old treatment plans are doable for a new patient. Treatment data was collected for patients with early-stage lung and head and neck cancer. A library of historical plans and feature-set were used to construct machine-learning classifications. Learning curves were used to analyze model accuracy and historical treatment plan matching was done. There are at least 45, 60, and 30 patients who need a sufficiently accurate model for classification of radiation therapy The database of previously approved treatment plans is accessible from the classification model. The case highlighted tradeoffs between the heart and chest wall dose while holding target dose constant in two historical plans. We show how this system can enable clinicians to use past decisions to help inform current assessments of patients, and we give 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 is a 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 Clinicians are able to be informed of tradeoffs of dose between critical structures early in treatment, allowing more time to choose the best course of treatment for individual patients.']

['A confluence of technological capabilities is creating an opportunity for machine learning and artificial intelligence (AI) to enable "smart" nanoengineered brain machine interfaces (BMI). This new generation of technologies will be able to communicate with the brain in ways that support contextual learning and adaptation to changing functional requirements. This applies to both invasive technologies aimed at restoring neurological function, as in the case of neural prosthesis, as well as non-invasive technologies enabled by signals such as electroencephalograph (EEG). Advances in computation, hardware, and algorithms that learn and adapt in a contextually dependent way will be able to leverage the capabilities that nanoengineering offers the design and functionality of BMI. We explore the enabling capabilities that these devices may exhibit, why they matter, and the state of the technologies necessary to build them. We also discuss a number of open technical challenges and problems that will need to be solved in order to achieve this.', 'Artificial intelligence and machine learning are being used to enable smart brain Machine interfaces. The new generation of technologies will be able to communicate with the brain in ways that will support contextual learning. This applies to both the case of a neural implant and non-invagrant technologies that use signals like theEEG. Advances in computation, hardware, and Algorithms that learn and adapt will be able to leverage capabilities that offer the design and functionality of BMI. We take 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 888-353-1299 A number of open technical challenges and problems will need to be solved in order to achieve this.']

['The brain is an information machine equipped with mind and consciousness, acquired through a long history of evolution. Artificial intelligence (AI) aims at the development of intelligent functions in computers. We show mechanisms of deep learning in AI, and compare them with brain functions. In particular, we consider the function of consciousness in the brain, and its relation to AI. Mathematical neuroscience provides a powerful method to both AI and brain science. We conclude by emphasizing that further interactions are important for both AI and brain science.', 'The brain is an information machine which is the result of a long history of evolution. Artificial intelligence is being used to develop intelligent function in computers. We compare the mechanisms of deep learning with brain functions. We looked at the function of consciousness in the brain and its relation to artificial intelligence. Computational neuroscience helps with brain science. We emphasize that further interactions are important for both brain science and artificial intelligence.']

["Performance analysis based on artificial intelligence together with game-related statistical models aims to provide relevant information before, during and after a competition. Due to the evaluation of handball performance focusing mainly on the result and not on the analysis of the dynamics of the game pace through artificial intelligence, the aim of this study was to design and validate a specific handball instrument based on real-time observational methodology capable of identifying, quantifying, classifying and relating individual and collective tactical behaviours during the game. First, an instrument validation by an expert panel was performed. Ten experts answered a questionnaire regarding the relevance and appropriateness of each variable presented. Subsequently, data were validated by two observers (1.5 and 2 years of handball observational analysis experience) recruited to analyse a Champions League match. Instrument validity showed a high accordance degree among experts (Cohen's kappa index (k) = 0.889). For both automatic and manual variables, a very good intra- ((automatic: Cronbach's alpha (α) = 0.984; intra-class correlation coefficient (ICC) = 0.970; k = 0.917) (manual: α = 0.959; ICC = 0.923; k = 0.858)) and inter-observer ((automatic: α = 0.976; ICC = 0.961; k = 0.874) (manual: α = 0.959; ICC = 0.923; k = 0.831) consistency and reliability was found. These results show a high degree of instrument validity, reliability and accuracy providing handball coaches, analysts, and researchers a novel tool to improve handball performance.", "Performance analysis based on artificial intelligence and game related statistical models can provide relevant information before, during and after a competition. The main goal of the study was to create a specific instrument based on real-time observational methodologies capable of identifying, quantifying, classifying and relating individual players. The panel of experts performed the validation on the instrument. Experts were asked about the relevance of each variable presented. The data was analysed by two observers who had 888-492-0 888-492-0's to analyse a match. The validity was high among the experts. A very good correlation for both variables is Cronbashis's alpha () and k. A high degree of instrument validity, reliability and accuracy has been shown in the results."]

['Epidemiological changes in retinopathy of prematurity (ROP) depend on neonatal care, neonatal mortality, and the ability to carefully titrate and monitor oxygen. We evaluate whether an artificial intelligence (AI) algorithm for assessing ROP severity in babies can be used to evaluate changes in disease epidemiology in babies from South India over a 5-year period. Retrospective cohort study. Babies (3093) screened for ROP at neonatal care units (NCUs) across the Aravind Eye Care System (AECS) in South India. Images and clinical data were collected as part of routine tele-ROP screening at the AECS in India over 2 time periods: August 2015 to October 2017 and March 2019 to December 2020. All babies in the original cohort were matched 1:3 by birthweight (BW) and gestational age (GA) with babies in the later cohort. We compared the proportion of eyes with moderate (type 2) or treatment-requiring (TR) ROP, and an AI-derived ROP vascular severity score (from retinal fundus images) at the initial tele-retinal screening exam for all babies in a district, VSS), in the 2 time periods. Differences in the proportions of type 2 or worse and TR-ROP cases, and VSS between time periods. Among BW and GA matched babies, the proportion [95% confidence interval {CI}] of babies with type 2 or worse and TR-ROP decreased from 60.9% [53.8%-67.7%] to 17.1% [14.0%-20.5%] (P < 0.001) and 16.8% [11.9%-22.7%] to 5.1% [3.4%-7.3%] (P < 0.001), over the 2 time periods. Similarly, the median [interquartile range] VSS in the population decreased from 2.9 [1.2] to 2.4 [1.8] (P < 0.001). In South India, over a 5-year period, the proportion of babies developing moderate to severe ROP has dropped significantly for babies at similar demographic risk, strongly suggesting improvements in primary prevention of ROP. These results suggest that AI-based assessment of ROP severity may be a useful epidemiologic tool to evaluate temporal changes in ROP epidemiology. Proprietary or commercial disclosure may be found after the references.', 'Changes in ROP can depend on factors such as the number of babies, mortality and the ability to monitor oxygen. The artificial intelligence would be used to evaluate changing disease epidemiology in babies from India over a 5- year period. The retrospective cohort study is conducted again. Babies were screened for ROP at the NCU at Aravind Eye Care System in South India. During the 2 time periods, all the babies were matched by birthweight, and all the images were collected as part of routine tele-ROP screening at the AECS in India. We compared the proportion of eyes that were type 2 or TR to the amount of eyes that were type 2 or TR at the initial tele-retinal screening exam for all babies in the district. There are differences in the proportions of type 2 or worse and TR-ROP cases. The proportion of babies with type 2 or worse decreased from 69.9% to 64.9% among BW and GA matching babies. The median value of service in the population decreased from 2.6 to 2.2. In South India, over the last five years the proportion of babies developing moderate to severe Rop has dropped significantly for babies at similar demographic risk. The results show that an assessment of severity using artificial intelligence can be used to evaluate temporal changes in epidemiology. References may reveal private or commercial information.']

['Artificial intelligence (AI) is a major branch of computer science that is fruitfully used for analyzing complex medical data and extracting meaningful relationships in datasets, for several clinical aims. Specifically, in the brain care domain, several innovative approaches have achieved remarkable results and open new perspectives in terms of diagnosis, planning, and outcome prediction. In this work, we present an overview of different artificial intelligent techniques used in the brain care domain, along with a review of important clinical applications. A systematic and careful literature search in major databases such as Pubmed, Scopus, and Web of Science was carried out using "artificial intelligence" and "brain" as main keywords. Further references were integrated by cross-referencing from key articles. 155 studies out of 2696 were identified, which actually made use of AI algorithms for different purposes (diagnosis, surgical treatment, intra-operative assistance, and postoperative assessment). Artificial neural networks have risen to prominent positions among the most widely used analytical tools. Classic machine learning approaches such as support vector machine and random forest are still widely used. Task-specific algorithms are designed for solving specific problems. Brain images are one of the most used data types. AI has the possibility to improve clinicians\' decision-making ability in neuroscience applications. However, major issues still need to be addressed for a better practical use of AI in the brain. To this aim, it is important to both gather comprehensive data and build explainable AI algorithms.', 'Artificial intelligence is a branch of computers that is used for a variety of purposes. There are several approaches to brain care that have achieved remarkable results and have opened new perspectives in terms of diagnosis, planning, and outcome prediction. We present an overview of the different artificial intelligent techniques that are utilized in brain care. A systematic literature search in major databases was carried out using artificial intelligence and the brain. Further references were integrated from a few key articles. There are 155 studies that used artificial intelligence to make use of it for different purposes. Artificial neural networks are rising in stature among analytic tools. Supporting machine and random forest are examples of classic machine learning approaches still used today. Task-specific algorithms help solve specific problems. Brain images are a popular type of data. Neural networks can be improved by the use of artificial intelligence in decision-making. Major issues still need to be looked at for a possible better use of Artificial Intelligence in the brain. To achieve this goal, it is important to gather comprehensive data.']

['This study examines related literature to propose a model based on artificial intelligence (AI), that can assist in the diagnosis of depressive disorder. Depressive disorder can be diagnosed through a self-report questionnaire, but it is necessary to check the mood and confirm the consistency of subjective and objective descriptions. Smartphone-based assistance in diagnosing depressive disorders can quickly lead to their identification and provide data for intervention provision. Through fast region-based convolutional neural networks (R-CNN), a deep learning method that recognizes vector-based information, a model to assist in the diagnosis of depressive disorder can be devised by checking the position change of the eyes and lips, and guessing emotions based on accumulated photos of the participants who will repeatedly participate in the diagnosis of depressive disorder.', "There is a study looking into the use of Artificial intelligence to help withdiagnosis of mental illness. The depression can be diagnosed through self-report questionnaires but it's important to check the mood and get the consistency of the descriptions. Data for intervention provision can be provided with the help they give to diagnose depressed people. A deep learning method that recognizes Vector-based information can be used to create a model to help in the diagnosis of depression, and the process of guessing emotions can be accomplished using accumulated pictures of the participants."]

['Schizophrenia is a psychiatric disorder presenting a lack of insight. Although insight changes over time, longitudinal studies of insight in schizophrenia are scarce. Furthermore, most previous studies on insight and intelligence have not measured full-scale IQ and have not been able to examine the relationship between detailed dimensions of cognitive function and insight. In this study, we assessed insight at two time points and assessed dimensions of cognitive function. A total of 163 patients with schizophrenia participated in the study. We evaluated insight at two time points to understand the patterns of change and examined the association between insight and clinical variables. Additionally, we examined the relationship between the dimensions of cognitive function and insight. The patients were divided into three groups based on their change in insight over time: stable at a low level of insight (poor insight), stable at a high level of insight (good insight), and changed in insight over time (unstable insight). Those in the poor insight group had lower general intelligence scores than those in the good insight and unstable insight groups. Regarding cognitive function, verbal comprehension was associated with the level of insight at baseline and follow-up. Regarding psychiatric symptoms, the poor insight group exhibited more severe symptoms than the other two groups, especially regarding positive symptoms. Our classification of patients based on changes in insight revealed that poor insight patients had impaired cognitive function, especially verbal comprehension, and more severe positive symptoms than good insight or unstable insight patients.', 'Schizophrenia has a lack of insight Longitudinal studies of insight are rare in the field. Most previous studies have notmeasured full-scale IQ, and so have not been able to examine the relationship between deep cognitive function and insight. Insight was assessed at two time points and dimensions of cognitive function were assessed. There are a total of 163 patients in the study We examined insight and clinical variables and found that they correlated with each other. We looked at the relationship between the dimensions of cognitive function and insight. The patients were separated into three different groups based on their change in insight over time: stable at a low level of Insight, stable at a high level of Insight, and instability. The good insight group had lower intelligence scores than the poor insight group. verbal comprehension was related with the level of insight at baseline and follow-up. The poor insight group had more severe symptoms than the other group and even the positive symptom group. Patients who had poor insight were found to have more serious cognitive problems than patients with good insight or unstable insight.']

['Changes in cerebral cortical regions occur in HIV-infected patients, even in those with mild neurocognitive disorders. Working memory / attention is one of the most affected cognitive domain in these patients, worsening their quality of life. Our objective was to assess whether cortical thickness differs between HIV-infected patients with and without working memory deficit. Forty-one adult HIV-infected patients with and without working memory deficit were imaged on a 1.5 T scanner. Working memory deficit was classified by composite Z scores for performance on the Digits and Letter-Number Sequencing subtests of the Wechsler Adult Intelligence Scale (third edition; WAIS-III). Cortical thickness was determined using FreeSurfer software. Differences in mean cortical thickness between groups, corrected for multiple comparisons using Monte-Carlo simulation, were examined using the query design estimate contrast tool of the FreeSurfer software. Greater cortical thickness in left pars opercularis of the inferior frontal gyrus, and rostral and caudal portions of the left middle frontal gyrus (cluster 1; p = .004), and left superior frontal gyrus (cluster 2; p = .004) was observed in HIV-infected patients with working memory deficit compared with those without such deficit. Negative correlations were found between WAIS-III-based Z scores and cortical thickness in the two clusters (cluster 1: ρ = -0.59; cluster 2: ρ = -0.47). HIV-infected patients with working memory deficit have regions of greater thickness in the left frontal cortices compared with those without such deficit, which may reflect increased synaptic contacts and/or an inflammatory response related to the damage caused by HIV infection.', 'Changes in cerebral cortical areas occur in patients with HIV. The quality of life of these patients is affected by working memory/Attention. Our aim was to determine if cortical thickness differences between HIV patients with and without working memory deficit were caused by the disease. Forty-one patients with HIV and without working memory deficits were scanned with a 1.5 T scanner. The performance on the letter-number and digit segments of the Wechler Adult Intelligence Scale are classified by the Z scores for working memory deficit. The thickness of the skin was determined using a software. There were differences in mean cortical thickness in the groups that were examined using the query design estimate contrast tool of the FreeSurfer software. There is a larger cortical thickness in left pars opercularis of the inferior frontal gyrus and left middle gyrus. There were correlations between the Z scores and the thickness of the cortex in the two clusters. HIV patients that have a working memory deficit have thicker regions in their left brain compared to the rest, which may be due to increased synaptic contacts and an inflammatory response related to the damage caused by HIV.']

["Policymakers require a systematic approach when planning for information technology needs in healthcare. The aim of this study was to obtain experts' predictions of future health information technology (HIT) needs until 2025 for Iran in relation to the relative importance of key technologies, expected timeframe of realisation, areas that may be impacted upon and obstacles to achieving these goals. This article presents results from the third phase (a Delphi study) of a larger mixed-method study. Policymakers from the Iranian Ministry of Health and faculty members from different medical universities across the country who were expert in the field of HIT were invited to participate (<i>n</i> = 61). Participants (39) completed the first-round questionnaire and 24 completed the second. The development of personal health records (<i>n</i> = 32, 82.0%), the development of clinical decision-making systems (<i>n</i> = 30, 76.9%) and the use of business intelligence for collecting and analysing clinical and financial data (<i>n</i> = 32, 82.0%) were predicted to occur after 2025. The healthcare areas predicted to experience the greatest impact from most HITs were facilitating patient-provider communication and improving healthcare quality. Key barriers to achieving HITs were related to weaknesses in planning and limited financial resources for most technologies. By identifying the areas of impact and the barriers to achieving the HIT goals, more accurate planning is possible and resources can be allocated according to priorities.", "Policy makers need a systematic approach to healthcare information technology needs. In order to obtain experts' predictions of future health information technology needs until 2025 for Iran, this study has to be conducted. The results from the third phase of the study are presented in the article. Faculty members from various medical universities across the country were invited to participate in the workshop to gain the experience of working in the field of HIT. The first-round questions were finished by 39 participants and the second by 24 The development of personal health records and the use of business intelligence for collecting and analyzing clinical and financial data were part of the study. Weakness in planning are two of the key obstacles to achieving HITs. By knowing the areas of impact and the obstacles that need to be conquered, planners can allocate resources in accordance with priorities."]

["The ongoing COVID-19 pandemic has profoundly affected millions of lives globally, with some individuals experiencing persistent symptoms even after recovering. Understanding and managing the long-term sequelae of COVID-19 is crucial for research, prevention, and control. To effectively monitor the health of those affected, maintaining up-to-date health records is essential, and digital health informatics apps for surveillance play a pivotal role. In this review, we overview the existing literature on identifying and characterizing long COVID manifestations through hierarchical classification based on Human Phenotype Ontology (HPO). We outline the aspects of the National COVID Cohort Collaborative (N3C) and Researching COVID to Enhance Recovery (RECOVER) initiative in artificial intelligence (AI) to identify long COVID. Through knowledge exploration, we present a concept map of clinical pathways for long COVID, which offers insights into the data required and explores innovative frameworks for health informatics apps for tackling the long-term effects of COVID-19. This study achieves two main objectives by comprehensively reviewing long COVID identification and characterization techniques, making it the first paper to explore incorporating long COVID as a variable risk factor within a digital health informatics application. By achieving these objectives, it provides valuable insights on long COVID's challenges and impact on public health.", "The ongoing COVID-19 flu has caused massive havoc worldwide, as some individuals experience persistent symptoms even after recovering. Understanding and managing the long-term sequelae is a must for research, prevention, and control. Digital health phins are used to aid in monitoring the health of people that have been affected, and keeping up-to-date health records is a must. We reviewed the existing literature on identifying and describing long COV manifestations, using Human Phenotype Ontology. The National COVID Collaborative (N4C) and Researching COVID to Enhance Recovery in artificial intelligence (ai) are outlined in this section. We present aconcept map of clinical pathways for long COVID, which offers insight into the data required and explores innovative frameworks for health informatics apps for tackling the long term effects of COVID-19. Long COVID's challenges and impact on public health are provided insights by achieving these objectives."]

["The presence of defects like gas bubble in fabricated parts is inherent in the selective laser sintering process and the prediction of bubble shrinkage dynamics is crucial. In this paper, two artificial intelligence (AI) models based on Decision Trees algorithm were constructed in order to predict bubble dissolution time, namely the Ensemble Bagged Trees (EDT Bagged) and Ensemble Boosted Trees (EDT Boosted). A metadata including 68644 data were generated with the help of our previously developed numerical tool. The AI models used the initial bubble size, external domain size, diffusion coefficient, surface tension, viscosity, initial concentration, and chamber pressure as input parameters, whereas bubble dissolution time was considered as output variable. Evaluation of the models' performance was achieved by criteria such as Mean Absolute Error (MAE), Root Mean Squared Error (RMSE) and coefficient of determination (R<sup>2</sup>). The results showed that EDT Bagged outperformed EDT Boosted. Sensitivity analysis was then conducted thanks to the Monte Carlo approach and it was found that three most important inputs for the problem were the diffusion coefficient, initial concentration, and bubble initial size. This study might help in quick prediction of bubble dissolution time to improve the production quality from industry.", "Predicting the bubble size in fabricated parts is crucial because of the defects inherent in the process. In order to predict bubble dissolution time, two Artificial Intelligence models based on Decision trees were constructed. The data was generated with the help of a tool. The initial bubble size, external domain size and chamber pressure were input parameters used by the models. The models' performance was evaluated by using three criteria; Mean Absolute Error (MAE),Root Mean Squared Error (RMSE) and the correlation of determination. The results show that there was an inverse relationship between the two between EDT Bagged and the 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 888-666-1846 The Monte Carlo approach yielded a sensitivity analysis that stated the most important inputs were the diffusion coefficients. This study can help predict bubble dissolution time to improve production quality."]

['In this study, we investigated the efficacy of a self-administered, online Social Intelligence Training (SIT) program aimed at enhancing psychological and relational well-being among a nationwide U.S. sample of custodial grandmothers. A two-arm randomized clinical trial (RCT) was conducted, where 349 grandmothers raising grandchildren aged 11-18 years were assigned to either SIT or an attention control condition (ACC). Participants self-completed online surveys at baseline and immediately post-intervention, in addition to follow-ups at three, six, and nine months post-intervention. First-order latent difference score models were used to compare SIT to ACC, across all times of measurement, along key indicators of psychological and relational well-being on an intent-to-treat basis. Although SIT was largely superior to ACC at yielding positive results, it appears that it attenuated longitudinal declines that occurred among ACC participants. SIT also exerted stronger effects on relational than psychological outcomes, with perceived relations with grandchildren being the most positively affected. Given that the historical time of this RCT unpredictably corresponded with the COVID-19 pandemic, we suspect that SIT helped offset declines in psychological and relational well-being that are widely documented to have resulted from the pandemic. Our overall positive findings support future use of the inexpensive and easily delivered SIT program under normal environmental conditions, with the vulnerable and geographically disperse population of custodial grandmothers.', 'The effectiveness of a self administered online Social Intelligence Training program was investigated in this study. S. sample of grandmothers 349 grandmothers who were raising their children between the ages of 11 and 18 were randomized to one of two treatment groups. The participants self-completed online surveys at baseline, and immediately after the intervention. Key indicators of psychological well-being were used along with first order model to compare SIT to ACC. It appears that there were declines in participants in the conference despite the positive results. The effects on perceived relations with a child were the most positive by virtue of their similarity to a grandchild. We think that the time of this RCT was marked by the COVID-19 Pandemic, which is why we believe that SIT helped to offset the declines in psychological and well-being that have been documented. The inexpensive and easily delivered program supports future use with vulnerable women in the community.']

['This exploratory study focuses on sequential bi-/multilinguals (specifically, nonimmigrant young Dutch native speakers who learned at least one foreign language (FL) at or after the age of 5) and investigates the impact of proficiency-based and amount-of-use-based degrees of multilingualism in different modalities (i.e., speaking, listening, writing, reading) on inhibition, disengagement of attention, and switching. Fifty-four participants completed a comprehensive background questionnaire, a nonverbal fluid intelligence task, a Flanker task, and the Trail Making Test. Correlational and regression analyses considering multilingualism related variables and other variables that may contribute to the cognitive abilities under investigation (e.g., years of formal education, socioeconomic status, physical activity, playing video-games) revealed that only proficiency-based degrees of multilingualism impacted cognitive abilities. Particularly, mean FL writing proficiency affected inhibition (i.e., significant positive flanker effect) and L2 listening proficiency influenced disengagement of attention (i.e., significant negative sequential congruency effect). Our findings suggest that only those speakers who have reached a certain proficiency threshold in more than one FL show a cognitive advantage, which, in our sample, emerged in inhibition only. Furthermore, our study suggests that, regarding the impact of proficiency-based degrees of multilingualism on cognitive abilities, for our participants the writing and listening modalities mattered most.', 'This exploratory study focuses on sequential bi-/multilinguals who are not immigrants but who learned at least one foreign language after the age of 5) and investigates the impact of degree of multilingualism. E. Intruder, disengagement of attention, and switch were discussed. Fifty-four participants were shown a comprehensive background questionnaire, and were given a series of tasks. Multilingualism related variables and other variables that may contribute to the cognitive abilities under investigation are analyzed in correlational and regression analyses. g In years of formal education, socioeconomic status, physical activity and playing video-games, cognitive abilities were the only ones impacted by degrees of multilingualism. Especially, mean FL writing ability affected inhibition... E. Positive flanker effect and L2 listening is influence on attention. E. Significant negative sequential congruency effect. Our findings show that only the speakers who are proficient in at least one FL show an advantage, which, in our sample, emerged in inhibition. The research suggests that the impact of degree of multilingualism on cognitive abilities was the most significant issue for our participants.']

["Diabetes Mellitus, one of the leading causes of death worldwide, has no cure to date and can lead to severe health complications, such as retinopathy, limb amputation, cardiovascular diseases, and neuronal disease, if left untreated. Consequently, it becomes crucial to be able to monitor and predict the incidence of diabetes. Machine learning approaches have been proposed and evaluated in the literature for diabetes prediction. This paper proposes an IoT-edge-Artificial Intelligence (AI)-blockchain system for diabetes prediction based on risk factors. The proposed system is underpinned by blockchain to obtain a cohesive view of the risk factors data from patients across different hospitals and ensure security and privacy of the user's data. We provide a comparative analysis of different medical sensors, devices, and methods to measure and collect the risk factors values in the system. Numerical experiments and comparative analysis were carried out within our proposed system, using the most accurate random forest (RF) model, and the two most used state-of-the-art machine learning approaches, Logistic Regression (LR) and Support Vector Machine (SVM), using three real-life diabetes datasets. The results show that the proposed system predicts diabetes using RF with 4.57% more accuracy on average in comparison with the other models LR and SVM, with 2.87 times more execution time. Data balancing without feature selection does not show significant improvement. When using feature selection, the performance is improved by 1.14% for PIMA Indian and 0.02% for Sylhet datasets, while it is reduced by 0.89% for MIMIC III.", 'Diabetes Mellitus is one of the leading causes of death for women, and can lead to many serious health problems, such as heart disease, and even amputation. The incidence of diabetes has to be monitored and predicted in order to be able to monitor it. Machine learning approaches have been proposed for prediction of diabetes. A paper proposes an artificial intelligence-blockchain system for predicting diabetes based on risk factors. The system is underpinned by the Internet of Things to securely store and share risk factors data from patients across the country. We show a comparison of different medical sensors, devices, and methods to collect and analyze the risk factors in the system. Our system used the most accurate random forest model, Logistic Regression (LR) and SupportVector Machine (SVP), as well as three real-life Diabetes to carry out the numerical experiments and comparative analysis. The proposed system is more accurate on average than the other models with more execution time because of its use of RF. Data balancing without selection does not show significant improvement. Features selection reduced the performance by 0.89% for MIMIC III and improved it by 1.14% for PIMA Indian.']

['Deep learning techniques, including convolutional neural networks (CNN), have the potential to improve breast cancer risk prediction compared to traditional risk models. We assessed whether combining a CNN-based mammographic evaluation with clinical factors in the Breast Cancer Surveillance Consortium (BCSC) model improved risk prediction. We conducted a retrospective cohort study among 23,467 women, age 35-74, undergoing screening mammography (2014-2018). We extracted electronic health record (EHR) data on risk factors. We identified 121 women who subsequently developed invasive breast cancer at least 1 year after the baseline mammogram. Mammograms were analyzed with a pixel-wise mammographic evaluation using CNN architecture. We used logistic regression models with breast cancer incidence as the outcome and predictors including clinical factors only (BCSC model) or combined with CNN risk score (hybrid model). We compared model prediction performance via area under the receiver operating characteristics curves (AUCs). Mean age was 55.9 years (SD, 9.5) with 9.3% non-Hispanic Black and 36% Hispanic. Our hybrid model did not significantly improve risk prediction compared to the BCSC model (AUC of 0.654 vs 0.624, respectively, p = 0.063). In subgroup analyses, the hybrid model outperformed the BCSC model among non-Hispanic Blacks (AUC 0.845 vs. 0.589; p = 0.026) and Hispanics (AUC 0.650 vs 0.595; p = 0.049). We aimed to develop an efficient breast cancer risk assessment method using CNN risk score and clinical factors from the EHR. With future validation in a larger cohort, our CNN model combined with clinical factors may help predict breast cancer risk in a cohort of racially/ethnically diverse women undergoing screening.', 'Deep learning techniques have the possibility to improve breast cancer risk prediction. We evaluated the impact of combining mammographic evaluation with clinical factors into the BCSC model to improve risk prediction. A retrospective cohort study was conducted among 23, 437 women who had been undergoing mammograms. We got the data on risk factors. We identified the 121 women that developed breast cancer a year or two after the baseline mammogram. CNN architecture was used to analyze mammographic evaluations using a pixels-wise approach. Logistic regression models with breast cancer incidence were used to derive the outcome and predictors. Model prediction performance is compared under the AUCs. There was a mean age of 55.8 years with 9.3% of it non- Hispanic. The hybrid model did not make any significant improvements over the BCSC model. The hybrid model was better in subgroup analyses in non-Hispanic blacks. Hispanics ( AUC 0.650 vs 0.595; p 0.05). We tried to develop an efficient method for assessing breast cancer risk based onCNN and clinical factors. Our CNN model and clinical factors may be able to predict breast cancer risk in a group of diverse women undergoing screening.']

['Digital health interventions can bridge barriers in access to treatment among individuals with chronic pain. This study aimed to evaluate the perceived needs, engagement, and effectiveness of the mental health app Wysa with regard to mental health outcomes among real-world users who reported chronic pain and engaged with the app for support. Real-world data from users (N=2194) who reported chronic pain and associated health conditions in their conversations with the mental health app were examined using a mixed methods retrospective observational study. An inductive thematic analysis was used to analyze the conversational data of users with chronic pain to assess perceived needs, along with comparative macro-analyses of conversational flows to capture engagement within the app. Additionally, the scores from a subset of users who completed a set of pre-post assessment questionnaires, namely Patient Health Questionnaire-9 (PHQ-9) (n=69) and Generalized Anxiety Disorder Assessment-7 (GAD-7) (n=57), were examined to evaluate the effectiveness of Wysa in providing support for mental health concerns among those managing chronic pain. The themes emerging from the conversations of users with chronic pain included health concerns, socioeconomic concerns, and pain management concerns. Findings from the quantitative analysis indicated that users with chronic pain showed significantly greater app engagement (P<.001) than users without chronic pain, with a large effect size (Vargha and Delaney A=0.76-0.80). Furthermore, users with pre-post assessments during the study period were found to have significant improvements in group means for both PHQ-9 and GAD-7 symptom scores, with a medium effect size (Cohen d=0.60-0.61). The findings indicate that users look for tools that can help them address their concerns related to mental health, pain management, and sleep issues. The study findings also indicate the breadth of the needs of users with chronic pain and the lack of support structures, and suggest that Wysa can provide effective support to bridge the gap.', 'There are barriers to treatment for individuals with chronic pain. A study was done to evaluate the effectiveness and perceived needs of the mental health app Wysa. A mixed method study looked at real-world data from users who reported chronic pain and health conditions in their mental health app discussions. An analysis of data from the app to assess perceived needs was used along with comparative macro analyses of flows to capture engagement. Pre-post assessment questionnaires, namelyPatient Health Questionnaire-9 (PHQ-9) and Generalized Anxiety Disorder Assessment-7 ( GAD-7) were analyzed to evaluate the effectiveness of Wysa. Health concerns, socioeconomic concerns, and pain management concerns are some of the themes emerging from conversations with users with chronic pain. Findings from the quantitative analysis indicated that the users with chronic pain showed a larger effect size than the users without chronic pain. Users with pre-post assessments showed significant improvements in group means and symptom scores in a small study. The findings show that users are looking for tools that they can use to address their concerns. The breadth of the needs of users and the lack of support structures were indicated by the study, and suggest that Wysa can provide effective support to bridge the gap.']

['The industrial internet of things (IIoT), a leading technology to digitize industrial sectors and applications, requires the integration of edge and cloud computing, cyber security, and artificial intelligence to enhance its efficiency, reliability, and sustainability. However, the collection of heterogeneous data from individual sensors as well as monitoring and managing large databases with sufficient security has become a concerning issue for the IIoT framework. The development of a smart and integrated IIoT infrastructure can be a possible solution that can efficiently handle the aforementioned issues. This paper proposes an AI-integrated, secured IIoT infrastructure incorporating heterogeneous data collection and storing capability, global inter-communication, and a real-time anomaly detection model. To this end, smart data acquisition devices are designed and developed through which energy data are transferred to the edge IIoT servers. Hash encoding credentials and transport layer security protocol are applied to the servers. Furthermore, these servers can exchange data through a secured message queuing telemetry transport protocol. Edge and cloud databases are exploited to handle big data. For detecting the anomalies of individual electrical appliances in real-time, an algorithm based on a group of isolation forest models is developed and implemented on edge and cloud servers as well. In addition, remote-accessible online dashboards are implemented, enabling users to monitor the system. Overall, this study covers hardware design; the development of open-source IIoT servers and databases; the implementation of an interconnected global networking system; the deployment of edge and cloud artificial intelligence; and the development of real-time monitoring dashboards. Necessary performance results are measured, and they demonstrate elaborately investigating the feasibility of the proposed IIoT framework at the end.', 'The integration of edge and cloud computing along with artificial intelligence, and cyber security are necessary for the success of the industrial internet of things. It has become a concern for the framework that the collection of heterogeneous data including individual sensors as well as monitoring and managing large databases have insufficient security. The development of a smart and integrated IIoT infrastructure can be a solution to handle the aforementioned issues. This paper proposes an infrastructure that includes heterogeneous data collection and storing capability, global communication and a real-time anomaly detection model. The development of smart data acquisition devices and the transfer of energy data to edge IIoTservers are intended to this end. The security protocol used on the server is also applied to the credentials. The server can exchange data with a secure message. Big data is handled by cloud databases. For detecting the anomalies of individual electrical appliances in real-time, an idea based on a group of isolation forest models is built and implemented on edge and cloud server as well. Users can monitor the system through online dashboards. Hardware design, the development of open-sourced IIoT server and databases, the deployment of edge and cloud artificial intelligence, and the development of real-time monitoring dashboard are all covered in this study. Performance results are measured and they show how feasibility studies can be done.']

["Smart medical uses the medical information platform and the current technological means to enable the process of sharing information between medical staff and medical equipment. The combination of current technology and the medical field has become the norm. In the future, more artificial intelligence technologies will be integrated into the medical field to promote the development of medical care. At present, the information on the Internet is very large and complex, and general search engines often do not have knowledge in certain professional fields and can only perform shallow keyword searches. Therefore, it is difficult to meet people's medical diagnosis needs, and smart medical care can solve these needs. Medical imaging refers to the technology or process of obtaining internal tissue images of a certain part of the human body for medical research, including medical imaging systems and medical image processing. Medical image processing refers to the further processing of the obtained images, the purpose of which is either to restore the original image that was not clear enough or to highlight some characteristic information in the image. The purpose of this paper is to study the research on the selection of T1 stage renal tumor resection based on the imaging MAP score under smart medical care. It is hoped that through smart medicine and medical imaging technology, it can help renal tumor resection, reduce the sequelae of renal tumor resection, and promote the development of medical services. This paper proposes applying natural language processing technology to the medical field, creating an intelligent diagnosis assistance system, and using the existing medical record data to realize the corresponding medical assistance functions. It studies the class imbalance problem prevalent in medical datasets and provides better solutions through ensemble learning techniques to improve classifier performance when the number of positive and negative samples is unbalanced. The experimental results in this paper show that the creatinine of patients undergoing renal tumor resection combined with smart medicine and imaging technology is stable at 75 mol/L, while the creatinine is stable at 71 mol/L in other methods. It shows that the postoperative effect of smart medical treatment and imaging technology is better.", "The medical information platform is the basis for smart medical, which uses the technology to share medical information with one another. The medical field and technological innovation have become commonplace. artificial intelligence technologies will be used in the Medical field to promote the care of patients. General search engines are often limited to doing shallow searches on the Internet because of the large amount of information and lack of knowledge of certain professional fields. Smart medical care can give people's medical diagnosis needs. Medical image processing can be defined as the process of using technology to obtain internal tissue pictures of some part of the human body. Medical image processing involves the further processing of the obtained images, which may be used to highlight a characteristic information in the image or restore the original image that was not clear enough. This paper is about the research surrounding T1 stage renal tumor resection using theMAP score as a basis for decision making. It's hoped that using smart medicine and medical technology will improve the lives of patients in the future. The paper proposes using natural language processing technology to add medical assistance functions and use existing medical record data to do so. The class imbalance problem in medical data can be studied and better solutions can be found through ensemble learning techniques. The results from the experiment show that a stable percentage of patients have their creatinine stable at around 75 mol/L outside of the surgical setting. The results show that smart medical treatment has a better effect on the body."]

['Deep learning is an active bioinformatics artificial intelligence field that is useful in solving many biological problems, including predicting altered epigenetics such as DNA methylation regions. Deep learning (DL) can learn an informative representation that addresses the need for defining relevant features. However, deep learning models are computationally expensive, and they require large training datasets to achieve good classification performance. One approach to addressing these challenges is to use a less complex deep learning network for feature selection and Machine Learning (ML) for classification. In the current study, we introduce a hybrid DL-ML approach that uses a deep neural network for extracting molecular features and a non-DL classifier to predict environmentally responsive transgenerational differential DNA methylated regions (DMRs), termed epimutations, based on the extracted DL-based features. Various environmental toxicant induced epigenetic transgenerational inheritance sperm epimutations were used to train the model on the rat genome DNA sequence and use the model to predict transgenerational DMRs (epimutations) across the entire genome. The approach was also used to predict potential DMRs in the human genome. Experimental results show that the hybrid DL-ML approach outperforms deep learning and traditional machine learning methods.', "Deep learning is used in the prediction of altered epigenetics, one of the problems it is useful in. Deep learning can show an informative representation to address the need for defining features. Deep learning models are very expensive and require a lot of training to achieve good classification performance. Machine learning and deep learning networks should be used to find feature and classification. The current study uses a deep neural network for neuralgia and a non-DL classifier The model was designed to predict transgenerational damage due to environmental toxicants and was used for the rat's whole genome as well as the sperm epimutations. The approach is used to determine what types of DMRs are in the human genome. The results show that the hybrid approach beats traditional methods."]

['Coronavirus disease 2019 (COVID-19) can result in deterioration of cardiac function, which is associated with high mortality. A simple point-of-care diagnostic test to screen for ventricular dysfunction would be clinically useful to guide management. We sought to review the clinical experience with an artificial intelligence electrocardiogram (AI ECG) to screen for ventricular dysfunction in patients with documented COVID-19. We examined all patients in the Mayo Clinic system who underwent clinically indicated electrocardiography and echocardiography within 2 weeks following a positive COVID-19 test and had permitted use of their data for research were included. Of the 27 patients who met the inclusion criteria, one had a history of normal ventricular function who developed COVID-19 myocarditis with rapid clinical decline. The initial AI ECG in this patient indicated normal ventricular function. Repeat AI ECG showed a probability of ejection fraction (EF) less than or equal to 40% of 90.2%, corroborated with an echocardiographic EF of 35%. One other patient had a pre-existing EF less than or equal to 40%, accurately detected by the algorithm before and after COVID-19 diagnosis, and another was found to have a low EF by AI ECG and echocardiography with the COVID-19 diagnosis. The area under the curve for detection of EF less than or equal to 40% was 0.95. This case series suggests that the AI ECG, previously shown to detect ventricular dysfunction in a large general population, may be useful as a screening tool for the detection of cardiac dysfunction in patients with COVID-19.', "Cardiac function can be adversely impacted by coronavirus disease, which is associated with high mortality. A simple test at the doctor's office is a good option to screen for ventricular problems. We examined all patients with a positive COVID-19 test who underwent clinicallyindicated electrocardiogram andechocardiography within two weeks. One patient who met the inclusion criteria was a history of normal ventricular function who developed Covid-19 myocarditis with rapid clinical decline. The patient had an initial assessment of normal ventricular function. A copy of a repeat ECG showed that the chance of ejection fraction was less than one-fifth of what it was. One patient had a pre-existing EF less than 40%, accurately detected by the algorithm before and after the COVID-19 diagnosis, and another had a low EF."]

["Existing literature provides evidence of the connection between emotional intelligence and resilience, both concepts being adversely related to perceived stress. Nevertheless, there is little evidence from cross-cultural and/or cross-country studies of the simultaneous relationship between these psychological variables. The objective of this study was to address this lack of research, examining the associations between emotional intelligence, resilience and perceived stress in a cross-country context. A total sample of 696 undergraduate students from two universities in the United States and the Basque Country (an autonomous community in northern Spain) participated in the study. Structural equation modeling was used to examine the effects of emotional intelligence and resilience that may affect students' perceived stress. The results revealed that emotional intelligence functions as a negative predictor of perceived stress through the mediating variable resilience for the American and Basque students. The findings suggest that university students with better emotional intelligence and resilience present lower perceived stress. Thus, improving emotional intelligence and resilience could prevent students from suffering perceived stress in higher education. Implications and directions for further research are discussed; in particular, it is highlighted that intervention programs that improve both EI and resilience could be helpful in reducing perceived stress.", "Literature shows the connection between emotional intelligence and resilience, both of which are related to stress. There is not a lot of evidence to suggest the relationship between these variables. There is a lack of research on the relationships between emotional intelligence, resilience, and perceived stress in a cross-country context. A sample of nearly 700 undergrad students from two universities in the US and the Basque Country were part of the study. Structural equation modeling was used to examine the effects of emotional intelligence and resilience on students' perception of stress. The results showed emotional intelligence as a negative predictor of perceived stress through the variable resilience of the American and Basque students. The findings indicate that university students with resilience present less stress. Students in colleges could be prevented from feeling stressed out due to the improved emotional intelligence and resilience. In regards to research, it's recommended that intervention programs that improve both resilience and EI are used, as they could be useful in reducing perceived stress."]

['Advances in artificial intelligence-based methods have led to the development and publication of numerous systems for auto-segmentation in radiotherapy. These systems have the potential to decrease contour variability, which has been associated with poor clinical outcomes and increased efficiency in the treatment planning workflow. However, there are no uniform standards for evaluating auto-segmentation platforms to assess their efficacy at meeting these goals. Here, we review the most frequently used evaluation techniques which include geometric overlap, dosimetric parameters, time spent contouring, and clinical rating scales. These data suggest that many of the most commonly used geometric indices, such as the Dice Similarity Coefficient, are not well correlated with clinically meaningful endpoints. As such, a multi-domain evaluation, including composite geometric and/or dosimetric metrics with physician-reported assessment, is necessary to gauge the clinical readiness of auto-segmentation for radiation treatment planning.', 'Advances in artificial intelligence led to the development of a large number of systems that are used for the purpose of auto-segmentation in radiotherapy. The systems have a chance to decrease the effects of the variability on the clinical outcomes. There are no consistent standards for evaluating auto-segmentation platforms. Here, we compare the most often used evaluation techniques, including geometric overlap, dosimetric parameters and time spent contoured. The data suggests that some geometric indices are not well correlated with clinically meaningful endpoints. A multi-domain evaluation, including geometric and dosimetric metrics with physician-reported assessment, is necessary in order to gauge the clinical readiness of auto-segmentation.']

['Some surgical patients require an arterial or central venous catheterization intraoperatively. This decision relied solely on the experience of individual anesthesiologists; however, these decisions are not easy for clinicians who are in an emergency or inexperienced. Therefore, applying recent artificial intelligence techniques to automatically extractable data from electronic medical record (EMR) could create a very clinically useful model in this situation. This study aimed to develop a model that is easy to apply in real clinical settings by implementing a prediction model for the preoperative decision to insert an arterial and central venous catheter and that can be automatically linked to the EMR. We collected and retrospectively analyzed data from 66,522 patients, > 18 years of age, who underwent non-cardiac surgeries from March 2019 to April 2021 at the single tertiary medical center. Data included demographics, pre-operative laboratory tests, surgical information, and catheterization information. When compared with other machine learning methods, the DNN model showed the best predictive performance in terms of the area under receiver operating characteristic curve and area under the precision-recall curve. Operation code information accounted for the largest portion of the prediction. This can be applied to clinical fields using operation code and minimal preoperative clinical information.', "There are surgical patients who need arteries or other central vein accesories. It's not easy for clinicians who are in an emergency to make these decisions, based on the experience of individual anesthesiologists. Artificial intelligence can create a very clinically useful model if it is used in this situation. The model developed by this study is easy to apply in real clinical settings by implementing prediction models for the decision to insert the arterial and central Vein catheter and it can be linked to the electronic Medical Records. We collected and analyzed data from 66,552 patients who underwent non-cardiac surgeries between March and April of this year. Demographics, pre-operative lab tests, surgical information, and catheter information were included in the data. The area under the receiver operating characteristic curve and area under the precision-recall curve were two areas where the model best performed when compared with other machine learning methods. The largest part of the prediction was the operation code information. This can be used in several clinical fields."]

['The world today is undergoing revolutionary changes in science, technology, and industry. These changes have optimized the surgical procedure for lung cancer treatment into a more minimally invasive, precise, comprehensive, industrialized, and patient-centered manner. The definition of minimally invasive treatment for lung cancer has been expanded from simply reducing the size of the skin incision to reserve the lung function. The principle of precision surgery is emphasized throughout the whole process of patient management including diagnosis and treatment. Multimodality therapy helps narrow the gaps between different disciplines and thus provides more personalized treatment for lung cancer patients. Integration of industrial techniques such as visualization, surgical robot, and artificial intelligence into medical practice will potentially lead to a revolution in thoracic surgical procedure. Today, thoracic surgeons are responsible for establishing a self-reliant surgical practice for Chinese patients with lung cancer. It is necessary to attach great importance to patient-centered care, move forward to review minimally invasive surgical procedure, and foster better practice for lung cancer management by keeping up with cutting-edge research on science and technology in the context of changes and challenges.', "Today's world is undergoing revolutionary changes. The surgical procedure for lung cancer has been improved because of these changes. The definition of minimally invasive treatment for lung cancer has been expanded to use the lung function as a factor in deciding the size of the skin incision. Precision surgery is emphasized in the whole process of patient management. Multimodality therapy can help narrow the gaps between disciplines. A revolution in thoracic surgical procedure is possible due to the integration of industrial techniques into medical practice. thoracic surgeons are responsible for establishing self reliant surgical practice for Chinese patients with lung cancer It's important to have great importance to patient-centered care, to move forward to review minimally-invasive surgical procedure and to keep up with cutting-edge research on science and Technology, in order to foster better practice for lung cancer management."]

['It is well established that a large proportion of paper banknotes in circulation contain traces of cocaine. Being able to discriminate between the innocent transfer of illicit drug particles acquired through everyday interactions with surfaces such as banknotes, as opposed to transfer resulting from criminal activities can provide valuable intelligence that can inform an investigation. With many countries adopting polymer banknotes as legal tender, it is important to consider the transfer of cocaine from these surfaces as well as the retention of these particulates on polymer banknotes for evaluative interpretation in crime reconstruction. This comparison study assessed three contact variables (force, time, and rotation) on the transfer of cocaine particulates from paper and polymer banknotes onto a human skin proxy. The persistence of cocaine particulates was assessed through a realistic scenario which mimicked a cash transaction. Quantifiable amounts of cocaine were transferred onto the human skin proxy across all of the contacts assessed, with a greater transfer observed with contacts involving polymer banknotes and those contacts which involved rotation. Following extensive handling, cocaine persisted on both banknote types, with paper banknotes retaining larger amounts of cocaine than polymer banknotes. These findings show that cocaine can persist on both paper and polymer banknotes for extended periods of time following handling and is therefore available for transfer. This transfer then readily occurs, even when contact is brief and involves relatively small forces. A key distinction between the banknote types was that cocaine particulates are more likely to transfer from polymer banknotes due to the lower retention rate of particulates on this surface. Such insights can aid in evaluating the relevance of illicit drug particles identified on items or persons of interest in crime reconstruction approaches.', 'It has long been known that a large amount of paper money contains traces of cocaine. Being able to discriminate between an innocent transfer of drug particles from one place to another, and a criminal transfer of drug particles from a different place, can provide valuable intelligence which can inform a police investigation. It is necessary to consider the transfer of cocaine from these surfaces as well as the retention of particulates on the polymer banknotes in order to reconstruct a crime. The transfer of cocaine particles from paper to a human skin proxy was compared with others. The persistence was assessed through a cash transaction. It was observed that greater amounts of cocaine were transferred to the human skin proxy with contacts that required rotation. cocaine continued on both paper and plastic bank notes following extensive handling. cocaine can remain on both paper and polymers for a long period of time, and is available for transfer. When contact is short, this transfer can be done quickly. The lower retention rate of particulates on the surface of the banknote makes cocaine transfer more likely. Such insights can aid in detecting the relevance of drug particles found on items or people of interest in crime reconstruction approaches.']

['Knowledge mining from synthetic biology journal articles for machine learning (ML) applications is a labor-intensive process. The development of natural language processing (NLP) tools, such as GPT-4, can accelerate the extraction of published information related to microbial performance under complex strain engineering and bioreactor conditions. As a proof of concept, we proposed prompt engineering for a GPT-4 workflow pipeline to extract knowledge from 176 publications on two oleaginous yeasts (<i>Yarrowia lipolytica</i> and <i>Rhodosporidium toruloides</i>). After human intervention, the pipeline obtained a total of 2037 data instances. The structured data sets and feature selections enabled ML approaches (e.g., a random forest model) to predict <i>Yarrowia</i> fermentation titers with decent accuracy (<i>R</i><sup>2</sup> of 0.86 for unseen test data). Via transfer learning, the trained model could assess the production potential of the engineered nonconventional yeast, <i>R. toruloides</i>, for which there are fewer published reports. This work demonstrated the potential of generative artificial intelligence to streamline information extraction from research articles, thereby facilitating fermentation predictions and biomanufacturing development.', 'Machine learning is a labor-intensive process for knowledge mining from synthetic biology journal articles. It can be helpful to speed up the extract of information related to microbial performance under complex strain engineering and bioreactor conditions, if there is a natural language processing tool available. In order to demonstrate that a GPT-4 Workflow is possible, we proposed prompt engineering to extract knowledge from about 180 publications on two yeasts. A total of 2037 data instances were obtained by the line after human intervention. The data sets had feature selections. G. There is a random forest model that can predict the titers for Yehudaia with good accuracy and no unseen test data (i>R/i>sup>2/sup> of 0.86 for unseen test data). Transfer learning allowed the model to assess the production potential of the engineered yeast. There are more published reports for this. This work demonstrated the potential of artificial intelligence to help with the development of biomanufacturing.']

['<b>Background:</b> A large recent study analyzed the relationship between multiple factors and neonatal outcome and in preterm births. Study variables included the reason for admission, indication for delivery, optimal steroid use, gestational age, and other potential prognostic factors. Using stepwise multivariable analysis, the only two variables independently associated with serious neonatal morbidity were gestational age and the presence of suspected intrauterine growth restriction as a reason for admission. This finding was surprising given the beneficial effects of antenatal steroids and hazards associated with some causes of preterm birth. Multivariable logistic regression techniques have limitations. Without testing for multiple interactions, linear regression will identify only individual factors with the strongest independent relationship to the outcome for the entire study group. There may not be a single "best set" of risk factors or one set that applies equally well to all subgroups. In contrast, machine learning techniques find the most predictive <i>groupings of factors</i> based on their frequency and strength of association, with no attempt to identify independence and no assumptions about linear relationships.<b>Objective:</b> To determine if machine learning techniques would identify specific clusters of conditions with different probability estimates for severe neonatal morbidity and to compare these findings to those based on the original multivariable analysis.<b>Materials and methods:</b> This was a secondary analysis of data collected in a multicenter, prospective study on all admissions to the neonatal intensive care unit between 2013 and 2015 in 10 hospitals. We included all patients with a singleton, stillborn, or live newborns, with a gestational age between 23 0/7 and 31 6/7 week. The composite endpoint, severe neonatal morbidity, defined by the presence of any of five outcomes: death, grade 3 or 4 intraventricular hemorrhage (IVH), and ≥28 days on ventilator, periventricular leukomalacia (PVL), or stage III necrotizing enterocolitis (NEC), was present in 238 of the 1039 study patients. We studied five explanatory variables: maternal age, parity, gestational age, admission reason, and status with respect to antenatal steroid administration. We concentrated on Classification and Regression Trees because the resulting structure defines clusters of risk factors that often bear resemblance to clinical reasoning. Model performance was measured using area under the receiver-operator characteristic curves (AUC) based on 10 repetitions of 10-fold cross-validation.<b>Results:</b> A hybrid technique using a combination of logistic regression and Classification and Regression Trees had a mean cross-validated AUC of 0.853. A selected point on its receiver-operator characteristic (ROC) curve corresponding to a sensitivity of 81% was associated with a specificity of 76%. Rather than a single curve representing the general relationship between gestational age and severe morbidity, this technique found seven clusters with distinct curves. Abnormal fetal testing as a reason for admission with or without growth restriction and incomplete steroid administration would place a 20-year-old patient on the highest risk curve.<b>Conclusions:</b> Using a relatively small database and a few simple factors known before birth it is possible to produce a more tailored estimate of the risk for severe neonatal morbidity on which clinicians can superimpose their medical judgment, experience, and intuition.', "A large study analyzed the relationship between multiple factors and preterm birth outcomes. Study variables included reasons for admission, indication for delivery, optimal steroid use, and other potential factors. Using a stepwise analysis, the only two variables associated with serious morbidity were the age of the fetus and the presence of growth restriction. It was unexpected given the positive effects of steroids on births. Limitations are faced by multivariable logistic regression techniques. Linear regression can identify individual factors with the strongest relationship to the outcome for the entire study group without testing for multiple interactions. There isn't a single set of risk factors that applies to all groups. Machine learning techniques look at factors using their associations and frequencies with no attempt to distinguish between independence and the linear relationships. To determine if machine learning techniques could identify different clusters of conditions that were not the same based on the original multivariable analysis. There was a secondary analysis of data from the prospective study of all admissions to the neonatal intensive care unit in 10 hospitals. The patients had a singleton, stillborn, or live newborn that was conceived between the ages of 23 0/7 and 31 /6 weeks. If there are five outcomes,death, grade 3 or 4 Intraventricular hemorrhage, and 28 days on ventilator are the most common. We studied several factors, including status with respect to steroid administration, maternal age, parity, and gestational age. We focused on the structure of classification and regression trees because it defines the clusters of risk factors that are similar to clinical reasoning. Model performance was measured using area under the AUC. A hybrid technique with a combination of regression and Logistic Regression had a cross-validated AUC of 0.853. The technique found seven clusters with distinct curves, instead of the general relationship between gestational age and severe morbidity that a single curve represented. Fetal testing is abnormal if it is a reason for admission with or without growth restriction and steroid administration is incomplete. It is possible to use a small database and few simple factors before birth to estimate the risk for newborn morbidity, allowing clinicians to superimpose their experience and intuition."]

["In the late 19th century, mutual autopsy societies formed, first in Paris, France (1876) and later in Philadelphia, Pennsylvania. Members, who were often a who's who of anthropologists, physicians, intellectuals, and highly accomplished citizens, pledged to submit their bodies for autopsies to be performed by living society members so that their brains could be weighed and surface topography studied, with the results to be correlated with the decedents' intelligence and personal strengths during life. To explore the short history of these societies, the science they produced, and their extensive newspaper coverage in the United States. Available primary and secondary historic sources were reviewed. The Societe Mutuelle d'Autopsie in Paris and the American Anthropometric Society in Philadelphia had different motives, as the former was heavily influenced by French Third Republic politics and secularism. The American press provided titillating coverage of both and was particularly fascinated by scandals. In America, Burt Wilder formed a splinter group and established the Wilder Brain Collection at Cornell University. In a period where many anthropologists were making untrue claims that brain and skull measurements were largely determined by race and sex, Wilder and Franklin P. Mall of the Wistar Institute in Philadelphia independently published carefully conducted studies proving this was not the case. Mutual autopsy societies and brain clubs conclusively established that brain weight was not an accurate predictor of intelligence but accomplished little else; they were phased out shortly after World War I but are the predecessor to modern-day brain banks.", "In the late 19th century, mutual autopsy societies were found in Paris, France and Philadelphia, Pennsylvania. In order for living society members to be able to determine the cause of death, members would have to give their bodies for autopsies, so that their brains could be weighed and studied. These societies have a short history as well as their science and newspaper coverage in the United States. Historical sources were reviewed. The American Anthropometric Society in Philadelphia had a different motives as it was influenced by French Third Republic politics and secularism. The American press gave lots of coverage of both scandals. The splinter group that was formed by Burt is considered to be the original group of the Wilder Brain Collection at Cornell University. A period where many anthropologists made false claims about race and sex being the main factor in determining brain and skull measurements. The Mall of theWistar Institute published studies showing this wasn't the case. Brain weight was phased out shortly after World War I after the societies concluded that it was not an accurate predictor of intelligence."]

["Governing emerging technologies is one of the most important issues of the twenty-first century, and primarily concerns the public, private, and social initiatives that can shape the adoption and responsible development of digital technologies. This study surveys the emerging landscape of blockchain and artificial intelligence (AI) governance and maps the ecosystem of emerging platforms within industry and public and civil society. We identify the major players in the public, private, and civil society organizations and their underlying motivations, and examine the divergence and convergence of these motivation and the way they are likely to shape the future governance of these emerging technologies. There is a broad consensus that these technologies represent the present and future of economic growth, but they also pose significant risks to society. Indeed, there is also considerable confusion and disagreement among the major players about navigating the delicate balance between promoting these innovations and mitigating the risks they pose. While some in the industry are calling for self-regulation, others are calling for strong laws and state regulation to monitor these technologies. These disagreements, are likely to remain for the foreseeable future and may derail the optimal development of governance ecosystems across jurisdictions. Therefore, we propose that players should consider erecting new safeguards and using existing frameworks to protect consumers and society from the harms and dangers of these technologies. For instance, through re-examining existing legal and institutional arrangements to check whether these cater for emerging issues with new technologies, and as needed make necessary update/amendments. Further, there may be cases where existing legal and regulated systems are completely outdated and can't cover for new technologies, for example, when AI is used to influence political outcomes, or crypto currency frauds, or AI-powered autonomous vehicles, such cases call of agile governance regimes. This is important because different players in government, industry, and civil are still coming to terms with the governance challenges that these emerging technologies pose to society, and no one has a clear answer on optimal way to promote these technologies, at the same time limit the dangers they pose to users.", "Digital technologies, which include emerging technologies like the Internet, is one of the biggest issues of the twenty-first century because of the public, private, and social initiatives that can shape its adoption. The emerging landscape of Blockchain and Artificial Intelligence governance is surveyed, as well as the state of the new platforms in industry and public and civil society. The major players in the public, private, and civil society organizations are identified with their underlying motives, their divergence and convergence, and how they may shape governance of emerging technologies. There is a lot of agreement that these technologies are important for the future, but also pose significant risks to the community. There is a lot of confusion and disagreement among the major players about trying to balance the risks of these innovations with the benefits of them. Some want self-regulation, others want strong laws to watch over these technologies. The disagreements are likely to last for the foreseeable future and will have an adverse effect on the development of governance in the jurisdiction. We propose that players use existing frameworks and install new safeguards to protect consumers and society from the harms and dangers of new technologies. In order to check whether existing legal and institutional arrangements cater to emerging issues with new technologies, as needed make necessary update/amendments. There can be cases where existing legal and regulated systems can't cope with new technologies like Artificial Intelligence and Artificial Intelligence powered cars, where there is a case of Agile governance regimes. There are a lot of players who are coming to terms with governance challenges of emerging technologies and no one knows an optimal way to promote them, at the same time limiting the dangers they pose to users."]

['Autism presents with significant phenotypic and neuroanatomical heterogeneity, and neuroimaging studies of the thalamus, globus pallidus and striatum in autism have produced inconsistent and contradictory results. These structures are critical mediators of functions known to be atypical in autism, including sensory gating and motor function. We examined both volumetric and fine-grained localized shape differences in autism using a large (<i>n</i>=3145, 1045-1318 after strict quality control), cross-sectional dataset of T1-weighted structural MRI scans from 32 sites, including both males and females (assigned-at-birth). We investigated three potentially important sources of neuroanatomical heterogeneity: sex, age, and intelligence quotient (IQ), using a meta-analytic technique after strict quality control to minimize non-biological sources of variation. We observed no volumetric differences in the thalamus, globus pallidus, or striatum in autism. Rather, we identified a variety of localized shape differences in all three structures. Including age, but not sex or IQ, in the statistical model improved the fit for both the pallidum and striatum, but not for the thalamus. Age-centered shape analysis indicated a variety of age-dependent regional differences. Overall, our findings help confirm that the neurodevelopment of the striatum, globus pallidus and thalamus are atypical in autism, in a subtle location-dependent manner that is not reflected in overall structure volumes, and that is highly non-uniform across the lifespan.', "There is significant heterogeneity in the thalamus, globus waldus and striatum inAutism, and these studies have produced conflicting results. These structures are involved in sensory gating and motor function, which are atypical inaustical. We examined both volumetric and fine-grained shape differences in a large and cross-sectional dataset of structural magnetic resonance scans from 32 sites, both males and females. Sex, age, and intelligence quotient are potentially important sources of heterogeneity and were studied by a meta-analytic technique. There are no visible differences in the thalamus, globus pallidus, or  striatum. We were able to identify a variety of shape differences in the three structures. The fit for the thalamus and the pallidum were improved by adding age, but not sex or IQ. There are a variety of age- dependent regional differences. The findings show that the brain area in question are atypical in people with the condition, and that it's not just the area in the middle of the skull."]

["Data mining is a powerful tool to reduce costs and mitigate errors in the diagnostic analysis and repair of complex engineered system, but it has yet to be applied systematically to the most complex and socially expensive system - the human body. The currently available approaches of knowledge-based and pattern-based artificial intelligence are unsuited to the iterative and often subjective nature of clinician-patient interactions. Furthermore, current electronic health records generally have poor design and low quality for such data mining. Bayesian methods have been developed to suggest multiple possible diagnoses given a set of clinical findings, but the larger problem is advising the physician on useful next steps. A new approach based on inverting Bayesian inference allows identification of the diagnostic actions that are most likely to disambiguate a differential diagnosis at each point in a patient's work-up. This can be combined with personalized cost information to suggest a cost-effective path to the clinician. Because the software is tracking the clinician's decision-making process, it can provide salient suggestions for both diagnoses and diagnostic tests in standard, coded formats that need only to be selected. This would reduce the need to type in free text, which is prone to ambiguities, omissions and errors. As the database of high-quality records grows, the scope, utility and acceptance of the system should also grow automatically, without requiring expert updating or correction.", "Data mining can be used to help the diagnostic analysis and repair of complex engineered system but it hasn't been applied similarly to the human body. The current approaches of artificial intelligence do not fit the subjective nature of clinician- patient interactions. Current health records are not good for data mining. The bigger problem for clinicians is advising the doctor on the next steps after reviewing a set of findings. It is possible to identify the diagnostic actions that are most likely to diagnose a differential diagnosis at some point in a workup. This can be combined with personal cost information to recommend a cost effective path to the clinician. Because of the software's tracking of the clinician's decision-making process it can provide options for diagnoses in standard formats that only need to be selected. This will reduce the need to type into free text that is prone to errors. The scope, utility and acceptance of the system should grow automatically as the database of high-quality records grows."]

['Intravenous thrombolysis decision-making and obtaining of consent would be assisted by an individualized risk-benefit ratio. Deep learning (DL) models may be able to assist with this patient selection. Clinical data regarding consecutive patients who received intravenous thrombolysis across two tertiary hospitals over a 7-year period were extracted from existing databases. The noncontrast computed tomography brain scans for these patients were then retrieved with hospital picture archiving and communication systems. Using a combination of convolutional neural networks (CNN) and artificial neural networks (ANN) several models were developed to predict either improvement in the National Institutes of Health Stroke Scale of ≥4 points at 24 hours ("NIHSS24"), or modified Rankin Scale 0-1 at 90 days ("mRS90"). The developed CNN and ANN were then applied to a test set. The THRIVE, HIAT, and SPAN-100 scores were also calculated for the patients in the test set and used to predict NIHSS24 and mRS90. Data from 204 individuals were included in the project. The best performing DL model for prediction of mRS90 was a combination CNN + ANN based on clinical data and computed tomography brain (accuracy = 0.74, F1 score = 0.69). The best performing model for NIHSS24 prediction was also the combination CNN + ANN (accuracy = 0.71, F1 score = 0.74). DL models may aid in the prediction of functional thrombolysis outcomes. Further investigation with larger datasets and additional imaging sequences is indicated.', 'Individualized risk-benefit ratios would be used to assist decision-making and consent. The patient selection may be assisted by deep learning models. There were patients who received IV fluid at one of the two tertiary hospitals for seven years. Hospital picture archives and communication systems were used to recover the brain scans of these patients. The National Institute of Health Stroke Scale of 4% points at 24 hours was predicted by several models by combining artificial neural networks with CNN. The test set had both CNN and ANN applied to it. The data from 204 individuals was used to calculate the score for patients in the test set and predict the score with the help of the THRIVE, HIAT, and SPAN-100 scores. The CNN + ANN combination is the best model for predicting mRS90 because of itsaccuracy and F1 score. The best model for prediction was the combination of CNN and ANN. The prediction of functional thrombolysis outcomes could be helped by the DL models. There is additional investigation with larger datasets.']

["Despite health care advances, artificial intelligence and government interventions aiming to improve the health and wellbeing of citizens, huge disparities and failures in care provision exist. This is demonstrated by the rising number of medical errors, increase in readmission rates and mortality rates, and the failure of many health systems to successfully cope with events, such as pandemics and natural disasters. This shortfall is in part because of the complexity of the health care system, the interconnectedness of various parts of service, funding models, the complexity of patients' conditions, patient and carer needs, and the clinical processes needed for patients via multiple providers. The objective of this paper is to describe the use of system thinking methodologies to address complex problems such as those in the public health and health services domains. A description of the system thinking methodology and its associated methods including causal loop diagrams, social network analysis and soft system methodology are described with examples in the health care setting. There are various models of knowledge translation that have been employed including the Joanna Briggs Institute model of implementation of evidence into practice, the triple C, and the Promoting Action on Research Implementation in Health Services. However, many of these models are neither scalable nor sustainable, and are most effective for localized projects implemented by trained clinicians and champions in relevant settings.System thinking is essentially a modelling process, which aims to create opportunities for change via an appreciation of perspective, and recognition that complex problems are a result of interconnected factors. The article argues that systems thinking applications need to move beyond that of addressing complex health issues pertaining to a population, and rather consider complex problems surrounding the delivery of high-quality health care. It is important that methods to implement systems thinking methodologies in health care settings are developed and tested.", 'Despite health care advances, artificial intelligence and government interventions, there are huge disparity in care provision. The rising number of medical errors, increase in mortality rates and the failure of many health systems to cope with natural disasters are signs of this. There is an interconnectedness of other parts of the health care services, funding models that make it hard to pay, and clinical processes that need multiple providers. System thinking methodologies are used to address certain public health and health services issues, according to the objectives of the paper. A description of the system thinking methodology and its associated method include examples of how it is used in the health care setting. The triple C is one of the models of knowledge translation employed, as is the Promoting Action on Research Implementation in Health Services. Many of these models are impractical and ineffectual for projects implemented by clinicians and champions. System thinking is a process with a goal of making opportunities for change through an appreciation of perspective and recognizing complex problems are a result of interwoven factors. The article makes the point that systems need to consider more than just health issues related to a population, as well as the delivery of high- quality health care. It is important that the methods for implementing systems thinking are developed.']

['Dynamic bone scintigraphy (DBS) is the first widely reliable and simple imaging modality in nuclear medicine that can be used to diagnose prosthetic joint infection (PJI). We aimed to apply artificial intelligence to diagnose PJI in patients after total hip or knee arthroplasty (THA or TKA) based on <sup>99m</sup>Tc-methylene diphosphonate (<sup>99m</sup>Tc-MDP) DBS. A total of 449 patients (255 THAs and 194 TKAs) with a final diagnosis were retrospectively enrolled and analyzed. The dataset was divided into a training and validation set and an independent test set. A customized framework composed of two data preprocessing algorithms and a diagnosis model (dynamic bone scintigraphy effective neural network, DBS-eNet) was compared with mainstream modified classification models and experienced nuclear medicine specialists on corresponding datasets. In the fivefold cross-validation test, diagnostic accuracies of 86.48% for prosthetic knee infection (PKI) and 86.33% for prosthetic hip infection (PHI) were obtained using the proposed framework. On the independent test set, the diagnostic accuracies and AUC values were 87.74% and 0.957 for PKI and 86.36% and 0.906 for PHI, respectively. The customized framework demonstrated better overall diagnostic performance compared to other classification models and showed superiority in diagnosing PKI and consistency in diagnosing PHI compared to specialists. The customized framework can be used to effectively and accurately diagnose PJI based on <sup>99m</sup>Tc-MDP DBS. The excellent diagnostic performance of this method indicates its potential clinical practical value in the future. • The proposed framework in the current study achieved high diagnostic performance for prosthetic knee infection (PKI) and prosthetic hip infection (PHI) with AUC values of 0.957 and 0.906, respectively. • The customized framework demonstrated better overall diagnostic performance compared to other classification models. • Compared to experienced nuclear medicine physicians, the customized framework showed superiority in diagnosing PKI and consistency in diagnosing PHI.', 'Dynamic bone scintigraphy is one of the most reliable and inexpensive ways to see bones in a nuclear medicine setting. We wanted to apply artificial intelligence to diagnose PJI in patients after total hip or knee surgery. A total of 449 patients with final diagnoses were analyzed. There were training and validation sets in the dataset. A framework composed of two data preprocessing techniques and a diagnosis model was compared to mainstream models and experienced nuclear medicine specialists on corresponding datasets The accuracies were obtained using the framework for the 5FOLD cross- validation test. The diagnostic accuracies were calculated on the independent test set. The framework showed better diagnostic performance, had better consistency in diagnoses, and was better at showing consistency in diagnoses than the other models. The framework can used to diagnose PJI based on the sup>99m/sup>Tc-MDP DBS. The diagnostic performance of the method shows potential value in the future. The framework in the present study achieved high diagnostic performance for knee infections and infections in hip joints. The framework demonstrated better diagnostic performance than other models. The tailored framework was superior to experienced nuclear medicine physicians in both of their areas of expertise.']

['The Cattell-Horn-Carroll (CHC) or three-stratum model of intelligence envisions human intelligence as a hierarchy. General intelligence (<i>g</i>) is situated at the top, under which are a group of broad intelligences such as verbal, visuospatial processing, and quantitative knowledge that pertain to more specific areas of reasoning. Some broad intelligences are people-centered, including personal, emotional, and social intelligences; others concern reasoning about things more generally, such as visuospatial and quantitative knowledge. In the present research, we conducted a meta-analysis of 87 studies, including 2322 effect sizes, to examine the average correlation between people-to-people intelligences relative to the average correlation between people-to-thing-centered intelligences (and similar comparisons). Results clearly support the psychometric distinction between people-centered and thing-centered mental abilities. Coupled with evidence for incremental predictions from people-centered intelligences, our findings provide a secure foundation for continued research focused on people-centered mental abilities.', 'The Cattell-Horn-Carroll model of intelligence is about a hierarchy of intelligence. General intelligence is a group of broad intelligences such as verbal, visuospatial processing, and quantitative knowledge that pertain to more specific areas of reasoning. Some broad intelligences are focused on people, while others worry about reasoning about things other than visuospatial and quantitative knowledge. We looked at the average correlation between people-to-people intelligences and the average correlation between people-to-thing-centeredintelligences. The results support the difference between people and things. The foundation for continued research focused on people-centered mental abilities can be found in our findings.']

["Donald Ewen Cameron is known as the Canadian psychiatrist behind the Montreal Experiments, a series of brainwashing experiments. As part of a larger Central Intelligence Agency (CIA) project known as MK Ultra, the CIA regarded these experiments as a potential military weapon during the Cold War. However, a closer look into Cameron's research and project MK Ultra shows that these experiments began long before Cameron was contacted by the CIA. Additionally, Cameron received funding for his experiments indirectly, so he was probably never aware the money was from the CIA. In this paper, I analyse the published work of Dr Cameron from the beginning of his career to his role in MK Ultra, and evaluate his own possible reasoning behind these experiments.", "The Montreal Experiments are a series of brain washing experiments. During the Cold War, the CIA regarded these experiments as a potential military weapon because they were part of a larger project. A closer look at the research and the project shows the experiments began long before. The CIA gave funding for the experiments, so he probably didn't know it. I analyse the work of Dr Cameron, from the start of his career to his role in MK Ultra, and evaluate his own thought process behind the experiments."]

['Artificial intelligence (AI) is poised to change much about the way we practice radiology in the near future. The power of AI tools has the potential to offer substantial benefit to patients. Conversely, there are dangers inherent in the deployment of AI in radiology, if this is done without regard to possible ethical risks. Some ethical issues are obvious; others are less easily discerned, and less easily avoided. This paper explains some of the ethical difficulties of which we are presently aware, and some of the measures we may take to protect against misuse of AI.', 'The way we practice medicine is going to change thanks to artificial intelligence. There is a lot of potential for patients to benefit from the power of artificial intelligence. If this is done without regard to ethical risk, there will be dangers inherent in the deployment of artificial intelligence. Some ethical issues are obvious, however others are more difficult to spot and avoid. This paper explains some of the ethical difficulties that we currently are aware of']

['Over the last decade, artificial intelligence (AI) has made an enormous impact on a wide range of fields, including science, engineering, informatics, finance, and transportation [...].', 'Artificial intelligence has made a huge impact in a wide range of fields over the last decade. There There are a lot of people who like to think that there are a lot of people who love to think that there are a lot of people who like to think that there are a lot of people who like to think that there are a lot of people who like to think that there are a ]']

["Ensuring correct radiograph view labeling is important for machine learning algorithm development and quality control of studies obtained from multiple facilities. The purpose of this study was to develop and test the performance of a deep convolutional neural network (DCNN) for the automated classification of frontal chest radiographs (CXRs) into anteroposterior (AP) or posteroanterior (PA) views. We obtained 112,120 CXRs from the NIH ChestX-ray14 database, a publicly available CXR database performed in adult (106,179 (95%)) and pediatric (5941 (5%)) patients consisting of 44,810 (40%) AP and 67,310 (60%) PA views. CXRs were used to train, validate, and test the ResNet-18 DCNN for classification of radiographs into anteroposterior and posteroanterior views. A second DCNN was developed in the same manner using only the pediatric CXRs (2885 (49%) AP and 3056 (51%) PA). Receiver operating characteristic (ROC) curves with area under the curve (AUC) and standard diagnostic measures were used to evaluate the DCNN's performance on the test dataset. The DCNNs trained on the entire CXR dataset and pediatric CXR dataset had AUCs of 1.0 and 0.997, respectively, and accuracy of 99.6% and 98%, respectively, for distinguishing between AP and PA CXR. Sensitivity and specificity were 99.6% and 99.5%, respectively, for the DCNN trained on the entire dataset and 98% for both sensitivity and specificity for the DCNN trained on the pediatric dataset. The observed difference in performance between the two algorithms was not statistically significant (p = 0.17). Our DCNNs have high accuracy for classifying AP/PA orientation of frontal CXRs, with only slight reduction in performance when the training dataset was reduced by 95%. Rapid classification of CXRs by the DCNN can facilitate annotation of large image datasets for machine learning and quality assurance purposes.", "Quality control of studies obtained from multiple facilities is an important requirement for machine learning. The objectives of the study were to test the performance of a deep neural network for automated classification of chest x-rays. We got 112,140 children's x-rays from theNIH ChestX-ray14 database and used it to perform more than 100,000 children's x-rays. The ResNet-18 DCNN was used for training, validation, and testing. A second DCNN was created in the same way as the first. The DCNN's performance on the test dataset was evaluated by using the ROC curves with the AUC and standard diagnostic measures. The DCNN's trained on the entire database had an acclimation rate of 1.0 and 0.997, and 98% of the time, they found AP and PA to be the same. The DCNN trained on the entire dataset received sensitivity and specificity of 99.6% and 98%, respectively. The performance difference between the two are not statistically significant. There is only slight reduction in performance when the training dataset is reduced, due to the high accuracy of our DCNNs. The DCNN can provide rapid classification of large image datasets."]

["Developing sophisticated device architectures is of great significance to go beyond Moore's law with versatility toward human-machine interaction and artificial intelligence. Tribotronics/tribo-iontronics offer a direct way to controlling the transport properties of semiconductor devices by mechanical actions, which fundamentally relies on how to enhance the tribotronic gating effect through device engineering. Here, we propose a universal method to enhance the tribotronic properties through electric double layer (EDL) capacitive coupling. By preparing an ion gel layer on top of tribotronic graphene transistor, we demonstrate a dual-mode field effect transistor (<i>i.e.</i>, a tribotronic transistor with capacitively coupled ion gel and an ion-gel-gated graphene transistor with a second tribotronic gate). The resulted tribotronic gating performances are greatly improved by twice for the on-state current and four times for the on/off ratio (the first mode). It can also be utilized as a multiparameter distance sensor with drain current increased by ∼600 μA and threshold voltage shifted by ∼0.8 V under a mechanical displacement of 0.25 mm (the second mode). The proposed methodology of EDL capacitive coupling offers a facile and efficient way to designing more sophisticated tribotronic devices with superior performance and multifunctional sensations.", "Developing sophisticated device architectures is important to go beyond Moore's law to include human- machine interaction and artificial intelligence. The tribotronic gating effect is an important part of how to control transport properties of Semiconductor Devices through mechanical actions. The method we propose is universal and can be applied to all tribotronic properties. We demonstrate dual-mode field effect transistor with ion gel on top of tribotronic graphene transistor. E. A tribotronic transistor with a second tribotronic gate and ion gel-gated graphene transistors For the on/off ratio, the tribotronic gating performances are better by four times. It's possible to use it as a multiparameter distance sensor with drain current increased by 6000 A and threshold voltage shifted by 0.8 V under a mechanical displacement of 0.25mm in the second mode. The proposed methodology of EDL capacitivecoupling offers an efficient way to designs more sophisticated tribotronic devices with superior performance and multifunctional sensations."]

["Islets transplanted for type-1 diabetes have their viability reduced by warm ischemia, dimethyloxalylglycine (DMOG; hypoxia model), oxidative stress and cytokine injury. This results in frequent transplant failures and the major burden of patients having to undergo multiple rounds of treatment for insulin independence. Presently there is no reliable measure to assess islet preparation viability prior to clinical transplantation. We investigated deep morphological signatures (DMS) for detecting the exposure of islets to viability compromising insults from brightfield images. Accuracies ranged from 98 % to 68 % for; ROS damage, pro-inflammatory cytokines, warm ischemia and DMOG. When islets were disaggregated to single cells to enable higher throughput data collection, good accuracy was still obtained (83-71 %). Encapsulation of islets reduced accuracy for cytokine exposure, but it was still high (78 %). Unsupervised modelling of the DMS for islet preparations transplanted into a syngeneic mouse model was able to predict whether or not they would restore glucose control with 100 % accuracy. Our strategy for constructing DMS' is effective for the assessment of islet pre-transplant viability. If translated into the clinic, standard equipment could be used to prospectively identify non-functional islet preparations unable to contribute to the restoration of glucose control and reduce the burden of unsuccessful treatments.", "Islets for type-1 diabetes might be less viable due to a number of conditions. Patients have to have to undergo multiple rounds of treatment for Diabetes in order to have a transplant failure. There is no reliable method to assess islet preparation viability prior to clinical transplantation. The exposure of islets to viability was looked at with deep morphological signatures. ROS damage was classified as an accuracies ranged from 98 percent to 68 percent. When islets were disaggregated to single cells, there was good accuracy left. It was still high in terms of accuracy even though Encapsulation reduced it. It was possible to model the islet preparations for the sake of predicting if or not they would restore the blood sugar level. The assessment of islet pre-transplant viability can be accomplished with the help of our strategy. Standard equipment may be used to identify non-functional islet preparations that can't contribute to restoration of the glycemic control that is required."]

["The relationship between obstructive sleep apnoea (OSA) and poorer neurobehavioural outcomes in school-age children is well established, but the relationship in obese children and adolescents, in whom OSA is more common, is not so well established. We aimed to investigate this relationship in 10-18-year-olds. Thirty-one participants with a mean body mass index (BMI) of 32.3 ± 4.9 enrolled. BMI-for-age cut-offs were used to define obesity. Participants underwent polysomnography and were classified into OSA (apnoea-hypopnoea index (AHI) > 2 per hour) and non-OSA (AHI ≤ 2) groups. Intelligence, memory and learning, academic achievement, behaviour and executive functioning were assessed using the Wechsler Abbreviated Scale of Intelligence, Wide Range Assessment of Memory and Learning 2, Wechsler Individual Achievement Test II (WIAT-II), Behavioural Assessment System for Children 2 and Behaviour Rating Inventory of Executive Function, respectively. Forty-eight per cent (15/31) were classified as having OSA, and 52% (16/31) as non-OSA. The obese cohort performed below the average of normative data on several neurobehavioural measures. WIAT-II maths scores were significantly lower (P = 0.034) in the OSA group than in the non-OSA group (means 84.5 vs. 94.6, respectively), losing significance after adjustment for IQ, age and gender. Self-reported school problems were significantly worse in the OSA group before and after multivariate adjustment (P = 0.010, Cohen's d = 1.02). No other significant differences were found. Results suggest that OSA may increase risk for some poorer educational and behavioural outcomes. The findings are reasonably consistent with and add to the evidence base of the few studies that have explored this relationship.", "Snoring and sleep apnoea are linked to better neurological outcomes in children, but obese children and adolescents aren't related. We hoped to investigate this relationship in young adults. Thirty-one people had a mean body mass index of more than 3. The cut-offs are used for obese people. The participants underwent polysomnography and were divided into two groups. The Wechlear Abbreviated Scale of Intelligence, Wide Range Assessment of Memory and Learning 2, Wechchler Individual Achievement Test II and Behaviour Rating Inventory were used. Forty-eight percent was classified as having osas, and half of that was non-osas. The obese cohort did some better than usual on several measures. The non-OSA group's math scores were almost exactly the same as those in the OSA group. After adjusting for age and gender, the significance is lost. The self- reported school problems were worse in the OSA group. There were no significant differences found. Some poorer educational and behavioural outcomes might be at increased risk. This relationship is explored in a few studies, the findings are consistent with them."]

['Diversity is supposed to create better groups and societies but sometimes fails. It is explained why the power of diversity may not create better groups in the current diversity prediction theory. Diversity may hurt civic life and introduce distrust. This is because the current diversity prediction theory is based on real numbers that ignore individual abilities. Its diversity prediction theory maximizes performance with infinite population size. Contrary to this, collective intelligence or swarm intelligence is not maximized by infinite population size, but by population size. The extended diversity prediction theory using the complex number allows us to express individual abilities or qualities. The diversity of complex numbers always produces better groups and societies. The wisdom of crowds, collective intelligence, swarm intelligence or nature-inspired intelligence is implemented in the current machine learning or artificial intelligence, called Random Forest. The problem of the current diversity prediction theory is detailed in this paper.', 'Diversity may sometimes fail in creating better groups and societies. The diversity prediction theory may not work out because of the power of diversity. Diversity may impact civic life. The theory of diversity is based on real numbers. The theory maximizes performance with infinite population size. Collective intelligence is maximized by population size. The complex number is used in extended diversity prediction theory and allows us to express individual abilities or qualities. The complexity of numbers can lead to better groups and societies. Random Forest is the current machine learning or artificial intelligence that integrateswisdom of crowds,collective intelligence, swarm intelligence or nature-inspired intelligence. The current diversity prediction theory has a problem that is detailed in this paper.']

['The COVID pandemic has been devastating for not only its direct impact on lives, physical health, socio-economic status of individuals, but also for its impact on mental health. Some individuals are affected psychologically more severely and will need additional care. However, the current health system is so fragmented and focused on caring for those infected that management of mental illness has been neglected. An integrated approach is needed to strengthen the health system, service providers and research to not only manage the current mental health problems related to COVID but develop robust strategies to overcome more long-term impact of the pandemic. A series of recommendations are outlined in this paper to help policy makers, service providers and other stakeholders, and research and research funders to strengthen existing mental health systems, develop new ones, and at the same time advance research to mitigate the mental health impact of COVID19. The recommendations refer to low, middle and high resource settings as capabilities vary greatly between countries and within countries. The recommendations for policy makers are focused on strengthening leadership and governance, finance mechanisms, and developing programme and policies that especially include the most vulnerable populations. Service provision should focus on accessible and equitable evidence-based community care models commensurate with the existing mental health capacity to deliver care, train existing primary care staff to cater to increased mental health needs, implement prevention and promotion programmes tailored to local needs, and support civil societies and employers to address the increased burden of mental illness. Researchers and research funders should focus on research to develop robust information systems that can be enhanced further by linking with other data sources to run predictive models using artificial intelligence, understand neurobiological mechanisms and community-based interventions to address the pandemic driven mental health problems in an integrated manner and use innovative digital solutions. Urgent action is needed to strengthen mental health system in all settings. The recommendations outlined can be used as a guide to develop these further or identify new ones in relation to local needs.', 'The devastating impact of the Covid pandemic has been felt all around the globe. Some individuals are more traumatised by it and need more care. The present health system is so fragmented that the management of mental illness has been neglected The health system, service providers and research need to be part of the approach to manage the current mental health problems related to COVID. Policy makers, service providers and other stakeholders are advised to strengthen their mental health systems and develop new ones in this paper Policy makers can strengthen leadership, governance, finance mechanisms and develop policies that include vulnerable populations. Mental health needs should be addressed by training existing primary care staff to cater for the increased demand of mental health services, by implementing prevention and promotion programmes tailored to the needs of local communities and by supporting civil society and employers to address the increased burden. Researchers and funders have to concentrate on research to develop robust, connected and innovative information systems that can be further enhanced by linking to other data sources, such as Artificial Intelligence and community-based interventions. Mental health system needs to be strengthened in all settings. The recommendations outlined can be used as a guide to develop additional ones.']

['(1) Background: The difficulty of pelvic operation is greatly affected by anatomical constraints. Defining this difficulty and assessing it based on conventional methods has some limitations. Artificial intelligence (AI) has enabled rapid advances in surgery, but its role in assessing the difficulty of laparoscopic rectal surgery is unclear. This study aimed to establish a difficulty grading system to assess the difficulty of laparoscopic rectal surgery, as well as utilize this system to evaluate the reliability of pelvis-induced difficulties described by MRI-based AI. (2) Methods: Patients who underwent laparoscopic rectal surgery from March 2019 to October 2022 were included, and were divided into a non-difficult group and difficult group. This study was divided into two stages. In the first stage, a difficulty grading system was developed and proposed to assess the surgical difficulty caused by the pelvis. In the second stage, AI was used to build a model, and the ability of the model to stratify the difficulty of surgery was evaluated at this stage, based on the results of the first stage; (3) Results: Among the 108 enrolled patients, 53 patients (49.1%) were in the difficult group. Compared to the non-difficult group, there were longer operation times, more blood loss, higher rates of anastomotic leaks, and poorer specimen quality in the difficult group. In the second stage, after training and testing, the average accuracy of the four-fold cross validation models on the test set was 0.830, and the accuracy of the merged AI model was 0.800, the precision was 0.786, the specificity was 0.750, the recall was 0.846, the F1-score was 0.815, the area under the receiver operating curve was 0.78 and the average precision was 0.69; (4) Conclusions: This study successfully proposed a feasible grading system for surgery difficulty and developed a predictive model with reasonable accuracy using AI, which can assist surgeons in determining surgical difficulty and in choosing the optimal surgical approach for rectal cancer patients with a structurally difficult pelvis.', "The difficulty of genital operation is influenced by a lot of things. It is impossible to define and assess the difficulty using conventional methods. Artificial intelligence has been used to make surgery more efficient, but it's not clear what role it plays in assessing the difficulty of rectal surgery. This study aimed to create a difficulty rating system for assessing the difficulty of rectal surgeries, as well as use the system to evaluate the reliability of the difficulties described by artificial intelligence. Patients who underwent rectal surgery during March to October this year were included and were divided into two groups. The two stages made up the study. The first stage proposed a difficulty graded system that could be used to assess the surgical difficulty caused by the appendix. The first stage of the study showed that the model that was built in the second stage would be able to make predictions about surgery difficulty. The harder group had higher rates of leaks, longer operation times, and poorer specimen quality. The average accuracy of four-fold cross validation models in the second stage was just over 0.8, and the precision was 0.786, the specificity was 0.750 and the recall was 0.846."]

['Thanks to the availability of multiomics data of individual cancer patients, precision medicine or personalized medicine is becoming a promising treatment for individual cancer patients. However, the association patterns, that is, the mechanism of response (MoR) between large-scale multiomics features and drug response are complex and heterogeneous and remain unclear. Although there are existing computational models for predicting drug response using the high-dimensional multiomics features, it remains challenging to uncover the complex molecular mechanism of drug responses. To reduce the number of predictors/features and make the model more interpretable, in this study, 46 signaling pathways were used to build a deep learning model constrained by signaling pathways, consDeepSignaling, for anti-drug response prediction. Multiomics data, like gene expression and copy number variation, of individual genes can be integrated naturally in this model. The signaling pathway-constrained deep learning model was evaluated using the multiomics data of ∼1000 cancer cell lines in the Broad Institute Cancer Cell Line Encyclopedia (CCLE) database and the corresponding drug-cancer cell line response data set in the Genomics of Drug Sensitivity in Cancer (GDSC) database. The evaluation results showed that the proposed model outperformed the existing deep neural network models. Also, the model interpretation analysis indicated the distinctive patterns of importance of signaling pathways in anticancer drug response prediction.', 'Individualized cancer treatment is becoming better thanks to the availability of multiomics data of individual cancer patients. The mechanisms of response between large-scale multiomics features and drug response are complex, heterogeneous and unclear. Although there are many models for predicting drug responses, it is not easy to find the mechanism of drug response. 46 signaling pathways have been used to build a deep learning model constrained by consDeepSignaling to make the model more interpretable and reduce the number of predictors. This model can be applied to multiomics data of individual genes. The model was evaluated using data from the Broad Institute Cancer Cell Line Encyclopedia and the corresponding drug-cancer cell lines response data from the Genomics of Drug Sensitivity in Cancer database. The proposal model performed better than the already existing deep neural network models. The model interpretation analysis showed that the patterns of signaling pathways were important.']

['Quantitative imaging analysis through radiomics is a powerful technology to non-invasively assess molecular correlates and guide clinical decision-making. There has been growing interest in image-based phenotyping for meningiomas given the complexities in management. We systematically reviewed meningioma radiomics analyses published in PubMed, Embase, and Web of Science until December 20, 2021. We compiled performance data and assessed publication quality using the radiomics quality score (RQS). A total of 170 publications were grouped into 5 categories of radiomics applications to meningiomas: Tumor detection and segmentation (21%), classification across neurologic diseases (54%), grading (14%), feature correlation (3%), and prognostication (8%). A majority focused on technical model development (73%) versus clinical applications (27%), with increasing adoption of deep learning. Studies utilized either private institutional (50%) or public (49%) datasets, with only 68% using a validation dataset. For detection and segmentation, radiomic models had a mean accuracy of 93.1 ± 8.1% and a dice coefficient of 88.8 ± 7.9%. Meningioma classification had a mean accuracy of 95.2 ± 4.0%. Tumor grading had a mean area-under-the-curve (AUC) of 0.85 ± 0.08. Correlation with meningioma biological features had a mean AUC of 0.89 ± 0.07. Prognostication of the clinical course had a mean AUC of 0.83 ± 0.08. While clinical studies had a higher mean RQS compared to technical studies, quality was low overall with a mean RQS of 6.7 ± 5.9 (possible range -8 to 36). There has been global growth in meningioma radiomics, driven by data accessibility and novel computational methodology. Translatability toward complex tasks such as prognostication requires studies that improve quality, develop comprehensive patient datasets, and engage in prospective trials.', 'Using radiomics is a powerful technology that can help guide clinical decisions. There is interest in phenotyping for meningiomas, given the complexity in management. We evaluated publication quality and performed a systematic review of the meningioma radiomics analyses published. A total of 170 publications were categorized into 5 different groups for radiomics applications to meningiomas: tumor detection and segmenting, classification across neurological diseases, feature correlation, and prognostication. A majority of the time they focused on technical model development rather than clinical applications. Studies utilized either private or public data, with the majority using a validation dataset. Radiomic model had a mean accuracy of 93.1  8.1% and a dice coefficients of 88.8%. There was a mean accuracy of 94.9%. The AUC of the tumor was 0.85  0.08. There is growth in meningioma radiomics due to better data accessibility and innovative methodology. Enhancing quality, developing patient data and participating in prospective trials are required to Translatability towards complex tasks.']

['Deep Learning and Machine Learning are becoming more and more popular as their algorithms get progressively better, and their use is expected to have the large effect on improving the health care system. Also, the pandemic was a chance to show how adding AI to healthcare infrastructure could help, since infrastructures around the world are overworked and falling apart. These new technologies can be used to fight COVID-19 because they are flexible and can be changed. Based on these facts, we looked at how the ML and DL-based models can be used to deal with the COVID-19 pandemic problem and what the pros and cons of each are. This paper gives a full look at the different ways to find COVID-19. We looked at the COVID-19 issues in a systematic way and then rated the methods and techniques for finding it based on their availability, ease of use, accuracy, and cost. We have also shown in pictures how well each of the detection techniques works. We did a comparison of different detection models based on the above factors. This helps researchers understand the different methods and the pros and cons of using them as the basis for their research. In the last part, we talk about the open challenges and research questions that come with putting these techniques together with other detection methods.', 'Machine Learning and Deep Learning are becoming more popular as their algorithm gets better and they are expected to have a significant effect on improving health care system. Also, adding Artificial Intelligence to the healthcare infrastructure could help since the global infrastructure is falling apart. They can be changed and can be used to fight COVID-19. We looked at the pros and cons of the different models used to deal with the COVID-19 epidemic. We looked at all the ways to find COVID-19 in the paper, and rated the different methods based on ease of use, accuracy, and cost. We showed how well each of the techniques worked. We compared different detection models based on these factors. It provides a better understanding of the different methods and pros and cons of using them as the basis for their research. In the last part, we talk about how open challenges can be used to integrate techniques with other detection methods.']

['Methamphetamine (MA) tablet production confers chemical and physical properties. This study developed a simple and effective physical characteristic profiling method for MA tablets with capital letter "WY" logos, which realized the discrimination between linked and unlinked seizures. Seventeen signature distances extracted from the "WY" logo were explored as factors for multivariate analysis and demonstrated to be effective to represent the features of tablets in the drug intelligence perspective. Receiver operating characteristic (ROC) curve was used to evaluate efficiency of different pretreatments and distance/correlation metrics, while "Standardization + Euclidean" and "Logarithm + Euclidean" algorithms outperformed the rest. Finally, hierarchical cluster analysis (HCA) was applied to the data set of 200 MA tablet seizures randomly selected from cases all around China in 2015, and 76% of them were classified into a group named after "WY-001." Moreover, the "WY-001" tablets occupied 51-80% tablet seizures from 2011 to 2015 in China, indicating the existence of a huge clandestine factory incessantly manufacturing MA tablets.', 'Chemical and physical properties are gained by the production of MA tablets. The discrimination between linked and unlinked seizures is realized when the profiling method for MA tablets withWY logos is used. Seventeen signature distances from the " WY" logo were examined as factors for multivariate analysis and found to be very effective for representing the features of tablets in the drug intelligence perspective. "Logarithm + Euclidean" and "standardization + Euclidean" were both better at evaluating efficiency of different pretreatments and distance metrics than the rest. The data of 200 MA tablet seizures randomly selected from cases around China in 2015, and 75% were classified into groups named after "WY-001." The "WY-001" tablets occupied 50% of the total seizures from 2011).']

["Children's first mathematics concept is their understanding of the quantities represented by number words (cardinal value), and the age at which they achieve this insight predicts their readiness for mathematics learning in school. We provide the first exploration of the factors that influence the age of becoming a cardinal principle knower (CPK), with a longitudinal study of 197 (94 boys) children from the beginning to the end of two years of preschool. Core symbolic and non-symbolic quantitative competencies at the beginning of preschool, as well as measures of intelligence, executive function, preliteracy skills, and parental education were used to predict timing of CPK status. Children who achieved early CPK status had higher IQ scores, knew more count words and numerals, and had a better intuitive understanding of relative quantity than their peers. Children who were delayed CPKs, in contrast, had deficits in executive function and poor preliteracy skills. The results add to our understanding of children's conceptual development in mathematics and have implications for the identification of at-risk children and design of interventions for them.", "Young children's first concept of mathematics is their ability to understand the quantities represented by number words. The first study on factors affecting the age of becoming aCK was conducted with almost 200 boys from preschool to senior year. Measures of executive function, pre literacy skills, and parental education were used to predict how long it would take for the child to grow up. Children who achieved early CPMK status had higher IQ scores, had better intuitive understanding of relative quantities, and had more count words and numerals than their peers. Children who were delayed were less likely to have poor preliteracy skills. The results add to our understanding of children's conceptual development in mathematics and have implications for the identification of at-risk children and the design of interventions for them."]

['Research on the etiology of dyslexia typically uses an approach based on a single core deficit, failing to understand how variations in combinations of factors contribute to reading development and how this combination relates to intervention outcome. To fill this gap, this study explored links between 28 cognitive, environmental, and demographic variables related to dyslexia by employing a network analysis using a large clinical database of 1,257 elementary school children. We found two highly connected subparts in the network: one comprising reading fluency and accuracy measures, and one comprising intelligence-related measures. Interestingly, phoneme awareness was functionally related to the controlled and accurate processing of letter-speech sound mappings, whereas rapid automatized naming was more functionally related to the automated convergence of visual and speech information. We found evidence for the contribution of a variety of factors to (a)typical reading development, though associated with different aspects of the reading process. As such, our results contradict prevailing claims that dyslexia is caused by a single core deficit. This study shows how the network approach to psychopathology can be used to study complex interactions within the reading network and discusses future directions for more personalized interventions.', 'The approach used for research in the etiology of dyslexia is based on a single core deficit, and fails to understand the differences in combination of factors that can contribute to reading growth. This study explored the links between 28 variables related to dyslexia with the help of a large clinical database of more than 1300 Elementary School children. Two areas of the network are connected, one contains reading and accuracy measures and the other intelligence-related measures. The phoneme awareness was related to speech information and the rapid name was related to visual information. We discovered that a variety of factors contribute to the development of atypical reading. The results show that there is not a single core deficit behind dyslexia. The network approach to psychopathology can be used to investigate complex interactions within the reading network.']

['Golgi is one of the core proteins of a cell, constitutes in both plants and animals, which is involved in protein synthesis. Golgi is responsible for receiving and processing the macromolecules and trafficking of newly processed protein to its intended destination. Dysfunction in Golgi protein is expected to cause many neurodegenerative and inherited diseases that may be cured well if they are detected effectively and timely. Golgi protein is categorized into two parts cis-Golgi and trans-Golgi. The identification of Golgi protein via direct method is very hard due to limited available recognized structures. Therefore, the researchers divert their attention toward the sequences from structures. However, owing to technological advancement, exploration of huge amount of sequences was reported in the databases. So recognition of large amount of unprocessed data using conventional methods is very difficult. Therefore, the concept of intelligence was incorporated with computational model. Intelligence based computational model obtained reasonable results, but the gap of improvement is still under consideration. In this regard, an intelligent automatic recognition model is developed in order to enhance the true classification rate of sub-Golgi proteins. In this approach, discrete and evolutionary feature extraction methods are applied on the benchmark Golgi protein datasets to excerpt salient, propound and variant numerical descriptors. After that, an oversampling technique Syntactic Minority over Sampling Technique is employed to balance the data. Hybrid spaces are also generated with combination of these feature spaces. Further, Fisher feature selection method is utilized to reduce the extra noisy and redundant features from feature vector. Finally, k-nearest neighbor algorithm is used as learning hypothesis. Three distinct cross validation tests are used to examine the stability and efficiency of the proposed model. The predicted outcomes of proposed model are better than the existing models in the literature so far. Finally, it is anticipated that the proposed model will provide the foundation to pharmaceutical industry in drug design and research community to innovate new ideas in the area of computational biology and bioinformatics.', 'Plants and animals have Golgi as one of their core Proteins. The macromolecules are processed by Golgi and sent to its intended destination. If a deficiency in Golgi is detected and treated appropriately, many neurodegenerative and inherited diseases could be cured. There are two parts to golgi, called cis-Golgi and trans-Golgi. The identification of Golgi is difficult due to a limited number of recognized structures. The researchers focus on the sequence from the structures. However, due to technological advancement, huge amount of sequence was reported in the databases. It can be hard to recognize large amount of data using conventional methods. Intelligence was incorporated with a model. The model obtained reasonable results but there is a gap of improvement. An automatic recognition model is developed to improve the classification rate of sub-GolgiProteins. The Golgi proteins dataset is being used to extract salient, pro pound and variant numerical descriptors. Syntactic minority over sampling technique is utilized to balance the data after that. The hybrid spaces are also created with the combination of feature spaces. The Fisher feature selection method can help reduce noise and redundant features. The learning hypothesis is used with the k-nearest neighbor algorithms. Three different cross validation tests have to be used to examine the stability of the model. The outcomes of the model proposed are better than previous models. Finally, it is anticipated that the model will create a foundation for thepharmaceutical industry in drug design and research to create new ideas in the area of computational biology and bioinformatics.']

['Chemotherapy and radiotherapy can produce treatment-related effects, which may mimic tumour progression. Advances in Artificial Intelligence (AI) offer the potential to provide a more consistent approach of diagnosis with improved accuracy. The aim of this study was to determine the efficacy of machine learning models to differentiate treatment-related effects (TRE), consisting of pseudoprogression (PsP) and radiation necrosis (RN), and true tumour progression (TTP). The systematic review was conducted in accordance with PRISMA-DTA guidelines. Searches were performed on PubMed, Scopus, Embase, Medline (Ovid) and ProQuest databases. Quality was assessed according to the PROBAST and CLAIM criteria. There were 25 original full-text journal articles eligible for inclusion. For gliomas: PsP versus TTP (16 studies, highest AUC = 0.98), RN versus TTP (4 studies, highest AUC = 0.9988) and TRE versus TTP (3 studies, highest AUC = 0.94). For metastasis: RN vs. TTP (2 studies, highest AUC = 0.81). A meta-analysis was performed on 9 studies in the gliomas PsP versus TTP group using STATA. The meta-analysis reported a high sensitivity of 95.2% (95%CI: 86.6-98.4%) and specificity of 82.4% (95%CI: 67.0-91.6%). TRE can be distinguished from TTP with good performance using machine learning-based imaging models. There remain issues with the quality of articles and the integration of models into clinical practice. Future studies should focus on the external validation of models and utilize standardized criteria such as CLAIM to allow for consistency in reporting.', 'Treatment effects from Chemotherapy and Radiotherapy can mimic disease progression. Artificial Intelligence has the ability to give a more consistent approach to diagnosis This study was to understand the efficacy of machine learning models in differentiating treatment-related effects and true cancer progression. The review was done in accordance with the guidelines. The searches were done on several databases. The quality was assessed using the PROBAST criteria. The journal has 25 original full-text articles that were eligible for inclusion. For gliomas, PsP versus TTP, RN versus   . For metastases: Which one is better, the RN or the RN vs. Two studies, highest AUC of 0.8. 9 studies in the gliomas PsP versus TTP group were analyzed using the STATA methodology. The meta-analysis had a high sensitivity and specificity. Using machine learning-based models, TRE can be distinguished from TTP. There are issues with integration of models and quality of articles. Future studies should focus on external validation of models and use standardized criteria to make consistency in reporting possible.']

['Decline in cognitive performance in old age is linked to both suboptimal neural processing in grey matter (GM) and reduced integrity of white matter (WM), but the whole-brain structure-function-cognition associations remain poorly understood. Here we apply a novel measure of GM processing-moment-to-moment variability in the blood oxygenation level-dependent signal (SDBOLD)-to study the associations between GM function during resting state, performance on four main cognitive domains (i.e., fluid intelligence, perceptual speed, episodic memory, vocabulary), and WM microstructural integrity in 91 healthy older adults (aged 60-80 years). We modeled the relations between whole-GM SDBOLD with cognitive performance using multivariate partial least squares analysis. We found that greater SDBOLD was associated with better fluid abilities and memory. Most of regions showing behaviorally relevant SDBOLD (e.g., precuneus and insula) were localized to inter- or intra-network "hubs" that connect and integrate segregated functional domains in the brain. Our results suggest that optimal dynamic range of neural processing in hub regions may support cognitive operations that specifically rely on the most flexible neural processing and complex cross-talk between different brain networks. Finally, we demonstrated that older adults with greater WM integrity in all major WM tracts had also greater SDBOLD and better performance on tests of memory and fluid abilities. We conclude that SDBOLD is a promising functional neural correlate of individual differences in cognition in healthy older adults and is supported by overall WM integrity.', 'The decline in cognitive performance in old age is linked to both poorer neural processing in grey matter and reduced integrity of white matter. We used a measure of GM processing variability for a study of the associations between cognitive function and resting state. E. , fluid intelligence, perceptual speed, and episodic memories are in a group of older people. The relationship between whole-GM SDBOLD with cognitive performance was modeled using a partial least squares analysis. We found that it was associated with better fluid abilities and memory. Most of the regions are relevant to SDBOLD. g The brain can be connected through inter- or internal network "Hubs" that integrate functional domains in the brain. Our results suggest that cognitive operations can be supported by the most flexible neural processing that is possible in hub regions. Older adults with greater integrity in all major tracts had improved performance on memory and fluid ability tests. We conclude that there is a link between the individual differences in cognitive abilities of older adults and the function of the neural tube.']

['Treatment planning for prostate volumetric modulated arc therapy (VMAT) can take 5-30 min per plan to optimize and calculate, limiting the number of plan options that can be explored before the final plan decision. Inspired by the speed and accuracy of modern machine learning models, such as residual networks, we hypothesized that it was possible to use a machine learning model to bypass the time-intensive dose optimization and dose calculation steps, arriving directly at an estimate of the resulting dose distribution for use in multi-criteria optimization (MCO). In this study, we present a novel machine learning model for predicting the dose distribution for a given patient with a given set of optimization priorities. Our model innovates upon the existing machine learning techniques by utilizing optimization priorities and our understanding of dose map shapes to initialize the dose distribution before dose refinement via a voxel-wise residual network. Each block of the residual network individually updates the initialized dose map before passing to the next block. Our model also utilizes contiguous and atrous patch sampling to effectively increase the receptive fields of each layer in the residual network, decreasing its number of layers, increasing model prediction and training speed, and discouraging overfitting without compromising on the accuracy. For analysis, 100 prostate VMAT cases were used to train and test the model. The model was evaluated by the training and testing errors produced by 50 iterations of 10-fold cross-validation, with 100 cases randomly shuffled into the subsets at each iteration. The error of the model is modest for this data, with average dose map root-mean-square errors (RMSEs) of 2.38 ± 0.47% of prescription dose overall patients and all optimization priority combinations in the patient testing sets. The model was also evaluated at iteratively smaller training set sizes, suggesting that the model requires between 60 and 90 patients for optimal performance. This model may be used for quickly estimating the Pareto set of feasible dose objectives, which may directly accelerate the treatment planning process and indirectly improve final plan quality by allowing more time for plan refinement.', "The number of plan options can be limited due to the fact that the final plan decision can take up to a month. It was thought it'd be possible to use a machine Learning model to come up with an estimate of the dose distribution in multi-criteria optimalization, based on its accuracy and speed. In this study we present a novel machine learning model for predicting dose distribution for a patient with a set of priorities. Our model is powered by the existing machine learning techniques and utilizes its learning techniques to modify the dose distribution in a voxel-wise residual network. The residual network has a block that updates a dose map on an individual basis. Our model uses a mixture of contiguous and atrous patch sampling to increase the receptive fields of each layer in the residual network, decreasing the number of layers, increasing model prediction and training speed, and discouraging overfitting without compromising on the accuracy. 100 cases were tested for the model. The training and testing errors produced were evaluated with a randomly shuffled 100th case in each iteration. The model's average Errors are 2.38  0.47% of prescription dose overall patients and all the patient testing sets in the data. The model was examined at smaller training set sizes that showed it required between 60 and ninety patients for optimal performance. The model may be used for estimating the Pareto set of feasible dose objectives which may bring about a faster treatment planning process."]

["Professor Henry Higgins in My Fair Lady said, 'Why can't a woman be more like a man?' Perhaps unintended, such narration extends to the reality of current drug development. A clear sex-gap exists in pharmaceutical research spanning from preclinical studies, clinical trials to post-marketing surveillance with a bias towards males. Consequently, women experience adverse drug reactions from approved drug products more often than men. Distinct differences in pharmaceutical response across drug classes and the lack of understanding of disease pathophysiology also exists between the sexes, often leading to suboptimal drug therapy in women. This review explores the influence of sex as a biological variable in drug delivery, pharmacokinetic response and overall efficacy in the context of pharmaceutical research and practice in the clinic. Prospective recommendations are provided to guide researchers towards the consideration of sex differences in methodologies and analyses. The promotion of disaggregating data according to sex to strengthen scientific rigour, encouraging innovation through the personalisation of medicines and adopting machine learning algorithms is vital for optimised drug development in the sexes and population health equity.", "InMy Fair Lady, Professor Henry Higgins said 'Why can't a woman be more like a man?' In pharmaceutical research there is a sex-gap with the research being skewed towards males. Women are more likely to have adverse drug reactions than men. Striking differences between the pharmaceutical response of genders leads to better drug therapy in women. The influence of sex is examined in relation to drug delivery, bio-availability and efficacy in the context of pharmaceutical research and practice in the clinic. Prospective recommendations can be used to help researchers consider sex differences in methodologies and analyses. The promotion of disaggregating data according to sex is very important for promoting scientific rigor, innovation through the personalisation of medicines and improving population health equity."]

['Recently, there has been a growing interest in developing AI-enabled chatbot-based symptom checker (CSC) apps in the healthcare market. CSC apps provide potential diagnoses for users and assist them with self-triaging based on Artificial Intelligence (AI) techniques using human-like conversations. Despite the popularity of such CSC apps, little research has been done to investigate their functionalities and user experiences. To do so, we conducted a feature review, a user review analysis, and an interview study. We found that the existing CSC apps lack the functions to support the whole diagnostic process of an offline medical visit. We also found that users perceive the current CSC apps to lack support for a comprehensive medical history, flexible symptom input, comprehensible questions, and diverse diseases and user groups. Based on these results, we derived implications for the future features and conversational design of CSC apps.', 'The healthcare market has a growing interest in developing chatbots-enabled symptom checker apps. Artificial intelligence techniques such as human-like conversations are used in CSC apps to assist users with possible diagnoses. Little research has been done to investigate the features and user experience of such apps. In order to do so, we did three reviews: a feature review, a user review and an interview study. The existing CSC apps lack the required functions to support the whole diagnostic process. Users perceive the current apps to lack a complete medical history, flexible symptom input and comprehensible questions, and that diverse diseases and user groups are not supported. The implications for the future features were derived based on these results.']

["This study explores current reading profiles and concurrent and early predictors of reading in children with autism spectrum disorder. Before the age of 3 years, the study cohort underwent a neurodevelopmental assessment following identification in a population-based autism screening. At age 8 years, reading, language and cognition were assessed. Approximately half of the sample (<i>n</i> = 25) were 'poor readers' at age 8 years, meaning that they scored below the normal range on tests of single word reading and reading comprehension. And 18 were 'skilled readers' performing above cut-offs. The final subgroup (<i>n</i> = 10) presented with a 'hyperlexic/poor comprehenders' profile of normal word reading, but poor reading comprehension. The 'poor readers' scored low on all assessments, as well as showing more severe autistic behaviours than 'skilled readers'. Group differences between 'skilled readers' and 'hyperlexics/poor comprehenders' were more subtle: these subgroups did not differ on autistic severity, phonological processing or non-verbal intelligence quotient, but the 'hyperlexics/poor comprehenders' scored significantly lower on tests of oral language. When data from age 3 were considered, no differences were seen between the subgroups in social skills, autistic severity or intelligence quotient. Importantly, however, it was possible to identify oral language weaknesses in those that 5 years later presented as 'poor readers' or 'hyperlexics'.", 'The study focuses on reading profiles and concurrent and early predictors of reading in children with the disorder. Before the age of 3 years, the study group underwent a psychological assessment. All children at age 8 were assessed for reading, language and cognitive ability. Half of the sample was a poor reader and they scored well below the normal range on tests of reading comprehension and single word reading. Eighteen were\'skilled readers\' performing above cut-offs. The final subgroup has a \'hyperlexic/poor comprehenders\' profile of normal word reading. The poor readers scored low on all of the assessments, showing more severe Autistic symptoms than skilled readers. The group differences between skilledReaders and Hyperlexics/poor comprehenders were more subtle and did not differ on some cognitive skills. There were no differences in the data from age 3 between the different groups. It was possible to see which oral language weaknesses existed in the five years after they were presented as "poor readers" or "hyperlexics".']

['A primary variant of social media, online support groups (OSG) extend beyond the standard definition to incorporate a dimension of advice, support and guidance for patients. OSG are complementary, yet significant adjunct to patient journeys. Machine learning and natural language processing techniques can be applied to these large volumes of unstructured text discussions accumulated in OSG for intelligent extraction of patient-reported demographics, behaviours, decisions, treatment, side effects and expressions of emotions. New insights from the fusion and synthesis of such diverse patient-reported information, as expressed throughout the patient journey from diagnosis to treatment and recovery, can contribute towards informed decision-making on personalized healthcare delivery and the development of healthcare policy guidelines. We have designed and developed an artificial intelligence based analytics framework using machine learning and natural language processing techniques for intelligent analysis and automated aggregation of patient information and interaction trajectories in online support groups. Alongside the social interactions aspect, patient behaviours, decisions, demographics, clinical factors, emotions, as subsequently expressed over time, are extracted and analysed. More specifically, we utilised this platform to investigate the impact of online social influences on the intimate decision scenario of selecting a treatment type, recovery after treatment, side effects and emotions expressed over time, using prostate cancer as a model. Results manifest the three major decision-making behaviours among patients, Paternalistic group, Autonomous group and Shared group. Furthermore, each group demonstrated diverse behaviours in post-decision discussions on clinical outcomes, advice and expressions of emotion during the twelve months following treatment. Over time, the transition of patients from information and emotional support seeking behaviours to providers of information and emotional support to other patients was also observed. Findings from this study are a rigorous indication of the expectations of social media empowered patients, their potential for individualised decision-making, clinical and emotional needs. The increasing popularity of OSG further confirms that it is timely for clinicians to consider patient voices as expressed in OSG. We have successfully demonstrated that the proposed platform can be utilised to investigate, analyse and derive actionable insights from patient-reported information on prostate cancer, in support of patient focused healthcare delivery. The platform can be extended and applied just as effectively to any other medical condition.', 'Online support groups are extended beyond the definition of a standard social media group in order to provide support and guidance for patients. OSG complement patient journeys. Machine learning and natural language processing can be used to extract information from the large volumes of text discussions accumulated in OSG. The fusion and synthesis of such disparate patient reported information can contribute to the development of healthcare policy guidelines Machine learning and natural language processing techniques have been used to develop an artificial intelligence based analytical framework The social interactions aspect, patient behaviours, decisions, demographic, clinical factors, and emotions are analysed. We used this platform to investigate the effect of online social influence on the intimate decision scenario of selecting a treatment type, recovery after treatment, side effects and emotions expressed over time Results show the decision-making behaviors of the patients. Each group showed different ways in which they expressed their emotions, and their opinions, in the twelve months following treatment. The gradual transition from emotional support to information and other support for other patients was also observed over time. The results of the study show that the expectations of patients on social media are high. The popularity of OSG proves that clinicians need to hear patient voices. We have proved that the proposed platform can be used to investigate, analyse and gain actionable insights from patient reported information in support of patient centred healthcare delivery. The platform can always be applied to any other medical condition.']

["Pediatricians are well positioned to discuss early life obesity risk, but optimal methods of communication should account for parent preferences. To help inform communication strategies focused on early life obesity prevention, we employed human-centered design methodologies to identify parental perceptions, concerns, beliefs, and communication preferences about early life obesity risk. We conducted a series of virtual human-centered design research sessions with 31 parents of infants <24 months old. Parents were recruited with a human intelligence task posted on Amazon's Mechanical Turk, via social media postings on Facebook and Reddit, and from local community organizations. Human-centered design techniques included individual short-answer activities derived from personas and empathy maps as well as group discussion. Parents welcomed a conversation about infant weight and obesity risk, but concerns about health were expressed in relation to the future. Tone, context, and collaboration emerged as important for obesity prevention discussions. Framing the conversation around healthy changes for the entire family to prevent adverse impacts of excess weight may be more effective than focusing on weight loss. Our human-centered design approach provides a model for developing and refining messages and materials aimed at increasing parent/provider communication about early life obesity prevention. Motivating families to engage in obesity prevention may require pediatricians and other health professionals to frame the conversation within the context of other developmental milestones, involve the entire family, and provide practical strategies for behavioral change.", "It is well positioned to discuss early life weight risk, but optimal methods of communication should account for Parent preferences. To help inform communication strategies focused on early life overweight prevention, we used human centered design methodologies. 31 parents of infants  24 months old were the subjects of virtual human-centered design research sessions. Local community organizations recruited parents to take the human intelligence tasks posted on Amazon's Mechanical Turk. Human-centered design techniques included individual short-answer activities that were derived from the personas and empath maps as well as a group discussion. Concerns about the future were expressed in relation to the baby weight discussion, but parents welcomed the discussion. Tone, context, and collaboration emerged as crucial in the discussions about Obesity prevention It is more efficient to talk about healthy changes for the entire family instead of focusing on weight loss. The human-centered approach provides a model for refining and designing messages for increasing parent communication. It is necessary for the whole family to be involved in the conversation and to provide strategies for behavior change in order to encourage families to engage in Obesity Prevention."]

['Artificial Intelligence (AI) offers potential opportunities to optimize clinical pharmacy services in community or hospital settings. The objective of this systematic literature review was to identify and analyse quantitative studies using or integrating AI for clinical pharmacy services. A systematic review was conducted using PubMed/Medline and Web of Science databases, including all articles published from 2000 to December 2021. Included studies had to involve pharmacists in the development or use of AI-powered apps and tools.. 19 studies using AI for clinical pharmacy services were included in this review. 12 out of 19 articles (63.1%) were published in 2020 or 2021. Various methodologies of AI were used, mainly machine learning techniques and subsets (natural language processing and deep learning). The datasets used to train the models were mainly extracted from electronic medical records (6 studies, 32%). Among clinical pharmacy services, medication order review was the service most targeted by AI-powered apps and tools (9 studies), followed by health product dispensing (4 studies), pharmaceutical interviews and therapeutic education (2 studies). The development of these tools mainly involved hospital pharmacists (12/19 studies). The development of AI-powered apps and tools for clinical pharmacy services is just beginning. Pharmacists need to keep abreast of these developments in order to position themselves optimally while maintaining their human relationships with healthcare teams and patients. Significant efforts have to be made, in collaboration with data scientists, to better assess whether AI-powered apps and tools bring value to clinical pharmacy services in real practice.', 'Artificial Intelligence offers potential opportunities to improve the services of community and hospital pharmacy. The aim of the systematic literature review was to identify and analyse quantitative studies using technology. In order for the review to be valid, all the articles published from 2000 to December 2021, were included. There is a 19 studies using artificial intelligence for clinical pharmacy services were included in the review. Machine learning techniques and subsets were used in 12 of 19 articles. Most of the data used to train the models was taken from electronic medical records. medication order review was a service most targeted by Artificial Intelligence-powered apps and tools, followed by health product Dispensing, pharmaceutical interviews, and therapeutic education. The development of these tools was mainly done by a hospital pharmacy. There are soon to be applications and tools for clinical pharmacy services. Keeping up with the latest developments is important for pharmacists to be the best they can be for patients and healthcare teams. In order to make a clear decision on whether artificial intelligence-powered apps and tools bring value to clinical pharmacy services, work needs to be done with data scientists.']

['Controversies and uncertainty persist in prostate cancer grading. To update grading recommendations. Critical review of the literature along with pathology and clinician surveys. Percent Gleason pattern 4 (%GP4) is as follows: (1) report %GP4 in needle biopsy with Grade Groups (GrGp) 2 and 3, and in needle biopsy on other parts (jars) of lower grade in cases with at least 1 part showing Gleason score (GS) 4 + 4 = 8; and (2) report %GP4: less than 5% or less than 10% and 10% increments thereafter. Tertiary grade patterns are as follows: (1) replace "tertiary grade pattern" in radical prostatectomy (RP) with "minor tertiary pattern 5 (TP5)," and only use in RP with GrGp 2 or 3 with less than 5% Gleason pattern 5; and (2) minor TP5 is noted along with the GS, with the GrGp based on the GS. Global score and magnetic resonance imaging (MRI)-targeted biopsies are as follows: (1) when multiple undesignated cores are taken from a single MRI-targeted lesion, an overall grade for that lesion is given as if all the involved cores were one long core; and (2) if providing a global score, when different scores are found in the standard and the MRI-targeted biopsy, give a single global score (factoring both the systematic standard and the MRI-targeted positive cores). Grade Groups are as follows: (1) Grade Groups (GrGp) is the terminology adopted by major world organizations; and (2) retain GS 3 + 5 = 8 in GrGp 4. Cribriform carcinoma is as follows: (1) report the presence or absence of cribriform glands in biopsy and RP with Gleason pattern 4 carcinoma. Intraductal carcinoma (IDC-P) is as follows: (1) report IDC-P in biopsy and RP; (2) use criteria based on dense cribriform glands (>50% of the gland is composed of epithelium relative to luminal spaces) and/or solid nests and/or marked pleomorphism/necrosis; (3) it is not necessary to perform basal cell immunostains on biopsy and RP to identify IDC-P if the results would not change the overall (highest) GS/GrGp part per case; (4) do not include IDC-P in determining the final GS/GrGp on biopsy and/or RP; and (5) "atypical intraductal proliferation (AIP)" is preferred for an intraductal proliferation of prostatic secretory cells which shows a greater degree of architectural complexity and/or cytological atypia than typical high-grade prostatic intraepithelial neoplasia, yet falling short of the strict diagnostic threshold for IDC-P. Molecular testing is as follows: (1) Ki67 is not ready for routine clinical use; (2) additional studies of active surveillance cohorts are needed to establish the utility of PTEN in this setting; and (3) dedicated studies of RNA-based assays in active surveillance populations are needed to substantiate the utility of these expensive tests in this setting. Artificial intelligence and novel grading schema are as follows: (1) incorporating reactive stromal grade, percent GP4, minor tertiary GP5, and cribriform/intraductal carcinoma are not ready for adoption in current practice.', 'There are uncertainties in the analysis of prostrate cancer. To update the grades. There is a critical review of the literature. In case of lower grade and with at least 1 part showing a gleason score, you should report a %gp4 under the %Gyre pattern. The tertiary grade pattern is replace "tertiary grade pattern" in RC with a minor tertiary pattern, and only used in cases with less than 5% of Gleason pattern 5. When multiple undesignated cores from a single MRI-targeted lesion are taken and an overall grade is given, this is called a global score. The term Grade Groups (GrGp) is the terminology adopted by major world organizations. It\'s important to report the carcinoma in biopsy and to use criteria that include dense cribriform glands and solid nests, as well as marked tumors. There are three things that must be done to establish the utility of PTEN in this area: additional studies are needed to establish this; and dedicated studies are needed to demonstrate the usefulness of the assays. cribri form/intraductal carcinoma and the percent GP4 are not ready for adoption in current practice, as well as the use of artificial intelligence.']

["The release of the 2020-2030 Strategic Plan for NIH Nutrition Research (SPNR) and its emphasis on precision nutrition has provided an opportunity to identify future nutrition research that addresses individual variability in response to diet and nutrition across the life span-including those relevant to the Strategic Vision of the National Heart, Lung, and Blood Institute (NHLBI). The SPNR and the NHLBI's Strategic Vision were developed with extensive input from the extramural research community, and both have 4 overarching strategic goals within which are embedded several objectives for research. For the SPNR, these include 1) spur discovery science and normal biological functions (e.g., role of the microbiome in health and disease), 2) population science to understand individual differences (e.g., biomarkers including 'omics that predict disease status), 3) emerging scientific areas of investigation and their application (e.g., data science, artificial intelligence), and 4) cross-cutting themes (e.g., training the scientific workforce and minority health and health disparities). These strategic goals and objectives serve as blueprints for research and training. Nutrition remains important in the prevention and treatment of heart, lung, blood, and sleep (HLBS) disorders and diseases, and the NHLBI has played a pivotal role in supporting nutrition research. In this paper, we report important gaps in the scientific literature related to precision nutrition in HLBS diseases. Research opportunities that could stimulate precision nutrition and their alignment with the SPNR and the NHLBI Strategic Vision Objectives are provided. These opportunities include 1) exploring individual differences in response to varying dietary patterns and nutrients; 2) investigating genetic/epigenetic, biological (e.g., microbiome, biomarkers), social, psychosocial, and environmental underpinnings of individual variability in diet; 3) elucidating the role of circadian rhythm and chrononutrition; and 4) applying implementation science research methods in precision nutrition interventions relevant to HLBS diseases.", "The 2020-2030 Strategic Plan for NIH Nutrition Research has given the opportunity to identify future nutrition research that addresses individual variability in response to diet and nutrition across the life span and those relevant to the National Heart, The NHLBI's Strategic Vision is based on input from the extramural research community, and has 4 overarching strategic goals within which are embedded several objectives for research. There are two things for theSPNR: spur discovery science and normal biological function. g Population science and the role of the gut in health and diseases. g , including the omics that predict disease status and other areas of investigation. g The four themes are: data science, artificial intelligence, and cross-cutting themes. g Training the scientific workforce and minority health and issues. These strategic goals and objectives are blueprints for research and training. The NHLBI plays a pivotal role in supporting Nutrition research, and nutrition remains important in the prevention, treatment, and cure of Heart, Lung, Blood, and Sleep (HLBS) disorders and diseases. There are gaps in the scientific literature related to precision Nutrition. There are research opportunities that could lead to the development of new precision nutrition drugs. There are two opportunities: exploring individual differences in response to different diet plans and vitamins, and investigating genetic and epigenetics. g , the role of hormones in diet, and the use of implementation science methods for precision nutrition interventions relevant to HLBS diseases."]

['An artificial intelligence-based method can predict distinct conformational states of membrane transporters and receptors.', 'The method can predict the state of the transporter']

['The Wechsler Adult Intelligence Scale (WAIS) has been used extensively to study intellectual abilities of special groups. Here, we report the results of an intellectually gifted group on the WAIS-IV. Gifted individuals are people who obtained scores equal to or greater than 2 standard deviations above the mean on an intelligence test. Hence, the current study aims first, to examine mean group performance data of gifted individuals on the WAIS-IV; second, to revalidate the pattern of performance identified in this special group in previous studies (i.e., verbal skills higher than all other abilities); third, to compare scatter measures across intellectual domains with a matched comparison group. A total of 130 gifted individuals (79 males) were administered the full battery and their performance was compared with a matched comparison group. Analyses revealed that gifted group displayed higher scores in all intellectual domains. Contrary to expectations, they showed the highest scores in perceptual reasoning tasks. A multivariate approach revealed that this ability was statistically different from all other domains within the gifted group. Moreover, gifted individuals showed higher discrepancies across intellectual domains than average-intelligence people. Findings have important practical implications to detect intellectual giftedness in adulthood.', "The Wechsler Adult Intelligence Scale has been used to study the intellectual abilities of special groups. The results of a group that was intellectually gifted are reported here. Individuals who have a score equal to or greater than 2 standard deviations above the mean on an intelligence test are called gifted. The current study seeks to examine performance data of gifted individuals on the WAIS-IV and then to identify the pattern of performance seen in the past studies. It's an e There is a match between scatter measures across intellectual domain and a matched comparison group. The performance of a group of 130 gifted individuals who were administered full batteries was compared with a matched group who did not. The analyses showed that the group with the higher scores had more intellectual ability. They showed the highest score on the perceptual reasoning tasks. The ability within the gifted group was different from the other areas. Compared to average intelligence individuals, gifted individuals showed higher discrepancies in intellectual domains. The findings should give some practical implications to detect intellectual giftedness in adulthood."]

['Emotional intelligence (EI) is increasingly viewed as one of the important skills required for a successful career and personal life. Consequently, efforts have been made to improve personal and group performance in EI, mostly in commercial organizations. However, these programs have not been widely applied in the health field. The aim of this study is to assess the impact of a unique special EI interventional process within the framework of an active hematology-oncology unit in a general hospital. This investigation employed a pre- and post-training design using the Bar-On Emotional Quotient Inventory (EQ-i) measure of EI, both before and after completion of training 10 months later. The training included personal and group EI assessments and 10 EI workshops, each 2 weeks apart and each lasting approximately 2 h. Results were compared to a control group of medical staff who did not undergo any EI training program during the same time period. Average total Bar-On EQ-i level at baseline for the group was 97.9, which increased significantly after the interventional process to a score of 105.6 (P = 0.001). There were also significant increases in all five main EQ-i scales, as well as for 12 of the 15 subscales. In contrast, the control group showed no significant differences in general EI level, in any of the five main scales or 15 EI subscale areas. This pilot study demonstrated the capability of a group intervention to improve EI of medical staff working in a hematology-oncological unit. The results are encouraging and suggest that the model program could be successfully applied in a large-scale interventional program.', "One of the skills required for a successful career and personal life is emotional intelligence. Efforts have been made to improve personal and group performance in commercial organizations. The programs aren't common in the health field. This study aims to assess the impact of a special special treatment in a general hospital. This investigation used the Bar-On Emotional-Ugliness Inventory, also known as the Q-i measure, before and after training is completed. The training consisted of assessments and workshops, each lasting approximately 2 h, and a personal and group EI assessment and 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 888-270-6611 The control group of medical staff didn't attend any EI training programs during the same time period. The average total bar-on-equivalence level for the group was 97.9, but increased significantly following the procedure of the interventional process to a score of 108.6. There was a significant increase in the number of scales in the five main ones and 12 subscales. The control group did not show a noticeable difference in the general levels of the five main scales. The pilot study was able to demonstrate the group intervention's ability to improve the EI of medical staff. The model program's results are very encouraging and could possibly be applied to a huge program."]

['Immunohistopathology is an essential technique in the diagnostic workflow of a kidney biopsy. Deep learning is an effective tool in the elaboration of medical imaging. We wanted to evaluate the role of a convolutional neural network as a support tool for kidney immunofluorescence reporting. High-magnification (×400) immunofluorescence images of kidney biopsies performed from the year 2001 to 2018 were collected. The report, adopted at the Division of Nephrology of the AOU Policlinico di Modena, describes the specimen in terms of "appearance," "distribution," "location," and "intensity" of the glomerular deposits identified with fluorescent antibodies against IgG, IgA, IgM, C1q and C3 complement fractions, fibrinogen, and <i>κ</i>- and <i>λ</i>-light chains. The report was used as ground truth for the training of the convolutional neural networks. In total, 12,259 immunofluorescence images of 2542 subjects undergoing kidney biopsy were collected. The test set analysis showed accuracy values between 0.79 ("irregular capillary wall" feature) and 0.94 ("fine granular" feature). The agreement test of the results obtained by the convolutional neural networks with respect to the ground truth showed similar values to three pathologists of our center. Convolutional neural networks were 117 times faster than human evaluators in analyzing 180 test images. A web platform, where it is possible to upload digitized images of immunofluorescence specimens, is available to evaluate the potential of our approach. The data showed that the accuracy of convolutional neural networks is comparable with that of pathologists experienced in the field.', 'There is an important procedure that can be used in the diagnosis of a Kidney Biopsy. Deep learning is used in medical imagery. We wanted to evaluate the benefit of a convolutional neural network as a support tool for reporting immunosuppressive disorders. The high-magnification images of organs were from 2001 to 2019. The report describes the specimen in its appearance, distribution, location, and intensity, based on fluorescent antibodies against IgA, IgM, and C. The training of the neural networks use the report as their ground truth. Of 2542 subjects undergoing a transplant, 12,259 immunofluorescence images were collected. The test set showed accuracy values between 0.79 and  0.94. The ground truth results obtained by the neural networks with respect to the ground truth showed similar values to three different people. Neural networks were as fast as human evaluators in analyzing test images. A web platform where it is possible to download images of the specimen is available to evaluate our approach. The data showed that the accuracy of neural networks indistinguishable from those experienced in the field.']

['Precocious puberty in girls is defined as the onset of pubertal changes before 8 years of age, and gonadotropin-releasing hormone (GnRH) agonist treatment is available for central precocious puberty (CPP). The gold standard for diagnosing CPP is the GnRH stimulation test. However, the GnRH stimulation test is time-consuming, costly, and requires repeated blood sampling. We aimed to develop an artificial intelligence (AI) prediction model to assist pediatric endocrinologists in decision making regarding the optimal timing to perform the GnRH stimulation test. We reviewed the medical charts of 161 girls who received the GnRH stimulation test from 1 August 2010 to 31 August 2021, and we selected 15 clinically relevant features for machine learning modeling. We chose the models with the highest area under the receiver operating characteristic curve (AUC) to integrate into our computerized physician order entry (CPOE) system. The AUC values for the CPP diagnosis prediction model (LH ≥ 5 IU/L) were 0.884 with logistic regression, 0.912 with random forest, 0.942 with LightGBM, and 0.942 with XGBoost. For the Taiwan National Health Insurance treatment coverage prediction model (LH ≥ 10 IU/L), the AUC values were 0.909, 0.941, 0.934, and 0.881, respectively. In conclusion, our AI predictive system can assist pediatric endocrinologists when they are deciding whether a girl with suspected CPP should receive a GnRH stimulation test. With proper use, this prediction model may possibly avoid unnecessary invasive blood sampling for GnRH stimulation tests.', "There is a treatment called central precocious puberty that helps to regulate pubertal changes before the age of 8. The GnRH stimulation test is the gold standard for detectingCPP. The GnRH stimulation test is time consuming and expensive. The model we tried to develop would assist the physicians with determining the optimal time to perform the GnRH stimulation test. A review of the medical charts of 161 girls who received the GnRH stimulation test in the year 2010 and a look at fifteen features for machine learning modeling were done. The models with the highest AUC were selected to integrate into our computerized physician order entry system. The AUC values were used to come to a conclusion on a prediction for the diagnosis. The Taiwan National Health Insurance treatment coverage prediction model AUC values were 0.909, 0.941, 0.934, and 0.881. Children's specialists can utilize our artificial intelligence system to help decide if a suspected child abuse victim should receive a GnRH stimulation test. This prediction model could possibly save blood samples for stimulation tests."]

['Sustainability development goals (SDGs) are regarded as a universal call to action with the overall objectives of planet protection, ending of poverty, and ensuring peace and prosperity for all people. In order to achieve these objectives, different AI technologies play a major role. Specifically, recommender systems can provide support for organizations and individuals to achieve the defined goals. Recommender systems integrate AI technologies such as machine learning, explainable AI (XAI), case-based reasoning, and constraint solving in order to find and explain user-relevant alternatives from a potentially large set of options. In this article, we summarize the state of the art in applying recommender systems to support the achievement of sustainability development goals. In this context, we discuss open issues for future research.', 'The main objectives of theSDGs areplanet protection, ending poverty, and ensure peace and prosperity for all people. Artificial intelligence plays a major role in achieving these objectives. The support can be given to individuals and organizations to achieve the goals. Recommender systems use artificial intelligence technology to find and explain user-relevant alternatives from potential large set of options. We summarize the state of the art of applying recommender systems to support sustainable development goals. In this case, open issues for future research are discussed.']

['Research in molecular sciences witnessed the rise and fall of Artificial Intelligence (AI)/ Machine Learning (ML) methods, especially artificial neural networks, few decades ago. However, we see a major resurgence in the use of modern ML methods in scientific research during the last few years. These methods have had phenomenal success in the areas of computer vision, speech recognition, natural language processing (NLP), etc. This has inspired chemists and biologists to apply these algorithms to problems in natural sciences. Availability of high performance Graphics Processing Unit (GPU) accelerators, large datasets, new algorithms, and libraries has enabled this surge. ML algorithms have successfully been applied to various domains in molecular sciences by providing much faster and sometimes more accurate solutions compared to traditional methods like Quantum Mechanical (QM) calculations, Density Functional Theory (DFT) or Molecular Mechanics (MM) based methods, etc. Some of the areas where the potential of ML methods are shown to be effective are in drug design, prediction of high-level quantum mechanical energies, molecular design, molecular dynamics materials, and retrosynthesis of organic compounds, etc. This article intends to conceptually introduce various modern ML methods and their relevance and applications in computational natural sciences. <i>Synopsis</i> Recent surge in the application of machine learning (ML) methods in fundamental sciences has led to a perspective that these methods may become important tools in chemical science. This perspective provides an overview of the modern ML methods and their successful applications in chemistry during the last few years.', 'Artificial Intelligence and Machine Learning methods, especially artificial neural networks, fell out of favor few decades ago in research. The use of modern methods in scientific research has gone up in the last few years. These methods have been used to excel in areas of computer vision, speech recognition, natural language processing, etc. Computational biologists and chemists are applying the technology to their problems in natural sciences. Availability of high performance Graphics Processing Unit (GPU) accelerators has allowed for a surge. In the field of sciences related to quantum physics, some techniques like theQM,DFT, or MM based methods, have not been found to be accurate or fast enough. Drug designer, prediction of high-level quantum mechanical energies, and molecular design are some of the areas in which the potential of ML methods is shown to be effective. The article intends to introduce a number of modern methods for Computational Natural Sciences. Machine learning methods, which have led to a surge in the application of such methods in fundamental sciences, may eventually become an important tool in chemical science. This perspective looks at the modern methods utilized in chemistry in the last few years.']

['Histological variant (HV) of bladder urothelial carcinoma (UC) is a significant factor for therapy management. We aim to assess the predictive performance of machine learning (ML)-based Computed Tomography radiomics of UC for HV. Volume of interest of 37 bladder UC tumors, of which 21 were pure and 16 were HV, were manually segmented. The extracted first- and second-order texture features (n = 117) using 3-D Slicer radiomics were compared to the radical cystectomy histopathological results. ML algorithms were performed to determine the significant models using Python 2.3, Pycaret library. The sample size was increased to 74 by synthetic data generation, and three outliers from the training set were removed (training dataset; n = 52, test dataset; n = 19). The predictive performances of 15 ML algorithms were compared. Then, the best two models were evaluated on the test set and ensembled by Voting Classifier. The ML algorithms demonstrated area under curve (AUC) and accuracy ranging 0.79-0.97 and 50%-90%, respectively on the train set. The best models were Gradient Boosting Classifier (AUC: 0.95, accuracy: 90%) and CatBoost Classifier (AUC: 0.97, accuracy: 85%). On the test set; the Voting Classifier of these two models demonstrated AUC, accuracy, recall, precision, and F1 scores as follows; 0.93, 79%, 86%, 67%, and 75%, respectively. ML-based Computed Tomography radiomics of UC can predict HV, a prognostic factor that is indeterminable by qualitative radiological evaluation and can be missed in the preoperative histopathological specimens.', "Therapy management is affected by histological variant ofUC. We aim to assess the machine learning-based Computed Tomography performance. The volume of interest of the bladder tumors was manually divided into purity and HV. The results of the radical cyst dissection was compared to the texture features using 3-D Slicer radiomics. The significant models were computed using the python library. The sample size was increased to 74 by synthetic data generation and the outliers from the training set were removed. The predictions performed by the 15 ML algorithms were compared. The models that had the best ratings were evaluated by the Voting Classifier. The area under curve and the accuracy were shown by the machines. The best models were the ones with an accuracy of 80% or greater. The Voting Classifier of the two models showed a AUC of 0.93 dollars and accuracy, recall, and F1 scores of 75%. It is possible to predict HV using computed tomography, a type of radio mic that looks like a picture of a human's face."]

["Prostate cancer is the most frequent cancer in men and a leading cause of cancer death. Determining a patient's optimal therapy is a challenge, where oncologists must select a therapy with the highest likelihood of success and the lowest likelihood of toxicity. International standards for prognostication rely on non-specific and semi-quantitative tools, commonly leading to over- and under-treatment. Tissue-based molecular biomarkers have attempted to address this, but most have limited validation in prospective randomized trials and expensive processing costs, posing substantial barriers to widespread adoption. There remains a significant need for accurate and scalable tools to support therapy personalization. Here we demonstrate prostate cancer therapy personalization by predicting long-term, clinically relevant outcomes using a multimodal deep learning architecture and train models using clinical data and digital histopathology from prostate biopsies. We train and validate models using five phase III randomized trials conducted across hundreds of clinical centers. Histopathological data was available for 5654 of 7764 randomized patients (71%) with a median follow-up of 11.4 years. Compared to the most common risk-stratification tool-risk groups developed by the National Cancer Center Network (NCCN)-our models have superior discriminatory performance across all endpoints, ranging from 9.2% to 14.6% relative improvement in a held-out validation set. This artificial intelligence-based tool improves prognostication over standard tools and allows oncologists to computationally predict the likeliest outcomes of specific patients to determine optimal treatment. Outfitted with digital scanners and internet access, any clinic could offer such capabilities, enabling global access to therapy personalization.", "Men are more than five times more likely to get breast cancer, and there is a leading cause of cancer death. It's difficult to decide on a therapy for a patient, as oncologists have to choose one with the lowest likelihood of toxicity and the highest likelihood of success. Non-specific and semi-quantitative tools are used by international standards. Many tissue-based genes have been attempted to address this, but most have limited validation in trials and are very expensive to run. The therapy personalization personalization tools that are needed remain accurate andScalable. We show how to use deep learning and machine learning to help personalize cancer therapy by predicting long-term outcomes using clinical data and digital histo pathology. We train andValidate models using randomized trials in hundreds of centers. A follow-up of about a 12 year was provided for 7 of the 7764 patients that had histopathological data. Our models have superior discriminatory performance compared to the most common risk-stratification tool-risk groups developed by theNCCN, ranging from 9.2% to 14.6% relative improvement in a held-out validation set. The artificial intelligence tool allows the doctor to compare outcomes of different patients to determine which ones get optimal treatment. Any clinic could provide the capabilities to give access to therapy personalization."]

['In this current digital landscape, artificial intelligence (AI) has established itself as a powerful tool in the commercial industry and is an evolving technology in healthcare. Cutting-edge imaging modalities outputting multi-dimensional data are becoming increasingly complex. In this era of data explosion, the field of cardiovascular imaging is undergoing a paradigm shift toward machine learning (ML) driven platforms. These diverse algorithms can seamlessly analyze information and automate a range of tasks. In this review article, we explore the role of ML in the field of cardiovascular imaging.', 'Artificial intelligence can be used in many sectors of the economy and healthcare. Multi-dimensional data has become increasingly complex. Machine learning is being used more and more in the field of cardiovascular images. The diverse Algorithms can analyze information and automate some tasks. In the article, we discuss the role of the machine learning.']

["Antipsychotics are frequently prescribed to children and adolescents for nonpsychotic indications. Guidelines recommend regularly assessing treatment response and adverse effects and the ongoing need for their use. We aimed to assess adherence to recommendations of available guidelines regarding monitoring antipsychotic use and to test the influence of children's age, sex, intelligence quotient, and diagnosis on adherence. We reviewed 426 medical records from 26 centers within 3 large Dutch child and adolescent psychiatry organizations, excluding children with schizophrenia, psychosis, mania, or an intelligence quotient below 70. We investigated whether there was regular assessment of treatment response, adverse events (physical and laboratory), and at least annual discussion of the need of continued use. On average, treatment response was assessed in 69.3% of the recommended treatment periods, height in 25.6%, weight in 30.6%, blood pressure in 20.6%, evaluation of adverse events in 19.4%, and cardiometabolic measures in 13.7%; discontinuation and/or continued need was discussed at least annually in 36.2%. Extrapyramidal and prolactin-related adverse effects, waist circumference, glucose, and lipids were rarely investigated. Higher age was associated with lower rates of assessment of treatment response. Most antipsychotics were prescribed long-term. In those children with sufficient documentation of the course of treatment, 57.7% was still using an antipsychotic 3 years after initiation. Our findings indicate insufficient adherence to guideline recommendations for monitoring antipsychotic use in children and adolescents, as well as long duration of use in the majority of children. Especially, older children are at higher risk of receiving suboptimal care.", "Children and adolescents are more often prescribed anti-psychotics. The need for their use should be assessed regularly. We wanted to see if the influence of the children's age, sex, intelligence quotient, and diagnoses were related to adherence. We looked at medical records of children with a range of mental illnesses, from kids with an intelligence quotient below 70, to kids with psychosis, mania, and schizophrenia. On average, treatment response was assessed in almost 70% of the recommended treatment periods, height in over 20%, weight in less than 30%, blood pressure in over 21% and cardiometabolic measures in over 9%. Extrapyramidal and prolactin related adverse effects rarely are investigated. The rates of assessment of treatment response were lower when there was higher age. A large amount of antipsychotics were prescribed long term. 54.7% of children with sufficient documentation were still using an intervention three years after starting. Our findings suggest that there is insufficient adherence to guideline recommendations for monitoring and long-term use of a medication in the majority of children. Older children are less likely to receive optimal care."]

['The intra-sphere and inter-sphere structural attributes of controlled release microsphere drug products can greatly impact their release profile and clinical performance. In developing a robust and efficient method to characterize the structure of microsphere drug products, this paper proposes X-ray microscopy (XRM) combined with artificial intelligence (AI)-based image analytics. Eight minocycline loaded poly(lactic-co-glycolic acid) (PLGA) microsphere batches were produced with controlled variations in manufacturing parameters, leading to differences in their underlying microstructures and their final release performances. A representative number of microspheres samples from each batch were imaged using high resolution, non-invasive XRM. Reconstructed images and AI-assisted segmentation were used to determine the size distribution, XRM signal intensity, and intensity variation of thousands of microspheres per sample. The signal intensity within the eight batches was nearly constant over the range of microsphere diameters, indicating high structural similarity of spheres within the same batch. Observed differences in the variation of signal intensity between different batches suggests inter-batch non-uniformity arising from differences in the underlying microstructures associated with different manufacturing parameters. These intensity variations were correlated with the structures observed from higher resolution focused ion beam scanning electron microscopy (FIB-SEM) and the in vitro release performance for the batches. The potential for this method for rapid at-line and offline product quality assessment, quality control, and quality assurance is discussed.', 'Drug products with controlled release can have great impact on their release profile and clinical performance. The method to describe the structure of the drug products that is proposed includes X-ray microscopy and Artificial intelligence. There were 8 minocycline loadedPLGA microsphere batches produced with different manufacturing parameters leading to differences of their underlying structures. A number of sample from each batches were imaged with high resolution XRM. A sample of thousands of microspheres was compared to be determined by the size distribution, XRM signal intensity, and intensity variation. The signal intensity within the eight batches was nearly constant over the range of spheres. Inter-batch non-uniformity is related to different manufacturing parameters and differences in the underlying structure. These intensity variations were correlated with the structures observed with higher resolution focused electron microscopes. The method for rapid at-line and offline quality assessment, quality control, and quality assurance is discussed.']

['Artificial intelligence (AI) plays a rapidly increasing role in clinical care. Many of these systems, for instance, deep learning-based applications using multilayered Artificial Neural Nets, exhibit epistemic opacity in the sense that they preclude comprehensive human understanding. In consequence, voices from industry, policymakers, and research have suggested trust as an attitude for engaging with clinical AI systems. Yet, in the philosophical and ethical literature on medical AI, the notion of trust remains fiercely debated. Trust skeptics hold that talking about trust in nonhuman agents constitutes a category error and worry about the concept being misused for ethics washing. Proponents of trust have responded to these worries from various angles, disentangling different concepts and aspects of trust in AI, potentially organized in layers or dimensions. Given the substantial disagreements across these accounts of trust and the important worries about ethics washing, we embrace a diverging strategy here. Instead of aiming for a positive definition of the elements and nature of trust in AI, we proceed <i>ex negativo</i>, that is we look at cases where trust or distrust are misplaced. Comparing these instances with trust expedited in doctor-patient relationships, we systematize these instances and propose a taxonomy of both misplaced trust and distrust. By inverting the perspective and focusing on negative examples, we develop an account that provides useful ethical constraints for decisions in clinical as well as regulatory contexts and that highlights how we should <i>not</i> engage with medical AI.', 'Artificial intelligence plays a role in healthcare. Many of these systems are using Artificial Neural Nets and are not compatible with fully comprehensive human understanding. Stakeholders in industries, policymakers, and research have all said that trust is an attitude that should be taken into account when engaging with clinical artificial intelligence systems. In the ethical literature on medical software, the notion of trust is still a contentious topic. Trust skeptics worry that talking about trust in nonhuman agents constitutes a category error and that they may be misinterpreted for ethics washing. Proponents of trust have addressed these concerns from a number of angles, clarifying different aspects of trust in technology. We embrace a different strategy for trust because of the significant differences. Instead of looking for a positive definition of the elements and nature of trust, we look at cases where trust or distrust is lost. We systematize instances and propose a list of both trust and distrust. By turning the perspective into a negative one, we can come up with an account that provides ethical constraints for decisions in clinical and regulatory contexts.']

['Prehospital care plays a critical role in improving patient outcomes, particularly in cases of time-sensitive emergencies such as trauma, cardiac failure, stroke, bleeding, breathing difficulties, systemic infections, etc. In recent years, there has been a growing interest in clinical research in prehospital care, and several challenges and opportunities have emerged. There is an urgent need to adapt clinical research methodology to a context of prehospital care. At the same time, there are many barriers in prehospital research due to the complex context, posing unique challenges for research, development, and evaluation. Among these, this review allows the highlighting of limited resources and infrastructure, ethical and regulatory considerations, time constraints, privacy, safety concerns, data collection and analysis, selection of a homogeneous study group, etc. The analysis of the literature also highlights solutions such as strong collaboration between emergency medical services (EMS) and hospital care, use of (mobile) health technologies and artificial intelligence, use of standardized protocols and guidelines, etc. Overall, the purpose of this narrative review is to examine the current state of clinical research in prehospital care and identify gaps in knowledge, including the challenges and opportunities for future research.', 'In cases of time-sensitive emergencies, prehospital care plays an important role in improving patient outcomes. There is a growing interest in clinical research inhospital care. Clinical research methodology needs to be changed in order to be applicable for prehospital care. There are many barriers in the prehospital research because of the complex context, posing unique challenges for research, development, and evaluation. This review allows the highlighting of limited resources and infrastructure, ethical and regulatory considerations, time constraints, privacy, safety concerns, data collection and analysis, selection of a group, and more. The solutions highlighted in the analysis of the literature include strong collaboration between emergency medical services and hospital care, use of mobile health technology, use of standardized protocols and guidelines, artificial intelligence, and more. The current state of research in pre hospital care is examined in this review, along with the gaps in knowledge and the opportunities for new research.']

["Lung cancer is the leading cause of cancer-related death in China. The effectiveness of low-dose computed tomography (LDCT) screening has been further validated in recent years, and significant progress has been made in research on identifying high-risk individuals, personalizing screening interval, and management of screen-detected findings. The aim of this study is to revise China national lung cancer screening guideline with LDCT (2018 version). The China Lung Cancer Early Detection and Treatment Expert Group (CLCEDTEG) designated by the China's National Health Commission, and China Lung Oncolgy Group experts, jointly participated in the revision of Chinese lung cancer screening guideline (2023 version). This revision is based on the recent advances in LDCT lung cancer screening at home and abroad, and the epidemiology of lung cancer in China. The following aspects of the guideline were revised: (1) lung cancer risk factors besides smoking were considered for the identification of high risk population; (2) LDCT scan parameters were further classified; (3) longer screening interval is recommended for individuals who had negative LDCT screening results for two consecutive rounds; (4) the follow-up interval for positive nodules was extended from 3 months to 6 months; (5) the role of multi-disciplinary treatment (MDT) in the management of positive nodules, diagnosis and treatment of lung cancer were emphasized. This revision clarifies the screening, intervention and treatment pathways, making the LDCT screening guideline more appropriate for China. Future researches based on emerging technologies, including biomarkers and artificial intelligence, are needed to optimize LDCT screening in China in the future. .", "Lung cancer is the leading cause of cancer-related death in China. The effectiveness of low-dose computed tomography screening has improved over time, as well as progress in several areas such as identifying high-risk individuals, and management of screen-detected findings The aim of this study is to revise the guidelines for lung cancer screening in China. The revision of Chinese lung cancer screening guidelines was helped by the China Lung Cancer Early Detection and Treatment Expert Group (CLCEDTEG) designated by the China's National Health Commission. The epidemiology of lung cancer in China is a topic of revision. The following modifications to the guideline were made: (1) lung cancer risk factor besides smoking was considered for the identification of high risk population; (2) LDCT scans were further classified; (3) longer screening intervals is recommended for individuals who had negative screening results for two consecutive rounds; and 4) the follow-up This revision makes the LDCT screening guideline for China more appropriate. In the future, future research using emerging technologies, such as artificial intelligence, need to be done to improve LDCT screening in China. There"]

['Patients with coronavirus disease 2019 (COVID-19) could develop severe disease requiring admission to the intensive care unit (ICU). This article presents a novel method that predicts whether a patient will need admission to the ICU and assesses the risk of in-hospital mortality by training a deep-learning model that combines a set of clinical variables and features in chest radiographs. This was a prospective diagnostic test study. Patients with confirmed severe acute respiratory syndrome coronavirus 2 infection between March 2020 and January 2021 were included. This study was designed to build predictive models obtained by training convolutional neural networks for chest radiograph images using an artificial intelligence (AI) tool and a random forest analysis to identify critical clinical variables. Then, both architectures were connected and fine-tuned to provide combined models. 2552 patients were included in the clinical cohort. The variables independently associated with ICU admission were age, fraction of inspired oxygen (<i>F</i> <sub>iO<sub>2</sub></sub> ) on admission, dyspnoea on admission and obesity. Moreover, the variables associated with hospital mortality were age, <i>F</i> <sub>iO<sub>2</sub></sub> on admission and dyspnoea. When implementing the AI model to interpret the chest radiographs and the clinical variables identified by random forest, we developed a model that accurately predicts ICU admission (area under the curve (AUC) 0.92±0.04) and hospital mortality (AUC 0.81±0.06) in patients with confirmed COVID-19. This automated chest radiograph interpretation algorithm, along with clinical variables, is a reliable alternative to identify patients at risk of developing severe COVID-19 who might require admission to the ICU.', "coronaviruses disease could be dangerous if patients need to sign up for the intensive care unit. This article describes a novel method that combines a set of clinical variables into a model to predict a patient's fate and assess their risk of in-hospital mortality. This was a prospective test study. Patients with confirmed acute respiratory syndrome coronaviruses 2 infection between March 2020 and January 2021 were included. The study used an artificial intelligence tool as well as a random forest analyses to identify critical clinical variables. Both architectures were connected and fine-tuned to provide combined models. The patients were included in the clinical cohort. The variables that were associated with admission were age, fraction of oxygen, and degree of dyspnoea. The variables that could be associated with hospitalization deaths were age, admission and dyspnoea. A model based on the randomness of randomness was developed to accurately predict admissions to hospitals and death rates in patients with confirmed COVID-19."]

['A decade of unprecedented progress in artificial intelligence (AI) has demonstrated a lot of interest in medical imaging research including nuclear cardiology. AI has a potential to reduce cost, save time and improve image acquisition, interpretation, and decision-making. This review summarizes recent researches and potential applications of AI in nuclear cardiology and discusses the pitfall of AI.', 'A decade of unprecedented progress in artificial intelligence shows a lot of interest in medical scans. Artificial intelligence has the ability to improve image acquisition, interpretation and decision making. There were recent research and potential applications in nuclear cardiology and this review reviews them.']

['To determine the association of verbal intelligence, a core constituent of health literacy, with diabetic complications and walking speed in people with Type 2 diabetes. This study was performed in 228 people with Type 2 diabetes participating in the Maastricht Study, a population-based cohort study. We examined the cross-sectional associations of score on the vocabulary test of the Groningen Intelligence Test with: 1) determinants of diabetic complications (HbA<sub>1c</sub> , blood pressure and lipid level); 2) diabetic complications: chronic kidney disease, neuropathic pain, self-reported history of cardiovascular disease and carotid intima-media thickness; and 3) walking speed. Analyses were performed using linear regression and adjusted in separate models for potential confounders and mediators. Significant age- and sex-adjusted associations were additionally adjusted for educational level in a separate model. After full adjustment, lower verbal intelligence was associated with the presence of neuropathic pain [odds ratio (OR) 1.18, 95% CI 1.02;1.36], cardiovascular disease (OR 1.14, 95% CI 1.01;1.30), and slower walking speed (regression coefficient -0.011 m/s, 95% CI -0.021; -0.002 m/s). These associations were largely explained by education. Verbal intelligence was not associated with blood pressure, glycaemic control, lipid control, chronic kidney disease or carotid intima-media thickness. Lower verbal intelligence was associated with the presence of some diabetic complications and with a slower walking speed, a measure of physical functioning. Educational level largely explained these associations. This implies that clinicians should be aware of the educational level of people with diabetes and should provide information at a level of complexity tailored to the patient.', 'To see if there is an association of verbal intelligence with health literacy and diabetes. The study was performed on 228 people with Diabetes who were participating in the Maastricht Study. The cross-sectional associations of score on the vocabulary test for the Groningen intelligence test have been looked at. Analyses were performed using different models for potential confounders. The education level is adjusted for the age and sex-adjusted associations. Lower verbal intelligence was found to be linked to cardiovascular disease, slow walking speed, and the presence of neuropathic pain. Education was used to explain these associations. Verbal intelligence was not associated with a number of diseases. Lower verbal intelligence was associated with slower walking speeds as well as some degree of diabetes. Educational level is what explained the associations. This means that clinicians should know what level of education people with diabetes have and provide information that is specific to the patient.']

['To describe the long-term outcome of children with prenatally diagnosed isolated complete agenesis of the corpus callosum (cACC). In this single-center case series, we reviewed retrospectively the charts of fetuses referred to our fetal therapy unit from January 2004 to July 2020 for a suspected anomaly of the corpus callosum (CC). Cases with prenatally diagnosed isolated cACC were included. Fetal karyotype and comparative genomic hybridization microarray of amniotic fluid, in addition to fetal magnetic resonance imaging, were offered to all pregnant women with a diagnosis of fetal CC malformation. The surviving children were enrolled in the neurodevelopmental follow-up program at our institution, which included postnatal magnetic resonance imaging, serial neurological examinations and neurodevelopmental evaluations with standardized tests according to age. Families living in remote areas or far from our institution were offered a structured ad-hoc phone interview. A total of 128 pregnancies with fetal CC malformation were identified (mean gestational age at diagnosis, 24.5 (range, 21-34) weeks), of which 53 cases were diagnosed prenatally with apparently isolated cACC. Of these, 12 cases underwent termination of pregnancy, one resulted in intrauterine demise at 24 weeks of gestation and 13 cases were lost to follow-up. Of the remaining 27 children, one was excluded due to an associated chromosomal anomaly (8p21.3q11.21 mosaic duplication) diagnosed after birth, which could have been detected prenatally if the parents had consented to amniocentesis. In the 26 children included in the analysis, neurodevelopmental follow-up was available for a median of 3 (range, 1-16) years. Three (11.5%) infants had severe neurodevelopmental impairment, two of which were diagnosed postnatally with a genetic syndrome (Mowat-Wilson syndrome and Vici syndrome) that would not have been diagnosed prenatally. Seven (26.9%) children had mild neurodevelopmental impairment and 16 (61.5%) had normal neurodevelopmental outcome. The Full-Scale Intelligence Quotients of the three children with severe neurodevelopmental impairment were 50, 64 and 63, respectively, while that of the remaining children was in the normal range (median, 101; range, 89-119). In 88% of the children with cACC included in this study, neurodevelopment was not severely impaired. However, long-term follow-up is recommended in all cases of congenital isolated cACC to recognize subtle neurodevelopmental disorders as early as possible. © 2022 International Society of Ultrasound in Obstetrics and Gynecology.', 'The outcome of children with a complete agenesis of the cortex was described. From January 2004 to July 2020 we reviewed the charts of pregnant women referred for fetal therapy. Some cases were diagnosed with cACC before birth. Fetal karyotype and comparative genes of amniotic fluid were offered to all pregnant women with a fetal CC malformation. The children in the follow-up program were required to have their neurological exams at a minimum age of 18. A structured phone interview was given to families living in remote areas. There are 128 pregnancies with fetal CC malformation that have been identified, of which 53 were prenatally diagnosed with cACC. Of these, 13 were terminated of the pregnancies, with one resulting in a death at 24 weeks of gestation and the other one dying. If the parents had consented to amniocentesis, the one child excluded due to an associated chromosomal anomaly could have been detected at an early age. The children included in the analysis had a median follow-up time of 3 years. Two of the infants had a genetic syndrome that would not have been diagnosed before birth, but did not have a diagnosis before they were born. There were seven children with mild neurodevelopmental impairment and 16 with normal outcomes. Three children with severe neurodevelopmental impairment had full scale intelligence quotients of 50, 64 and 63, while the rest had normal quotients. The study showed that in 85% of the children with cACC. In all congenital isolated cACC cases long-term follow-up is advised to recognize subtle neurodevelopmental disorders as early as possible. International Society of Ultrasound in Obstetrics and Gynecology was founded in 1922.']

['Fatty liver disease has a high and increasing prevalence worldwide, is associated with adverse cardiovascular events and higher long-term medical costs, and may lead to liver-related morbidity and mortality. There is an urgent need for accurate, reproducible, accessible, and noninvasive techniques appropriate for detecting and quantifying liver fat in the general population and for monitoring treatment response in at-risk patients. CT may play a potential role in opportunistic screening, and MRI proton-density fat fraction provides high accuracy for liver fat quantification; however, these imaging modalities may not be suited for widespread screening and surveillance, given the high global prevalence. US, a safe and widely available modality, is well positioned as a screening and surveillance tool. Although well-established qualitative signs of liver fat perform well in moderate and severe steatosis, these signs are less reliable for grading mild steatosis and are likely unreliable for detecting subtle changes over time. New and emerging quantitative biomarkers of liver fat, such as those based on standardized measurements of attenuation, backscatter, and speed of sound, hold promise. Evolving techniques such as multiparametric modeling, radiofrequency envelope analysis, and artificial intelligence-based tools are also on the horizon. The authors discuss the societal impact of fatty liver disease, summarize the current state of liver fat quantification with CT and MRI, and describe past, currently available, and potential future US-based techniques for evaluating liver fat. For each US-based technique, they describe the concept, measurement method, advantages, and limitations. <sup>©</sup> RSNA, 2023 <i>Online supplemental material is available for this article.</i> Quiz questions for this article are available through the Online Learning Center.', 'There is a high prevalence of cholesterol-related diseases, and that may lead to increased healthcare costs. It is crucial that we find accurate, accessible, and non-destructive ways to measure the response to treatment for liver fat in the general population and at-risk patients. There may be a role for computed tomographic screening but the high globalPrevalence may preclude the use of other, more conventional, screening technologies. US is a good option for screening and surveillance. Some well established qualitative signs of the fat are reliable for moderate, severe and mild ste atosis but are not reliable for subtle changes over time. New and emerging measures of sound and fat in the body hold promise. Evolving techniques, such as multiparametric modeling, are still on the horizon. The authors discuss the impact of fatty liver disease, summarize the current state ofLiver fat with CT and MRI and describe past, currently available or potential future US- based techniques for evaluatingLiver fat They describe the concept, measurement method, advantages and limitations of the US-based techniques. There are online supplemental material in this article. The online learning center has quiz questions for the article.']

['<b>Introduction and aim :</b> The examination of trait emotional intelligence as an important component of adolescent psychological adjustment and coping has received a great deal of attention. Trait emotional intelligence is expected to reduce the vulnerability to emotional problems by reducing mood deterioration in adverse situations. Most research to date has addressed the regulation of negative affective states, with less attention paid to the responses to positive affect. Thus, the aim of this research was to examine the cross-sectional and prospective associations between trait emotional intelligence dimensions (i.e., trait emotional attention, trait emotional clarity, and trait emotional repair), response styles to negative affect (i.e., depressive rumination and distraction) and response to positive affect (i.e., emotion-focused and self-focused positive rumination and dampening) in adolescence. <b>Methods:</b> A 1-year follow-up study was conducted with a sample of 880 adolescents (52.4% girls) aged 14-17 years old (<i>M</i> = 14.74, <i>SD</i> = 0.68) who were enrolled in 18 high schools in Andalusia (Spain). Participants completed self-report measures of trait emotional intelligence, response to negative affect and response styles to positive affect. To analyse the data, hierarchical regression analyses and path analysis were performed. <b>Results:</b> Our results showed that high trait emotional attention was cross-sectionally and longitudinally associated with more dampening of positive affect and more depressive rumination. Furthermore, high trait emotional repair was cross-sectionally and longitudinally related to more distraction to negative affect and more self-focused positive rumination. Some gender differences were also found; girls reported higher trait emotional attention, higher dampening, and higher depressive rumination. Furthermore, boys reported higher trait emotional repair, higher self-focused positive rumination and higher distraction to negative affect. <b>Conclusions and discussion:</b> Our findings provide longitudinal evidence of the relationships between trait emotional intelligence and responses to both positive and negative affect during adolescence. Consequently, interventions designed to promote resilience during adolescence could target the development of more adaptive responses to both negative and positive affect within the framework of school-based emotional education programmes.', "The study of trait emotional intelligence as a component of adolescent psychological adjustment and cope has gotten a lot of attention. It is expected that trait emotional intelligence reduces the vulnerability to emotional issues. There is less attention given to the responses to positive affect in research that has been done to date. The aim of this research was to look at the cross-sectional and prospective association between trait emotional intelligence dimensions. e There are two response styles to negative affect, trait emotional attention, and trait emotional clarity. It's an e Responses to positive affect, as well as the responses to depression and distraction. It's e. Positive and emotion focused in adolescence. A follow-up study was performed with more than 800 adolescents who were aged 14 to 17 years old. People completed measures of their emotional intelligence and response styles. The analysis of the data was performed. Our results show that high trait emotional attention is associated with a greater effect of positive affect and greater depression. High trait emotional repair was related to distraction tonegative affect and more self-focused positive. Girls reported higher trait emotional attention, whereas boys reported lower trait emotional attention. Boys reported higher trait emotional repair, self-focused positive rumination and distraction to negative affect. There is longitudinal evidence to support the idea that trait emotional intelligence and responses to both positive and negative affect during adolescence are related. School-based emotional education programmes could be used to help the development of more adaptive responses to both negative and positive affect in adolescence."]

['The incidence and mortality rates of lung cancer are high worldwide, where non-small cell lung cancer (NSCLC) accounts for more than 85% of lung cancer cases. Recent non-small cell lung cancer research has been focused on analyzing patient prognosis after surgery and identifying mechanisms in connection with clinical cohort and ribonucleic acid (RNA) sequencing data, including single-cell ribonucleic acid (scRNA) sequencing data. This paper investigates statistical techniques and artificial intelligence (AI) based non-small cell lung cancer transcriptome data analysis methods divided into target and analysis technology groups. The methodologies of transcriptome data were schematically categorized so researchers can easily match analysis methods according to their goals. The most widely known and frequently utilized transcriptome analysis goal is to find essential biomarkers and classify carcinomas and cluster NSCLC subtypes. Transcriptome analysis methods are divided into three major categories: Statistical analysis, machine learning, and deep learning. Specific models and ensemble techniques typically used in NSCLC analysis are summarized in this paper, with the intent to lay a foundation for advanced research by converging and linking the various analysis methods available.', 'Lung cancer is the leading cause of cancer death in the world, with non-small cell lung canceraccounting for 85% of lung cancer cases. The purpose of recent non-small cell lung cancer research has been to analyze patient progess after surgery This paper explores how statistical techniques and artificial intelligence are used for lung cancer transcriptome data analysis. The methods of transcriptome data are categorized so that researchers can easily match their analysis methods with their goals. The most common transcriptome analysis goal is to find essential markers and classify tumors and non-small cell lung cancer. There are three types of transcriptome analysis: Statistical analysis, machine learning, and deep learning. Specific models and ensemble techniques used in a lung cancer study are summarized in this paper in an attempt to lay a foundation for advanced research.']

["Recent advances in machine learning (ML) have transformed the landscape of energy exploration, including hydrocarbon, CO<sub>2</sub> storage, and hydrogen. However, building competent ML models for reservoir characterization necessitates specific in-depth knowledge in order to fine-tune the models and achieve the best predictions, limiting the accessibility of machine learning in geosciences. To mitigate this issue, we implemented the recently emerged automated machine learning (AutoML) approach to perform an algorithm search for conducting an unconventional reservoir characterization with a more optimized and accessible workflow than traditional ML approaches. In this study, over 1000 wells from Alberta's Athabasca Oil Sands were analyzed to predict various key reservoir properties such as lithofacies, porosity, volume of shale, and bitumen mass percentage. Our proposed workflow consists of two stages of AutoML predictions, including (1) the first stage focuses on predicting the volume of shale and porosity by using conventional well log data, and (2) the second stage combines the predicted outputs with well log data to predict the lithofacies and bitumen percentage. The findings show that out of the ten different models tested for predicting the porosity (78% in accuracy), the volume of shale (80.5%), bitumen percentage (67.3%), and lithofacies classification (98%), distributed random forest, and gradient boosting machine emerged as the best models. When compared to the manually fine-tuned conventional machine learning algorithms, the AutoML-based algorithms provide a notable improvement on reservoir property predictions, with higher weighted average f1-scores of up to 15-20% in the classification problem and 5-10% in the adjusted-R<sup>2</sup> score for the regression problems in the blind test dataset, and it is achieved only after ~ 400 s of training and testing processes. In addition, from the feature ranking extraction technique, there is a good agreement with domain experts regarding the most significant input parameters in each prediction. Therefore, it is evidence that the AutoML workflow has proven powerful in performing advanced petrophysical analysis and reservoir characterization with minimal time and human intervention, allowing more accessibility to domain experts while maintaining the model's explainability. Integration of AutoML and subject matter experts could advance artificial intelligence technology implementation in optimizing data-driven energy geosciences.", 'Recent advances with machine learning have changed the landscape of energy exploration. In order to make competent models for geosciences, specific in-depth knowledge is required. The new machine learning approach has been implemented to find unconventional reservoir characterization with a more optimal and accessible workflows. A study of over 1000 wells from the Athabasca oil sand showed different reservoir properties The first stage of the proposed workflows includes predicting the volume of the oil and gas by using well log data and the second stage integrates the predictions into well log data. The models that did the best work for predicting the porosity were the Bitumen percentage, distributed random forest, and gradient boosting machine. When compared with manually fine-tuned machine learning, the auto-lmc-based algos provide a huge improvement on the predictions of the properties of the reservoir, with higher weighted average f1- scores of up to 15-20% for the classification problem and up to 5% in the adjusted- There is an agreement with domain experts about the most significant input parameters for each prediction. The evidence shows that the AutoML process is powerful in doing advanced analysis with less time and human intervention, and allows more accessibility to domain experts. In data driven energy geosciences, integration of AutoML and subject matter experts could improve the implementation of Artificial Intelligence.']

['Computer-aided diagnostic systems are emerging in the field of gastrointestinal endoscopy. In this study, we assessed the clinical performance of the computer-aided detection (CADe) of colonic adenomas using a new endoscopic artificial intelligence system. This was a single-center prospective randomized study including 415 participants allocated into the CADe group (n = 207) and control group (n = 208). All endoscopic examinations were performed by experienced endoscopists. The performance of the CADe was assessed based on the adenoma detection rate (ADR). Additionally, we compared the adenoma miss rate for the rectosigmoid colon (AMRrs) between the groups. The basic demographic and procedural characteristics of the CADe and control groups were as follows: mean age, 54.9 and 55.9 years; male sex, 73.9% and 69.7% of participants; and mean withdrawal time, 411.8 and 399.0 s, respectively. The ADR was 59.4% in the CADe group and 47.6% in the control group (p = 0.018). The AMRrs was 11.9% in the CADe group and 26.0% in the control group (p = 0.037). The colonoscopy with the CADe system yielded an 11.8% higher ADR than that performed by experienced endoscopists alone. Moreover, there was no need to extend the examination time or request the assistance of additional medical staff to achieve this improved effectiveness. We believe that the novel CADe system can lead to considerable advances in colorectal cancer diagnosis.', "There are computer aided diagnostic systems. The results of the study assessed the clinical performance of the new computer-aided detection system for colonic adenomas. The study included 423 participants who were allocated into theCADe group and the control group. Endoscopists performed all the endoscopic exams. The performance of the CADe was evaluated based on the adenoma detection rate. The adenoma miss rate was compared between the groups The demographic and procedural characteristics of the CADe and control group were as follows: median age, 54.9 and 55.9 years; median male sex, 74.9%; and mean withdrawal time, 411.8 and 399.0 s, respectively. The control group reported a 47.6% ratio while the control group's ADR was 59.4%. The two groups that the AMRrs were in were the control group and the control group with a 26.0% share. The colonoscopy with the CADe system produced an 11.8% higher final result than did the endoscopists alone. There was no need to ask the medical staff to help with the exam time extension. We believe that the novel Cad system can lead to major improvements in colorectal cancer diagnosis."]

['Due to the growing need to provide better global healthcare, computer-based and robotic healthcare equipment that depend on artificial intelligence has seen an increase in development. In order to evaluate artificial intelligence (AI) in computer technology, the Turing test was created. For evaluating the future generation of medical diagnostics and medical robots, it remains an essential qualitative instrument. We propose a novel methodology to assess AI-based healthcare technology that provided verifiable diagnostic accuracy and statistical robustness. In order to run our test, we used a state-of-the-art AI model and compared it to radiologists for checking how generalized the model is and if any biases are prevalent. We achieved results that can evaluate the performance of our chosen model for this study in a clinical setting and we also applied a quantifiable method for evaluating our modified Turing test results using a meta-analytical evaluation framework. His test provides a translational standard for upcoming AI modalities. Our modified Turing test is a notably strong standard to measure the actual performance of the AI model on a variety of edge cases and normal cases and also helps in detecting if the algorithm is biased towards any one type of case. This method extends the flexibility to detect any prevalent biases and also classify the type of bias.', 'Due to the growing need to provide better global healthcare, computer-based and robotic healthcare equipment that depend on artificial intelligence has experienced an increase in development. The Turing test was created to evaluate artificial intelligence in computer technology. For evaluating the future generation of medical diagnostics, it is still an essential qualitative instrument. A novel methodology for evaluating the accuracy and statistical robustness of healthcare technology is proposed. In order to conduct a test, we used a state-of-the-artAI model which we compared to the radiology model to make sure there were no biases. We achieved a quantifiable method for evaluating our modified Turing test results and also achieved some positive results in our study, which will help us in our future work. His test gives a standard for upcoming artificial intelligence procedures. The Turing test is a standard that helps to measure the performance of the AI model on a variety of edge cases and normal cases as well as help to detect if the algorithms is biased towards any one type of case. This method can be used to detect biases and classify biases.']

["Living reviews are an increasingly popular research paradigm. The purpose of a 'living' approach is to allow rapid collation, appraisal and synthesis of evolving evidence on an important research topic, enabling timely influence on patient care and public health policy. However, living reviews are time- and resource-intensive. The accumulation of new evidence and the possibility of developments within the review's research topic can introduce unique challenges into the living review workflow. To investigate the potential of software tools to support living systematic or rapid reviews, we present a narrative review informed by an examination of tools contained on the Systematic Review Toolbox website. We identified 11 tools with relevant functionalities and discuss the important features of these tools with respect to different steps of the living review workflow. Four tools (NestedKnowledge, SWIFT-ActiveScreener, DistillerSR, EPPI-Reviewer) covered multiple, successive steps of the review process, and the remaining tools addressed specific components of the workflow, including scoping and protocol formulation, reference retrieval, automated data extraction, write-up and dissemination of data. We identify several ways in which living reviews can be made more efficient and practical. Most of these focus on general workflow management, or automation through artificial intelligence and machine-learning, in the screening process. More sophisticated uses of automation mostly target living rapid reviews to increase the speed of production or evidence maps to broaden the scope of the map. We use a case study to highlight some of the barriers and challenges to incorporating tools into the living review workflow and processes. These include increased workload, the need for organisation, ensuring timely dissemination and challenges related to the development of bespoke automation tools to facilitate the review process. We describe how current end-user tools address these challenges, and which knowledge gaps remain that could be addressed by future tool development. Dedicated web presences for automatic dissemination of in-progress evidence updates, rather than solely relying on peer-reviewed journal publications, help to make the effort of a living evidence synthesis worthwhile. Despite offering basic living review functionalities, existing end-user tools could be further developed to be interoperable with other tools to support multiple workflow steps seamlessly, to address broader automatic evidence retrieval from a larger variety of sources, and to improve dissemination of evidence between review updates.", "Living reviews are becoming more popular in research. It's the purpose of a 'living' approach in order to allow rapid collation, appraisal and synthesis of evolving evidence on an important research topic, allowing timely influence on patient care and public health policy. Living reviews are very time consuming. New evidence and developments within the research topic can cause challenges in the living review. A narrative review of the tools available on the Systematic Review Toolbox website is presented to help investigate the use of software in living systematic or rapid reviews. We looked at different steps of the living review process and found some important features of the 11 tools we identified. The review process was encompassed by four tools, including NestedKnowledge, SWIFT-Active Screener, DistillerSR, and EPPI-Reviewer. We believe that living reviews can be made more efficient and practical. The screening process is the main focus of most of them. More complex uses of automation mainly target living rapid reviews to increase the speed of production or evidence maps to broaden their scope. There are some challenges and barriers to incorporating tools into the living review process. Increased workload, need for organisation, ensure timely dissemination, and challenges relating to the development of automation tools help to explain these. Current end-user tools address these challenges, and which knowledge gaps can be addressed by future tool development. Dedicated web presences for automatic dissemination of in-progress evidence updates, rather than merely relying on peer-reviewed journal publications, help to make the effort of a living evidence synthesis worthwhile. Existing end-user tools can be further developed to be  compatible with other tools to support multiple steps easily, to address broader automatic evidence retrieve from a larger variety of sources, and to improve dissemination of evidence between review updates."]

['The aim of this study was to investigate the effects of plaque-related factors on the diagnostic performance of an artificial intelligence coronary-assisted diagnosis system (AI-CADS). Patients who underwent coronary computed tomography angiography (CCTA) and invasive coronary angiography (ICA) were retrospectively included in this study. The degree of stenosis in each vessel was collected from CCTA and ICA, and the information on plaque-related factors (plaque length, plaque type, and coronary artery calcium score (CAC)) of the vessels with plaques was collected from CCTA. In total, 1224 vessels in 306 patients (166 men; 65.7 ± 10.1 years) were analyzed. Of these, 391 vessels in 249 patients showed significant stenosis using ICA as the gold standard. Using per-vessel as the unit, the area under the curves of coronary stenosis ≥ 50% for AI-CADS, doctor, and AI-CADS + doctor was 0.764, 0.837, and 0.853, respectively. The accuracies in interpreting the degree of coronary stenosis were 56.0%, 68.1%, and 71.2%, respectively. Seven hundred fifty vessels showed plaques on CCTA; plaque type did not affect the interpretation results by AI-CADS (chi-square test: p = 0.0093; multiple logistic regression: p = 0.4937). However, the interpretation results for plaque length (chi-square test: p < 0.0001; multiple logistic regression: p = 0.0061) and CACs (chi-square test: p < 0.0001; multiple logistic regression: p = 0.0001) were significantly different. AI-CADS has an ability to distinguish ≥ 50% coronary stenosis, but additional manual interpretation based on AI-CADS is necessary. The plaque length and CACs will affect the diagnostic performance of AI-CADS. • AI-CADS can help radiologists quickly assess CCTA and improve diagnostic confidence. • Additional manual interpretation on the basis of AI-CADS is necessary. • The plaque length and CACs will affect the diagnostic performance of AI-CADS.', 'The aim of the study was to investigate the effect of plaque-related factors on the performance of an artificial intelligence Coronary-Assisted Diagnosis System. The patients who underwentCTA andICA were excluded from the study. The degree of stenosis in each vessel was collected from C CTA and in the same way, the information on plaque-related factors (plaque length, plaque type, and coronary calcium score of the vessels with plaques were collected from CCTA) was collected from CCTA. There were 1224) vessels in the patients, of which 133 were analysed. 391 vessels in 258 patients have major stenosis using ICA, the gold standard. The area under the curves of coronary stents was calculated using per-vessel as a unit. The accuracies in interpreting the coronary stenosis degree were high. The plaques type did not affect the interpretation results of the CCTA. The results of the chi-square test for plaque length and multiple logistic regression were not the same. Additional manual interpretation is needed based on the ability of AI- CADS to distinguish 50%. The long plaque and short CACs will affect the performance. CCTA can be quickly assessed by the use of artificial intelligence. Additional manual interpretation is necessary on a case-by-case basis. Diagnostic performance of AI-cads will be affected by the length of plaque and the amount of CACs.']

["We have recently tested an automated machine-learning algorithm that quantifies left ventricular (LV) ejection fraction (EF) from guidelines-recommended apical views. However, in the point-of-care (POC) setting, apical 2-chamber views are often difficult to obtain, limiting the usefulness of this approach. Since most POC physicians often rely on visual assessment of apical 4-chamber and parasternal long-axis views, our algorithm was adapted to use either one of these 3 views or any combination. This study aimed to (1) test the accuracy of these automated estimates; (2) determine whether they could be used to accurately classify LV function. Reference EF was obtained using conventional biplane measurements by experienced echocardiographers. In protocol 1, we used echocardiographic images from 166 clinical examinations. Both automated and reference EF values were used to categorize LV function as hyperdynamic (EF>73%), normal (53%-73%), mildly-to-moderately (30%-52%), or severely reduced (<30%). Additionally, LV function was visually estimated for each view by 10 experienced physicians. Accuracy of the detection of reduced LV function (EF<53%) by the automated classification and physicians' interpretation was assessed against the reference classification. In protocol 2, we tested the new machine-learning algorithm in the POC setting on images acquired by nurses using a portable imaging system. Protocol 1: the agreement with the reference EF values was good (intraclass correlation, 0.86-0.95), with biases <2%. Machine-learning classification of LV function showed similar accuracy to that by physicians in most views, with only 10% to 15% cases where it was less accurate. Protocol 2: the agreement with the reference values was excellent (intraclass correlation=0.84) with a minimal bias of 2.5±6.4%. The new machine-learning algorithm allows accurate automated evaluation of LV function from echocardiographic views commonly used in the POC setting. This approach will enable more POC personnel to accurately assess LV function.", 'We recently tested an automated machine-learning program that quantifies certain parameters from the guidelines-recommended apical views. In the point-of-care setting, an apical 2-chamber views are difficult to obtain, limiting the usefulness of this approach. Since visual assessment of apical 4-and parasternal long- axis views is a common way of measuring PoC physicians, we had adapted our algorithm to use either one or both of these views. The purpose of the study was to test the accuracy of automated estimates and to find out if they could be used to classify function. echocardiographers obtained the reference EF from conventional biplane measures. We used echocardiographic images from 166 clinical examinations in the first protocol. The automated and reference values were used to determine whether the function was hyperdynamic, normal, mildly-to-moderately, or severely reduced. 10 experienced physicians estimated the function of the LV in each view. The accuracy of detecting reducedLV function is assessed against the reference classification. The new machine- learning algorithm was tested in the setting of the POC to see how it would behave. The agreement with the reference values was good in Protocol 1. The machine-learning classification of LV function was almost the same as it had been by physicians in most views. The agreement with the values was excellent and there was a minimal bias. The new machine-learning model is able to accurately evaluate LV function from different echocardiographic views. More personnel will be able to assess LV function.']

['The research evaluated the opinion of those in charge of the administrative management of the logistics and supply chain of medical and pharmaceutical stocks of health care centers in the north of Chile and a potential improvement of their operations through the use of artificial intelligence (AI). The identification of the problem arose from the empirical analysis, where serious deficiencies in the manual handling and management of the stock of medicines and hospital supplies were evidenced. This deficiency does not allow a timely response to the demand of the logistics and supply chain, causing stock ruptures in health centers. Based on this finding, we asked ourselves how AI was observed as the most efficient tool to solve this difficulty. The results were obtained through surveys of personnel in charge of hospital and pharmacy supplies. The questions focused on the level of training, seniority in positions related to the problem, knowledge of regulations, degree of innovation in the procedures used in logistics and supply chain and procurement. However, a very striking fact was related to the importance of the use of AI, where, very surprisingly, 64.7% considered that it would not help to reduce human errors generated in the areas analyzed.', 'The administrative management of the logistics and supply chain of medical and pharmaceutical stock in Health care centers in the north of chile was evaluated by the research, and a potential improvement of operations through artificial intelligence was assessed. The empirical analysis showed that serious deficiencies in the manual handling and management of the stock of medicines and hospital supplies were present. There is a lack of availability of stock in Health Centers due to lack of availability of logistics and supply chain. We asked how artificial intelligence was able to solve the difficulty. The results were obtained from surveys of hospital employees. The degree of innovation, knowledge of regulations, level of training and seniority were some of the questions. A very striking fact was related to the fact that more than half of the people thought that using artificial intelligence would not help to reduce human errors.']

['Voice-based personal assistants using artificial intelligence (AI) have been widely adopted and used in home-based settings. Their success has created considerable interest for its use in healthcare applications; one area of prolific growth in AI is that of voice-based virtual counselors for mental health and well-being. However, in spite of its promise, building realistic virtual counselors to achieve higher-order maturity levels beyond task-based interactions presents considerable conceptual and pragmatic challenges. We describe one such conceptual challenge-cognitive plausibility, defined as the ability of virtual counselors to emulate the human cognitive system by simulating how a skill or function is accomplished. An important cognitive plausibility consideration for voice-based agents is its ability to engage in meaningful and seamless interactive communication. Drawing on a broad interdisciplinary research literature and based on our experiences with developing two voice-based (voice-only) prototypes that are in the early phases of testing, we articulate two conceptual considerations for their design and use-conceptualizing voice-based virtual counselors as communicative agents and establishing virtual co-presence. We discuss why these conceptual considerations are important and how it can lead to the development of voice-based counselors for real-world use.', 'Home-based settings haveWidespread adoption and use of voice-based personal assistants using artificial intelligence. Their success has created interest for its use in healthcare applications and that is one of the factors behind the growth of voice-based virtual counselors. Building realistic virtual counselors to get to higher-order maturity levels beyond task-based interactions presents considerable conceptual and pragmatic challenges. We describe how virtual counselors can emulate the human cognitive system by replicating a skill or function. An important cognitive plausibility consideration is its ability to engage in seamless andinteractive communication. Drawing on the research literature and our experiences with developing two voice-based prototypes that are in early stages of testing, we articulate two conceptual interests for their design and use. We talked about why conceptual considerations are important and how they can lead to the development of voice-based counselors for real-world use.']

['<b>Background:</b> Multiplex tissue analysis has revolutionized our understanding of the tumor microenvironment (TME) with implications for biomarker development and diagnostic testing. Multiplex labeling is used for specific clinical situations, but there remain barriers to expanded use in anatomic pathology practice. <b>Methods:</b> We review immunohistochemistry (IHC) and related assays used to localize molecules in tissues, with reference to United States regulatory and practice landscapes. We review multiplex methods and strategies used in clinical diagnosis and in research, particularly in immuno-oncology. Within the framework of assay design and testing phases, we examine the suitability of multiplex immunofluorescence (mIF) for clinical diagnostic workflows, considering its advantages and challenges to implementation. <b>Results:</b> Multiplex labeling is poised to radically transform pathologic diagnosis because it can answer questions about tissue-level biology and single-cell phenotypes that cannot be addressed with traditional IHC biomarker panels. Widespread implementation will require improved detection chemistry, illustrated by InSituPlex technology (Ultivue, Inc., Cambridge, MA) that allows coregistration of hematoxylin and eosin (H&E) and mIF images, greater standardization and interoperability of workflow and data pipelines to facilitate consistent interpretation by pathologists, and integration of multichannel images into digital pathology whole slide imaging (WSI) systems, including interpretation aided by artificial intelligence (AI). Adoption will also be facilitated by evidence that justifies incorporation into clinical practice, an ability to navigate regulatory pathways, and adequate health care budgets and reimbursement. We expand the brightfield WSI system "pixel pathway" concept to multiplex workflows, suggesting that adoption might be accelerated by data standardization centered on cell phenotypes defined by coexpression of multiple molecules. <b>Conclusion:</b> Multiplex labeling has the potential to complement next generation sequencing in cancer diagnosis by allowing pathologists to visualize and understand every cell in a tissue biopsy slide. Until mIF reagents, digital pathology systems including fluorescence scanners, and data pipelines are standardized, we propose that diagnostic labs will play a crucial role in driving adoption of multiplex tissue diagnostics by using retrospective data from tissue collections as a foundation for laboratory-developed test (LDT) implementation and use in prospective trials as companion diagnostics (CDx).', 'Multiplex tissue analysis is changing our understanding of the tumor microenvironment. Multiplex labeling can be used for certain scenarios, but it still requires more work in an anatomic pathology practice. We looked at the aspects of Ihc and related tests that are used in the U.S. We review strategies used in both research and clinical diagnoses. We look at the advantages and challenges of using the mIFF in a clinical diagnostic system within the framework of the design and testing phases. Multiplex labeling is poised to transform the diagnosis of diseases because it can answer questions about tissue-level biology that can\'t be addressed with traditional IHCS panels. Widespread implementation will require improved detection chemistry, illustrated by InSituPlex technology. There\'s more standardization and interoperability of data resources and the integration of digital pathology images into a whole slide. Adoption will also be assisted by evidence that supports inclusion into clinical practice, an ability to navigate regulatory pathways, and adequate health care budgets and reimbursement. The "pixel pathway" idea has been expanded to include more than one way to work with samples, suggesting that adoption can be accelerated by data standardization. Multiplex labeling can be used to complement next generation Sequencing in Cancer Diagnosis, by allowing Pathologists to visualize and understand every cell in a tissue biopsies slide. Diagnostic labs will need to be a key part in driving the adoption of multiplex diagnostics if retrospective data from tissue collections is used for the purpose of test implementation.']

['The rise of emerging technologies such as Big Data, the Internet of Things, and artificial intelligence, which requires efficient power schemes, is driving brainstorming in data computing and storage technologies. In this study, merely relying on the fundamental structure of two memristors and a resistor, arbitrary Boolean logic can be reconfigured and calculated in two steps, while no additional voltage sources are needed beyond "±V<sub>P</sub> " and 0, and all state reversals are based on memristor set switching. Utilizing the proposed logic scheme in an elegant form of unity structure and minimum cost, the implementation of a 1-bit adder is demonstrated economically, and a promising circuit scheme for the N-bit adder is exhibited. Some critical issues including the crosstalk problem, energy consumption, and peripheral circuits are further simulated and discussed. Compared with existing works on memristive logic, such methods support building a memristor-based digital in-memory calculation system with high functional reconfigurability, simple voltage sources, and low power and area consumption.', 'Big Data, the Internet of Things, and artificial intelligence, which requires efficient power schemes, is driving brainstorming in data computing and storage technologies. Absent the fundamental structure of two memristors and aresistive the logic can be calculated in two steps and without the need for any additional voltages. The implementation of a 1-bit adder is demonstrated economically and a circuit scheme for the N-bit adder is displayed Critical issues including the crosstalk problem, energy consumption, and peripheral circuits are discussed Compared with existing works on memristive logic, such approaches support building a memristor-based digital in-memory calculation system with high functioning, easy to use, and low power and area consumption.']

['Cytology-based triaging is commonly used to manage the care of women with positive human papillomavirus (HPV) results, but it suffers from subjectivity and a lack of sensitivity and reproducibility. The diagnostic performance of an artificial intelligence-enabled liquid-based cytology (AI-LBC) triage approach remains unclear. Here, we compared the clinical performance of AI-LBC, human cytologists and HPV16/18 genotyping at triaging HPV-positive women. HPV-positive women were triaged using AI-LBC, human cytologists and HPV16/18 genotyping. Histologically confirmed cervical intraepithelial neoplasia grade 2/3 or higher (CIN2+/CIN3+) were accepted as thresholds for clinical performance assessments. Of the 3514 women included, 13.9% (n = 489) were HPV-positive. The sensitivity of AI-LBC was comparable to that of cytologists (86.49% vs 83.78%, P = 0.744) but substantially higher than HPV16/18 typing at detecting CIN2+ (86.49% vs 54.05%, P = 0.002). While the specificity of AI-LBC was significantly lower than HPV16/18 typing (51.33% vs 87.17%, P < 0.001), it was significantly higher than cytologists at detecting CIN2+ (51.33% vs 40.93%, P < 0.001). AI-LBC reduced referrals to colposcopy by approximately 10%, compared with cytologists (51.53% vs 60.94%, P = 0.003). Similar patterns were also observed for CIN3+. AI-LBC has equivalent sensitivity and higher specificity compared with cytologists, with more efficient colposcopy referrals for HPV-positive women. AI-LBC could be particularly useful in regions where experienced cytologists are few in number. Further investigations are needed to determine triaging performance through prospective designs.', 'It is common for the care of women with positive human pylommision (HPV) results to be rendered by genology-based Triaging, however it suffers from subjectivity and a lack of sensitivity and reproducibility. There is no clear performance of the Artificial Intelligence-enabled liquid-based cytology approach. We compared the clinical performance of all three. In order to determine if the woman was positive for the virus, she used the following: Ai-LBC, human cytologists and genotyping. The thresholds for clinical performance assessments were based on histologically confirmed cases of neoplasia grade 1/3 or higher. 38.2% of the women were positive for the human immunodeficiency virus. The sensitivity of the conjugate was comparable to that of a cytologist, but significantly better than the conjugate typing of a vaccine. The specificity of the program was lower than the others, but it was higher than the others at detecting CIN2+. The referrals to colposcopic surgeries were reduced by 10%, compared with the doctors. Similar patterns were also observed. The more efficient colpscopy referrals forHPV positive women is due to the higher specificity and sensitivity of the AI-LBC. In regions with a lot of experienced cytologists, the use of AI-LBC could be useful. Potential designs are necessary to determine triaging performance.']

['Cognitive consequences at school age associated with prenatal methylmercury (MeHg) exposure may need to take into account nutritional and sociodemographic cofactors as well as relevant genetic polymorphisms. A subsample (n = 1,311) of the Avon Longitudinal Study of Parents and Children (Bristol, UK) was selected, and mercury (Hg) concentrations were measured in freeze-dried umbilical cord tissue as a measure of MeHg exposure. A total of 1135 children had available data on 247 single-nucleotide polymorphisms (SNPs) within relevant genes, as well as the Wechsler Intelligence Scale for Children Intelligence Quotient (IQ) scores at age 8 years. Multivariate regression models were used to assess the associations between MeHg exposure and IQ and to determine possible gene-environment interactions. Hg concentrations indicated low background exposures (mean = 26 ng/g, standard deviation = 13). Log10-transformed Hg was positively associated with IQ, which attenuated after adjustment for nutritional and sociodemographic cofactors. In stratified analyses, a reverse association was found in higher social class families (for performance IQ, P value for interaction = 0.0013) among whom there was a wider range of MeHg exposure. Among 40 SNPs showing nominally significant main effects, MeHg interactions were detected for rs662 (paraoxonase 1) and rs1042838 (progesterone receptor) (P < 0.05) and for rs3811647 (transferrin) and rs2049046 (brain-derived neurotrophic factor) (P < 0.10). In this population with a low level of MeHg exposure, there were only equivocal associations between MeHg exposure and adverse neuropsychological outcomes. Heterogeneities in several relevant genes suggest possible genetic predisposition to MeHg neurotoxicity in a substantial proportion of the population. Future studies need to address this possibility.', 'Cognitive consequences associated with MeHg exposure as a child need to take into account genetic factors and cofactors. The mercury concentration in freeze-dried umbilical cord tissue was used to measure MeHg exposure, because a sub sample was selected for the study. There were a total of 1135 children who had data on 247 single-nucleotide polymorphisms within their genes. The associations between MeHg exposure and IQ and possible interaction between genes were investigated with multi-variable regression models. The mean of the Hg concentrations was 26 and the standard deviation was 13. The positive association of log10-transformed Hg with IQ diminished after adjustment. Higher social class families who had a wider range of MeHg exposure were found to be linked with a reverse association for performance IQ. For rs662, rs1042838, and rs3811648, MeHg interactions were found, all showing nominally significant main effects. There were no observable correlations between MeHg exposure and adverse neuroscience outcomes in this population. A large portion of the population might have a genetic susceptibility to meHg neurodegenerative disease. Future studies need to address the possibility.']

['Sensory feedback from wearables can be effective to learn better movement through enhanced information and engagement. Facilitating greater user cognition during movement practice is critical to accelerate gains in motor function during rehabilitation following brain or spinal cord trauma. This preliminary study presents an approach using an instrumented glove to leverage sense of agency, or perception of control, to provide training feedback for functional grasp. Seventeen able-bodied subjects underwent training and testing with a custom-built sensor glove prototype from our laboratory. The glove utilizes onboard force and flex sensors to provide inputs to an artificial neural network that predicts achievement of "secure" grasp. Onboard visual and audio feedback was provided during training with progressively shorter time delay to induce greater agency by intentional binding, or perceived compression in time between an action (grasp) and sensory consequence (feedback). After training, subjects demonstrated a significant reduction (<i>p</i> < 0.05) in movement pathlength and completion time for a functional task involving grasp-move-place of a small object. Future work will include a model-based algorithm to compute secure grasp, virtual reality immersion, and testing with clinical populations.', 'Wearables give sensory feedback to help you learn a new way to move. It is important to speed up gains in motor function during rehabilitation after brain or spine cord trauma. An instrumented glove is an approach used to give training feedback for functional grasp. A custom-built sensor glove prototype was used during training and testing for 17 able-bodied subjects. The glove uses onboard force and flex sensors to make predictions about the "secure" grasp. Training with time delays for visual and audio feedback was provided to induce greater agency by intentional binding or perceived compression between actions. The reduction in movement path length and completion time was shown by the subjects after training. Future work will include a model-based system to compute secure grasp, virtual reality immersion and testing with clinical populations.']

['To identify and contrast risk factors for six-month pediatric asthma readmissions using traditional models (Cox proportional-hazards and logistic regression) and artificial neural-network modeling. This retrospective cohort study of the 2013 Nationwide Readmissions Database included children 5 to 18 years old with a primary diagnosis of asthma. The primary outcome was time to asthma readmission in the Cox model, and readmission within 180 days in logistic regression. A basic neural network construction with 2 hidden layers and multiple replications considered all dataset variables and potential variable interactions to predict 180-day readmissions. Logistic regression and neural-network models were compared on area-under-the receiver-operating curve. Of 18,489 pediatric asthma hospitalizations, 1858 were readmitted within 180 days. In Cox and logistic models, longer index length of stay, public insurance, and nonwinter index admission seasons were associated with readmission risk, whereas micropolitan county was protective. In neural-network modeling, 9 factors were significantly associated with readmissions. Four overlapped with the Cox model (nonwinter-month admission, long length of stay, public insurance, and micropolitan hospitals), whereas 5 were unique (age, hospital bed number, teaching-hospital status, weekend index admission, and complex chronic conditions). The area under the curve was 0.592 for logistic regression and 0.637 for the neural network. Different methods can produce different readmission models. Relying on traditional modeling alone overlooks key readmission risk factors and complex factor interactions identified by neural networks.', "Traditional models and artificial neural-network modeling are being used to identify risk factors for asthma. There were children 5 to 18 years old with a primary diagnosis of asthma included in the retrospective cohort study. The Cox model led to asthma being readmitted within 180 days. A basic neural network construction consisting of two hidden layers and multiple replications was used to predict 180-day readmissions. Logistic regression and neural-network models were compared. Over 18,700 hospitalizations for asthma were readmitted within 180 days. Micropolitan county was protective compared to Cox models because it had longer index lengths of stay, as well as public insurance and non winter index admission seasons. 9 factors were associated with readmissions, in neural-network modeling. The four that didn't overlap were: non-winter-month admission, long length of stay, public insurance, and micropolitan hospitals. Logistic regression area was under the curve with a 0.637 area. Different methods can produce different models. Traditional modeling alone ignores key risks and complex interactions identified by neural networks."]

['Adequate privacy protection is crucial for implementing modern AI algorithms in medicine. With Fully Homomorphic Encryption (FHE), a party without access to the secret key can perform calculations and advanced analytics on encrypted data without taking part of either the input data or the results. FHE can therefore work as an enabler for situations where computations are carried out by parties that are denied plain text access to sensitive data. It is a scenario often found with digital services that process personal health-related data or medical data originating from a healthcare provider, for example, when the service is delivered by a third-party service provider located in the cloud. There are practical challenges to be aware of when working with FHE. The current work aims to improve accessibility and reduce barriers to entry by providing code examples and recommendations to aid developers working with health data in developing FHE-based applications. HEIDA is available on the GitHub repository: https://github.com/rickardbrannvall/HEIDA.', 'Privacy protection is very important in relation to modern medicine. Without access to the secret key a party can do calculations and perform advanced analytics on the data without taking part of the data or the results. In the situation where plain text access is not given, FHE can be used to help carry out computations by parties who are denied access to sensitive data. The scenario is often found where a third-party service provider is located in the cloud and is tasked with delivering the service to the public. Practical challenges to be aware of while working with FHE. The current work is looking for ways to improve accessibility and reduce Barriers to Entry by providing code examples and recommendations for developers working with health data in developing FHE-based applications. HeIDA is available in a repository. Rickard Brann, who also goes by the title Heida, can be found at: com/rickard Brann.']

['To develop a deep learning model, with the aid of ChatGPT, for thyroid nodules, utilizing ultrasound images. The cytopathology of the fine needle aspiration biopsy (FNAB) serves as the baseline. After securing IRB approval, a retrospective study was conducted, analyzing thyroid ultrasound images and FNAB results from 1,061 patients between January 2017 and January 2022. Detailed examinations of their demographic profiles, imaging characteristics, and cytological features were conducted. The images were used for training a deep learning model to identify various thyroid pathologies. ChatGPT assisted in developing this model by aiding in code writing, preprocessing, model optimization, and troubleshooting. The model demonstrated an accuracy of 0.81 on the testing set, within a 95% confidence interval of 0.76 to 0.87. It presented remarkable results across thyroid subgroups, particularly in the benign category, with high precision (0.78) and recall (0.96), yielding a balanced F1-score of 0.86. The malignant category also displayed high precision (0.82) and recall (0.92), with an F1-score of 0.87. The study demonstrates the potential of artificial intelligence, particularly ChatGPT, in aiding the creation of robust deep learning models for medical image analysis.', 'To create a model using a deep learning method. The baseline is served by the pathology of the fine needle aspiration biopsy. The retrospective study analyzed 1,061 patients from January to January 2022 and analyzed their demographic profiles. The model was trained to identify anomalies in the thyroid. Code writing, preprocessing and model optimization were helped by the help of CHATG PPT. The model displayed remarkable results, with a balanced F1- score of 0.8, across all the subgroup, as well as high precision and recall.']

['The conventional detection of exogenous drugs in equine doping samples has been used for confirmation and subsequent prosecution of participants responsible. In recent years, alternative methods using indirect detection have been investigated due to the expanding number of pharmaceutical agents available with the potential of misuse. The monitoring of endogenous biomarkers such as hydrocortisone (HC) has been studied in equine urine with an international threshold of 1 μg/ml established; however, there is no current threshold for equine plasma. The aim of this research was to investigate plasma concentrations of HC and cortisone (C) in race day samples compared to an administration of Triamcinolone Acetonide (TACA). The reference population (n = 1150) provided HC (6 to 145 ng/ml) and C (0.7 to 13 ng/ml) levels to derive the HC to C ratio (HC/C). Population reference limits (PRLs) were proposed for HC/C values at 0.2 (lower) and 61 (upper). Administration of TACA resulted in down-regulation of HC/C values below the estimated PRLs for up to 96 h post-administration. This indirect detection period was longer than the detection of TACA for 72 h. The use of individual reference limits (IRLs) for HC/C values was investigated to support the Equine Biological Passport (EBP), an intelligence model developed by Racing NSW for longitudinal monitoring of biomarkers.', 'The prosecution of participants using the conventional detection of drugs in equine sample has been done. In recent years, alternative methods have been investigated due to the fact that there are more pharmaceutical agents available that can be used for misuse. The international threshold for hydrocortisone has not been established, but there is a threshold for equine plasma. The aim of the research was to see how concentrations of the two drugs were compared to one another. The reference population provided the HC to C ratio. Population reference limits were proposed for values of 0.2 and 61. The down-regulation of the values below the estimated PRLs was caused by the administration of TACA. The indirect detection period was longer than the 74 h detection of TACA. Racing New South Wales developed a horse biological passport to support the use of individual reference limits.']

['Very preterm children often have difficulties in behavioral functioning, but there is large heterogeneity in the severity of these difficulties and in the combination of the difficulties observed. Few studies so far addressed this heterogeneity by examining whether more homogeneous subtypes of behavioral functioning can be identified. To identify behavioral subtypes in a group of very preterm children, examine whether such subtypes are related to neonatal medical complications and/or parental education level (to better understand origins) and to examine whether such subtypes are associated with IQ and neurocognitive deficits in attention and executive function (to study underlying mechanisms of dysfunction). Cross-sectional cohort study. 135 very preterm (gestational age < 30 weeks and/or birthweight < 1000 g) children aged 8-12 years. Parent and teacher questionnaires covering a broad range of behavioral domains, parental education level, neonatal medical complications, short-form Wechsler Intelligence Scale for Children-III and performance-based attention and executive function measures. Cluster analysis indicated two behavioral subtypes: a subtype characterized by low behavioral problems (76% of children) and a subtype characterized by high behavioral problems across behavioral domains (24% of children). Lower parental education level, lower IQ and poorer verbal working memory, visuospatial working memory and inhibition were associated with the high problems subtype, but neonatal medical complications were not. The majority of very preterm children was assigned to the low behavioral problems subtype. However, if problems do occur, they are wide-spread across behavioral domains and accompanied by problems in neurocognitive domains.', "There's large heterogeneity in the severity of the difficulties and there's also a combo of the difficulties observed in very preterm children. There are few studies currently examining heterogeneity by looking at whether there are more compatible subtypes of behavioral functioning. To identify behavioral subtypes in very premature children, take into account whether the subtypes are related to the medical issues of the child and the education level of the child's parents. A cross-sectional cohort study was done. The children are either very premature (born before 30 weeks and weigh less than 1000 g) or very premature (12 years of age). The parent and teacher questionnaires include a broad range of behavioral, educational and health topics. A subgroup with 70% of people having low behavioral problems is one that the analysis indicated was a subtypes with high behavioral problems. Maternal education level, lower IQ and less verbal working memory were associated with high problems, but the children's medical outcomes were not affected. Most very premature children were assigned the low behavioral problems. Problems can occur across a wide range of behavioral and cognitive areas."]

['In mass casualty incidents (MCI), the number of casualties can far exceed the capacity of medical emergency units to treat and transport in a very short period of time. A rapid MCI triage according to the severity of their injuries, can not only effectively use limited medical resources, but also improve the survival rate of injured patients. With the emergence of artificial intelligence (AI) and augmented reality (AR), smart glasses have been developed and used in different scenarios, and have achieved remarkable results in the medical field. This article focuses on the role and advantages of smart glasses in the triage of MCI, while proposing the problems in the application of smart glasses. At the same time, we elaborate on the development status of smart glasses in the triage, and discuss the application trend and development direction of smart glasses in the triage of pre-hospital injuries.', 'In a mass casualty incident, the number of casualties can exceed the capacities of medical emergency units to treat and provide transportation in a short time. The survival rate for injured patients increases with the use of a rapid MCI. Smart glasses have developed and are used in different scenarios, achieving remarkable results in the medical field with the emergence of artificial intelligence and augmented reality. This article addresses the role and advantages of smart glasses while also looking at the problems that need to be solved with smart glasses. We discuss the development of smart glasses in the triage, as well as the application trend and direction of smart glasses, in addition to the development of smart glasses in the pre-hospital injuries.']

["To study the performance of artificial intelligence (AI) for detecting pleural pathology on chest radiographs (CXRs) using computed tomography as ground truth. Retrospective study of subjects undergoing CXR in various clinical settings. Computed tomography obtained within 24 hours of the CXR was used to volumetrically quantify pleural effusions (PEfs) and pneumothoraxes (Ptxs). CXR was evaluated by AI software (INSIGHT CXR; Lunit) and by 3 second-year radiology residents, followed by AI-assisted reassessment after a 3-month washout period. We used the area under the receiver operating characteristics curve (AUROC) to assess AI versus residents' performance and mixed-model analyses to investigate differences in reading time and interreader concordance. There were 96 control subjects, 165 with PEf, and 101 with Ptx. AI-AUROC was noninferior to aggregate resident-AUROC for PEf (0.82 vs 0.86, P < 0.001) and Ptx (0.80 vs 0.84, P = 0.001) detection. AI-assisted resident-AUROC was higher but not significantly different from the baseline. AI-assisted reading time was reduced by 49% (157 vs 80 s per case, P = 0.009), and Fleiss kappa for Ptx detection increased from 0.70 to 0.78 (P = 0.003). AI decreased detection error for PEf (odds ratio = 0.74, P = 0.024) and Ptx (odds ratio = 0.39, P < 0.001). Current AI technology for the detection of PEf and Ptx on CXR was noninferior to second-year resident performance and could help decrease reading time and detection error.", 'The use of computed tomography as a ground truth to study the performance of artificial intelligence in detecting lung pathology on chest radiographs. The retrospective is about subjects that have undergone CXR in clinical settings. Within 24 hours of the CXR, computed tomography had been used to quantify the effusions. After a 3-month washout period, the 3 second-yearradiology residents had their scans analyzed by the Artificial Intelligence software. We used the area under the receiver operating characteristics curve to assess the artificial intelligence (ai) of people and to investigate differences in reading time. There were 96 control subjects and 165 PEf subjects. Aggregate resident-AUROC was noninferior to artificial intelligence-AUROC for PEf and Ptx. The baseline was not vastly different from anai-assisted resident-AUROC. Fleiss for kappa Ptx detection increased from 0.70 to 0.78 thanks to the reduction in the read time and the use of the artificial intelligence system. The detection error was decreased for PEf and Ptx. The current technology for the detection of PEf and Ptx was not good for the second- year resident performance.']

['In 1934, senior registrar Augusta Rasmussen (1895–1979) published a study of 77 cases involving sexual offences. She found that the women involved had suffered no mental injury from the abuse. In 1947, she published a study of the intelligence level of 310 Norwegian women who had formed relationships with German soldiers during the occupation. She found that nearly all of them were more or less retarded. Her conclusions, however, were not scientifically valid. Here we present Rasmussen’s biography, academic background and scientific activity.', "In 1934, Augusta Rasmussen worked with 77 cases of sexual crimes. She found that the women did not suffer a mental injury. The intelligence level of Norwegian women who had relationships with Germans during the occupation was studied. She found that most of them were retarded. Her conclusions weren't valid. Here we present some information on the person, including their biography, academic background, and scientific activity."]

['Artificial intelligence is useful for building objective and rapid personal identification systems. It is important to research and develop personal identification methods as social and institutional infrastructure. A critical consideration during the coronavirus disease 2019 pandemic is that there is no contact between the subjects and personal identification systems. The aim of this study was to organize the recent 5-year development of contactless personal identification methods that use artificial intelligence. This study used a scoping review approach to map the progression of contactless personal identification systems using artificial intelligence over the past 5 years. An electronic systematic literature search was conducted using the PubMed, Web of Science, Cochrane Library, CINAHL, and IEEE Xplore databases. Studies published between January 2016 and December 2020 were included in the study. By performing an electronic literature search, 83 articles were extracted. Based on the PRISMA flow diagram, 8 eligible articles were included in this study. These eligible articles were divided based on the analysis targets as follows: (1) face and/or body, (2) eye, and (3) forearm and/or hand. Artificial intelligence, including convolutional neural networks, contributed to the progress of research on contactless personal identification methods. This study clarified that contactless personal identification methods using artificial intelligence have progressed and that they have used information obtained from the face and/or body, eyes, and forearm and/or hand.', 'Artificial intelligence is useful for building identification systems. Personal identification methods should be research and developed to be used in social and institutional infrastructure. During the coronaviruses disease outbreak, there is no contact between the subjects and identification systems. The goal of this study was to develop methods for Personal Identification that use Artificial Intelligence. This study was used to map out the progression of personal identification systems using artificial intelligence. An electronic systematic literature search can be conducted using several databases. The study included studies published between January 2016 and December 2020. Eighty-three articles were obtained by performing an electronic literature search. Eight eligible articles were included in the study, based on the flow diagram. The eligible articles have been divided according to the analysis targets of (1) face and/or body, (2) eye, and (3) forearm and/or hand. The advancement of research on personal identification methods is due to artificial intelligence and neural networks. Information obtained from the face and/or body, eyes, forearm and hand, is used by personal identification methods using artificial intelligence to progress, according to this study.']

['Lung cancer is the leading cause of cancer deaths among both men and women, representing approximately 25% of cancer fatalities each year. The treatment landscape for non-small cell lung cancer (NSCLC) is rapidly evolving due to the progress made in biomarker-driven targeted therapies. While advancements in targeted treatments have improved survival rates for NSCLC patients with actionable biomarkers, long-term survival remains low, with an overall 5-year relative survival rate below 20%. Artificial intelligence/machine learning (AI/ML) algorithms have shown promise in biomarker discovery, yet NSCLC-specific studies capturing the clinical challenges targeted and emerging patterns identified using AI/ML approaches are lacking. Here, we employed a text-mining approach and identified 215 studies that reported potential biomarkers of NSCLC using AI/ML algorithms. We catalogued these studies with respect to BEST (Biomarkers, EndpointS, and other Tools) biomarker sub-types and summarized emerging patterns and trends in AI/ML-driven NSCLC biomarker discovery. We anticipate that our comprehensive review will contribute to the current understanding of AI/ML advances in NSCLC biomarker research and provide an important catalogue that may facilitate clinical adoption of AI/ML-derived biomarkers.', "25% of cancer deaths each year are attributable to lung cancer, which accounts for the majority of deaths among both men and women. The treatment landscape for non-small cell lung cancer has changed rapidly due to the progress in targeted therapies. While the survival rate for patients with actionable markers has improved, the long-term survival rate is still low. It's not clear if the promise of machine learning in biomarker discovery is still present in the trials that capture clinical challenges and emerging patterns. There are 215 studies using the text-mining approach to pick out the potential NSCLC markers. We summarized emerging patterns and trends in the research into the discovery of the best biomarkers for lung cancer. The current understanding of artificial intelligence and machine learning in research on non-small stature lung cancer is expected with our review to contribute and provide an important catalogue that may support the adoption of artificial intelligence in clinical practice."]

['Machine learning has become a powerful tool for systems biologists, from diagnosing cancer to optimizing kinetic models and predicting the state, growth dynamics, or type of a cell. Potential predictions from complex biological data sets obtained by "omics" experiments seem endless, but are often not the main objective of biological research. Often we want to understand the molecular mechanisms of a disease to develop new therapies, or we need to justify a crucial decision that is derived from a prediction. In order to gain such knowledge from data, machine learning models need to be extended. A recent trend to achieve this is to design "interpretable" models. However, the notions around interpretability are sometimes ambiguous, and a universal recipe for building well-interpretable models is missing. With this work, we want to familiarize systems biologists with the concept of model interpretability in machine learning. We consider data sets, data preparation, machine learning methods, and software tools relevant to omics research in systems biology. Finally, we try to answer the question: "What is interpretability?" We introduce views from the interpretable machine learning community and propose a scheme for categorizing studies on omics data. We then apply these tools to review and categorize recent studies where predictive machine learning models have been constructed from non-sequential omics data.', "Machine leard has become a great tool for systems biologists, from detecting cancer to predicting the state of the cell. Potential predictions from complex biological data sets can be found, but they are often not the main objective of biological research. We need to explain a decision that is based on a prediction in order to know the mechanisms of a disease. Machine learning models need to be extended in order to gain knowledge. There is a recent trend of design 'interpretable' models. There is a universal recipe for building well-interpretable models that is missing. The aim of this work is to orient systems biologists to the concept of model interpretability in machine learning. We consider many different types of data sets, data preparation, machine learning methods and software tools. We try to answer the question of interpretation, introducing views from the machine learning community and proposing a scheme for categorizing studies on omics data. We use these tools to review studies where machine learning models have been constructed from non-sequential data."]

["This research explored how intelligence interviewees mentally identify the relevant information at their disposal. We theorized that interviewees estimate the interviewer's objectives based on how they frame any attempt to solicit information. Then interviewees organize the information they possess into item designations that pragmatically correspond to the perceived interviewer-objective. The more an interviewer specifies what they want to know, the more the interviewee will mentally designate information items corresponding with that objective. To examine the theory, we conducted two identical experiments wherein participants assumed the role of an informant with one of two dispositions. They were to be cooperative or resistant when undergoing an interview. The interviewer posed specific or ambiguous questions. In Study 1 (<i>N</i> = 210), interviewees identified applicable information items based on their interviewer's questions. And interviewees answered their interviewer's questions in Study 2 (<i>N</i> = 199). We aimed to demonstrate that question type influences mental designations and disposition affects disclosures. Disposition had a stronger influence on interviewees' disclosure than when reasoning about what the interviewer wants to know. But contrary to our expectations, mental designation preferences indicated that interviewees generally assume interviewers want to know complete details, irrespective of question specificity. We suggest avenues for future research.", "The research looked at intelligence interviews to see how they mentally identify the relevant information. We theorize that interviews estimate the interviewer's objectives by how they frame any attempt to solicit information. The information they have is arranged into items that correspond to their perceived interviewer objective. When interviewing a person, the more an interviewer specifies what they want to know. To examine the theory, we tested the theory in two identical experiments. They were to be willing to face an interview. Specific or ambiguous questions were posed by the interviewer. The interviewees identified items based on the questions they were asked. In Study 2, the questions were asked by their interviewer. We wanted to show how question type and mental designation affect disclosures. Disposition had more to do with the way that the interviewees tell their story than with what the interviewer wants to know. Our expectations were incorrect, because mental designation preferences indicated that interviewers wanted to know complete details, regardless of question specificity. avenues for future studies have been suggested by us."]

['Virtual monochromatic images (VMI) are increasingly used in clinical practice as they improve contrast-to-noise ratio. However, due to their different appearances, the performance of artificial intelligence (AI) trained on conventional CT images may worsen. The goal of this study was to assess the performance of an established AI algorithm trained on conventional polychromatic computed tomography (CT) images (CPI) to detect pulmonary embolism (PE) on VMI. Paired 60 kiloelectron volt (keV) VMI and CPI of 114 consecutive patients suspected of PE, obtained with a detector-based spectral CT scanner, were retrospectively analyzed by an established AI algorithm. The CT pulmonary angiography (CTPA) were classified as positive or negative for PE on a per-patient level. The reference standard was established using a comprehensive method that combined the evaluation of the attending radiologist and three experienced cardiothoracic radiologists aided by two different detection tools. Sensitivity, specificity, positive and negative predictive values and likelihood ratios of the algorithm on VMI and CPI were compared. The prevalence of PE according to the reference standard was 35.1% (40 patients). None of the diagnostic accuracy measures of the algorithm showed a significant difference between CPI and VMI. Sensitivity was 77.5% (95% confidence interval (CI) 64.6-90.4%) and 85.0% (73.9-96.1%) (p = 0.08) on CPI and VMI respectively and specificity 96.0% (91.4-100.0%) and 94.6% (89.4-99.7%) (p = 0.32). Diagnostic performance of the AI algorithm that was trained on CPI did not drop on VMI, which is reassuring for its use in clinical practice. A commercially available AI algorithm, trained on conventional polychromatic CTPA, could be safely used on virtual monochromatic images. This supports the sustainability of AI-aided detection of PE on CT despite ongoing technological advances in medical imaging, although monitoring in daily practice will remain important. • Diagnostic accuracy of an AI algorithm trained on conventional polychromatic images to detect PE did not drop on virtual monochromatic images. • Our results are reassuring as innovations in hardware and reconstruction in CT are continuing, whilst commercial AI algorithms that are trained on older generation data enter healthcare.', 'Virtual images improve the contrast-to noise ratio. Artificial intelligence may not do as well if it is trained on conventional image. The purpose of this study was to get to the bottom of the performance of an existing artificial intelligence tool to detect a lung disease on amography images. A set of 60 keVVMI and CRPI of 114 patients suspected of PE, obtained with a detector-basedCTScan, were retrospectively analyzed by an established artificial intelligence program. Positive or negative for PE was the classification for the computed pulmonary arteriosy. The reference standard was set by combining the evaluation of the attending physician and three cardiothoracic radiologists with two different detection tools. Positive and negative predictive values, sensitivity, specificity, and likelihood ratios were compared on the basis of their use. ThePrevalence of PE was found to be 35.1% by the reference standard. There was nothing to show a difference between the two diagnostic accuracy measures. The sensitivity was 77.5% and specificity 95.6% on the two measures. It is reassuring that the Diagnostic performance of the Artificial Intelligence system that was trained did not drop on VMI. A commercially available machine learning method could be used to create virtual images. Monitoring daily practice is very important and supports the sustainable future of detecting PE on CT with the help of artificial intelligence. There was no drop in diagnostic accuracy of the Artificial Intelligence program used to detect PE on virtual images. Our results are reassuring as new technology in reconstruction and hardware is continuing, as well as the commercial use of older generation data in healthcare.']

['Longstanding interest has been directed toward the etiology of sociopolitical attitudes. Personality traits have been posited as antecedents; however, most work addressing such links has been limited to cross-sectional study designs. The current study used data from two large (both Ns > 8,700), longitudinal cohorts of individuals from the United Kingdom who were parent-assessed on a measure of temperament (assessing anxiety, conduct problems, and hyperactivity) at age 5 or 7 years and on a range of sociopolitical attitudes at age 30 or 33 years. In both cohorts, higher levels of childhood conduct problems predicted higher levels of economic and political discontent in adulthood. These associations were still evident when controlling for sex, childhood intelligence, and parental social class. In both cohorts, this pathway was partially mediated by educational attainment and achieved social class. These findings are consistent with the perspective that early-life temperament gives rise to adult political sentiment.', 'Longstanding interest has focused on the causes of socio political attitudes. Most work addressing links between personality traits and other factors has consisted of cross-sectional study designs. The current study used data from two large longitudinal cohorts of individuals from the United Kingdom that had been assessed for temperament at age 5 or 7 years old. In both batches, higher levels of conduct problems in childhood predicted more unhappiness in adulthood. The association between sex, intelligence, and parental social class was still evident. The pathway was influenced by educational advancement and achieved social class. These findings are consistent with a view of early-life upbringings giving rise to adult political sentiment.']

["Chest pain accounts for approximately 6% of Emergency Department (ED) attendances and is the most common reason for emergency hospital admission. For many years, our approach to diagnosis has required patients to stay in hospital for at least 6-12 h to undergo serial biomarker testing. As less than one fifth of the patients undergoing investigation actually has an acute coronary syndrome (ACS), there is tremendous potential to reduce unnecessary hospital admissions. Recent advances in diagnostic technology have improved the efficiency of care pathways. Decision aids such as the Thrombolysis in Myocardial Infarction (TIMI) risk score and the History, Electrocardiogram, Age, Risk factors and Troponin (HEART) score enable rapid 'rule out' of ACS within hours of patients arriving in the ED. With high sensitivity cardiac troponin (hs-cTn) assays, approximately one third of patients can have ACS 'ruled out' with a single blood test, and up to two thirds could have an acute myocardial infarction 'ruled out' with a second sample taken after as little as 1 h. Building on those recent advances, this paper presents an overview of the principles behind the development of the Troponin-only Manchester Acute Coronary Syndromes (T-MACS) decision aid. This clinical prediction model could be used to 'rule out' and 'rule in' ACS following a single blood test and to calculate the probability of ACS for every patient. The future potential of this approach is then addressed, including practical applications of artificial intelligence, shared decision making, near-patient testing and personalized medicine.", "According to the Emergency Department's attendance figures, chest pain is the most common reason for ED attendances. Patients have been required to stay in the hospital for a minimum of 6 h for serial diagnosis testing for many years. According to researchers, there is tremendous potential to reduce hospital admissions if less than 5% of patients have an acute coronary syndrome. Diagnostic technology has led to improvement in the efficiency of care pathways. Within hours of patients arriving in the ED, the decision aids can allow rapid 'rule out' of Acute Cardiac Syndrome, such as the Thrombosis in Myocardial Infarction (TIMI) risk score, and the History, Electrocardiogram, Age, and Risk factors. Up to two thirds of patients can have an acute infarction considered out with a second sample taken and a single cardiac troponin (hs-cTn) test, if they have high sensitivity cardiac troponin (hs-cTn) tests. This paper details the principles involved in the development of the Troponin-only Manchester Acute Coronary Syndromes (T-MACS) decision aid. The model can be used to rule out the possibility of accidents after a single blood test, and to calculate the chance of accidents for every patient. Practical applications of artificial intelligence, shared decision making, near-patient testing and personalized medicine were addressed."]

['A multimodal deep-learning (MDL) framework is presented for predicting physical properties of a ten-dimensional acrylic polymer composite material by merging physical attributes and chemical data. The MDL model comprises four modules, including three generative deep-learning models for material structure characterization and a fourth model for property prediction. The approach handles an 18-dimensional complexity, with ten compositional inputs and eight property outputs, successfully predicting 913 680 property data points across 114 210 composition conditions. This level of complexity is unprecedented in computational materials science, particularly for materials with undefined structures. A framework is proposed to analyze the high-dimensional information space for inverse material design, demonstrating flexibility and adaptability to various materials and scales, provided sufficient data are available. This study advances future research on different materials and the development of more sophisticated models, drawing the authors closer to the ultimate goal of predicting all properties of all materials.', 'The framework for combining physical attributes and chemical data is an MDL. Three generative deep-learning models for material structure characterization and a fourth model for property prediction are included in the MDL model. The approach handles an 18-dimensional complex, with ten inputs and eight property outputs. Computational materials science is known for the high level of complexity. If enough data are available, the framework is proposed to analyze the high-dimensional information space for inverse material design, demonstrating flexibility and adaptability. The authors of the study will be closer to predicting the properties of all materials, as a result of the study advances future research on different materials and more sophisticated models.']

['This article provides an overview of the implications of ChatGPT and other large language models (LLMs) for dental medicine. ChatGPT, a LLM trained on massive amounts of textual data, is adept at fulfilling various language-related tasks. Despite its impressive capabilities, ChatGPT has serious limitations, such as occasionally giving incorrect answers, producing nonsensical content, and presenting misinformation as fact. Dental practitioners, assistants, and hygienists are not likely to be significantly impacted by LLMs. However, LLMs could affect the work of administrative personnel and the provision of dental telemedicine. LLMs offer potential for clinical decision support, text summarization, efficient writing, and multilingual communication. As more people seek health information from LLMs, it is crucial to safeguard against inaccurate, outdated, and biased responses to health-related queries. LLMs pose challenges for patient data confidentiality and cybersecurity that must be tackled. In dental education, LLMs present fewer challenges than in other academic fields. LLMs can enhance academic writing fluency, but acceptable usage boundaries in science need to be established. While LLMs such as ChatGPT may have various useful applications in dental medicine, they come with risks of malicious use and serious limitations, including the potential for misinformation. Along with the potential benefits of using LLMs as an additional tool in dental medicine, it is crucial to carefully consider the limitations and potential risks inherent in such artificial intelligence technologies.', "This article compares the implications of large language models for dentistry. The LLM in linguistics trained on massive amounts of text data is able to fulfill other language related tasks. Despite its impressive capabilities, it has serious limitations including giving incorrect answers, producing nonsensical content, and presenting misinformation as fact. The impact on dental practitioners is not likely to be significant. Administrative personnel and dental clinics may be affected by LLMs. It's possible for clinical decision supports, text summarization, and multilingual communication with LLMs. It is essential to safeguard against inaccurate and biased responses to health- related queries as more people seek health information from LLMs. Patient data confidentiality and cybersecurity face challenges. Compared to other academic fields, dental education has fewer challenges. There are appropriate usage boundaries in science that need to be established. While LLMs such as the one in the ChatGPST have various useful applications in dental medicine, they are also fraught with potential risks and serious limitations. While the benefits of using LLMs as an extra tool in dental medicine can be seen, they need to be considered carefully."]

["Autoinjectable devices continue to provide real-life benefits for patients with chronic conditions since their widespread adoption 30 years ago with the rise of macromolecules. Nonetheless, issues surrounding adherence, patient administration techniques, disease self-management, and data outcomes at scale persist despite product design innovation. The interface of drug device combination products and digital health technologies formulates a value proposition for next-generation autoinjectable devices to power the delivery of precision care at home and achieve the full potential of biologics. Success will largely be dependent on biopharma's digital health maturity to implement this framework. This viewpoint measures the digital health maturity of the top 15 biopharmaceutical companies in the US biologics autoinjector market and establishes the framework for next-generation autoinjectable devices powering home-based precision care and the need for formal digital health training.", "The rise of macromolecules has resulted in improved benefits for patients with chronic conditions since autoinjectable devices were adopted 30 years ago. Adherence, patient administration and disease self-management are issues that persist despite product design innovations. Drug device combinations are connected to digital health technologies to give a value proposition for next-generation autoinjectable devices. Success will be dependent on bioPharma's digital health maturity. The viewpoint measures the digital health maturity of the top 15 US manufacturers of autoinjectors and establishes the framework for next- generation devices capable of home-based precision care as well as the need for formal digital health training."]

['Multisystem iron poisoning is a major concern for long-term beta-thalassemia management. Quantitative MRI-based techniques routinely show iron overload in heart, liver, endocrine glands and kidneys. However, data on the brain are conflicting and monitoring of brain iron content is still matter of debate. This 3T-MRI study applied a well validated high-resolution whole-brain quantitative MRI assessment of iron content on 47 transfusion-dependent (mean-age: 36.9 ± 10.3 years, 63% females), 23 non-transfusion dependent (mean-age: 29.2 ± 11.7 years, 56% females) and 57 healthy controls (mean-age: 33.9 ± 10.8 years, 65% females). Clinical data, Wechsler Adult Intelligence Scale scores and treatment regimens were recorded. Beside whole-brain R2* analyses, regional R2*-values were extracted in putamen, globus pallidum, caudate nucleus, thalamus and red nucleus; hippocampal volumes were also determined. Regional analyses yielded no significant differences between patients and controls, except in those treated with deferiprone that showed lower R2*-values (p<0.05). Whole-brain analyses of R2*-maps revealed strong age-R2* correlations (r<sup>2</sup>=0.51) in both groups and clusters of significantly increased R2*-values in beta-thalassemia patients in the hippocampal formations and around the Luschka foramina; transfusion treatment was associated with additional R2* increase in dorsal thalami. Hippocampal formation R2*-values did not correlate with hippocampal volume; hippocampal volume did not differ between patients and controls. All regions with increased R2*-values shared a strict anatomical contiguity with choroid plexuses suggesting a blooming effect as the likely cause of R2* increase, in agreement with the available histopathologic literature evidence. According to our MRI findings and the available histopathologic literature evidence, concerns about neural tissue iron overload in beta-thalassemia appear to be unjustified.', 'Long-term management of beta-thalassemia is affected by multi system iron poisoning. Iron overload can be found in the heart, liver, kidneys and other parts of the body. Monitoring of brain iron content is still a topic of debate, as data on the brain are conflicting. The Iron content assessment on 47 of the 63 females and 23 non-fusion- dependent patients was done using a 3T-MRI study. Clinical data as well as Wechsler SCALE scores and treatment regimen were recorded. The whole-brain R2* analyses were also used to calculate the regional R2* values. There were no significant differences between patients and controls. Analyses of the whole-brain R2*-maps showed strong age-R2* correlations in both groups and in clusters. Hippocampal formation R2*-values of patients and controls did not correlate with Hippocampal volume. The histopathologic evidence supports the presence of a blooming effect in the areas with increased R2* values. Our findings and literature demonstrate that the concerns about iron overload in neural tissues are unwarranted.']

["Numerous nature inspired algorithms have been suggested to enable robotic swarms, mobile sensor networks and other multi-agent systems to exhibit various self-organized behaviors. Swarm intelligence and swarm robotics research have been underway for a few decades and have produced many such algorithms based on natural self-organizing systems. While a large body of research exists for variations and modifications in swarm intelligence algorithms, there have been few attempts to unify the underlying agent level design of these widely varying behaviors. In this work, a design paradigm for a swarm of agents is presented which can exhibit a wide range of collective behaviors at swarm level while using minimalistic single-bit communication at the agent level. The communication in the proposed paradigm is based on waves of 'ping'-signals inspired by strategies for communication and self organization of slime mold (Dictyostelium discoideum) and fireflies (lampyridae). The unification of common collective behaviors through this Wave Oriented Swarm Paradigm (WOSP) enables the control of swarms with minimalistic communication and yet allowing the emergence of diverse complex behaviors. It is demonstrated both in simulation and using real robotic experiments that even a single-bit communication channel between agents suffices for the design of a substantial set of behaviors. Ultimately, the reader will be enabled to combine different behaviours based on the paradigm to develop a control scheme for individual swarms.", 'Various nature inspired ideas have been suggested to help with robotic swarms, mobile sensor networks and other multi-agent systems. Many of the systems used to make the swarm intelligence and swarm robotic research are derived from natural self-organizing systems. The underlying agent level design of swarm intelligence algorithms are widely varied, with little attempt to unify it. In this work, a design paradigm for a swarm of agents can show a wide range of collective behaviors while using minimal single-bit communication at the agent level. The communication in the proposal is based on the strategies of communication that involve the growth of the Dictyostelium discoideum. The Wave Oriented Swarm Symposium (WOSP) allows the control of swarms with minimal communication, yet allowing emergence of diverse complex behaviors. It is demonstrated with a single-bit communication channel between agents that it is possible to design a large set of behaviors. The reader will be able to combine various behaviours based on the paradigm in order to develop a control scheme for individual swarms.']

["Music training has repeatedly been claimed to positively impact children's cognitive skills and academic achievement (literacy and mathematics). This claim relies on the assumption that engaging in intellectually demanding activities fosters particular domain-general cognitive skills, or even general intelligence. The present meta-analytic review (N = 6,984, k = 254, m = 54) shows that this belief is incorrect. Once the quality of study design is controlled for, the overall effect of music training programs is null ([Formula: see text] ≈ 0) and highly consistent across studies (τ<sup>2</sup> ≈ 0). Results of Bayesian analyses employing distributional assumptions (informative priors) derived from previous research in cognitive training corroborate these conclusions. Small statistically significant overall effects are obtained only in those studies implementing no random allocation of participants and employing non-active controls ([Formula: see text] ≈ 0.200, p < .001). Interestingly, music training is ineffective regardless of the type of outcome measure (e.g., verbal, non-verbal, speed-related, etc.), participants' age, and duration of training. Furthermore, we note that, beyond meta-analysis of experimental studies, a considerable amount of cross-sectional evidence indicates that engagement in music has no impact on people's non-music cognitive skills or academic achievement. We conclude that researchers' optimism about the benefits of music training is empirically unjustified and stems from misinterpretation of the empirical data and, possibly, confirmation bias.", "Music training is said to increase children's academic achievement and impact on cognitive skills. This claim is based on the idea that engaging in intellectually demanding activities fosters certain cognitive skills. The meta-analytic review shows that the belief is incorrect. The effect of music training is null if the quality of the study design is controlled for. Results of previous research in cognitive training corroborate these conclusions. Only studies that use no random allocation of participants and non active controls have statistically significant benefits. Music training is useless regardless of the outcome measure. g They include speed-related, verbal, non- verbal, etc. Participants' age and duration. There is a lot of evidence that indicates that engaging in music is not related to educational skills or cognitive abilities. There is a conclusion we reached regarding researchers' optimism about the benefits of music training."]

["The relationship between political environment and health services accessibility (HSA) has not been the focus of any specific studies. The purpose of this study was to address this gap in the literature by examining the relationship between political environment and HSA. This relationship that HSA indicators (physicians, nurses and hospital beds per 10 000 people) has with political environment was analyzed with multiple least-squares regression using the components of democracy (electoral processes and pluralism, functioning of government, political participation, political culture, and civil liberties). The components of democracy were represented by the 2011 Economist Intelligence Unit Democracy Index (EIUDI) sub-scores. The EIUDI sub-scores and the HSA indicators were evaluated for significant relationships with multiple least-squares regression. While controlling for a country's geographic location and level of democracy, we found that two components of a nation's political environment: functioning of government and political participation, and their interaction had significant relationships with the three HSA indicators. These study findings are of significance to health professionals because they examine the political contexts in which citizens access health services, they come from research that is the first of its kind, and they help explain the effect political environment has on health.", "The relationship between political environment and health services accessibility has not be studied in any specific way. The goal of the study was to examine the relationship between political environment and HSA. The relationship between HSA indicators and politics was analyzed with the components of democracy used to figure it out, including electoral processes and pluralism, functioning of government, political participation, political culture, and civil liberties. The components of democracy were represented by theEIUDI sub-scores. There are significant relationships between the eiUDI sub-scores and the HSA indicators. We found that two components of a nation's political environment, functioning of government and political participation, and their interaction, had important relationships with the three HSA indicators. The study findings help explain how the political environment affects health because they examine the political contexts in which citizens can access health services."]

['In order to limit the spread of the novel betacoronavirus (SARS-CoV-2), it is necessary to detect positive cases as soon as possible and isolate them. For this purpose, machine-learning algorithms, as a field of artificial intelligence, have been recognized as a promising tool. The aim of this study was to assess the utility of the most common machine-learning algorithms in the rapid triage of children with suspected COVID-19 using easily accessible and inexpensive laboratory parameters. A cross-sectional study was conducted on 566 children treated for respiratory diseases: 280 children with PCR-confirmed SARS-CoV-2 infection and 286 children with respiratory symptoms who were SARS-CoV-2 PCR-negative (control group). Six machine-learning algorithms, based on the blood laboratory data, were tested: random forest, support vector machine, linear discriminant analysis, artificial neural network, k-nearest neighbors, and decision tree. The training set was validated through stratified cross-validation, while the performance of each algorithm was confirmed by an independent test set. Random forest and support vector machine models demonstrated the highest accuracy of 85% and 82.1%, respectively. The models demonstrated better sensitivity than specificity and better negative predictive value than positive predictive value. The F1 score was higher for the random forest than for the support vector machine model, 85.2% and 82.3%, respectively. This study might have significant clinical applications, helping healthcare providers identify children with COVID-19 in the early stage, prior to PCR and/or antigen testing. Additionally, machine-learning algorithms could improve overall testing efficiency with no extra costs for the healthcare facility.', 'Positive cases of the novelBetacoronaviruses can be detected as soon as possible to stop the spread. Machine-Learning algorithms, a field of artificial intelligence, has been seen as promising for this purpose. To assess the utility of the most common machine-learning algorithms in the rapid triage of children with suspected CO-VID-19, the study used easy to navigate and inexpensive laboratory parameters. A study was done on children who had respiratory symptoms after being found to have a confirmed case of the disease. Six machine-learning simulations based on the blood laboratory data were tested. Each iteration of the training set was verified by an independent sample of test sets. Random forest and support machine models had the best accuracy. The models have negative and positive predictive values. The F1 score was higher for the random forest than for the support machine model. This study might be used to help identify children with COVID-22 in the early stages. The healthcare facility could benefit from the improved testing efficiency that will be achieved by machine-learning.']

["The impact of a decision support tool designed to embed contextual mission factors was investigated. Contextual information may enable operators to infer the appropriateness of data underlying the automation's algorithm. Research has shown the costs of imperfect automation are more detrimental than perfectly reliable automation when operators are provided with decision support tools. Operators may trust and rely on the automation more appropriately if they understand the automation's algorithm. The need to develop decision support tools that are understandable to the operator provides the rationale for the current experiment. A total of 17 participants performed a simulated rapid retasking of intelligence, surveillance, and reconnaissance (ISR) assets task with manual, decision automation, or contextual decision automation differing in two levels of task demand: low or high. Automation reliability was set at 80%, resulting in participants experiencing a mixture of reliable and automation failure trials. Dependent variables included ISR coverage and response time of replanning routes. Reliable automation significantly improved ISR coverage when compared with manual performance. Although performance suffered under imperfect automation, contextual decision automation helped to reduce some of the decrements in performance. Contextual information helps overcome the costs of imperfect decision automation. Designers may mitigate some of the performance decrements experienced with imperfect automation by providing operators with interfaces that display contextual information, that is, the state of factors that affect the reliability of the automation's recommendation.", "The impact of a decision support tool was studied. Contextual information can help to see if the data underlying the automation's algorithm is appropriate. When operators are provided with decision support tools, imperfect automation is less expensive than it was before. Operators can trust the automation more if they understand it. The rationale for the current experiment is based on the need to develop decision support tools that are understandable to the operator. In a simulation, 17 participants performed a rapid retasking of intelligence, surveillance, and reconnaissance assets task with three types of decision automation but different levels of task demand. There was a mixture of reliability and automation failures among participants. ISR coverage was included with dependent variables. Reliable automation was able to improve the coverage. Performance was hurt by imperfect automation but some decrements were reduced because of contextual decision automation. textual information is able to overcome the costs of decision automation. Designers may try to mitigate the performance decrements experienced with imperfect automation by giving operators the interface that display contextual information, that is, the state of factors that affect the reliability of the automation's recommendation."]

['The early detection and accurate histopathological diagnosis of gastric cancer increase the chances of successful treatment. The worldwide shortage of pathologists offers a unique opportunity for the use of artificial intelligence assistance systems to alleviate the workload and increase diagnostic accuracy. Here, we report a clinically applicable system developed at the Chinese PLA General Hospital, China, using a deep convolutional neural network trained with 2,123 pixel-level annotated H&E-stained whole slide images. The model achieves a sensitivity near 100% and an average specificity of 80.6% on a real-world test dataset with 3,212 whole slide images digitalized by three scanners. We show that the system could aid pathologists in improving diagnostic accuracy and preventing misdiagnoses. Moreover, we demonstrate that our system performs robustly with 1,582 whole slide images from two other medical centres. Our study suggests the feasibility and benefits of using histopathological artificial intelligence assistance systems in routine practice scenarios.', 'The chances of successful treatment increased due to the early detection and accurate histopathological diagnosis. The use of artificial intelligence assistance systems could be used to alleviate the workload because of the worldwide shortage of pathologists. The Chinese PLA General Hospital developed a clinically applicable system using deep convolutional neural network training and 2,123 H&E-stained slide images. The model achieved a sensitivity of 100% and a specificity of 86.6% on a real world test dataset with 3,212 whole slide images. We show that the system can help the pathologists. Our system performs robustly with 1,582 slides from other medical centres. Our study suggests the benefits and feasibility of using histopathological Artificial Intelligence Assistance systems.']

['This work was aimed at designing a deep-learning-based approach for MR image phase unwrapping to improve the robustness and efficiency of traditional methods. A deep learning network called PHU-NET was designed for MR phase unwrapping. In this network, a novel training data generation method was proposed to simulate the wrapping patterns in MR phase images. The wrapping boundary and wrapping counts were explicitly estimated and used for network training. The proposed method was quantitatively evaluated and compared to other methods using a number of simulated datasets with varying signal-to-noise ratio (SNR) and MR phase images from various parts of the human body. The results showed that our method performed better in the simulated data even under an extremely low SNR. The proposed method had less residual wrapping in the images from various parts of human body and worked well in the presence of severe anatomical discontinuity. Our method was also advantageous in terms of computational efficiency compared to the traditional methods. This work proposed a robust and computationally efficient MR phase unwrapping method based on a deep learning network, which has promising performance in applications using MR phase information.', 'A deep learning-based approach was designed to improve the robustness and efficiency of traditional methods. A deep learning network called PHU-NET was built. A method to Generate Training Data was proposed to mimic the MR phase images. Network training utilized the estimated wrapping boundary and wrapping counts. The proposed method was evaluated in detail by using a number of simulations with different signal-to-noise ratios and MR phase images from various parts of the human body. The results showed that we had a method that performed better in the simulation. There were less residual wrap in the images from various parts of the human body. Our method was very efficient compared to the traditional methods. A deep learning network and a robust, computationally efficient MR phase unwrapping method were proposed in this work.']

['Frailty is a serious physical disorder affecting the elderly all over the world. However, the frail elderly have low physical fitness, which limits the effectiveness of current exercise programs. Inspired by this, we attempted to integrate Baduanjin and strength and endurance exercises into an exercise program to improve the physical fitness and alleviate frailty among the elderly. Additionally, to achieve the goals of personalized medicine, machine learning simulations were performed to predict post-intervention frailty. A total of 171 frail elderly individuals completed the experiment, including a Baduanjin group (BDJ), a strength and endurance training group (SE), and a combination of Baduanjin and strength and endurance training group (BDJSE), which lasted for 24 weeks. Physical fitness was evaluated by 10-meter maximum walk speed (10 m MWS), grip strength, the timed up-and-go test (TUGT), and the 6 min walk test (6 min WT). A one-way analysis of variance (ANOVA), chi-square test, and two-way repeated-measures ANOVA were carried out to analyze the experimental data. In addition, nine machine learning models were utilized to predict the frailty status after the intervention. In 10 m MWS and TUGT, there was a significant interactive influence between group and time. When comparing the BDJ group and the SE group, participants in the BDJSE group demonstrated the maximum gains in 10 m MWS and TUGT after 24 weeks of intervention. The stacking model surpassed other algorithms in performance. The accuracy and precision rates were 75.5% and 77.1%, respectively. The hybrid exercise program that combined Baduanjin with strength and endurance training proved more effective at improving fitness and reversing frailty in elderly individuals. Based on the stacking model, it is possible to predict whether an elderly person will exhibit reversed frailty following an exercise program.', 'The elderly are affected by Frailty all over the world. The frail elderly have poor physical fitness which limits the effectiveness of current exercise programs. The physical fitness of the elderly needed to be improved so we tried to combine strength and endurance exercises into an exercise program. Machine learning simulations were used to determine if post-injury frailty was a risk for personalized medicine. A total of 171 frail elderly people have completed the experiment, which was a combination of the Baduanjin and strength and endurance training groups. The fitness was measured by walk speed (10 mWMRS), grip strength, timed up-and-go test, as well as a walk test. Experimental data was analyzed using a one-way analysis of variance, chi- square test, and two-way repeated-measures ANOVA. Nine machine learning models were used to pick out the frailest people after the intervention. The influence between the group and time was high in 10 m MWS. Participants in the BDJ group had the best gains in 10 m MWS and the most gains in 24 weeks of intervention. The model was the superior one in terms of performance. There was an accuracy and precision rate of 75.5% and 77.1%, respectively. It is found that a combined exercise program of Baduanjin with strength and endurance training can help reverse the effects of old age. The stacking model can be used to calculate the likelihood of an elderly person showing frailty following an exercise program.']

['Two relatively independent research traditions have developed that address emotion management. The first is the emotion regulation (ER) tradition, which focuses on the processes which permit individuals to influence which emotions they have, when they have them, and how they experience and express these emotions. The second is the emotional intelligence (EI) tradition, which focuses-among other things-on individual differences in ER. To integrate these two traditions, we employed the process model of ER (Gross, 1998b) to review the literature on EI. Two key findings emerged. First, high EI individuals shape their emotions from the earliest possible point in the emotion trajectory and have many strategies at their disposal. Second, high EI individuals regulate their emotions successfully when necessary but they do so flexibly, thereby leaving room for emotions to emerge. We argue that ER and EI traditions stand to benefit substantially from greater integration.', 'Two research traditions are related to emotion management. The emotion regulation (ER) tradition focuses on the processes which allow a person to affect certain emotions and how they express them. The emotional intelligence tradition focuses on individual differences in ER. To integrate the two traditions, we utilized the process model of ER. The two key findings were identified. First, high EI people shape their Emotions from the earliest possible point in the emotion trajectory, and have many strategies to use. The second point is that high Ei individuals do so freely, and they leave room for emotions to emerge. ER and EI traditions could benefit substantially from greater integration.']

['Pregnancy is a diabetogenic state characterized by relative insulin resistance, enhanced lipolysis, elevated free fatty acids and increased ketogenesis. In this setting, short period of starvation can precipitate ketoacidosis. This sequence of events is recognized as "accelerated starvation." Metabolic acidosis during pregnancy may have adverse impact on fetal neural development including impaired intelligence and fetal demise. Short periods of starvation during pregnancy may present as severe anion gap metabolic acidosis (AGMA). We present a 41-year-old female in her 32nd week of pregnancy, admitted with severe AGMA with pH 7.16, anion gap 31, and bicarbonate of 5 mg/dL with normal lactate levels. She was intubated and accepted to medical intensive care unit. Urine and serum acetone were positive. Evaluation for all causes of AGMA was negative. The diagnosis of starvation ketoacidosis was established in absence of other causes of AGMA. Intravenous fluids, dextrose, thiamine, and folic acid were administered with resolution of acidosis, early extubation, and subsequent normal delivery of a healthy baby at full term. Rapid reversal of acidosis and favorable outcome are achieved with early administration of dextrose containing fluids.', 'There is a diabetogenic state when pregnant, with factors including lipolysis and increased free fat acids. A short period of starvation can lead to acidosis. A sequence of events is recognized as starvation. Fetal neural development may be adversely impacted by fetal acidosis during pregnancy. There may be short periods of starvation during pregnancies. A female in the 32nd week of her unborn child was admitted with a range of issues, including acidosis and anion gap. She was admitted to a medical intensive care unit after being intubated. Urine and acetone were present. Evaluation of all causes of the same thing was negative. There was no other cause of AGMA for the diagnosis of starvation keto acidosis. Fetal fluids, vitamins, and acid were given to the baby with the expectation of a normal delivery. Early administration has been found to reverse acidosis and result in favorable outcomes.']

["The influence of individual differences on learners' study time allocation has been emphasised in recent studies; however, little is known about the role of individual thinking styles (analytical versus intuitive). In the present study, we explored the influence of individual thinking styles on learners' application of agenda-based and habitual processes when selecting the first item during a study-time allocation task. A 3-item cognitive reflection test (CRT) was used to determine individuals' degree of cognitive reliance on intuitive versus analytical cognitive processing. Significant correlations between CRT scores and the choices of first item selection were observed in both Experiment 1a (study time was 5 seconds per triplet) and Experiment 1b (study time was 20 seconds per triplet). Furthermore, analytical decision makers constructed a value-based agenda (prioritised high-reward items), whereas intuitive decision makers relied more upon habitual responding (selected items from the leftmost of the array). The findings of Experiment 1a were replicated in Experiment 2 notwithstanding ruling out the possible effects from individual intelligence and working memory capacity. Overall, the individual thinking style plays an important role on learners' study time allocation and the predictive ability of CRT is reliable in learners' item selection strategy.", "The influence of individual differences on learners' study allocation has become more apparent, however there is little known about the role of individual thinking styles. The influence of individual thinking styles on learner's application of agenda-based and habitual processes when selecting first item during study-time allocation task was explored in the present study. The degree of cognitive dependency on intuitive and analytical processing was determined by using a 3-item cognitive reflection test. The choices of first item selection in both Experiment 1a and 1b were found to be correlated with CRT scores. Analytical decision makers relied on prioritised high reward items, while intuitive decision makers relied on responding to selected items in the leftmost part of the array. In Experiment 2, the findings were the same as those in Experiment 1 and there were no effects from individual Intelligence or working memory capacity. While the individual thinking style plays a significant role on learners' study time allocation, the reliability of the predictr is reliable in learners' item selection strategy."]

['Excessive internet use is shown to be cross sectionally associated with lower cognitive functioning and reduced volume of several brain areas. However, the effects of daily internet use on the development of verbal intelligence and brain structures have not been investigated. Here, we cross sectionally examined the effects of the frequency of internet use on regional gray/white matter volume (rGMV/rWMV) and verbal intelligence as well as their longitudinal changes after 3.0 ± 0.3 (standard deviation) years in a large sample of children recruited from the general population (mean age, 11.2 ± 3.1 years; range, 5.7-18.4 years). Although there were no significant associations in cross sectional analyses, a higher frequency of internet use was found to be associated with decrease of verbal intelligence and smaller increase in rGMV and rWMV of widespread brain areas after a few years in longitudinal analyses. These areas involve areas related to language processing, attention and executive functions, emotion, and reward. In conclusion, frequent internet use is directly or indirectly associated with decrease of verbal intelligence and development to smaller gray matter volume at later stages.', "It has been shown that excessive internet use is related to reduced cognitive functioning. The effects of daily internet use on brain structures have not been studied. We looked at the effects of internet use on gray and white matter volume and verbal intelligence in children recruited from the same community. In longitudinal analyses the amount of internet use had a correlation with changes in verbal intelligence and the amount of dopamine in the brains of the people who use the internet the most and the amount of dopamine in the brains of the people who don't use it the most. These areas relate to language processing, attention, and executive functions. Frequent internet use is linked with lower verbal intelligence and the decrease of gray matter volume at later stages."]

['First-degree relatives of patients diagnosed with schizophrenia (SZ-FDRs) show similar patterns of brain abnormalities and cognitive alterations to patients, albeit with smaller effect sizes. First-degree relatives of patients diagnosed with bipolar disorder (BD-FDRs) show divergent patterns; on average, intracranial volume is larger compared to controls, and findings on cognitive alterations in BD-FDRs are inconsistent. Here, we performed a meta-analysis of global and regional brain measures (cortical and subcortical), current IQ, and educational attainment in 5,795 individuals (1,103 SZ-FDRs, 867 BD-FDRs, 2,190 controls, 942 schizophrenia patients, 693 bipolar patients) from 36 schizophrenia and/or bipolar disorder family cohorts, with standardized methods. Compared to controls, SZ-FDRs showed a pattern of widespread thinner cortex, while BD-FDRs had widespread larger cortical surface area. IQ was lower in SZ-FDRs (d = -0.42, p = 3 × 10<sup>-5</sup> ), with weak evidence of IQ reductions among BD-FDRs (d = -0.23, p = .045). Both relative groups had similar educational attainment compared to controls. When adjusting for IQ or educational attainment, the group-effects on brain measures changed, albeit modestly. Changes were in the expected direction, with less pronounced brain abnormalities in SZ-FDRs and more pronounced effects in BD-FDRs. To conclude, SZ-FDRs and BD-FDRs show a differential pattern of structural brain abnormalities. In contrast, both had lower IQ scores and similar school achievements compared to controls. Given that brain differences between SZ-FDRs and BD-FDRs remain after adjusting for IQ or educational attainment, we suggest that differential brain developmental processes underlying predisposition for schizophrenia or bipolar disorder are likely independent of general cognitive impairment.', 'There are similar patterns of brain abnormality and cognitive alterations in relatives of patients withSchizophrenia, although effect sizes are smaller. There are different patterns in relation to first degree relatives of patients with bipolar disorder and there are different information regarding cognitive alterations in the patients. In 5, 755 people, we did a meta-analysis of global and regional brain measures. SZ-FDRs had larger cortical areas than the controls. The IQ was lower in SZ-FDRs with a weak evidence. Both groups have similar educational levels. Adjusting for education and IQ changes the group effects on brain measures Brain abnormality in SZ-FDRs were less pronounced in the expected direction. Structural brain abnormality are shown in the SZ-FDRs and BD-FDRs. Both had lower IQ scores than the controls. The brain differences between SZ-FDRs and BD-FDRs are likely not caused by general cognitive impairment.']

['Microscopic image examination is fundamental to clinical microbiology and often used as the first step to diagnose fungal infections. In this study, we present classification of pathogenic fungi from microscopic images using deep convolutional neural networks (CNN). We trained well-known CNN architectures such as DenseNet, Inception ResNet, InceptionV3, Xception, ResNet50, VGG16, and VGG19 to identify fungal species, and compared their performances. We collected 1079 images of 89 fungi genera and split our data into training, validation, and test datasets by 7:1:2 ratio. The DenseNet CNN model provided the best performance among other CNN architectures with overall accuracy of 65.35% for top 1 prediction and 75.19% accuracy for top 3 predictions for classification of 89 genera. The performance is further improved (>80%) after excluding rare genera with low sample occurrence and applying data augmentation techniques. For some particular fungal genera, we obtained 100% prediction accuracy. In summary, we present a deep learning approach that shows promising results in prediction of filamentous fungi identification from culture, which could be used to enhance diagnostic accuracy and decrease turnaround time to identification.', 'Clinical microbiology is often used as the first step to diagnose infections. In this study, we presented classification of the fungi from images. We trained well known CNN architectures such as ResNet and Inception and compared their performances. We split the data into training, validation, and test datasets after taking 1079 images of 89 fungi genera. The best CNN model was the DenseNetCNN model, which provided overall accuracy of 65.35% for top 1 prediction and 75.19% for top 3 predictions. Excluding rare genera and applying data augmentation techniques have improved the performance by at least 80%. We were able to receive 100% prediction accuracy for some specific genera. A deep learning approach that shows promising results in predicting the identification of filamentous fungi could be used to improve the accuracy and speed up the identification process.']

['When using a smartwatch to obtain electrocardiogram (ECG) signals from multiple leads, the device has to be placed on different parts of the body sequentially. The ECG signals measured from different leads are asynchronous. Artificial intelligence (AI) models for asynchronous ECG signals have barely been explored. We aimed to develop an AI model for detecting acute myocardial infarction using asynchronous ECGs and compare its performance with that of the automatic ECG interpretations provided by a commercial ECG analysis software. We sought to evaluate the feasibility of implementing multiple lead-based AI-enabled ECG algorithms on smartwatches. Moreover, we aimed to determine the optimal number of leads for sufficient diagnostic power. We extracted ECGs recorded within 24 hours from each visit to the emergency room of Ajou University Medical Center between June 1994 and January 2018 from patients aged 20 years or older. The ECGs were labeled on the basis of whether a diagnostic code corresponding to acute myocardial infarction was entered. We derived asynchronous ECG lead sets from standard 12-lead ECG reports and simulated a situation similar to the sequential recording of ECG leads via smartwatches. We constructed an AI model based on residual networks and self-attention mechanisms by randomly masking each lead channel during the training phase and then testing the model using various targeting lead sets with the remaining lead channels masked. The performance of lead sets with 3 or more leads compared favorably with that of the automatic ECG interpretations provided by a commercial ECG analysis software, with 8.1%-13.9% gain in sensitivity when the specificity was matched. Our results indicate that multiple lead-based AI-enabled ECG algorithms can be implemented on smartwatches. Model performance generally increased as the number of leads increased (12-lead sets: area under the receiver operating characteristic curve [AUROC] 0.880; 4-lead sets: AUROC 0.858, SD 0.008; 3-lead sets: AUROC 0.845, SD 0.011; 2-lead sets: AUROC 0.813, SD 0.018; single-lead sets: AUROC 0.768, SD 0.001). Considering the short amount of time needed to measure additional leads, measuring at least 3 leads-ideally more than 4 leads-is necessary for minimizing the risk of failing to detect acute myocardial infarction occurring in a certain spatial location or direction. By developing an AI model for detecting acute myocardial infarction with asynchronous ECG lead sets, we demonstrated the feasibility of multiple lead-based AI-enabled ECG algorithms on smartwatches for automated diagnosis of cardiac disorders. We also demonstrated the necessity of measuring at least 3 leads for accurate detection. Our results can be used as reference for the development of other AI models using sequentially measured asynchronous ECG leads via smartwatches for detecting various cardiac disorders.', 'When using a wrist accessory to get alectrocardiogram (ECG) signal from multiple leads, it is important to place it on separate parts of the body. The signals from different leads are not connected. Artificial intelligence models for ECG signals are still being explored. We designed an artificial intelligence model for detecting acute myocardial infarction using electrocardiograms and then compared its performance to automatic interpretations of electrocardiograms provided by a commercial software. The feasibility of using multiple lead-based Artificial intelligence-enabled ECGs on watches was examined. We wanted to figure out the optimal number of leads for diagnostic power. Patients aged 20 years or older were the ones who were taken to the emergency room of the Ajou University Medical Center. The diagnostic code that a patient would enter for a acute myocardial infarction was used to label the episodes. We derived asynchronous Electrocardiogram lead sets from standard 12lead reports and used that information to prototype the situation that would occur via the sequential recording of ECG leads via wristwatches. We used various targeting lead sets to test the model during the training phase and randomly masked the remaining lead channels during testing. The sensitivity of the leads given in the lead sets was compared favorably with the ones given in the automatic ECG interpretation provided by a commercial software. Our results suggest that multiple lead-based artificial intelligence-enabled ecg can be implemented on a smartwatches. The number of leads increased, and the model performed better. It is necessary to measure at least 3 leads, ideally more than 2 leads, in order to minimize the chance of not detecting acute myocardial infarction in a certain location or direction. We showed the feasibility of several lead-based artificial intelligence-enabled and automated Cardiac Disorders diagnosis on wristwatches. We demonstrated the need for measuring at least 3 leads. We can use our results as a reference for developing other artificial intelligence models that use sequential measured asynchronous heart rate leads.']

['With the increasing integration of artificial intelligence (AI) in health care, AI chatbots like ChatGPT-4 are being used to deliver health information. This study aimed to assess the capability of ChatGPT-4 in answering common questions related to abdominoplasty, evaluating its potential as an adjunctive tool in patient education and preoperative consultation. A variety of common questions about abdominoplasty were submitted to ChatGPT-4. These questions were sourced from a question list provided by the American Society of Plastic Surgery to ensure their relevance and comprehensiveness. An experienced plastic surgeon meticulously evaluated the responses generated by ChatGPT-4 in terms of informational depth, response articulation, and competency to determine the proficiency of the AI in providing patient-centered information. The study showed that ChatGPT-4 can give clear answers, making it useful for answering common queries. However, it struggled with personalized advice and sometimes provided incorrect or outdated references. Overall, ChatGPT-4 can effectively share abdominoplasty information, which may help patients better understand the procedure. Despite these positive findings, the AI needs more refinement, especially in providing personalized and accurate information, to fully meet patient education needs in plastic surgery. Although ChatGPT-4 shows promise as a resource for patient education, continuous improvements and rigorous checks are essential for its beneficial integration into healthcare settings. The study emphasizes the need for further research, particularly focused on improving the personalization and accuracy of AI responses. This journal requires that authors assign a level of evidence to each article. For a full description of these Evidence-Based Medicine ratings, please refer to the Table of Contents or the online Instructions to Authors www.springer.com/00266 .', "Artificial intelligence like chatGPT-4 is being used in health care to deliver information. The purpose of the study was to determine whether or not to use the ChatGPt-4 in patient education and consult with patients following abdominoplasty. The American Society of Plastic Surgery provided a question list for these questions. An experienced plastic surgeon evaluated the responses generated by the Artificial intelligence to get a clear picture of how skilled the system is in providing patient-centered information. The study showed that this is useful for answering common queries. personalized advice was hard to get and it didn't always provide correct or outdated references. The information that you share with the patients can help you better understand the procedure. Despite positive findings the Artificial Intelligence needs to be improved to better serve the needs of patients in plastic surgery. Continuous improvements and rigorous checks are needed for its good integration into healthcare settings. The research focuses on improving personalization and accuracy of responses with the help of artificial intelligence. Each article in the journal must have a level of evidence assigned to it. You can refer to the table of contents or instructions to authors to learn about the ratings. The springer. com/00266"]

['The investigation of the human intelligence, cognitive systems and functional complexity of human brain is significantly facilitated by high-performance computational platforms. In this paper, we present a real-time digital neuromorphic system for the simulation of large-scale conductance-based spiking neural networks (LaCSNN), which has the advantages of both high biological realism and large network scale. Using this system, a detailed large-scale cortico-basal ganglia-thalamocortical loop is simulated using a scalable 3-D network-on-chip (NoC) topology with six Altera Stratix III field-programmable gate arrays simulate 1 million neurons. Novel router architecture is presented to deal with the communication of multiple data flows in the multinuclei neural network, which has not been solved in previous NoC studies. At the single neuron level, cost-efficient conductance-based neuron models are proposed, resulting in the average utilization of 95% less memory resources and 100% less DSP resources for multiplier-less realization, which is the foundation of the large-scale realization. An analysis of the modified models is conducted, including investigation of bifurcation behaviors and ionic dynamics, demonstrating the required range of dynamics with a more reduced resource cost. The proposed LaCSNN system is shown to outperform the alternative state-of-the-art approaches previously used to implement the large-scale spiking neural network, and enables a broad range of potential applications due to its real-time computational power.', 'High-performance platforms have made possible the investigation of the human intelligence, cognitive systems and functional complexity. The benefits of high biological realism and large network scale are emphasized in this paper. A large-scale cortico-basal ganglia-thalamocortical loop simulation is carried out using a 3D network- on- chip. A novel architecture is presented to address multinuclei neural network communication, which is not currently solved in previous NoC studies. The foundation of the large-scale realization can be found in the use of less memory resources and less DSP resources at the single neuron level. A more reduced resource cost can be demonstrated by investigating bifurcation behaviors and ionic dynamics in modified models. Compared to the state-of-the-art approaches used to implement the large-scale spiking Neural Network, the proposed system is shown to be the better system due to its real-time computational power.']

["Developing drugs for treating Alzheimer's disease has been extremely challenging and costly due to limited knowledge of underlying mechanisms and therapeutic targets. To address the challenge in AD drug development, we developed a multi-task deep learning pipeline that learns biological interactions and AD risk genes, then utilizes multi-level evidence on drug efficacy to identify repurposable drug candidates. Using the embedding derived from the model, we ranked drug candidates based on evidence from post-treatment transcriptomic patterns, efficacy in preclinical models, population-based treatment effects, and clinical trials. We mechanistically validated the top-ranked candidates in neuronal cells, identifying drug combinations with efficacy in reducing oxidative stress and safety in maintaining neuronal viability and morphology. Our neuronal response experiments confirmed several biologically efficacious drug combinations. This pipeline showed that harmonizing heterogeneous and complementary data/knowledge, including human interactome, transcriptome patterns, experimental efficacy, and real-world patient data shed light on the drug development of complex diseases.", "It has been very challenging and cost prohibitive to develop drugs to treat Alzheimer's disease due to a lack of knowledge. We developed a deep- learning program to learn biological interactions and AD risk genes in order to identify re-purposable drug candidates. Using the model, we ranked candidates based on evidence from a number of different areas. We tested the top candidates in cells, identifying drug combinations that reduced stress and safety. Several drug combinations were confirmed. The data related to human interaction, transcriptome patterns and experimental efficacy were included in the data."]

['Restrictions in movement and closure of borders imposed by the Sars-Cov- 2 worldwide pandemic have affected the global illicit drug market, including cocaine trafficking. In this scenario, comparing cutting agents added to the cocaine and the drug purity are valuable strategies to understand how the drug trade has been impacted by the pandemic. In this work, 204 cocaine salt materials seized in the Brazilian Federal District, before (2019) and during COVID-19 pandemics (2020) were analyzed by two analytical techniques: gas chromatography-mass spectrometry (GC-MS) and Fourier-transform infrared spectroscopy (FTIR). Statistical analyses, including Principal Component Analysis (PCA), were applied to evaluate the COVID-19 pandemic impact in the local market. Bibliometric analysis was performed as a forensic intelligence tool. From 2019-2020, cocaine average purity decreased 26 % while the frequency of cutting agents, as caffeine and anesthetics (lidocaine, tetracaine) increased. The high percentage of unknown were increased. Different cocaine profiling seized in 2020 showed new cutting agents, such as Irganox 1076, and Irgafos 168, indicating a trend on new adulterants/diluents introduced in the local market to mitigate the local drug shortage. Also in 2020, there was an increase in the local cocaine seizures, despite of the cocaine drug purity decreased by 26 % compared to 2019. Taken together, these data showed that the covid-19 pandemics has impacted cocaine trafficking in the Brazilian Federal District, an increase in cocaine seizures, which may indicate greater demand for the drug and, specially, changes in the cocaine purity and cutting agents profiling showing how traffickers tried to minimize difficulties in crossing the Brazilian border during COVID-19 restrictions. The information is relevant since Brazil is one of the major departure points for traded cocaine to the world. Bibliometric analysis showed that Irgafos 168 and Irganox 1076 were consistently identified as cocaine cutting agents for the first time.', "The international drug market has been affected by the restrictions on movement that have been imposed. The drug market has been affected by the pandemic, so it's useful to compare cutting agents added to cocaine and the purity of the drug. The cocaine salt materials were analyzed using two analytical techniques; gas  chromatography-mass spectrometry and fTIR, and were seized in Brazil's Federal District. There were analysis done to evaluate the impact of the COvid-19 outbreak on the local market. Bibliometrics were used as a forensic intelligence tool. The purity of cocaine decreased as cutting agents increased. The high percentage of unknown went up. There was a rise in the popularity of new cutting agents such as Irgafos 168 and Irganox 1076 in the local market. The increase in local cocaine seizures could have been caused by the covid-19 STDs, as the purity of the cocaine fell in 2019. Since Brazil is one of the main departure ports for cocaine to the world it is relevant. The first time that Irgufos 168 and Irganox 1076 were identified as cocaine cutting agents was in Bibliometric analysis."]

['Hepatic biopsy is the gold standard for staging nonalcoholic fatty liver disease (NAFLD). Unfortunately, accessing the liver is invasive, requires a multidisciplinary team and is too expensive to be conducted on large segments of the population. NAFLD starts quietly and can progress until liver damage is irreversible. Given this complex situation, the search for noninvasive alternatives is clinically important. A hallmark of NAFLD progression is the dysregulation in lipid metabolism. In this context, recent advances in the area of machine learning have increased the interest in evaluating whether multi-omics data analysis performed on peripheral blood can enhance human interpretation. In the present review, we show how the use of machine learning can identify sets of lipids as predictive biomarkers of NAFLD progression. This approach could potentially help clinicians to improve the diagnosis accuracy and predict the future risk of the disease. While NAFLD has no effective treatment yet, the key to slowing the progression of the disease may lie in predictive robust biomarkers. Hence, to detect this disease as soon as possible, the use of computational science can help us to make a more accurate and reliable diagnosis. We aimed to provide a general overview for all readers interested in implementing these methods.', "There is a very good chance of staging nonalcoholic fatty hem disease. It's too expensive to conduct aLiver procedure on large segments of the population and it's not easy, because it involves a multi-disciplinary team. When NAFLD starts quietly, there's a chance it will progress to liver damage. It's important to search for non-drug alternatives in this situation. A hallmark of this type of progression is the change in metabolism. Recent improvements in the area of machine learning increases the interest in evaluating whether multi-omics data analysis performed on peripheral blood can improve human interpretation. The use of machine learning can be used to identify sets of lipids as a predictor of the progression of NAFLD. It is believed that this approach could help improve the diagnosis accuracy and predict the future risk of the disease. Predictive robust markers could be the key to slowing the progression of the disease. Computational science can help us to make a more accurate and reliable diagnosis of this disease, so that it can be detected as soon as possible. We aimed to give an overview to all of the readers interested in implementing these methods."]

['Modern retinal imaging creates gigantic amounts of data (big data) of anatomic information. At the same time patient numbers and interventions are increasing exponentially. Introduction of artificial intelligence (AI) for optimization of personalized therapy and diagnosis. Deep learning was introduced for automated segmentation and recognition of risk factors and activity levels in retinal diseases. Automated algorithms enable the precise identification and quantification of retinal fluid in all compartments. Early detection of retinopathy in diabetes or glaucoma or risk determination for the development of age-related macular degeneration (AMD) are possible as well as an individual visual prognosis and evaluation of the need for retreatment in intravitreal injection therapy. Methods using AI constitute a breakthrough perspective for the introduction of individualized medicine and optimization of diagnosis and therapy, screening and prognosis.', 'Big data is the information in the data from anatomic information. The patient numbers are increasing greatly. Artificial intelligence is used for individualized therapy and diagnosis. Risk factors in eye diseases can be found in deep learning. The identification and quantification of the fluid in all the compartments was done using automated programs. There are many possibilities for the detection of various eye diseases, such as vision impairment due to age-Related Mactis, and for the evaluation of need for retreatment in vitreal injection therapy. The introduction of individualized medicine, and the utilization of artificial intelligence are some of the new methods used in medicine.']

['We evaluated the effects of liver transplantation (LT) in children with Niemann-Pick disease (NPD) type B. From October 2006 to October 2018, 7 of 1512 children who received LT at Ren Ji Hospital were diagnosed as NPD type B. The median age at diagnosis was 12 months (6-14 months) with initial presentations of hepatosplenomegaly, growth retardation, repeated pneumonia, and diarrhea. Even after comprehensive supporting treatments, all patients developed liver dysfunction, severe interstitial pulmonary disease, compromised lung function, and hypersplenism, with hypertriglyceridemia in 4 patients. They were transferred to our hospital for transplantation (median age, 6.5 years; range, 2.2-8.6 years). Among them, 4 patients received living donor LT, and 3 received whole-liver orthotopic LT. Splenectomy was conducted spontaneously. All patients are alive with a median follow-up of 10 months (range, 5-53 months). Liver function normalized within 3 weeks after transplantation and maintained stability. Thrombocytopenia and leukopenia were cured, as was hypertriglyceridemia. Strikingly, pulmonary disease was relieved after transplantation, as evidenced by resolution of interstitial lung disease and restored lung function. Bronchitis occurred only once among the 3 patients with a quick recovery during follow-up. Catch-up growth was observed in all patients, especially in 1 male patient, as his height z score increased from -3.9 to -1 at 4 years after transplantation. Patients with follow-up longer than 10 months indicated significant psychomotor ability improvement. Hypotonia was relieved in 4 patients after transplantation. However, intelligence developmental delay still existed in 4 patients during the follow-up. Three of them have been receiving intelligence recovery therapy, although the longterm effect needs more investigation. In conclusion, LT is a safe and effective treatment for patients with NPD type B with severe liver and pulmonary dysfunction.', 'The effects on children with aNPD type B have been evaluated. In the period of October 2006 to October 2018), 7 of 1512 children who received latus at Ren Ji Hospital were diagnosed as NPD type b. The median age was 12 months, with the first presentations of growth retardation, pneumonia, and diarrhea being 12 months. There are four patients who have hypertriglyceridemia and all four have developed severe interstitial pulmonary disease. They arrived at our hospital from our hospital with a median age of 6.5 years. There were 4 patients that received living donors and 3 that received whole-liver orthotopic donors. There was no reason for it to be conducted spontaneously. All patients are alive and have a 10 month follow-up. Within 3 weeks after transplantation the function of the organs normalized. Both typhustopenia and hypertriglyceridemia were cured. Strikingly, it was shown that lung disease was relieved after transplant. There was just one instance where bronchitis occurred among 3 patients who had quick recovery. One male transplant patient saw his height Z score increase from -40 to + 4 years later, as his catch-up growth was seen in all patients. Patients with follow-up that lasted 10 months had improvements to their psychomotor ability. Hypotonia was relieved in 4 patients. Intelligence developmental delays were still present in 4 patients following up. Three of them have been receiving therapy for their intelligence. There are patients withNPD type B who can benefit from lt.']

['The treatment of cancer during pregnancy is challenging because of the involvement of two individuals and the necessity of a multidisciplinary approach. An important concern is the potential impact of chemotherapy on the developing fetus. The authors review the available literature on neonatal and long-term outcome of children prenatally exposed to chemotherapy. Chemotherapy administered during first trimester of pregnancy results in increased congenital malformations (7.5 - 17% compared to 4.1 - 6.9% background risk), whereas normal rates are found during second or third trimester. Intrauterine growth restriction is seen in 7 - 21% (compared to 10%), but children develop normal weight and height on the long term. Children are born preterm in 67.1%, compared to 4% in general population. Normal intelligence, attention, memory and behavior are reported, although intelligence tends to decrease with prematurity. Global heart function remains normal, although small differences are seen in ejection fraction, fractional shortening and some diastolic parameters. No secondary cancers or fertility problems are encountered, but follow up periods are limited. Most evidence is based on retrospective studies with small samples and limited follow up periods, methodology and lack of control groups. A large prospective case-control study with long-term follow up is needed in which confounding factors are well considered.', "The treatment of cancer during pregnancies is difficult because of the involvement of two people. Chemo is an aspect of growing up that concerns many people. There is literature on the outcome of children who have been exposed to the drug. During the first two thirds of a baby's life there is a risk of congenital malformations, but there is a risk during the third. The rate of female fertility is 7 or 9% compared to 10% and children are born with normal weight and height on the long term. There are children born in the preterm zone. Intelligence tends to decrease with prematurity. Small differences are seen in ejection fraction, fractional shortening and some of the other parameters of global heart function. There are no problems with secondary cancer, and there are limited follow up periods. Most evidence involves retrospective studies with small samples, lack of control groups and a limited follow up period. A large case-controlled study with a long-term follow up is needed"]

['Complete reporting is essential for clinical research. However, the endorsement of reporting guidelines in radiological journals is still unclear. Further, as a field extensively utilizing artificial intelligence (AI), the adoption of both general and AI reporting guidelines would be necessary for enhancing quality and transparency of radiological research. This study aims to investigate the endorsement of general reporting guidelines and those for AI applications in medical imaging in radiological journals, and explore associated journal characteristic variables. This meta-research study screened journals from the Radiology, Nuclear Medicine & Medical Imaging category, Science Citation Index Expanded of the 2022 Journal Citation Reports, and excluded journals not publishing original research, in non-English languages, and instructions for authors unavailable. The endorsement of fifteen general reporting guidelines and ten AI reporting guidelines was rated using a five-level tool: "active strong", "active weak", "passive moderate", "passive weak", and "none". The association between endorsement and journal characteristic variables was evaluated by logistic regression analysis. We included 117 journals. The top-five endorsed reporting guidelines were CONSORT (Consolidated Standards of Reporting Trials, 58.1%, 68/117), PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses, 54.7%, 64/117), STROBE (STrengthening the Reporting of Observational Studies in Epidemiology, 51.3%, 60/117), STARD (Standards for Reporting of Diagnostic Accuracy, 50.4%, 59/117), and ARRIVE (Animal Research Reporting of In Vivo Experiments, 35.9%, 42/117). The most implemented AI reporting guideline was CLAIM (Checklist for Artificial Intelligence in Medical Imaging, 1.7%, 2/117), while other nine AI reporting guidelines were not mentioned. The Journal Impact Factor quartile and publisher were associated with endorsement of reporting guidelines in radiological journals. The general reporting guideline endorsement was suboptimal in radiological journals. The implementation of reporting guidelines for AI applications in medical imaging was extremely low. Their adoption should be strengthened to facilitate quality and transparency of radiological study reporting.', 'For clinical research complete reporting is essential. Despite the endorsement of reporting guidelines in the radiological journals, it is unclear. Adoption of general and artificial intelligence reporting guidelines would help improve quality and transparency in radiological research. The study aims to investigate the endorsements of general reporting guidelines, and those for artificial intelligence applications in medical imaging, and explore associated journal characteristic variables. The study excluded journals not publishing original research in non-English languages, journals who did not publish original research in the Nuclear Medicine and Medical Identification category, and journals that did not have instructions for authors. The guidelines were rated using a five level tool: "active strong", "passive weak", "passive moderate", and "none" Logistic Regression Analysis was performed on the relationships between endorsement and journal characteristic variables. We included many journals. The endorsed reporting guidelines are CONSORT (58.1%) and PRISMA (52%). The most used artificial intelligence guidelines were listed in the checklists, but other guidelines were not mentioned. The publisher and the Journal Impact Factor quartile were associated with endorsements of reporting guidelines. The general reporting guideline endorsement was not good in the journals. The implementation of reporting guidelines for medical applications was low. They should be strengthened to make sure that they are able to report quality and transparency.']

['Major depressive disorder (MDD) is associated with staggering personal and economic costs, a major proportion of which stem from impaired psychosocial and occupational functioning. Few studies have examined the impact of depression-related cognitive dysfunction on work functioning. We examined the association between neurocognitive and work functioning in employed patients with MDD. Employed adult outpatients (n=36) with MDD of at least moderate severity (≥23 on the Montgomery Asberg Depression Rating Scale, MADRS) and subjective cognitive complaints completed neurocognitive tests (CNS Vital Signs computerized battery) and validated self-reports of their work functioning (LEAPS, HPQ) before and after 8 weeks of open-label treatment with flexibly-dosed desvenlafaxine 50-100mg/day. Relationships between neurocognitive tests and functional measures were examined using bivariate correlational and multiple regression analyses, as appropriate. An ANCOVA model examined whether significant change in neurocognitive performance, defined as improvement of ≥1SD in the Neurocognition Index (NCI) from baseline to post-treatment, was associated with improved outcomes. Patients showed significant improvements in depressive symptom, neurocognitive, and work functioning measures following treatment with desvenlafaxine (e.g., MADRS response=77% and MADRS remission=49%). There were no significant correlations between changes in NCI or cognitive domain subscales and changes in MADRS, LEAPS, or HPQ scores. However, patients demonstrating significant improvement in NCI scores (n=11, 29%) had significantly greater improvement in clinical and work functioning outcomes compared to those without NCI improvement. The limitations of this study include small sample size, lack of a placebo control group, and lack of a healthy comparison group. Our sample also had more years of education and higher premorbid intelligence than the general population. There were no significant correlations between changes in neurocognitive and work functioning measures in this study. However, meaningful improvement in neurocognitive functioning with desvenlafaxine was associated with greater improvement in both mood and occupational outcomes. This suggests that addressing cognitive dysfunction may improve clinical and occupational outcomes in employed patients with MDD. However, the relationship between neurocognitive and work functioning in MDD is complex and requires further study.', "Major depression (MDD) can have huge economic and personal costs because of its impact on psychosocial and occupational functioning. The impact of depression related cognitive impairment on work functioning has been studied only a few times. We examined the relationship between the function of the brain and work. Employed adult outpatients completed a number of cognitive tests, including theCNS Vital Signs computerized battery and self-reports of their works functioning. The relationships between the tests and the measures are examined using multiple regression and bivariate correlational analyses. An ANCOVA model looked at whether a change in neurocognitive performance, which is the improvement of the 1SD in the NCI from baseline to post-treatment, was associated with improved outcomes. Patients were shown to benefit from treatment with desvenlafaxine g The response was 70% and the remission was 42%. There was no correlation between the changes in the cognitive domain subscales and the changes in the other scores. Patients who had a significant improvement in the scores had more improved clinical and work functioning outcomes than those with no improvement. The study's limitations include a small sample size, lack of a placebo control group, and lack of a healthy comparison group. Our sample had more years of education than the general population. There were no correlations between changes in neurosciences and the measures of work functioning. A greater improvement in mood and occupational outcomes was connected with a significant improvement in brain functioning with desvenlafaxine. This suggests that addressing cognitive problems could improve outcomes for employed patients with MDD. A study needs to be done about the relationship between work and brain functions in MDD."]

['Gastroenterology, hepatology and digestive endoscopy are rapidly evolving disciplines with significant advances in the diagnostics and treatment in the entire gastrointestinal tract. The aim of our article was to summarize new perspectives on relevant situations in gastroenterology and hepatology like acute pancreatitis, functional dyspepsia, rational indication of proton pump inhibitors, inflammatory bowel diseases (IBD), cholestatic liver diseases, alcohol induced hepatitis, non-alcoholic fatty live disease (NAFLD) and patophysiology of bilirubin and bile acids. Digestive endoscopy represents an interventional part of gastroenterology and key recent topics are mentioned like pancreatic cancer screening, arteficial intelligence, resection of low-risk neoplastic lesions, enteroscopy techniques, cholangio- and pancreatiscopy and extraluminal expansion of endoscopy techniques by means of endoscopic submucosal and transmural dissection, endoscopic myotomy and lumen apposing stents.', 'The diagnostic and treatment of the gastrointestinal tract are of the fastest evolving disciplines. The aim of the article was to summarize new perspectives regarding gastroenterology and hepatology. Recent topics include pancreatic cancer screening, arteficial intelligence, surgery to remove low risk neoplastic tumors, cholangio- and pancreatiscopy, and the expansion of endoscopy techniques by means of endoscopic.']

['This article is part of a For-Discussion-Section of Methods of Information in Medicine about the paper "Combining Health Data Uses to Ignite Health System Learning" written by John D. Ainsworth and Iain E. Buchan [1]. It is introduced by an editorial. This article contains the combined commentaries invited to independently comment on the paper of Ainsworth and Buchan. In subsequent issues the discussion can continue through letters to the editor. With these comments on the paper "Combining Health Data Uses to Ignite Health System Learning", written by John D. Ainsworth and Iain E. Buchan [1], the journal seeks to stimulate a broad discussion on new ways for combining data sources for the reuse of health data in order to identify new opportunities for health system learning. An international group of experts has been invited by the editor of Methods to comment on this paper. Each of the invited commentaries forms one section of this paper.', 'The paper "Integrating Health Data Use to Increase Health System Learning" was published in Methods of Information in Medicine and was written by John D. Iain E and Ainsworth are related. There is a person named Buchan. The editorial introduced it. The commentaries were invited to independently comment on the paper. Through letters to the editor, the discussion can continue. The paper " combining health data uses to spark health system learning" was written by John D. Iain E and Ainsworth are related. The journal seeks to spark a broad discussion of the reuse of health data in order to identify new opportunities for health system learning. The editor of Methods invited an international group of experts to comment on the paper. The commentaries form part of the paper.']

['Several factors are involved in obtaining the competence of providing spiritual care in nursing students. The purpose of this study was to explain the relationship between moral intelligence and the professional self-concept with the competency of the nursing students in providing spiritual care to promote nursing education. The present study was descriptive-analytics and the participants were the 7<sup>th</sup> and 8<sup>th</sup> semester nursing students of the processing district 6 of the medical sciences universities. The data were collected using questionnaires "Nursing student competencies for providing spiritual care," "Nursing professional self-concept" and "Moral intelligence" questionnaire and analysed using correlation and regression tests. The results showed that there is a significant relationship between the moral intelligence and the students competency in providing spiritual care (<i>r</i> = 0.24, <i>P</i> < 0.001), while this relationship was not significant for the professional self-concept (<i>r</i> = 0.045). Furthermore, based on the results of multiple regression analysis, the moral intelligence (<i>B</i> = 0.28, <i>P</i> = 0.001), gender (<i>B</i> = -4.23, <i>P</i> = 0.048), place of living (<i>B</i> = 10.59, <i>P</i> = 0.046), and adherence to religion (<i>B</i> = -11.82, <i>P</i> = 0.002) were the predictors of students\' competency in providing the spiritual care. According to the results of this study, it is suggested that by applying strategies to strengthen the moral intelligence of the students, such as holding-related workshops as well as reinforcing religiosity, the student\'s competency in providing the spiritual care to the patients is reinforced.', 'The nursing students have to get the competency of giving spiritual care. This study was supposed to demonstrate the relationship between professional self-concept and the ability of the nursing students to provide spiritual care in promoting nursing education. The nursing students of the processing district 6 of the medical sciences universities were included in the study. The data was collected using questionnaires "Nursing student competencies for providing spiritual care," "Nursing professional self-concept" and "Moral intelligence" and analysed. The results showed that the moral intelligence is connected to the students competency in spiritual care however this relationship is not significant for the professional self-concept. The moral intelligence was based on the results of multiple regression analysis. The results of the study suggest that there are ways that the students can strengthen their moral intelligence in order to give better spiritual guidance to patients.']

['Simultaneous Localisation And Mapping (SLAM) has long been recognised as a core problem to be solved within countless emerging mobile applications that require intelligent interaction or navigation in an environment. Classical solutions to the problem primarily aim at localisation and reconstruction of a geometric 3D model of the scene. More recently, the community increasingly investigates the development of Spatial Artificial Intelligence (Spatial AI), an evolutionary paradigm pursuing a simultaneous recovery of object-level composition and semantic annotations of the recovered 3D model. Several interesting approaches have already been presented, producing object-level maps with both geometric and semantic properties rather than just accurate and robust localisation performance. As such, they require much broader ground truth information for validation purposes. We discuss the structure of the representations and optimisation problems involved in Spatial AI, and propose new synthetic datasets that, for the first time, include accurate ground truth information about the scene composition as well as individual object shapes and poses. We furthermore propose evaluation metrics for all aspects of such joint geometric-semantic representations and apply them to a new semantic SLAM framework. It is our hope that the introduction of these datasets and proper evaluation metrics will be instrumental in the evaluation of current and future Spatial AI systems and as such contribute substantially to the overall research progress on this important topic.', 'SLAM is a core problem to be solved by countless emerging mobile applications that must use navigation and interact with the environment. Classical solutions focus on localising and reconstructing a geometric 3D model of the scene. The evolution of Spatial Artificial Intelligence is pursuing simultaneous recovery of object-level composition and semantic annotations of a recovered 3D model. Several approaches have been presented, such as creating maps that are both Geometric and Semantic, rather than just accurate and robust localisation performance. As such, they need more ground truth information. For the first time we propose synthetic datasets that include accurate ground truth information about the scene composition, as well as individual object shapes We propose to put evaluation metrics on all aspects of joint representations, and then apply them to a new SLAM framework. We want the introduction to these dataset as well as the proper evaluation metrics to be an important part in the evaluation of current and future Spatial Artificial Intelligence Systems.']

['Machine learning (ML) already accelerates discoveries in many scientific fields and is the driver behind several new products. Recently, growing sample sizes enabled the use of ML approaches in larger omics studies. This work provides a guide through a typical analysis of an omics dataset using ML. As an example, this chapter demonstrates how to build a model predicting Drug-Induced Liver Injury based on transcriptomics data contained in the LINCS L1000 dataset. Each section covers best practices and pitfalls starting from data exploration and model training including hyperparameter search to validation and analysis of the final model. The code to reproduce the results is available at https://github.com/Evotec-Bioinformatics/ml-from-omics .', 'Several new products have been created as a result of machine learning. The use of ML approaches in bigger studies was made more possible by the growing sample sizes. This work provides a guide to analyzing an omics datasets The chapter shows how to build a model forecasting Drug-InducedLiver Injury from a transcriptomics data contained in a dataset. The first part is about the data exploration, the second about the model training and the fourth is the analysis of the final model. It is possible to reproduce the results. .com/Evotec-bioinformatics']

['Esophagogastroduodenoscopy (EGD) is generally a safe procedure, but adverse events often occur. This highlights the necessity of the quality control of EGD. Complete visualization and photo documentation of upper gastrointestinal (UGI) tracts are important measures in quality control of EGD. To evaluate these measures in large scale, we developed an AI-driven quality control system for EGD through convolutional neural networks (CNNs) using archived endoscopic images. We retrospectively collected and labeled images from 250 EGD procedures, a total of 2599 images from eight locations of the UGI tract, using the European Society of Gastrointestinal Endoscopy (ESGE) photo documentation methods. The label confirmed by five experts was considered the gold standard. We developed a CNN model for multi-class classification of EGD images to one of the eight locations and binary classification of each EGD procedure based on its completeness. Our CNN model successfully classified the EGD images into one of the eight regions of UGI tracts with 97.58% accuracy, 97.42% sensitivity, 99.66% specificity, 97.50% positive predictive value (PPV), and 99.66% negative predictive value (NPV). Our model classified the completeness of EGD with 89.20% accuracy, 89.20% sensitivity, 100.00% specificity, 100.00% PPV, and 64.94% NPV. We analyzed the credibility of our model using a probability heatmap. We constructed a CNN model that could be used in the quality control of photo documentation in EGD. Our model needs further validation with a large dataset, and we expect our model to help both endoscopists and patients by improving the quality of EGD procedures.', 'Esophagogastroduodenoscopy (EGD) is often associated with adverse events, but is generally a good procedure. The necessity of quality control ofEGD is highlighted. Quality control of EGD can only be achieved by complete visualization and photo documentation. To evaluate these measures in large scale, we built an AI-driven quality control system forEGD through convolutional neural networks. We compared images from 258EGD procedures with images from eight locations of the UGI tract using the European Society of Gastrointestinal Endoscopy (ESGE) photo documentation methods. The gold standard was confirmed by five experts. A CNN model for multi class classification ofEGD images was developed. The CNN model classified theEGD images into eight tracts, with each tract having a certain amount of positive and negative predictive value. The model showed that the completeness of EGD was between 89.0% accuracy, 89.20% sensitivity, 100.0% specificity, 100.1% PVP, and 64.94% NPV. We used the probability heatmap to analyze the credibility of our model. ACNN model could be used for the quality control inEGD. Our model needs to go through further research with a large dataset in order to improve the quality ofEGD procedures.']

['The objective of this study was to investigate the efficacy of an artificial intelligence-assisted 3D planning system (AIHIP) in total hip arthroplasty by direct anterior approach and assess the reliability of the AIHIP preoperative program in terms of both interobserver and intraobserver agreement. A retrospective analysis was conducted on patients who underwent unilateral primary THA via direct anterior approach from June 2019 to March 2022. Participants were randomly assigned to receive either the AIHIP system (n = 220) or the 2D template (control group) (n = 220) for preoperative planning. The primary outcome aimed to evaluate the correspondence between the prosthesis selected intro-operation and the one planned preoperatively, as well as to calculate the intraclass correlation coefficient (ICC). Secondary outcomes included operation time, intraoperative blood loss, fluoroscopy times, Harris hip score (HHS), lower limb length difference (LLD), femoral offset (FO), and bilateral femoral offset difference. No significant differences were observed in gender, age, body mass index (BMI), aetiology, and American Society of Anesthesiologists (ASA) score between the two groups. Both planning methods exhibited good intraobserver agreement for component planning (ICC: 0.941-0.976). Interobserver agreement for component planning was comparable between the two methods (ICC: 0.882-0.929). In the AIHIP group, the accuracy of acetabular cup and femoral stem prosthetics planning significantly improved, with accuracies within the size range of ± 0 and ± 1 being 76.8% and 90.5% and 79.5% and 95.5%, respectively. All differences between two groups were statistically significant (p < 0.05). Patients receiving AIHIP preoperative planning experienced shorter operation times, reduced intraoperative blood loss, fewer fluoroscopy times, and lower leg length discrepancy (LLD) (p < 0.05). Moreover, they demonstrated a higher Harris hip score (HHS) at three days post-surgery (p < 0.05). However, no significant differences were found in femoral offset (FO), difference of bilateral femoral offsets, and HHS at 1 month after the operation. Utilizing AIHIP for preoperative planning of direct anterior approach THA can significantly enhance the accuracy of prosthetic sizing with good reliability, decrease operation time, reduce intraoperative blood loss, and more effectively restore the length of both lower limbs. This approach has greater clinical application value.', 'This study was to investigate the efficacy and reliability of a direct anterior approach to total hip arthroplasty using an artificial intelligence-assisted 3D planning system. There were patients who underwent THA in June/March of this year who were randomly assigned to receive either the 2D template or the AIHIP system The primary outcome was to evaluate the relationship between the prosthesis that was chosen and the one which was planned. The outcomes include operations time, blood loss, FIP times, Harris hip score, and lower limb length difference. There were no significant differences between the two groups in several categories. Both planning methods exhibited agreement for component planning. There was an agreement for component planning between the two methods. The accuracies of acetabULAR cup and femoral stem prosthetics planning were much improved in the AIHIP group. The differences between two groups were statistically significant. Patients who received preoperative planning experienced shorter surgery times, reduced operation times and lower leg length discrepancy. They demonstrated a higher Harris hip score at three days after the surgery. There were no significant differences found in three different areas of the body one month after the operation. The use of Artificial intelligence in surgery can improve the accuracy of replacement limbs, decrease the time taken to operate, reduce the amount of blood loss and increase the length of limbs. This approach has more value in the clinical area.']

['Although many researchers talk about a "patient database," they typically are not referring to a database at all, but instead to a spreadsheet of curated facts about a cohort of patients. This article describes relational database systems and how they differ from spreadsheets. At their core, spreadsheets are only capable of describing one-to-one (1:1) relationships. However, this article demonstrates that clinical medical data encapsulate numerous one-to-many relationships. Consequently, spreadsheets are very inefficient relative to relational database systems, which gracefully manage such data. Databases provide other advantages, in that the data fields are "typed" (that is, they contain specific kinds of data). This prevents users from entering spurious data during data import. Because each record contains a "key," it becomes impossible to add duplicate information (ie, add the same patient twice). Databases store data in very efficient ways, minimizing space and memory requirements on the host system. Likewise, databases can be queried or manipulated using a highly complex language called SQL. Consequently, it becomes trivial to cull large amounts of data from a vast number of data fields on very precise subsets of patients. Databases can be quite large (terabytes or more in size), yet still are highly efficient to query. Consequently, with the explosion of data available in electronic health records and other data sources, databases become increasingly important to contain or order these data. Ultimately, this will enable the clinical researcher to perform artificial intelligence analyses across vast amounts of clinical data in a way heretofore impossible. This article provides initial guidance in terms of creating a relational database system.', 'Although a patient database is used by many researchers, they usually use a spreadsheet of facts about a group of people. The article describes the differences between database systems and spreadsheets. At their core, spreadsheets are just a way of describing relationships. The article illustrates that there are many one-to-many relationships in the clinical medical data. The inefficiency of spreadsheets is due to the fact that they are very inefficient relative to database systems. The data fields are typed, meaning they contain specific kinds of data. Users can not enter spurious data during data import. Due to the fact that each record has a "key", it\'s impossible to add duplicate information to it. Databases store data in efficient ways and help reduce space and memory requirements on the host system. A database can be queried or manipulated using a highly complex language. It becomes fairly simple to remove a lot of data from a lot of fields. Large databases can be very efficient in query. With the explosion of data in electronic health records and other sources, databases are more important to contain or order these data. This will make it much easier to perform artificial intelligence analyses on vast amounts of clinical data. This article gives some initial guidance on creating a database system.']

['Artificial intelligence, represented by deep learning, has received increasing attention in the field of oral and maxillofacial medical imaging, which has been widely studied in image analysis and image quality improvement. This narrative review provides an insight into the following applications of deep learning in oral and maxillofacial imaging: detection, recognition and segmentation of teeth and other anatomical structures, detection and diagnosis of oral and maxillofacial diseases, and forensic personal identification. In addition, the limitations of the studies and the directions for future development are summarized.', 'Artificial intelligence has been studied in the field of oral and maxillofacial medical image analysis and has been found to improve image quality. This narrative review shows how deep learning can be used in detecting and differentiating oral and oral-maxillofacial diseases. The directions for future development are summarized in the limitations of the studies.']

['Transfusion-associated hyperkalemia (TAH) is a potentially life-threatening complication of red blood cell (RBC) transfusion. Previously, we reported features of RBC transfusions from 35 pediatric patients (TAH group) who had hyperkalemia with RBC transfusion in one-year period at four facilities. In this study, we used multivariate analyses and artificial intelligence to compare the TAH group to newly collected control group (non-TAH group) to identify factors associated with TAH occurrence. A review of RBC transfusion with TAH was compared to non-TAH group who did not develop TAH with RBC transfusion at each facility during the same one-year period. The non-TAH group included 12 patients each in 5 age groups. Wilcoxon rank-sum tests recursive feature elimination, least absolute shrinkage, and selection operator (LASSO), and other artificial intelligence techniques were employed to identify the most salient features associated with predicting specific clinical outcomes for TAH occurrence. Pre-transfusion creatinine, comorbidities of kidney and/or liver dysfunctions, and total transfused volume within 12 h (tV-12) per kg and per estimated total blood volume (eTBV) showed statistically significant differences between TAH and non-TAH groups. Multivariate analysis revealed the biggest factor in TAH occurrence was tV-12/kg followed by age of RBC units. The thresholds of risks were tV-12/kg of 30 ml/kg, tV-12/eTBV of 30%, and RBC unit age of 7.95 days. The study findings suggest that the biggest factor on TAH occurrence is tV-12/kg. More importantly, 30% of eTBV transfusion could cause TAH in patients with multiple comorbidities.', 'Transfusion-associated hyperkalemia, also known as Tah, can be a potentially life-threatening result of red blood cell tranfusion. 35 children had hyperkalemia with a one-year period of four facilities when they received RBC transfusions, and we reported features of their treatment. In this study, we compared the TAH group to a non-TAH group to find factors related to the occurrence of TAH. A non-TAH group who did not have it at the same facilities during the one-year period, were compared to a review of the group who did have it. There were 12 patients in each of the 5 age groups in the non-Tah group. Wilcoxon rank-sum tests and other artificial intelligence techniques were used to identify features associated with predicting TAH occurrence. The pre-transfusion creatinine, comorbidities of the two organs, and total transfused volume within a twelve hour period showed significant differences between TAH and non TAH groups. The most influential factor was tV-12/ kilogram followed by age of the units. The threshold of risk were tV-12/L of 30 kilogram, tV-12/eTBV 30% and the age of the unit at 7.90 days. The study concluded that tV-12/kg is the main factor in TAH occurrence. 30% of eTBV could cause TAH in patients with more than one condition.']

['We present a new artificial intelligence-powered method to predict 3-year hepatocellular carcinoma (HCC) recurrence by analysing the radiomic profile of contrast-enhanced CT (CECT) images that was validated in patient cohorts. This retrospective cohort study of 224 HCC patients with follow-up for at least 3 years was performed at a single centre from 2012 to 2019. Two groups of radiomic signatures were extracted from the arterial and portal venous phases of pre-operative CECT. Then, the radiological model (RM), deep learning-based radiomics model (DLRM), and clinical & deep learning-based radiomics model (CDLRM) were established and validated in the area under curve (AUC), calibration curve, and clinical decision curve. Comparison of the clinical baseline variables between the non-recurrence (<i>n</i> = 109) and recurrence group (<i>n</i> = 115), three clinical independent factors (Barcelona Clinic Liver Cancer staging, microvascular invasion, and α-fetoprotein) were incorporated into DLRM for the CDLRM construction. Among the 30 radiomic features most crucial to the 3 year recurrence rate, the selection from deep learning-based radiomics (DLR) features depends on CECT. through the Gini index. In most cases, CDLRM has shown superior accuracy and distinguished performance than DLRM and RM, with the 0.98 AUC in the training cohorts and 0.83 in the testing. This study proposed that DLR-based CDLRM construction would be allowed for the predictive utility of 3-year recurrence outcomes of HCCs, providing high-risk patients with an effective and non-invasive method to possess extra clinical intervention. This study has highlighted the predictive value of DLR in the 3-year recurrence rate of HCC.', "An artificial intelligence-powered method was used to predict the recurrence of hepatocellular carcinoma in a patient cohort, using the radiomic profile of contrast-enhanced CT images. There were 224 HCC patients who had follow up for at least 3 years. The AUC, calibration curve, and clinical decision Curve were established and validated using the radiological model, deep learning-based radiomics model, and clinical & deep learning-based radiomics model. There is a comparison between the baseline variables for the non-recurrence and recurrence groups. The 30 radiomic features most important to the 3 year recurrence rate depend on CECT. Through the Gini index. In the testing, it has been shown that the performance of CDLRM has been better than the results of the training and testing. The study proposed that an effective and non-invasive way to provide additional clinical interventions could be used for high-risk patients. The study highlighted the correlation of DLR's to the likelihood of a re-occurrence of the disease."]

['The purpose of this study was to improve the differentiation between malignant and benign thyroid nodules using deep learning (DL) in category 4 and 5 based on the Thyroid Imaging Reporting and Data System (TI-RADS, TR) from the American College of Radiology (ACR). From June 2, 2017 to April 23, 2019, 2082 thyroid ultrasound images from 1396 consecutive patients with confirmed pathology were retrospectively collected, of which 1289 nodules were category 4 (TR4) and 793 nodules were category 5 (TR5). Ninety percent of the B-mode ultrasound images were applied for training and validation, and the residual 10% and an independent external dataset for testing purpose by three different deep learning algorithms. In the independent test set, the DL algorithm of best performance got an AUC of 0.904, 0.845, 0.829 in TR4, TR5, and TR4&5, respectively. The sensitivity and specificity of the optimal model was 0.829, 0.831 on TR4, 0.846, 0.778 on TR5, 0.790, 0.779 on TR4&5, versus the radiologists of 0.686 (<i>P</i>=0.108), 0.766 (<i>P</i>=0.101), 0.677 (<i>P</i>=0.211), 0.750 (<i>P</i>=0.128), and 0.680 (<i>P</i>=0.023), 0.761 (<i>P</i>=0.530), respectively. The study demonstrated that DL could improve the differentiation of malignant from benign thyroid nodules and had significant potential for clinical application on TR4 and TR5.', 'The purpose of the study was to improve the identification of benign and malignant thyroid nodule using deep learning. Of the 988 consecutive patients with confirmed pathology who had had their thyroid waves collected from June 2 to April 23, 2082 were classified as either category 4 or category 5. Ninety percent of the B-mode images were applied for validation in training and an independent external dataset was used for testing. The best performance achieved in the test set was a AUC of 0.904., 0.845, and 0.829 in TR4, TR5 and TR4&5 respectively. The specificity and sensitivity of theoptimal model was close to that of radiologists at 0.766 and 0.686.']

['With the rapid development of science and technology, artificial intelligence (AI) has been widely used in the diagnosis and prognosis of various spine diseases. It has been proved that AI has a broad prospect in accurate diagnosis and treatment of spine disorders. On May 7, 2022, the Web of Science (WOS) Core Collection database was used to identify the documents on the application of AI in the field of spine care. HistCite and VOSviewer were used for citation analysis and visualization mapping. A total of 693 documents were included in the final analysis. The most prolific authors were Karhade A.V. and Schwab J.H. United States was the most productive country. The leading journal was <i>Spine</i>. The most frequently used keyword was spinal. The most prolific institution was Northwestern University in Illinois, USA. Network visualization map showed that United States was the largest network of international cooperation. The keyword "machine learning" had the strongest total link strengths (TLS) and largest number of occurrences. The latest trends suggest that AI for the diagnosis of spine diseases may receive widespread attention in the future. AI has a wide range of application in the field of spine care, and an increasing number of scholars are committed to research on the use of AI in the field of spine care. Bibliometric analysis in the field of AI and spine provides an overall perspective, and the appreciation and research of these influential publications are useful for future research.', 'Artificial intelligence has been very useful in diagnosis and prediction of spine diseases due to the rapid development of technology. It\'s proven that Artificial intelligence can be used to diagnosis and treat spine disorders. The documents were identified using the website\'s Core Collection database in the field of spine care. The visualization mapping was done with HistCite and VOSviewer. There were 693 documents included in the final analysis. Karhade A was the most prolific author. V. and Schwab J. H. The United States was most productive. Spine was the journal with the leading journal. The most used word was spinal. The most prolific school in this country was the University of Illinois. The United States was the largest network of international cooperation. "Machine learning" got the best total link strengths and the biggest number of occurrences. The current trends suggest that there will likely be a lot more attention given to the use of Artificial Intelligence for the diagnosis and treatment of spine diseases in the future. An increasing number of scholars aim to research the use of artificial intelligence in the field of spine care An appreciation and research of influential publications in the field of spine and artificial intelligence are useful for future research.']

["Artificial intelligence (AI) has a considerable present and future influence on healthcare. Nurses, representing the largest proportion of healthcare workers, are set to immensely benefit from this technology. However, the overall adoption of new technologies by nurses is quite slow, and the use of AI in nursing is considered to be in its infancy. The current literature on AI in nursing lacks conceptual clarity and consensus, which is affecting clinical practice, research activities, and theory development. Therefore, to set the foundations for nursing AI knowledge development, the purpose of this concept analysis is to clarify the conceptual components of AI in nursing and to determine its conceptual maturity. A concept analysis following Morse's approach was conducted, which examined definitions, characteristics, preconditions, outcomes, and boundaries on the state of AI in nursing. A total of 18 quantitative, qualitative, mixed-methods, and reviews related to AI in nursing were retrieved from the CINAHL and EMBASE databases using a Boolean search. Presently, the concept of AI in nursing is immature. The characteristics and preconditions of the use of AI in nursing are mixed between and within each other. The preconditions and outcomes on the use of AI in nursing are diverse and indiscriminately reported. As for boundaries, they can be more distinguished between robots, sensors, and clinical decision support systems, but these lines can become more blurred in the future. As of 2021, the use of AI in nursing holds much promise for the profession, but conceptual and theoretical issues remain.", 'Artificial intelligence has a large influence on healthcare. This technology will greatly benefit nurses, representing the largest proportion of healthcare workers. The use of artificial intelligence in nursing is in its infancy, and the adoption of new technology by the nurses is slow. The current literature on artificial intelligence in nursing lacks conceptual clarity and consensus which is impacting clinical practice, research activities, and theory development. To clarify the concept components of artificial intelligence in nursing and to determine their conceptual maturity is the purpose of the concept analysis. The state of Artificial Intelligence in nursing was studied in a concept analysis, using definitions, characteristics, preconditions, outcomes, and boundaries. A number of quantitative, qualitative, mixed-methods and reviews related to Artificial Intelligence was found in the EMBASE and CINAHL databases. The concept of automation is immature in nursing. There are characteristics and preconditions that are different in nursing. There are many preconditions and outcomes reported on the use of Artificial Intelligence in nursing. The boundaries can be more understood between various technologies, but the lines may become blurred in the future. The use of artificial Intelligence in nursing will lead to much promise, but not much.']

['Gerontechnology based on Artificial Intelligence (AI) is expected to fulfill the promise of the so-called 4p-medicine and enable a predictive, personalized, preventive, and participatory elderly care. Although empirical evidence shows positive health outcomes, commentators are concerned that AI-based gerontechnology could bring along the disruption of elderly care. A systematic conceptualization of these concerns is lacking. In this paper, such a conceptualization is suggested by analyzing the risks of AI in elderly care as "4d-risks": the depersonalization of care through algorithm-based standardization, the discrimination of minority groups through generalization, the dehumanization of the care relationship through automatization, and the disciplination of users through monitoring and surveillance. Based on the 4d-model, strategies for a patient-centered AI in elderly care are outlined. Whether AI-based gerontechnology will actualize the 4p-perspective or bring about the 4d-scenario depends on whether joint efforts of users, caregivers, care providers, engineers, and policy makers will be made.', 'Geron technology based on Artificial Intelligence is going to allow for elderly care to be tailored and preventive. Despite empirical evidence showing positive health outcomes, commentators are concerned that artificial intelligence may bring along the disruption of elderly care. There is not a systematic conceptualization of these concerns. The paper states that the risks of technology in care are four-pronged and include: the depersonalization of care through computers, the discrimination of minority groups, and the dehumanization of care. Strategies for a patient-centered artificial intelligence in elderly care are outlined based on the 4d-model. Whether users, caregivers, care providers, engineers, policy makers, and other stakeholders will be involved in the development of the 4d-scenario depends on whether or not there will be joint efforts by users, caregivers, care providers, engineers, and policy makers.']

['characterize the scientific production of Brazil and Spain in regard to methodological aspects and aspects of health-related quality of life experienced by cancer patients receiving chemotherapy in both countries. integrative literature review was conducted using the following databases: CINAHL, MEDLINE, SCOPUS and CUIDEN and the electronic libraries PubMed and SciELO, conducted in September 2013. a total of 28 papers met the inclusion criteria. The synthesis of knowledge was presented in three categories of analysis: assessment of quality of life in different types of cancer; sociodemographic factors that influenced quality of life; and type of cancer and interventions that improve quality of life. Chemotherapy affects health-related quality of life and the most important factors were: age, sex, chemotherapy protocol, type of surgery, stage of the disease, educational level, and emotional intelligence. Complementary therapies such as acupuncture, guided visualization, prayers and exercise were positive and reduced side effects. the results showed a poor level of evidence, since 86% of the studies were cross-sectional descriptive studies; the instrument most frequently used to measure health-related quality of life was EORTC QLQ C-30 and more studies were conducted in Brazil than in Spain.', 'The Brazilian and Spanish scientific production of research related to aspects of health related quality of life experienced by cancer patients receiving chemotherapy. The reviews were conducted using various databases and electronic libraries. The analysis of knowledge includes assessment of life in different types of cancer, sociodemographic factors that influenced it, and the type of cancer and interventions that improve it. The most important factors were age, sex, cancer stage, type of surgery, educational level, and emotional intelligence. The negative side effects of some therapies were greatly reduced. Most of the studies were cross-sectional descriptive and the most frequently used instrument was EORTC qualquc C30 compared to Spain.']

['Artificial intelligence (AI) and in particular radiomics has opened new horizons by extracting data from medical imaging that could be used not only to improve diagnostic accuracy, but also to be included in predictive models contributing to treatment stratification of cancer. Head and neck cancers (HNC) are associated with higher recurrence rates, especially in advanced stages of disease. It is considered that approximately 50% of cases will evolve with loco-regional recurrence, even if they will benefit from a current standard treatment consisting of definitive chemo-radiotherapy. Radiotherapy, the cornerstone treatment in locally advanced HNC, could be delivered either by the simultaneous integrated boost (SIB) technique or by the sequential boost technique, the decision often being a subjective one. The principles of radiobiology could be the basis of an optimal decision between the two methods of radiation dose delivery, but the heterogeneity of HNC radio-sensitivity makes this approach difficult. Radiomics has demonstrated the ability to non-invasively predict radio-sensitivity and the risk of relapse in HNC. Tumor heterogeneity evaluated with radiomics, the inclusion of coarseness, entropy and other first order features extracted from gross tumor volume (GTV) in multivariate models could identify pre-treatment cases that will benefit from one of the approaches (SIB or sequential boost radio-chemotherapy) considered the current standard of care for locally advanced HNC. Computer tomography (CT) simulation and daily cone beam CT (CBCT) could be chosen as imaging source for radiomic analysis.', 'Radiomics and artificial intelligence have allowed the creation of new ways of combining medical data with artificial intelligence, to derive data for improved diagnostic accuracy as well as being included in diagnostic models that will contribute to treatment of cancer. There is a higher chance of a head and neck cancer coming back in advanced stages. Even if a current standard treatment consisting of definitive chemo-radiotherapy can be used in the majority of cases, loco-regional recurrence may evolve if that is the case. The simultaneous integrated boost technique and the sequential boost technique could both be used to deliver radiotherapy in HNC. The principles of radiobiology are an optimal decision between the two methods of delivery of radiation dose, but heterogeneity of HNC radio-sensitivity makes this approach difficult. The risk of relapse in HNC can be predicted non-invasively by Radiomics. Tumor heterogeneity evaluated with radiomics could be used to identify pre-treatment cases that will benefit from a radio-chemotherapy strategy. Computer tomography simulation and daily cone beamCT are available for use for analysis of radionuclide.']

['Anaplastic lymphoma kinase (ALK) rearrangement status examination has been widely used in clinic for non-small cell lung cancer (NSCLC) patients in order to find patients that can be treated with targeted ALK inhibitors. This study intended to non-invasively predict the ALK rearrangement status in lung adenocarcinomas by developing a machine learning model that combines PET/CT radiomic features and clinical characteristics. Five hundred twenty-six patients of lung adenocarcinoma with PET/CT scan examination were enrolled, including 109 positive and 417 negative patients for ALK rearrangements from February 2016 to March 2019. The Artificial Intelligence Kit software was used to extract radiomic features of PET/CT images. The maximum relevance minimum redundancy (mRMR) and least absolute shrinkage and selection operator (LASSO) logistic regression were further employed to select the most distinguishable radiomic features to construct predictive models. The mRMR is a feature selection method, which selects the features with high correlation to the pathological results (maximum correlation), meanwhile retain the features with minimum correlation between them (minimum redundancy). LASSO is a statistical formula whose main purpose is the feature selection and regularization of data model. LASSO method regularizes model parameters by shrinking the regression coefficients, reducing some of them to zero. The feature selection phase occurs after the shrinkage, where every non-zero value is selected to be used in the model. Receiver operating characteristic (ROC) analysis was used to evaluate the performance of the models, and the performance of different models was compared by the DeLong test. A total of 22 radiomic features were extracted from PET/CT images for constructing the PET/CT radiomic model, and majority of these features used were based on CT features (20 out of 22), only 2 PET features were included (PET percentile 10 and PET difference entropy). Moreover, three clinical features associated with ALK mutation (age, burr and pleural effusion) were also employed to construct a combined model of PET/CT and clinical model. We found that this combined model PET/CT-clinical model has a significant advantage to predict the ALK mutation status in the training group (AUC = 0.87) and the testing group (AUC = 0.88) compared with the clinical model alone in the training group (AUC = 0.76) and the testing group (AUC = 0.74) respectively. However, there is no significant difference between the combined model and PET/CT radiomic model. This study demonstrated that PET/CT radiomics-based machine learning model has potential to be used as a non-invasive diagnostic method to help diagnose ALK mutation status for lung adenocarcinoma patients in the clinic.', 'The status examination of the ALK is used in clinics to find patients with ALK tumors that can be treated with a targeted treatment. The aim of the study is to predict the rearrangement status of lung adenocarcinomas with a machine learning model. There were patients with positive and negative scans from February 2016 to March 2019. The best way to pick radiomic features for models is by using a regression to select the most distinguishable features. The mR is a feature selection method, which picks features with high correlation to the results, and then retains them with minimum correlation between them. LASSO is a statistical formula and is used for feature selection and regularization of the data model. The LASSO method regularizes model parameters by decreasing the regression coefficients. The feature selection phase happens after the model has been created. The performance of the models was evaluated by using ROC analysis and the performance of different models was compared by the De Long test. A number of radiomic features that were taken from images of the PET/CT were used to construct the model, however a majority of these features were based on the CT images. The clinical features associated with the ALK variation were used to create a model. The training group has a significant advantage over the testing group when it comes to predicting the ALK status. There is no noticeable difference between the two models. The findings show that a machine learning model can be utilized to help diagnose lung adenocarcinoma patients in a clinic.']

['This study delves into the intricate synergy between teacher mindfulness and the quality of teacher-student relationships, with a specific and deliberate focus on the mediating influence of emotional intelligence. The way teachers engage with their students not only impacts the learning outcomes but also contributes significantly to the overall classroom atmosphere. Understanding the underlying mechanisms that drive these relationships is crucial for educators and policymakers alike. This research seeks to shed light on these critical dynamics. To investigate this multifaceted interplay, a participant pool of 369 Chinese English teachers was assembled. The research employed a comprehensive approach to data collection, utilizing self-report questionnaires completed by the instructors. Structural equation modeling, a robust statistical technique, was employed to rigorously analyze the collected data. The data analysis unveiled a robust and direct association between teacher mindfulness and the quality of teacher-student relationships. Beyond this primary link, a noteworthy revelation emerged: emotional intelligence, as measured through our analysis, was identified as a pivotal mediating factor in this relationship. This finding highlights the intricate web of emotions, awareness, and interpersonal interactions that underpin effective teaching and positive teacher-student relationships. These significant findings underscore the critical roles of teacher mindfulness and emotional intelligence in shaping the educational landscape. The implications of this study reach far beyond academia, extending to the development of tailored educational interventions and support strategies.', "The study looked at how teacherMindfulness and the relationship between students and their teachers affect the quality of their relationship. The way teachers interact with their students can impact their learning outcomes and contribute to the overall classroom atmosphere. Understanding the underlying mechanisms that drive relationships is important. This research wants to shed light on some of these dynamics. A pool of 369 Chinese English teachers was assembled to investigate this interplay. The research used questionnaires to collect data, as well as a comprehensive approach to data collection. Structural equation modeling was used to analyze the data. The data analysis shows a correlation between the quality of teacher-student relationships and teacher awareness. Through our analysis, it was found that emotional intelligence was a key factor in this relationship. A finding shows that a positive teacher-student relationship is dependent upon emotion, awareness, and touch. The critical roles of teacher mental clarity and emotional intelligence in the educational landscape has been underscored by the findings. The study's implications are far beyond the confines of academia."]

["What is the intra- and inter-centre reliability in embryo grading performed according to the Istanbul Consensus across several IVF clinics? Forty Day 3 embryos and 40 blastocysts were photographed on three focal planes. Senior and junior embryologists from 65 clinics were invited to grade them according to the Istanbul Consensus (Study Phase I). All participants then attended interactive training where a panel of experts graded the same embryos (Study Phase II). Finally, a second set of pictures was sent to both embryologists and experts for a blinded evaluation (Study Phase III). Intra-centre reliability was reported for Study Phase I as Cohen's kappa between senior and junior embryologists; inter-centre reliability was instead calculated between senior/junior embryologists and experts in Study Phase I versus III to outline improvements after training (i.e. upgrade of Cohen's kappa category according to Landis and Koch). Thirty-six embryologists from 18 centres participated (28% participation rate). The intra-centre reliability was (i) substantial (0.63) for blastomere symmetry (range -0.02 to 1.0), (ii) substantial (0.72) for fragmentation (range 0.29-1.0), (iii) substantial (0.66) for blastocyst expansion (range 0.19-1.0), (iv) moderate (0.59) for inner cell mass quality (range 0.07-0.92), (v) moderate (0.56) for trophectoderm quality (range 0.01-0.97). The inter-centre reliability showed an overall improvement from Study Phase I to III, from fair (0.21-0.4) to moderate (0.41-0.6) for all parameters under analysis, except for blastomere fragmentation among senior embryologists, which was already moderate before training. Intra-centre reliability was generally moderate/substantial, while inter-centre reliability was just fair. The interactive training improved it to moderate, hence this workflow was deemed helpful. The establishment of external quality assessment services (e.g. UK NEQAS) and the avant-garde of artificial intelligence might further improve the reliability of this key practice for embryo selection.", "How reliable is embryo grading done in the multiple clinics that use the Istanbul Consensus? The Istanbul Consensus (study phase I) is used to grade embryologists from 65 clinics. The panel of experts graded the same embryos, and all participants attended interactive training. A second set of pictures were sent to experts to be blinded. The reliability between senior and junior embryologist was reported as Cohen's Ku Klux; however, the reliability between junior and senior embryologists was reported as inter-centre reliability. Is that what it is? Cohen's category has been upgraded according to the two men. The 33 embryologists that took part had a 20% participation rate. The reliability was found to be (i) substantial for blastomere symmetry, (ii) substantial for fragmenting, and (iv) substantial for blastocyst expansion. The inter- centre reliability showed an overall improvement from the Study Phase I to III from fair to moderate, but only for blastomere of senior embryologists, which was already moderate before training. Inter-centre reliability was fairly stable, while the inter-centre reliability was moderate. The interactive training improved it to moderate. The establishment of external quality assessment services g Artificial intelligence and UK NEQAS might make this practice better for embryo selection."]

['The indoor location-based control system estimates the indoor position of a user to provide the service he/she requires. The major elements involved in the system are the localization server, service-provision client, user application positioning technology. The localization server controls access of terminal devices (e.g., Smart Phones and other wireless devices) to determine their locations within a specified space first and then the service-provision client initiates required services such as indoor navigation and monitoring/surveillance. The user application provides necessary data to let the server to localize the devices or allow the user to receive various services from the client. The major technological elements involved in this system are indoor space partition method, Bluetooth 4.0, RSSI (Received Signal Strength Indication) and trilateration. The system also employs the BLE communication technology when determining the position of the user in an indoor space. The position information obtained is then used to control a specific device(s). These technologies are fundamental in achieving a "Smart Living". An indoor location-based control system that provides services by estimating user\'s indoor locations has been implemented in this study (First scenario). The algorithm introduced in this study (Second scenario) is effective in extracting valid samples from the RSSI dataset but has it has some drawbacks as well. Although we used a range-average algorithm that measures the shortest distance, there are some limitations because the measurement results depend on the sample size and the sample efficiency depends on sampling speeds and environmental changes. However, the Bluetooth system can be implemented at a relatively low cost so that once the problem of precision is solved, it can be applied to various fields.', 'The indoor control system estimates the indoor position of a person so that they can provide the service they need. The localization server is one of the major elements involved in the system. The server that controls access is located in Canada. G. It is possible to determine their locations within a specified space and then the service-provision client steps in to initiate required services such as indoor navigation and monitoring/surveillance. The user application needs to give the server the necessary data to localize the devices, or give the user the option to receive services from the client. The major technological elements are the inside space partition method, RSSI, and trilateration. The system uses BLE communication technology to figure out a user\'s position indoors. The position information is used to control the device. This is a "Smart Living" and these technologies are fundamental. An indoor location-based control system that estimates user\'s indoor locations has been used in the study. The second scenario introduces an experimental method for getting valid samples from the RSSI dataset but does it work well or not? There are some limitations to the method because the results depend on sample size and sampling speed The Bluetooth system can be implemented at a low price in order to have it apply to some fields once the issue of precision is solved.']

["Model card reports aim to provide informative and transparent description of machine learning models to stakeholders. This report document is of interest to the National Institutes of Health's Bridge2AI initiative to address the FAIR challenges with artificial intelligence-based machine learning models for biomedical research. We present our early undertaking in developing an ontology for capturing the conceptual-level information embedded in model card reports. Sourcing from existing ontologies and developing the core framework, we generated the Model Card Report Ontology. Our development efforts yielded an OWL2-based artifact that represents and formalizes model card report information. The current release of this ontology utilizes standard concepts and properties from OBO Foundry ontologies. Also, the software reasoner indicated no logical inconsistencies with the ontology. With sample model cards of machine learning models for bioinformatics research (HIV social networks and adverse outcome prediction for stent implantation), we showed the coverage and usefulness of our model in transforming static model card reports to a computable format for machine-based processing. The benefit of our work is that it utilizes expansive and standard terminologies and scientific rigor promoted by biomedical ontologists, as well as, generating an avenue to make model cards machine-readable using semantic web technology. Our future goal is to assess the veracity of our model and later expand the model to include additional concepts to address terminological gaps. We discuss tools and software that will utilize our ontology for potential application services.", 'Model cards are designed to provide a description of machine learning models. The Bridge2ai initiative is interested in the report document to address the FAIR challenges with artificial intelligence. We demonstrate our early work on a database for taking the conceptual-level information in model card reports. The Model Card Report Ontology was created from existing ontologies and core framework. An OWL2-based artifact was developed by our development efforts. OBO Foundry ontology standard concepts and properties are used in the current release. The software reasoner did not indicate any logical inconsistencies. The coverage and usefulness of our model was demonstrated with sample model cards, which were useful in transforming static model card reports to a computrable format for machine-based processing. The benefit of our work is that it uses a variety of terms as well as the standard scientific rigor, and as a result it can be used to make model cards machine-readable using semantic web technology. Our future goal is to expand the model to address terminological gaps once we know the truth of it. We discuss the tools and software that will use our ontology.']

["Tuberculosis (TB) was the leading infectious cause of mortality globally prior to COVID-19 and chest radiography has an important role in the detection, and subsequent diagnosis, of patients with this disease. The conventional experts reading has substantial within- and between-observer variability, indicating poor reliability of human readers. Substantial efforts have been made in utilizing various artificial intelligence-based algorithms to address the limitations of human reading of chest radiographs for diagnosing TB. This systematic literature review (SLR) aims to assess the performance of machine learning (ML) and deep learning (DL) in the detection of TB using chest radiography (chest x-ray [CXR]). In conducting and reporting the SLR, we followed the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines. A total of 309 records were identified from Scopus, PubMed, and IEEE (Institute of Electrical and Electronics Engineers) databases. We independently screened, reviewed, and assessed all available records and included 47 studies that met the inclusion criteria in this SLR. We also performed the risk of bias assessment using Quality Assessment of Diagnostic Accuracy Studies version 2 (QUADAS-2) and meta-analysis of 10 included studies that provided confusion matrix results. Various CXR data sets have been used in the included studies, with 2 of the most popular ones being Montgomery County (n=29) and Shenzhen (n=36) data sets. DL (n=34) was more commonly used than ML (n=7) in the included studies. Most studies used human radiologist's report as the reference standard. Support vector machine (n=5), k-nearest neighbors (n=3), and random forest (n=2) were the most popular ML approaches. Meanwhile, convolutional neural networks were the most commonly used DL techniques, with the 4 most popular applications being ResNet-50 (n=11), VGG-16 (n=8), VGG-19 (n=7), and AlexNet (n=6). Four performance metrics were popularly used, namely, accuracy (n=35), area under the curve (AUC; n=34), sensitivity (n=27), and specificity (n=23). In terms of the performance results, ML showed higher accuracy (mean ~93.71%) and sensitivity (mean ~92.55%), while on average DL models achieved better AUC (mean ~92.12%) and specificity (mean ~91.54%). Based on data from 10 studies that provided confusion matrix results, we estimated the pooled sensitivity and specificity of ML and DL methods to be 0.9857 (95% CI 0.9477-1.00) and 0.9805 (95% CI 0.9255-1.00), respectively. From the risk of bias assessment, 17 studies were regarded as having unclear risks for the reference standard aspect and 6 studies were regarded as having unclear risks for the flow and timing aspect. Only 2 included studies had built applications based on the proposed solutions. Findings from this SLR confirm the high potential of both ML and DL for TB detection using CXR. Future studies need to pay a close attention on 2 aspects of risk of bias, namely, the reference standard and the flow and timing aspects. PROSPERO CRD42021277155; https://www.crd.york.ac.uk/prospero/display_record.php?RecordID=277155.", "Tuberculosis was the leading infectious cause of mortality prior to COVID-19 and chest radiography has an important role to play. The reliability of human readers is shown by the substantial variability in the experts' reading. Several artificial intelligence-based methods have been utilized to address the limitations of human reading of chest radiographs. This systematic literature review is intended to evaluate the ability of machine learning and deep learning to detect lung disease using chest x-ray. We followed the guidelines for reporting about the SLR. The records were identified through some of the world's largest databases. 47 studies that met inclusion criteria were included in the records we independently screened, reviewed, and assessed. The risk of bias was performed using Quality Assessment of Diagnostic Accuracy Studies version two. Montgomery County and Shenzhen have both been used in both the included studies and the best-selling data sets. More than one in seven studies claimed to have used the option of using the option of ldl. Most studies use the human radiologist's report as the standard. The most popular approaches to ML were the support machine, k-nearest neighbors, and random forest. The four most popular applications were ResNet-50, VGG-16, VGG-19 and AlexNet. Four performance metrics were used to measure performance. In the performance results, the accuracy and sensitivity of the machines was higher than that of the models. We used data from 10 studies to estimate the pooled sensitivity and specificity of the techniques. The 17 studies that were thought of having unclear risks for the bias assessment were grouped together as part of the flow and timing aspect. Two studies have built applications based on the solutions. Findings from the SLR confirm the high potential of both lm and dll in detection of Tuberculosis. Future studies should pay close attention to the reference standard, and the flow and timing aspects. PROSPERO crd It's york. A. The display record for UK."]

['Conditional survival (CS) provides dynamic prognostic estimates by considering the patients existing survival time. Since CS for endemic nasopharyngeal carcinoma (NPC) is lacking, we aimed to assess the CS of endemic NPC and establish a web-based calculator to predict individualized, conditional site-specific recurrence risk. Using an NPC-specific database with a big-data intelligence platform, 10,058 endemic patients with non-metastatic stage I-IVA NPC receiving intensity-modulated radiotherapy with or without chemotherapy between April 2009 and December 2015 were investigated. Crude CS estimates of conditional overall survival (COS), conditional disease-free survival (CDFS), conditional locoregional relapse-free survival (CLRRFS), conditional distant metastasis-free survival (CDMFS), and conditional NPC-specific survival (CNPC-SS) were calculated. Covariate-adjusted CS estimates were generated using inverse probability weighting. A prediction model was established using competing risk models and was externally validated with an independent, non-metastatic stage I-IVA NPC cohort undergoing intensity-modulated radiotherapy with or without chemotherapy (n = 601) at another institution. The median follow-up of the primary cohort was 67.2 months. The 5-year COS, CDFS, CLRRFS, CDMFS, and CNPC-SS increased from 86.2%, 78.1%, 89.8%, 87.3%, and 87.6% at diagnosis to 87.3%, 87.7%, 94.4%, 96.0%, and 90.1%, respectively, for an existing survival time of 3 years since diagnosis. Differences in CS estimates between prognostic factor subgroups of each endpoint were noticeable at diagnosis but diminished with time, whereas an ever-increasing disparity in CS between different age subgroups was observed over time. Notably, the prognoses of patients that were poor at diagnosis improved greatly as patients survived longer. For individualized CS predictions, we developed a web-based model to estimate the conditional risk of local (C-index, 0.656), regional (0.667), bone (0.742), lung (0.681), and liver (0.711) recurrence, which significantly outperformed the current staging system (P < 0.001). The performance of this web-based model was further validated using an external validation cohort (median follow-up, 61.3 months), with C-indices of 0.672, 0.736, 0.754, 0.663, and 0.721, respectively. We characterized the CS of endemic NPC in the largest cohort to date. Moreover, we established a web-based calculator to predict the CS of site-specific recurrence, which may help to tailor individualized, risk-based, time-adapted follow-up strategies.', "Predictive estimates can be provided by looking at the patients existing survival time. We decided to establish a web-based calculator to predict individualized site-specific risks after learning that there wasn't an instrument to assess the degree of suppression in endemic nasopharyngeal carcinoma. There were 10,178 endemic patients who were receiving intensity-modulated radiation between April 9 and December 29th. There were estimates of disease-free survival, loco regional relapse-freesurvival, distant metastasis-free survival, and CNP-SS. inverse probability weighted covariate-adjusted forecasts were generated. A prediction model was established and was validation with an independent, non- metastatic, stage I-iva NPC cohort who were undergoing intensity-modulated radiotherapy. The primary cohort had a median follow-up of over sixty months. The 5-year COS grew from 86.3% to 88.7%, 89.8% to 87.3%, and 87.6% at diagnosis. The difference in estimates of prognostic factor subgroups of each endpoint were quite visible at diagnosis and lessened over time, but the disparity in estimate between different age groups was more pronounced. As patients lived longer, the prognoss of poor patients improved greatly. We have created a model that uses the internet to estimate conditional risk, which has a much higher chance of being accurate than the current staging system. An external validation cohort of 61.3 months yielded a mean of 0.672, 0.736, 0.754, and 0.721, which were the C-indices for the performance of this web-based model. The largest cohort to date has characterized the central nervous system of the endemic NPC. A web-based calculator to predict the CS of site-specific recurrence has been created."]

["ChatGPT has sparked extensive discussions within the healthcare community since its November 2022 release. However, potential applications in the field of psychiatry have received limited attention. Deep learning has proven beneficial to psychiatry, and GPT is a powerful deep learning-based language model with immense potential for this field. Despite the convenience of ChatGPT, this advanced chatbot currently has limited practical applications in psychiatry. It may be used to support psychiatrists in routine tasks such as completing medical records, facilitating communications between clinicians and with patients, polishing academic writings and presentations, and programming and performing analyses for research. The current training and application of ChatGPT require using appropriate prompts to maximize appropriate outputs and minimize deleterious inaccuracies and phantom errors. Moreover, future GPT advances that incorporate empathy, emotion recognition, personality assessment, and detection of mental health warning signs are essential for its effective integration into psychiatric care. In the near future, developing a fully-automated psychotherapy system trained for expert communication (such as psychotherapy verbatim) is conceivable by building on foundational GPT technology. This dream system should integrate practical 'real world' inputs and friendly AI user and patient interfaces via clinically validated algorithms, voice comprehension/generation modules, and emotion discrimination algorithms based on facial expressions and physiological inputs from wearable devices. In addition to the technology challenges, we believe it is critical to establish generally accepted ethical standards for applying ChatGPT-related tools in all mental healthcare environments, including telemedicine and academic/training settings.", 'Since the release of the film in November of last year, there have been discussions in the healthcare community. Some of the applications in the field of psychiatry have been overlooked. GPT is a powerful deep learning based language model that has the potential to change the way we see Psychiatry. This advanced machine is a limited use for psychiatry despite the convenience of the system. It may be used to support psychiatrists in a variety of tasks such as completing medical records, supporting communications between clinicians and patients and polishing academic writings. The current application and training require appropriate Prompts to maximize appropriate outputs and minimize errors. Enhancing emotional and mental health assessment are essential for integration into mental health care. Building on GPT technology is possible to develop a fully-automated psychotherapy system that can be trained for expert communication. A dream system should integrate real world inputs and user interface via clinically-proven tools, as well as the use of facial expressions and heartbeats from mobile devices. We believe that it is important to put in place generally accepted ethical standards for using ChatGTP related tools in mental healthcare environments']

['The pedagogical beliefs (e.g., beliefs or "mindsets" concerning the malleability of intelligence) that teachers hold may have a far-reaching impact on their teaching behavior. In general, two basic mindsets can be distinguished with regard to the malleability of intelligence: fixed (entity) and growth (incremental). In this article, we present two studies investigating the associations between teachers\' mindset and (1) their appraisal of students\' achievements and (2) the feedback they provide. Study 1 focuses on the associations between mindset and appraisal. The findings reveal an association between growth mindset and the appraisal of increasing student achievements. Study 2 investigates the impact of teachers\' mindset on the amount and type of oral feedback they provide to their students. Contrarily to expectations, the findings reveal a significant negative correlation between mindset and the amount of feedback.', "The beliefs about teaching g Some teachers hold beliefs about the malleability of intelligence that could affect their teaching behavior. There are two basic mindsets that are distinguished with regard to the malleability of intelligence. In this article, we investigate the relationship between teachers' mindset and their appraisal of the students' achievements and the feedback they provide. The studies focuses on the associations between mindset and appraisal. The findings show that there is an association between growth mindset and student achievements. Study 2 tries to figure out the impact of the teachers mindset on their students' oral feedback. The findings show a correlation between the amount of feedback and the mindset."]

['We previously demonstrated that Machine learning (ML) algorithms can accurately estimate drug area under the curve (AUC) of tacrolimus or mycophenolate mofetil (MMF) based on limited information, as well as or even better than maximum a posteriori Bayesian estimation (MAP-BE). However, the major limitation in the development of such ML algorithms is the limited availability of large databases of concentration vs. time profiles for such drugs. The objectives of this study were: (i) to develop a Xgboost model to estimate tacrolimus inter-dose AUC based on concentration-time profiles obtained from a literature population pharmacokinetic (POPPK) model using Monte Carlo simulation; and (ii) to compare its performance with that of MAP-BE in external datasets of rich concentration-time profiles. The population parameters of a previously published PK model were used in the mrgsolve R package to simulate 9000 rich interdose tacrolimus profiles (one concentration simulated every 30 min) at steady-state. Data splitting was performed to obtain a training set (75%) and a test set (25%). Xgboost algorithms able to estimate tacrolimus AUC based on 2 or 3 concentrations were developed in the training set and the model with the lowest RMSE in a ten-fold cross-validation experiment was evaluated in the test set, as well as in 4 independent, rich PK datasets from transplant patients. ML algorithms based on 2 or 3 concentrations and a few covariates yielded excellent AUC estimation in the external validation datasets (relative bias < 5% and relative RMSE < 10%), comparable to those obtained with MAP-BE. In conclusion, Xgboost machine learning models trained on concentration-time profiles simulated using literature POPPK models allow accurate tacrolimus AUC estimation based on sparse concentration data. This study paves the way to the development of artificial intelligence at the service of precision therapeutic drug monitoring in different therapeutic areas.', 'We showed that machine learning may be better than a maximum Bayesian estimation of the drug area under a curve. There are limited availability of large databases of concentration which is the biggest limitation in the development of such neural networks. Time profiles for drugs like that. To develop a Xgboost model and to determine tacrolimus AUC from concentration-time profiles obtained from a literature population pharmacokinetic model using Monte Carlo simulation. The population parameters of a previous published model was used to predict 9000 tacrolimus profiles in the mrgsolve R package. The data was split so that the training and test sets were obtained. The model with the lowest RMSE was evaluated in a ten-fold cross-validation experiment, as well as in the training set. The AUC estimation is comparable to those obtained with the MAP- BE model, which used two or 3 concentrations. In conclusion, Xgboost machine learning models trained on concentration-time profiles can be used to model the tacrolimus AUC estimation based on sparse concentration data. This study paves the way to artificial intelligence which can be used for the service of precision therapeutic drug monitoring.']

['Contemporary psychiatric diagnosis still relies on the subjective symptom report of the patient during a clinical interview by a psychiatrist. Given the significant variability in personal reporting and differences in the skill set of psychiatrists, it is desirable to have objective diagnostic markers that could help clinicians differentiate patients from healthy individuals. A few recent studies have reported retinal vascular abnormalities in patients with schizophrenia (SCZ) using retinal fundus images. The goal of this study was to use a trained convolution neural network (CNN) deep learning algorithm to detect SCZ using retinal fundus images. A total of 327 subjects [139 patients with Schizophrenia (SCZ) and 188 Healthy volunteers (HV)] were recruited, and retinal images were acquired using a fundus camera. The images were preprocessed and fed to a convolution neural network for the classification. The model performance was evaluated using the area under the receiver operating characteristic curve (AUC). The CNN achieved an accuracy of 95% for classifying SCZ and HV with an AUC of 0.98. Findings from the current study suggest the potential utility of deep learning to classify patients with SCZ and assist clinicians in clinical settings. Future studies need to examine the utility of the deep learning model with retinal vascular images as biomarkers in schizophrenia with larger sample sizes.', 'A subjective symptom report is used in a clinical interview for a new psychiatric diagnosis. It is desirable for clinicians to have markers for certain illnesses to help differentiate between them and healthy individuals, because of variability in personal reporting and skill set in psychiatrists. In addition, it is desirable for clinicians to have markers for certain illnesses to help differentiate between them and healthy individuals, because of variability Patients with schizophrenia can be seen with certain types ofneurysms using a range of fundus images. The aim of the study was to use a trained convolution Neural Network and deep learning technique to detect ScZ. A total of 327 people with Schizophrenia and 188 healthy volunteers were recruited and used a fundus camera to take pictures of their eyes. The images were sent to a neural network for classification. The area under the  AUC was used to evaluate the model performance. The current study suggests that deep learning may be able to classify patients with certain conditions. In the future, studies need to look at the utility of the deep learning model with pictures from the brain']

['Prosthetic breast reconstruction rates have risen in the United States, whereas autologous techniques have stagnated. Meanwhile, single-institution data demonstrate that physician payments for prosthetic reconstruction are rising, while payments for autologous techniques are unchanged. This study aims to assess payment trends and variation for tissue expander and free flap breast reconstruction. The Blue Health Intelligence database was queried from 2009 to 2013, identifying women with claims for breast reconstruction. Trends in the incidence of surgery and physician reimbursement were characterized by method and year using regression models. There were 21,259 episodes of breast reconstruction, with a significant rise in tissue expander cases (incidence rate ratio, 1.09; p < 0.001) and an unchanged incidence of free flap cases (incidence rate ratio, 1.02; p = 0.222). Bilateral tissue expander cases reimbursed 1.32 times more than unilateral tissue expanders, whereas bilateral free flaps reimbursed 1.61 times more than unilateral variants. The total growth in adjusted tissue expander mean payments was 6.5 percent (from $2232 to $2378) compared with -1.8 percent (from $3858 to $3788) for free flaps. Linear modeling showed significant increases for tissue expander reimbursements only. Surgeon payments varied more for free flaps (the 25th to 75th percentile interquartile range was $2243 for free flaps versus $987 for tissue expanders). The incidence of tissue expander cases and reimbursements rose over a period where the incidence of free flap cases and reimbursements plateaued. Reasons for stagnation in free flaps are unclear; however, the opportunity cost of performing this procedure may incentivize the alternative technique. Greater payment variation in autologous reconstruction suggests the opportunity for negotiation with payers.', 'Prosthetic breast reconstruction rates have increased while autologous techniques have been stagnant in the United States. According to single-institution data, physician payments for replacement limbs are rising, while payments for autologous techniques are not. The study is designed to assess payment trends and variations for breast reconstruction. The Blue Health Intelligence database was queried to find women with claims for breast reconstruction. The incidence of surgery was characterized by method and year using models. There was a significant rise in tissue expander cases, with the incidence increasing from 0.94 to 1.10, and an unchanged incidence of free flap cases, with the incidence remaining at 1.02. Bilateral tissue expander cases were more efficient than the unilateral tissue expanders. The mean payments for tissue expander went up from $2232 to 2378 while the mean payments for free flaps went down from $3858 to $3782. There was significant increases for tissue expander reimbursements only. The 25th to 75th percentile interquartile range showed that surgeon payments varied more for free flaps. The incidence of tissue expander cases went up over the last year, while the incidence of free flap cases went down. Reasons for the stagnation in free flaps are not clear; however the opportunity cost may be an incentive to use alternative technique. There is an opportunity for negotiations with payers because of the higher payment variation in autologous reconstruction.']

["Artificial intelligence (AI) is a field of computer science that aims to mimic human thought processes. AI techniques have been applied in cardiovascular medicine to explore novel genotypes and phenotypes in existing diseases, improve the quality of patient care, enabling cost-effectiveness, and reducing readmission and mortality rates. The potential of AI in cardiovascular medicine is tremendous; however, ignorance of the challenges may overshadow its potential clinical impact. This paper gives a glimpse of AI's application in cardiovascular clinical care and discusses its potential role in facilitating precision cardiovascular medicine.", "Artificial intelligence is a science that seeks to mimic humans' thought processes. Artificial intelligence methods have been used to explore novel syndromes in the cardiovascular medicine to improve care, and possibly reduce mortality. There is a tremendous amount of potential in cardiovascular medicine but little awareness of the challenges. The paper gives a glimpse of the use of Artificial Intelligence in Cardiovascular Clinical Care and discusses its usefulness."]

['Artificial intelligence (AI) is an emerging technology that facilitates everyday tasks and automates tasks in various fields such as medicine. However, the emergence of a language model in academia has generated a lot of interest. This paper evaluates the potential of ChatGPT, a language model developed by OpenAI, and DALL-E 2, an image generator, in the writing of scientific articles in ophthalmology. The selected topic is the complications of the use of silicone oil in vitreoretinal surgery. ChatGPT was used to generate an abstract and a structured article, suggestions for a title and bibliographical references. In conclusion, despite the knowledge demonstrated by this tool, the scientific accuracy and reliability on specific topics is insufficient for the automatic generation of scientifically rigorous articles. In addition, scientists should be aware of the possible ethical and legal implications of these tools.', "Artificial intelligence is a technology that helps you with everyday tasks and can help in medicine. The emergence of a language model in college has generated a lot of interest. This paper explores the potential of the language model and the generator of an image in the writing of scientific articles in eye care. Silicone oil's use in skin care and eye procedures is the topic chosen. The method is used to generate an abstract and a structure for an article. Despite the knowledge demonstrated by this tool, accuracy and reliability on specific topics are not enough for automatic generation of rigorous articles. Scientists should be alert to the possible ethical and legal implications of the tools."]

['Understanding the effects of antiretroviral treatment (ART) interruption on neurocognition and quality of life (QoL) are important for managing unplanned interruptions and planned interruptions in HIV cure research. Children previously randomized to continuous (continuous ART, n = 41) vs. planned treatment interruption (PTI, n = 47) in the Pediatric European Network for Treatment of AIDS (PENTA) 11 study were enrolled. At study end, PTI children resumed ART. At 1 and 2 years following study end, children were assessed by the coding, symbol search and digit span subtests of Wechsler Intelligence Scale for Children (6-16 years old) or Wechsler Adult Intelligence Scale (≥17 years old) and by Pediatrics QoL questionnaires for physical and psychological QoL. Transformed scaled scores for neurocognition and mean standardized scores for QoL were compared between arms by t-test and Mann-Whitney U test, respectively. Scores indicating clinical concern were compared (<7 for neurocognition and <70 for QoL tests). Characteristics were similar between arms with a median age of 12.6 years, CD4⁺ of 830 cells/μl and HIV RNA of 1.7 log10copies/ml. The median cumulative ART exposure was 9.6 in continuous ART vs. 7.7 years in PTI (P = 0.02). PTI children had a median of 12 months off ART and had resumed ART for 25.2 months at time of first assessment. Neurocognitive scores were similar between arms for all tests. Physical and psychological QoL scores were no different. About 40% had low neurocognitive and QoL scores indicating clinical concern. No differences in information processing speed, sustained attention, short-term memory and QoL functioning were observed between children previously randomized to continuous ART vs. PTI in the PENTA 11 trial.', 'In HIV cure research, understanding the effects of ART interruption is important for management. Children previously were randomized to continuous. The planned treatment interruption in the PETA 11 study was conducted. The children resumed ART at the end of the study. The children were assessed at 2 years following the end of the study using a variety of questionnaires, including but not limited to: coding, symbol search and digit span, Wechsler Adult Intelligence Scales, and physical and psychological questionnaires. t-test and Mann-Whitney U tests were used to compare scales for neurocognition and mean standardized scores. The scores indicated clinical concern were compared. The arms had the same characteristics with a median age of 12 years, CD4+ of 830 cells/l and HIV RNAs of 1.7 log10copies/ml. The median cumulative exposure was 9.6. A long time in the APHIS Children have a median of 12 months off ART, and then they start back up for 25.2 months at the first assessment. All tests had the same scores on the brains of the arms. Physical and psychological scores were not completely different. Some of the 40% had low scores on cognitive and quality of life tests. Children who were randomized to continuous ART had no differences in information processing speed, attention, short-term memory or QoL functioning. The trial of PENTA 11']

['This study aims to assess the short-term outcome of total hip arthroplasty for treating developmental dysplasia of the hip (DDH) using artificial intelligence (AI)-assisted three-dimensional (3D) preoperative planning technology. Between January 2020 and July 2022, a total of 61 patients with DDH (31 males, 30 females, mean age: 59.2±10.4 years; range, 35 to 78 years) were retrospectively analyzed. The patients were divided into two groups as those in the observation group of AI-assisted 3D preoperative planning technology (n=34) and the control group of traditional two-dimensional X-ray template planning technology (n=27). Perioperative data of the patients were recorded and analyzed. All patients were followed for more than one year, and no hip dislocation, aseptic loosening, periprosthetic fracture, periprosthetic infection or revision occurred. The accuracy of the planning was based on the agreement between the preoperative planning model and the intraoperative model. The accuracy of preoperative planning for the acetabular prosthesis and femoral prosthesis in the observation group was significantly higher than in the control group. No statistically significant difference was found in the postoperative abduction (p=0.416) and anteversion (p=0.225) between the groups. In the observation group, 91.2% of the acetabular cups were implanted within the Lewinnek safe zone (66.7% in the control group) and 88.2% were within the Callanan safe zone (63% in the control group). There was a statistically significant difference between the two groups in terms of the postoperative lower-limb length discrepancy (p=0.004), which was significantly improved in both groups compared to preoperative values (p<0.01 for all). The postoperative Harris hip score in both groups was significantly improved compared to preoperative scores (p<0.01); however, there was no statistically significant difference between the two groups (p=0.098). Our study results suggest that AI-assisted 3D preoperative planning is evidently more successful than traditional 2D X-ray template planning for predicting prosthesis size. This method seems to be advantageous in acetabular cup positioning, as well as in lower-limb length restoration.', "The study is about evaluating the short-term outcome of total hip replacement for treatment of DDH using artificial intelligence and 3D planning technology. There are a total of 61 patients between January 2020 and July 2022, who have DDH, ranging in age from 35 to 78 years. There were two groups of patients with the first being the observation group of Artificial Intelligence-assisted 3D preoperative planning technology and the second being the control group of the traditional two-dimensional X-ray template planning technology. The patients' surgery data was recorded and analyzed. All patients were follow for more than one year and there were no problems. The accuracy of the planning was determined by the agreement between the preoperative model and the intraoperative model. The control group's accuracy for planning the acetabular prosthesis was not as high as that of the observation group. There was no significant difference between the groups in terms of abduction and anteversion. In the observation group 92 percent of the acetabular cups were implanted within the Lewinnek safe zone. There was a significant difference between the groups in terms of the lower-limb length discrepancy. There was no significant difference between the two groups when it came to the Harris hip score. Our results suggest that 3D planning is more successful than traditional 2D X-ray planning. This method has been seen to work for acetabular cup positioning and lower-limb length restoration."]

['This review describes the history of development of a new line of chemical reagents that prompts to significantly reevaluate the application of scanning electron microscopy (SEM) in medical and biological studies, particularly in ophthalmology; considers the establishing of SEM as an analytical method; covers the problems in its application associated with the needs of clinical medicine and the complexities of biological sample preparation for electron microscopy. The article also presents in chronological order the technical solutions associated with creating a unique line of reagents for supravital staining. The multitude of technical solutions allows considering SEM as a method of express diagnostics. The review discusses examples of practical application of these methods for solving certain cases in clinical ophthalmology. The niche of SEM is considered among other methods of clinical diagnostics, as well as its future development involving the use of artificial intelligence.', 'The history of development of a new line of chemical reagents, which prompted to significantly reexamine the application of scanning electron microscopy in medical and biological studies, is described in this review. The chronological order of the technical solutions for creating a unique line of reagents for supravital staining was presented in the article. It is possible to consider the use of SEM as a method of express diagnostics. The use of these methods in the field of clinical eye diseases are discussed in the review. The niche of SEM is considered among other methods of clinical diagnostics as well as its future development involving the use of artificial intelligence.']

['Single-point incremental forming (SPIF) is a technology that allows incremental manufacturing of complex parts from a flat sheet using simple tools; further, this technology is flexible and economical. Measuring the forming force using this technology helps in preventing failures, determining the optimal processes, and implementing on-line control. In this paper, an experimental study using SPIF is described. This study focuses on the influence of four different process parameters, namely, step size, tool diameter, sheet thickness, and feed rate, on the maximum forming force. For an efficient force predictive model based on an adaptive neuro-fuzzy inference system (ANFIS), an artificial neural network (ANN) and a regressions model were applied. The predicted forces exhibited relatively good agreement with the experimental results. The results indicate that the performance of the ANFIS model realizes the full potential of the ANN model.', "Single-point incremental forming is a flexible and economical way to manufacture complex parts from a flat sheet. Using this technology helps in preventing failures, determining the optimal processes, and implementing on-line control. An experimental study using SPIF is described in the paper. The study focuses on the influence of four process parameters on maximum forming force. There is an adaptive Neural network and a regressions model applied for an efficient force predictive model. The predicted forces seemed to mesh with the experiment. The results show that the ANN model's full potential is realized by the performance of the ANFIS model."]

["The purpose of this study was to evaluate the predictive value of optical coherence tomography (OCT) and OCT angiography (OCTA) parameters at baseline on lesion's activity at the 1-year follow-up in type 1 macular neovascularizations (MNVs) treated with 1-year fixed regimen of intravitreal aflibercept injections (q8 IAIs). All patients were imaged by structural OCT to evaluate central macular thickness (CMT), subretinal fluid (SRF), subretinal hyper-reflective material (SHRM), intraretinal fluid (IRF) and intraretinal hyper-reflective dots (HRDs), and by Swept-Source OCTA to measure baseline MNV area, perfusion density (PD), vessel length density (VLD), and vessel diameter index. At the end of q8 IAI, patients were classified in two groups: active-MNV (A-MNV) and inactive-MNV (I-MNV), considering the OCT signs of activity. Three binary logistic regression models were developed: (1) OCT-based, (2) OCTA-based, and (3) OCT/OCTA-based model. Thirty-one treatment-naive type 1 MNVs were enrolled (13 A-MNV and 18 I-MNV). No differences were observed in baseline OCT and OCTA characteristics between A-MNV and I-MNV. Among the models developed, model 3 that combined OCT/OCTA parameters showed a performance of 87.5% and excellent sensitivity for A-MNV lesions (100%). By analyzing the model, the A-MNV group appears more likely to show at baseline SRF, greater CMT, wider MNV area, and lower PD and VLD compared to I-MNV. Our study demonstrated that the combination of baseline OCT and OCTA parameters allowed to achieve a good models' performance in the prediction of MNV activity permitting to correctly classifying the active lesions at the end of follow-up period, with excellent sensitivity. OCT/OCTA could integrate statistical models potentially useful for artificial intelligence.", 'The purpose of this study was to assess the correlation of the two parameters at the beginning of the study in order to determine the best way to apply them to type 1 mVs treated with a fixed regimen. The patients were all imaged by structuralOCT in order to check for various conditions. Patients were classified into two groups based on the OCT signs of activity. The models were developed in three separate phases: (1) OCT-based, (2) OCTA-based, and (3) OCT/OCTA-based model. Thirty-one treatment naive type 1 MNVs were selected. There were no differences between A-MNV and I-MNV. The model 3 that incorporated OCT/OCTA parameters had an excellent sensitivity for A-MNV. The A-MNV group appears to show in the model at a variety of points compared to I-MNV. Our research showed that the combination of baseline OCT and OCTA parameters allowed for a good model performance, with excellent sensitivity at the end of the study Statistical models could be useful for artificial intelligence.']

['To construct predictive models of diabetes complications (DCs) by big data machine learning, based on electronic medical records. Six groups of DCs were considered: eye complications, cardiovascular, cerebrovascular, and peripheral vascular disease, nephropathy, diabetic neuropathy. A supervised, tree-based learning approach (XGBoost) was used to predict the onset of each complication within 5 years (task 1). Furthermore, a separate prediction for early (within 2 years) and late (3-5 years) onset of complication (task 2) was performed. A dataset of 147.664 patients seen during 15 years by 23 centers was used. External validation was performed in five additional centers. Models were evaluated by considering accuracy, sensitivity, specificity, and area under the ROC curve (AUC). For all DCs considered, the predictive models in task 1 showed an accuracy > 70 %, and AUC largely exceeded 0.80, reaching 0.97 for nephropathy. For task 2, all predictive models showed an accuracy > 70 % and an AUC > 0.85. Sensitivity in predicting the early occurrence of the complication ranged between 83.2 % (peripheral vascular disease) and 88.5 % (nephropathy). Machine learning approach offers the opportunity to identify patients at greater risk of complications. This can help overcoming clinical inertia and improving the quality of diabetes care.', 'The predictions of diabetes events can be created using big data machine learning. Eye complications, cardiovascular, cerebrovascular, and peripheral veins disease were considered among the groups of DCs. To predict the start of each problem within 5 years, a supervised, tree-based learning approach was used. There was a prediction for early and late the start of a complication. A dataset of 146,866 patients were seen over the course of 15 years by 23 centers. Outside validation was done in five more centers. The ROC curve is used to evaluate models by considering their accuracy, sensitivity, specificity, and area. The models for task 1 had a 70% accuracy and reached 0.80 for nephropathy. Predictive models had a accuracy of at least 70 percent and an AUC of above 0.85 for task 2. The potential for machine learning approach to identify patients at greater risk. Improving the quality ofDiabetes care can be improved with this.']

['The aim of the study is to develop artificial intelligence (AI) algorithm based on a deep learning model to predict mortality using abbreviate injury score (AIS). The performance of the conventional anatomic injury severity score (ISS) system in predicting in-hospital mortality is still limited. AIS data of 42,933 patients registered in the Korean trauma data bank from four Korean regional trauma centers were enrolled. After excluding patients who were younger than 19 years old and those who died within six hours from arrival, we included 37,762 patients, of which 36,493 (96.6%) survived and 1269 (3.4%) deceased. To enhance the AI model performance, we reduced the AIS codes to 46 input values by organizing them according to the organ location (Region-46). The total AIS and six categories of the anatomic region in the ISS system (Region-6) were used to compare the input features. The AI models were compared with the conventional ISS and new ISS (NISS) systems. We evaluated the performance pertaining to the 12 combinations of the features and models. The highest accuracy (85.05%) corresponded to Region-46 with DNN, followed by that of Region-6 with DNN (83.62%), AIS with DNN (81.27%), ISS-16 (80.50%), NISS-16 (79.18%), NISS-25 (77.09%), and ISS-25 (70.82%). The highest AUROC (0.9084) corresponded to Region-46 with DNN, followed by that of Region-6 with DNN (0.9013), AIS with DNN (0.8819), ISS (0.8709), and NISS (0.8681). The proposed deep learning scheme with feature combination exhibited high accuracy metrics such as the balanced accuracy and AUROC than the conventional ISS and NISS systems. We expect that our trial would be a cornerstone of more complex combination model.', 'The study wants to develop a deep learning model to predict mortality using an injury score. The performance of the conventional injury severity score in predicting mortality is still limited. There were 42,933 patients registered in the Korean trauma data bank. We included 37,762 patients after removing the younger patients and those who died within six hours of arrival. To enhance the performance of the model, we reduced the codes to reflect the organ location. The input features were compared to the totalAIS and the 6 categories of the region. The systems were compared with models from the internet. The performance of 12 features and models was evaluated. The highest accuracy is associated with Region46 with DNN, followed by that of Region 6 with DNN, and thenAIS with DNN. The highest AUROC is related to the region with DNN, followed by Region 6 with the same number. A proposed deep learning scheme with features exhibited high accuracy metrics, such as balanced accuracy. We expect the trial to be a key component of the more complex model.']

["Number line accuracy (NL accuracy) shows improvement over the course of a school education. However, there are practically no cross-country longitudinal studies of NL accuracy over the whole course of elementary school. This study investigated the developmental trajectories of NL accuracy and its types across the elementary school years in two countries-Russia and Kyrgyzstan. The analyses were carried out on the data collected from the sample of 508 schoolchildren at Grades 1, 2, 3 and 4 (aged 6.4-11.9 years) from Russia and Kyrgyzstan, who were surveyed as part of the 'Cross-cultural Longitudinal Analysis of Student Success' project. The participants were administered the 'Number Line' computerized test task and a paper-and-pencil 'Standard Progressive Matrices' test at the end of each academic year. During the course of the elementary school education, NL accuracy increases nonlinearly in both samples from Grade 1 to Grade 4, with a pronounced increase in the rate of improvement from the first to the second year. Cross-country differences in NL accuracy were observed during each year of schooling as well as in the growth of NL accuracy. The development of NL accuracy is described by a model with two developmental types: (1) 'high start and growth' (93% of the pooled sample) and (2) 'low start and no growth' (7%). Both NL accuracy and the rate of its growth during elementary school depend on educational conditions. Cross-country differences in the distribution of schoolchildren by these two developmental types were statistically insignificant.", 'Over the course of an education, number line accuracy shows an improvement. There are longitudinal studies of NL accuracy in one country, but not in another. The study was about the NL accuracy in the elementary school years in two countries, Russia and Russia. The data from the sample of schools in Russia and Kyrgyzstan were used to perform analyses on their student success. The number line computerized test task and paper and pencil standard progressive maxims test was administered to participants. NL accuracy increases in both samples from Grade 1 to Grades 4 when compared to the first year, with a pronounced increase in the second year. There were cross country differences in the growth and accuracy of NL. A model describing development of NL accuracy consists of a high start and growth model and a low start and no growth model. The accuracy of NL is influenced by the educational conditions at elementary school. The distribution of school children in cross-country differences was trivial.']

["Automatic deep-learning models used for sleep scoring in children with obstructive sleep apnea (OSA) are perceived as black boxes, limiting their implementation in clinical settings. Accordingly, we aimed to develop an accurate and interpretable deep-learning model for sleep staging in children using single-channel electroencephalogram (EEG) recordings. We used EEG signals from the Childhood Adenotonsillectomy Trial (CHAT) dataset (n = 1637) and a clinical sleep database (n = 980). Three distinct deep-learning architectures were explored to automatically classify sleep stages from a single-channel EEG data. Gradient-weighted Class Activation Mapping (Grad-CAM), an explainable artificial intelligence (XAI) algorithm, was then applied to provide an interpretation of the singular EEG patterns contributing to each predicted sleep stage. Among the tested architectures, a standard convolutional neural network (CNN) demonstrated the highest performance for automated sleep stage detection in the CHAT test set (accuracy = 86.9% and five-class kappa = 0.827). Furthermore, the CNN-based estimation of total sleep time exhibited strong agreement in the clinical dataset (intra-class correlation coefficient = 0.772). Our XAI approach using Grad-CAM effectively highlighted the EEG features associated with each sleep stage, emphasizing their influence on the CNN's decision-making process in both datasets. Grad-CAM heatmaps also allowed to identify and analyze epochs within a recording with a highly likelihood to be misclassified, revealing mixed features from different sleep stages within these epochs. Finally, Grad-CAM heatmaps unveiled novel features contributing to sleep scoring using a single EEG channel. Consequently, integrating an explainable CNN-based deep-learning model in the clinical environment could enable automatic sleep staging in pediatric sleep apnea tests.", "Automatic deep-learning models in use for sleep scoring are perceived as black boxes and limited in their implementation in clinical settings. To develop a model for sleep staging in children using single-channel eeG recordings we aimed to. The CHAT dataset and a clinical sleep database had signals from the ceet. The deep-learning architectures were developed to automatically classify sleep stages. Grad-CAM, an example of artificial intelligence, was applied to provide an interpretation of the EEG patterns that contribute to each sleep stage. The most accurate sleep stage detection in the CHAT test set was achieved by a standard neural network. The CNN-based total sleep time estimation showed strong agreement in the clinical dataset. The CNN's decision-making process can be influenced by the features of the sleep stage. Grad-CAM heatmaps were able to identify and analyze epochs within a recording that were likely to be misclassified and reveal different sleep stages within them. The features that contribute to sleep scoring using a single EEG channel are unveiled by Grad-CAM heatmaps. Automatic sleep staging is possible by integrating a CNN deep-learning model in the clinical environment."]

['There is a burgeoning interest in the use of deep neural network in diabetic retinal screening. To determine whether a deep neural network could satisfactorily detect diabetic retinopathy that requires referral to an ophthalmologist from a local diabetic retinal screening programme and an international database. Retrospective audit. Diabetic retinal photos from Otago database photographed during October 2016 (485 photos), and 1200 photos from Messidor international database. Receiver operating characteristic curve to illustrate the ability of a deep neural network to identify referable diabetic retinopathy (moderate or worse diabetic retinopathy or exudates within one disc diameter of the fovea). Area under the receiver operating characteristic curve, sensitivity and specificity. For detecting referable diabetic retinopathy, the deep neural network had an area under receiver operating characteristic curve of 0.901 (95% confidence interval 0.807-0.995), with 84.6% sensitivity and 79.7% specificity for Otago and 0.980 (95% confidence interval 0.973-0.986), with 96.0% sensitivity and 90.0% specificity for Messidor. This study has shown that a deep neural network can detect referable diabetic retinopathy with sensitivities and specificities close to or better than 80% from both an international and a domestic (New Zealand) database. We believe that deep neural networks can be integrated into community screening once they can successfully detect both diabetic retinopathy and diabetic macular oedema.', "The use of deep neural network is being used for diabetes screening. To find out if a deep neural network could detect diabetes, and how much it would cost to get an eye done. An audit is retrospective. Swelling of the retina is seen in the photos from the Otago and Messidor international databases. The curve shows the ability of a deep neural network to determine if there is a referable wound. The receiver's operating characteristic curve has an area under it. The deep neural network has an area under receiver operating characteristic curve of 0.901. A deep neural network can detect referable diabetes with a low sensitivity and a high specificity from both international and New Zealand databases. Once deep neural networks can successfully detect both diabetics will community screening be possible."]

['The global society is currently facing a rise in the elderly population. The concept of successful aging (SA) appeared in the gerontological literature to overcome the challenges and problems of population aging. SA is a subjective and multidimensional concept with many ambiguities regarding its meaning or measuring. This study aimed to propose an intelligent predictive model to predict SA. In this retrospective study, the data of 784 elderly people were used to develop and validate machine learning (ML) methods. Data pre-processing was first performed. First, an adaptive neuro-fuzzy inference system (ANFIS) was proposed to predict SA. Then, the predictive performance of the proposed model was compared with three ML algorithms, including multilayer perceptron (MLP) neural network, support vector machine (SVM), and random forest (RF) based on accuracy, sensitivity, precision, and F-score metrics. The findings indicated that the ANFIS model with gauss2mf built-in membership function (MF) outperformed the other models with accuracy, sensitivity, precision, and F-score of 91.57%, 95.18%, 92.31%, and 92.94%, respectively. The predictive performance of ANFIS is more efficient than the other ML models in SA prediction. The development of a decision support system (DSS) using our prediction model can provide healthcare administrators and policymakers with a reliable and responsive tool to improve elderly outcomes.', "A rising elderly population is threatening the world's society. The geronologic literature states the concept of successful aging (SA) to overcome the challenges and problems of population aging. SA has many incomprehensible meanings and is thus a multidimensional concept. There was a study that intended to propose a model for predicting SA. The data of  794 old people were used in this study to train machine learning methods. Data pre-processing was first done. The adaptive neuro-fuzzy inference system is proposed to predict SA. After the simulation, the model was compared with three different machine learning strategies, including a MLP neural network, support machine, and random forest based on accuracy, sensitivity, precision, and F- score metrics. The models with accuracy, sensitivity, precision, and F- score were all found to be superior to the ANFIS model. The prediction performance of other models is more efficient than the prediction of ANFIS. Our prediction model can give healthcare administrators and policymakers a reliable and responsive tool to improve elderly outcomes, thanks to the development of a decision support system."]

['To determine whether the prior performance of maternity services, as measured by Royal College of Obstetricians and Gynaecologists performance indicators, is associated with ratings by the Care Quality Commission at subsequent inspection, and whether performance changes occur after inspection. We used hospital activity data from 176 maternity sites inspected between October 2013 and March 2016 to generate a set of performance indicators developed by the Royal College of Obstetricians and Gynaecologists. We linked these data to Care Quality Commission data on inspection dates and rating scores and used regression models, controlling for site level effects, to estimate the relationships between inspection ratings and performance indicators before and after inspections. Coefficients measuring the relationship between indicator performance and subsequent inspection rating score had wide confidence intervals which crossed zero suggesting no statistically significant relationship prior to inspection. The same absence of statistical significance was observed for changes in indicator performance after inspection. The use of routine data for performance monitoring is becoming increasingly important as regular inspection is costly and regulators require accurate and timely intelligence. However, we found no statistically significant relationships between inspection ratings and performance indicators before or after inspections in maternity services. This calls into question the validity and reliability of the performance indicators, the inspection process and ratings, or both, as measures of performance.', 'The Care Quality Commission reviews the performance of maternity services after a visit, and whether or not the prior performance indicator measured by the Royal College of Obstetricians and Gynaecologists is associated with ratings. We used the data from the Maternity Sites inspected between October 2013 and March 2016 to generate indicators that the Royal College of Obstetricians and Gynaecologists developed. We used regression models to estimate the relationships between inspection ratings and performance indicators before and after inspections and linked these data to Care Quality Commission data. Coefficients that measured the relationship between indicator performance and subsequent inspection rating score had wide confidence intervals, suggesting there was no statistically significant relationship prior to inspection. Changes in indicator performance after inspection had no statistical significance. As regular inspection is expensive and the regulators demand more accurate and timely intelligence, the use of routine data for performance monitoring is increasing. Inspection ratings and performance indicators did not show a significant relationship before or after the maternity services inspections. It calls into question the validity and reliability of the performance indicators, the ratings and the inspection process.']

['<b><i>Background:</i></b> The Glu-Urea-Lys (EUK) pharmacophore as prostate-specific membrane antigen (PSMA)-targeted ligand was synthesized, radiolabeled with <sup>99m</sup>Tc-tricarbonyl-imidazole-BPS chelation system, and biological activities were evaluated. The strategy [2 + 1] ligand is applied for tricarbonyl labeling. (5-imidazole-1-yl)pentanoic acid as a monodentate ligand and bathophenanthroline disulfonate (BPS) as a bidentate ligand formed a chelate system with <sup>99m</sup>Tc-tricarbonyl. EUK-pentanoic acid-imidazole and EUK were evaluated for PSMA active site using AutoDock 4 software. <b><i>Materials and Methods:</i></b> EUK-pentanoic acid-imidazole was synthesized in two steps. BPS was radiolabeled with <sup>99m</sup>Tc-tricarbonyl at 100°C for 30 min. The purified <sup>99m</sup>Tc(CO)<sub>3</sub>(H<sub>2</sub>O)BPS was used to radiolabel EUK-pentanoic acid-imidazole at 100°C, 30 min. Radiochemical purity, Log P, and stability studies were carried out within 24 h. Affinity of <sup>99m</sup>Tc(CO)<sub>3</sub>BPS-imidazole-EUK was performed in the saturation binding studies using LNCaP cells at 37°C for 1 h with a range of 0.001-1000 nM radiolabeled compound range. Internalization studies were performed in LNCaP cells with 1000 nM radiolabeled compound incubated for (0-2) h at 37°C. Biodistribution was studied in normal male Balb/c mice. The artificial intelligence predicts the uptake of radiolabeled compound in tumor. <b><i>Results:</i></b> The structures of synthesized compounds were confirmed by mass spectroscopy. Radiochemical purity, Log P, and protein binding were ≥95%, -0.2%, and 23%, respectively. The radiolabeled compound was stable in saline and human plasma within 24 h with radiochemical purity ≥90%. There was no release of <sup>99m</sup>Tc within 4 h in competition with histidine. The affinity was 82 ± 26.38 nM, and the activity increased inside the cells over time. Biodistribution studies showed radioactivity accumulation in kidneys less than <sup>99m</sup>Tc-HYNIC-PSMA. There was a moderate accumulation of radioactivity in the liver and intestine. <b><i>Conclusion:</i></b> Based on the results, <sup>99m</sup>Tc(CO)<sub>3</sub>BPS-imidazole-EUK can potentially be used as an imaging agent for studies at prostate bed and distal areas. The chelate system can be potentially labeled with rhenium for imaging studies (fluorescent or scintigraphy) and therapy.', 'The radiolabeled EUK was used as a stimulator of the PSMA. The strategy is applied for labeling tricarbonyl. A Chelate system was formed by the conjugates of imidazole-1-yl and bathophenanthroline disulfonate. EUK and imidazole were evaluated for the PSMA active sites. The EUK-pentanoic acid-imidazole was synthesised in two steps. The BPS was labeled with sup>99m/sup>Tc-tricarbonyl at 100C for 30 min. The radiolabel EUK-pentanoic acid-imidazole was made using the sup>99m/sup>Tc(CO)sub>3/sub> Within 24 hours, radioactivity purity, Log P, and stability studies were carried out. The saturation binding studies were performed using LNCaP cells at 37C for 1 h. LNCaP cells with 1000 nM radiolabeled compound were subjected to Internalization studies while standing at a temperature of 37C. The distribution was looked at in male Balb/c mice. The artificial intelligence predicts what radiolabeled compound will do in a Tumor. Mass staxation confirmed the structures of the synthesized compounds. Log P, Radiochemical purity, andProtein binding were all over the place. The radiolabeled compound was stable with a radiochemical purity of 90%. There was no release of sup>99m/sup>Tc within 4h. The activity inside the cells increased over the course of time. The radioactivity in the kidneys were less than 99 million. There was a low amount of radioactivity in the organs. Based on the results, sup>99m/sup>Tc(CO)sub>3 can possibly be used. The system could possibly be labeled with rhenium for therapy.']

["<b>Background:</b> Some meta-analyses have demonstrated the association between emotional intelligence (EI) and different health indicators. With the increase of suicide cases in the world, more and more professionals have been interested in the link between both variables. <b>Aim:</b> To study all the available evidence on the association between EI and suicidal behavior. <b>Method:</b> We systematically reviewed all available literature (in English or Spanish) on the relationship between both variables through the main databases. <b>Results:</b> Twenty-five articles were included. EI and suicidal behavior correlated inversely in almost all the articles that the Emotional Quotient Inventory (EQ-i), The Emotional Intelligence Test (EIT), The Spanish Wong and Law Emotional Intelligence Scale (WLEIS), and The Schutte Emotional Intelligence Scale (EIS/SSEIT), Barchard's Emotional Intelligence Scale, were used, that is, the higher suicidal behavior level the lower the EI score. The same results were found in two out of four investigations that used the Trait Meta-Mood Scale (TMMS-24) between clarity (emotional understanding) and emotional repair (emotional regulation) and suicidal behavior. Two out of three studies that used the Mayer-Salovey-Caruso Emotional Intelligence Test (MSCEIT) found that only the Strategic component of EI (emotional understanding and regulation) was a protective factor. <b>Conclusions:</b> The results appear to indicate that a high level of EI plays an important role in protecting against suicidal behavior, and should thus be integrated into suicide prevention programs.", 'The association between emotional intelligence and health indicators is shown in a few meta-analyses. More and more professionals are interested in the link between both variables as there is an increase in suicide cases. To study all the information on the correlation between EI and suicidal behavior. We reviewed the literature on the relationship between the variables through main databases. There are 25 articles in the results. The Schutte Emotional Intelligence Scale and The Spanish Wong and Law Emotional Intelligence Scale were correlated with suicidal behavior. The results of both investigations used a scale that measures clarity, emotional understanding, and emotional repair. Two of the three studies that used theMSCEIT found that only the Strategic component of EI was a protective factor. The results show that high levels of EI are related to protecting against suicidal behavior and should be used in suicide prevention programs.']

["The advent of Industry 4.0 introduced new ways for businesses to evolve by implementing maintenance policies leading to advancements in terms of productivity, efficiency, and financial performance. In line with the growing emphasis on sustainability, industries implement predictive techniques based on Artificial Intelligence for the purpose of mitigating machine and equipment failures by predicting anomalies during their production process. In this work, a new dataset that was made publicly available, collected from an industrial blower, is presented, analyzed and modeled using a Sequence-to-Sequence Stacked Sparse Long Short-Term Memory Autoencoder. Specifically the right and left mounted ball bearing units were measured during several months of normal operational condition as well as during an encumbered operational state. An anomaly detection model was developed for the purpose of analyzing the operational behavior of the two bearing units. A stacked sparse Long Short-Term Memory Autoencoder was successfully trained on the data obtained from the left unit under normal operating conditions, learning the underlying patterns and statistical connections of the data. The model was evaluated by means of the Mean Squared Error using data from the unit's encumbered state, as well as using data collected from the right unit. The model performed satisfactorily throughout its evaluation on all collected datasets. Also, the model proved its capability for generalization along with adaptability on assessing the behavior of equipment similar to the one it was trained on.", "The advent of Industry 4 changed the way businesses can evolve and improve their productivity, efficiency, and financial performance. With the growing emphasis on sustainable practices, industries implement techniques based on Artificial Intelligence to combat machine and equipment failures by predicting anomalies. A public dataset from an industrial blower that is used to model a program using a Sequence- to-Sequence stacked Sparse Long Short-Term Memory autoencoder is presented and analyzed in this work. The right and left mounted ball bearing units were measured during the normal operational condition and an encumbered operational state. An analysis of the operational behavior of the two bearing units is what the model was created for. The Long Short-Term Memory Autoencoder was trained on the data from the left unit and learned patterns and connections. The model was evaluated using data from the state of the unit and data from the correct unit. The model performed well throughout its evaluation. The model proved it's ability to generalize and adapt to assess the behavior of equipment similar to one that was trained on."]

['Iniencephaly is a severe developmental abnormality of the craniovertebral junction in which the head is retroflexed dramatically. Anatomic studies have identified striking changes in the vertebrae and skull: marked lordosis of the cervical vertebrae, duplicated cervical vertebrae, irregularly fused cervical vertebrae, a widened foramen magnum and a small posterior fossa. The affected infant appears to have no neck, as the skin of the face is continuous with the chest and the skin of the posterior scalp is continuous with the skin of the back. Iniencephaly is considered a rare neural tube defect. The frequency has been higher in geographic areas in which the rates of occurrence of anencephaly and myelomeningocele were high. Most affected fetuses are either stillborn or die soon after birth. However, one affected individual is an adult with normal intelligence. A malformations surveillance program can identify an unselected group of infants with iniencephaly. This approach can determine the prevalence rate, the frequency of associated malformations, and the occurrence of close relatives with other neural tube defects. Over 41 years, the surveillance of 289,365 births identified eight fetuses and newborn infants with iniencephaly. Five of the eight had either an additional encephalocele or a thoracic myelomeningocele. Two of the eight affected infants had a sibling or a cousin with anencephaly. These findings suggest a relationship between the occurrence of iniencephaly and the most common neural tube defects, anencephaly and myelomeningocele. Recent experience confirms that this complex neural tube defect is not always lethal. Birth Defects Research 110:128-133, 2018. © 2018 Wiley Periodicals, Inc.', "Iniencephaly is a major abnormality of the skull in which the head is retroflexed dramatically. Anatomic studies show that there are changes to the skull and vertebrae. The skin of the face, chest and back are all connected by the skin of the back. Iniencephaly is a rare neural tube defect. In areas with high rates of anencephaly and myelomeningocele, thefrequency has been higher. The majority of affected fetuses die shortly after birth. A person with normal intelligence is one of the affected individuals. The malformations program can identify babies with Iniencephaly who aren't selected. This approach can determine the prevalence rate and the frequency of close relatives with neural tube defects. The newborn babies were identified with iniencephaly over a 41-year period. Five of the eight had something. Two infants had a sibling or cousin with an anencephaly. There is evidence that shows an association between the occurrence of iniencephaly and neural tube defects. Recent experience confirms that neural tube defects can not be fatal. Birth Defects Research takes place in the year 2018)."]

["<b>Background:</b> Although primary lung cancer is rare in children, chest CT is commonly performed to assess for lung metastases in children with cancer. Lung nodule computer-aided detection (CAD) systems have been designed and studied primarily using adult data, and such systems' efficacy on pediatric patients is poorly understood. <b>Objective:</b> To evaluate the diagnostic performance in children of traditional and deep-learning CAD systems trained with adult data for the detection of lung nodules on chest CT scans and to compare such systems' ability to generalize to children versus to other adults. <b>Methods:</b> This retrospective study included pediatric and adult chest CT test sets. The pediatric test set comprised 59 CT scans in 59 patients (30 male, 29 female; mean age, 13.1 years; age range, 4-17 year), obtained from November 30, 2018 to August 31, 2020; lung nodules were annotated by fellowship-trained pediatric radiologists as reference standard. The adult test set was the publicly available adult Lung Nodule Analysis (LUNA) 2016 subset 0, containing 89 deidentified scans with previously annotated nodules. The test sets were processed through the FlyerScan (traditional) and Medical Open Network for Artificial Intelligence (MONAI) (deep learning) lung nodule detection CAD systems, which had been trained on separate sets of adult CT scans. Sensitivity and false positive (FP) frequency were calculated for nodules measuring 3-30 mm; non-overlapping 95% CIs indicated significant differences. <b>Results:</b> Operating at two FPs per scan, FlyerScan and MONAI exhibited significantly lower detection sensitivities on pediatric testing data of 68.4% (197/288, 95% CI: 65.1-73.0) and 53.1% (153/288, 95% CI: 46.7-58.4), respectively, than on adult LUNA 2016 subset 0 testing data of 83.9% (94/112, 95% CI: 79.1-88.0) and 95.5% (107/112, 95% CI: 90.0-98.4), respectively. Mean nodule size was smaller (p<.001) in the pediatric (5.4±3.1 mm) than adult (11.0±6.2 mm) testing data. <b>Conclusions:</b> Adult-trained traditional and deep-learning-based lung nodule CAD systems had significantly lower detection sensitivity on pediatric data than on adult data at matching false-positive frequency. The performance difference may relate to smaller size of pediatric lung nodules. <b>Clinical Impact:</b> The results indicate a need for pediatric-specific lung nodule CAD systems trained on pediatric-specific data.", "Although cancer is rare in children, chest x-rays are routinely performed to look for lung damage in children with cancer. Lung disease computer-aided detection systems use adult data, and the effectiveness of such systems on children is not understood. To compare the diagnostic performance of children with other adults who have been trained with adult data for the detection of lung nodule on chest CT scans and to investigate the abilities of traditional and deep-learning CAD systems to generalize to children. The retrospective study included both children's and adult chest CT test sets. 59 patients (30 male, 29 female; mean age, 13.1 years; age range, 4-17 year); obtained from November 30, 2018, to August 31, 2020, all had CT scans, which were annotated as reference standard. The public adult Lung Nodule Analysis (LUNA 2016) set contained 89 deidentified scans with previously annotated nodules. There are separate sets of adult CT scans trained in the system of deep learning which processed the test sets. Significant differences were indicated in the sensitivity and false positive frequencies for the 3-30mm diameter nodules. Flyer Scan and MONAI exhibited lower detection sensitivities compared to other companies on their testing results. In the adult testing data there was a mean Size was less in the children's data. The difference in detection sensitivity between the two data types was found by Adult-trained traditional and deep-learning-based lungCAD systems. Smaller size of lung nodules may relate to the performance difference. The results indicate that there needs to be a need for lung nodule computer aided dispatch systems for children."]

['Although pharmaceutical companies conduct clinical trials of novel human epidermal growth factor receptor 2 (HER2)-low-directed drugs, diagnosing HER2-low cancer by immunohistochemistry (IHC) and in situ hybridization (ISH) remains challenging. This study investigates the performance of first-in-kind computerized intelligence to classify samples across gene expression levels and differentiate HER2-low tumors. We classified 251 samples: 142 primary invasive breast cancers (IBCs), 75 ductal carcinomas in situ (DCIS), and 34 mammaplasties (reference) using mRNA expression data from the QuantiGene Plex 2.0 assay. We used <i>g3mclass</i> probabilistic software to assess the number of classes in the assay data, the mean and the variance in each class, diagnostic cutoffs, and the prevalence of each class in the study population. HER2-low (IHC score of 1+ or 2+/ISH-) accounted for 31% of IBC. First, we discovered that HER2-low tumors were represented by cases with normal <i>ERBB2</i> transcript levels that were expected to produce physiologic levels of HER2 (70%) and cases with abnormally upregulated unamplified <i>ERBB2</i> (30%). We termed the latter cancers <i>ERBB2</i>-up as they do not meet the standard definitions for <i>ERBB2</i> overexpression and amplification. Second, HER2-low IBC classified as <i>ERBB2</i>-up had not only abnormally increased luminal growth and adhesion markers (<i>ERBB2</i>, <i>ESR1</i>, <i>PGR</i>, <i>IGF1R</i>, <i>VAV2</i>, <i>VAV3</i>, <i>KRT8</i>, <i>CDH1</i>) but also downregulated myoepithelial marker (<i>KRT5</i>). The vascularization (<i>RAP1</i> and <i>C3G</i>), immune cell infiltration (<i>VAV1</i>), and mesenchymal transition (<i>CDH2</i>) markers were dysregulated. Finally, in the independent cohort of DCIS, 40% of HER2-low DCIS shared similar traits with HER2-low IBC except for rare downregulation of <i>KRT5</i> and no change in <i>C3G</i>, <i>VAV1</i>, and <i>CDH2.</i> We demonstrated how innovative bioinformatic tools could help diagnose cancer across the spectrum of <i>ERBB2</i> expression to aid decision making for HER2-low.', "Although pharmaceutical companies conduct clinical trials for novel human epidermal growth factor 2 (HER2), they are still challenging to diagnose. The performance of the first-in-kind computerized intelligence to classify samples to differentiate them from low-infiltrating tumors was investigated. The 251 samples were classified using the data from the QuantiGene Plex 2.0. We used software to assess the number of classes in the assays data, the mean and the variance in each class, and the prevalence of each class in the study population. 32% of Integrated was accounted for by HER2 score of 1+ or ISH-. First, we found that tumors with low HER2 levels were represented by cases that had normalERBB2 transcript levels that were expected to produce physiologic levels of HER2. The latter cancers are termed ERBB2 up because they don't meet the standard definitions for overexpression and amplification. Second, the HER2-low ibc was classified as i>ERBB2/i>-up and there were also increased luminal growth and markers of attachment. The markers were dys. In the self-contained cohort of DCIS, less than half of them shared the same characteristics, except for rare down regulation of C3G."]

['Deep learning (DL) is an advanced machine learning approach used in different areas such as image analysis, bioinformatics, and natural language processing. A convolutional neural network (CNN) is a representative DL model that is highly advantageous for imaging recognition and classification This study aimed to develop a CNN using lateral cervical spine radiograph to detect cervical spondylotic myelopathy (CSM). We retrospectively recruited 207 patients who visited the spine center of a university hospital. Of them, 96 had CSM (CSM patients) while 111 did not have CSM (non-CSM patients). CNN algorithm was used to detect cervical spondylotic myelopathy. Of the included patients, 70% (145 images) were assigned randomly to the training set, while the remaining 30% (62 images) to the test set to measure the model performance. The accuracy of detecting CSM was 87.1%, and the area under the curve was 0.864 (95% CI, 0.780-0.949). The CNN model using the lateral cervical spine radiographs of each patient could be helpful in the diagnosis of CSM.', "Deep learning is an advanced machine learning approach used in different applications. A CNN is a model that is useful for detecting disease and being able to distinguish it from others. 207 patients were recruited to the spine center of the hospital. There were 96 that were CSM patients and 61 that weren't. CNN used a tool to find spondylotic myelopathy. The tested model performance was measured by 30% of the patients, who were randomly assigned to the training set. The area under the curve was 0.864, and the detecting accuracy was 87.1%. The CNN model can be used to assist in the diagnosis of cleft palate."]

["The objective of this study was to validate the NIH Toolbox Cognition Battery (NIHTB-CB) in Zambian children with and without HIV-infection. Children living with HIV and HIV-exposed, uninfected (HEU) children completed traditional neuropsychological and NIHTB-CB tasks. Using pairwise correlation and a linear regression model we measured associations between traditional measure composite scores and parental ratings of children's abilities, and NIHTB-CB scores. A Receiver Operating Characteristic (ROC) curve was developed to identify participants with impairment. 389 children, 8-17 years old participated. NIHTB-CB and traditional measures converged well as a whole and when comparing analogous individual tests across the two batteries. The NIHTB-CB composite score discriminated between the groups and was positively associated with external criteria for cognitive function: parental ratings of intelligence and school performance. Some English vocabulary and/or an unfamiliar cultural context presented challenges. NIHTB-CB was associated with children's everyday cognitive abilities, though future use may require linguistic and cultural adaptation.", "The purpose of the study was to see if the Toolbox Cognition Battery was valid in a child with HIV-infection. Children living with HIV and HIV-exposed were able to complete traditional psychological tasks. We measured associations between the traditional measure scores and the ratings of children's abilities by using a pairwise correlation and linear regression model. The curve was developed to identify participants with Impairment. 389 children were involved. When comparing individual tests across the two batteries, the measures that are tied together as a whole seem to work out well. The groups were rated differently by the score because of their intelligence and school performance. There were challenges associated with some English vocabulary and cultural context. The daily cognitive ability of children was associated withNIHtB-CB, though future use may require linguistic and cultural adaptation."]

['With the explosive development of smart wearable devices, a serious situation that a large amount of energy waste and environmental pollution caused by electronic discarding needs to be solved urgently. Here, as a throwaway waste material, a chewed gum can be reused for the preparation of wearable iontronics simply. A new gum sensor was constructed by regularly stretching a chewed gum in 6 M NaCl aqueous or even a Chinese edible salt solution for increasing the ionic conductivity. This gum sensor can be shaped arbitrarily, and the preparation process is green, pollution-free, with low energy consumption, and repeatable. Herein, this gum sensor can be utilized for real-time human healthcare monitoring effectively (i.e., facial mood changes, finger flexion, long time walking, and continuous ankle movement) and shows a fast response time of 297 ms and a reliable cycling performance in monitoring body motions. Furthermore, the gum sensor (containing edible salt) can act as a signal transmitter for intelligent information encryption and transmission in the light of the international Morse code with excellent repeatability and stability. Hence, this work will greatly possess wide potential application prospects in wearable electronics and information encryption. This gum sensor also provides a ponderable option in the next generation of artificial intelligence devices, which can address the troubles of material selections in sensor preparation.', 'With the rapid development of smart Wearable Devices, there is a serious issue of energy waste and environmental pollution that needs to be fixed quickly. The preparation of Wearable iontronics can be made using chewed gum as a waste material. The new gum sensor was made by stretching a chewed gum and a Chinese solution for increased ionic conductivity. The gum sensor can be shaped and the process is green, pollution-free, and repeatable. Here in, this gum sensor can be used to monitor humans in real-time. E. A good example of this is when the results show facial mood changes, finger flexibility, long walking and continuous ankle movement. The gum sensor can act as a signal transmitter for an electronic message in the light of the international morse code with excellent repeatability and stability. This work will have potential application possibilities in many areas. The next generation of artificial intelligence devices can have a ponderable option in the gum sensor, which can address the troubles of material selections in sensor preparation.']

['The deoxyribonucleotide (DNA) molecule is a stable carrier for large amounts of genetic information and provides an ideal storage medium for next-generation information processing technologies. Technologies that process DNA information, representing a cross-disciplinary integration of biology and computer techniques, have become attractive substitutes for technologies that process electronic information alone. The detailed applications of DNA technologies can be divided into three components: storage, computing, and self-assembly. The quality of DNA information processing relies on the accuracy of DNA reading. Nanopore detection allows researchers to accurately sequence nucleotides and is thus widely used to read DNA. In this paper, we introduce the principles and development history of nanopore detection and conduct a systematic review of recent developments and specific applications in DNA information processing involving nanopore detection and nanopore-based storage. We also discuss the potential of artificial intelligence in nanopore detection and DNA information processing. This work not only provides new avenues for future nanopore detection development, but also offers a foundation for the construction of more advanced DNA information processing technologies.', 'The deoxyribonucleotide molecule is stable and has great potential for storing next- generation information processing technology. There aretechnologies that process DNA information which are associated with a cross-cutting integration of biology and computer techniques, which are now an attractive substitute for technologies that process electronic information alone. There are three parts to the detailed applications of the DNA technologies. The accuracy of the read is crucial to the quality of the data. Researchers can read DNA by using the nanpore detection method. This paper shows the principles and development history of nanopore detection and reviews recent developments and specific applications in DNA information processing. Artificial intelligence is considered to be the future of detection ofpores in the human body. This work will give new pathways for the development of a future nanopore detection system, while also providing a foundation for the construction of more advanced DNA information Processing technologies.']

['To respond to global challenges of environmental contaminations, pursue more advanced material technologies, and achieve novel biomedical therapies, a variety of plasmas have been applied to wastewater and food processing, biomaterial treatments, and plasma-liquid ignitions. As these applications highly depend on the plasma-liquid interactions, researchers are now focusing on the physical and chemical reactions on the plasma-liquid interface. With massive publications reporting the molecular transfers, chemical pathways, and their effects on plasma treatments, this work provides a new point of view that the plasma-liquid interface can be manipulated by the chamber structure. In the experiment, plasma jet expansion in water is recorded in a cylinder chamber and a stepped-wall one. Data collected from the images show that the stepped-wall structure results in a shorter axial interface propagation, a small volume, more symmetry for the plasma jet, and more stability for the interface. To discover the physical mechanism behind these phenomena, we derived the momentum and energy equations for the plasma-liquid interface during its propagation. Those equations reveal how the stepped-wall structure can be used to manipulate the interface behaviors. Along with our experimental and theoretical investigation of the plasma-liquid interface, such information also sheds light on how the chamber wall structure can be used to manipulate the interface chemical reaction rates, stability, and expansion rate. This work is thus a basis of the future optimization for plasma-liquid treatments and ignitions which will be equipped with a flexible wall controlled by artificial intelligence to automatically achieve a variety of plasma treatment requirements.', "To respond to global environmental challenges, a variety of compounds have been applied to wastewater and food processing. The physical and chemical reactions of the liquid and the solid are now being focused on by researchers. This work shows that the chamber structure can be manipulated by the plasma-liquid interface with results reported in massive publications. The expansion of water's surface plasm in a cylinder chamber and stepped-wall experiment is recorded. Data collected from the images shows that the stepped-wall structure has many advantages in relation to the interface. We were able to understand the physical mechanism of these phenomena with the help of the momentum and energy equations. The equations show how the structure can be manipulated. Information about the chamber wall structure and how it is used for manipulating the interface chemical reaction rates, stability, and expansion rate is also information about the experimental and theoretical investigation of the plasma-liquid interface. The flexible wall controlled by artificial intelligence that will control the current and future performance of the treatments is a result of this work."]

['The aim of this research is to determine the order and importance of impacts of particular anthropological characteristics and technical and tactical competence on success in taekwondo according to opinions of top taekwondo instructors (experts). Partial objectives include analysis of metric characteristics of the measuring instrument, and determining differences between two disciplines (sparring and technical discipline of patterns) and two competition systems (WTF and ITF). In accordance with the aims, the research was conducted on a sample of respondents which consisted of 730 taekwondo instructors from 6 continents and from 69 countries (from which we selected 242 instructors), who are at different success levels in both taekwondo competition systems (styles) and two taekwondo disciplines. The respondents were divided into 3 qualitative subsamples (OST-USP-VRH) using the dependant variable of accomplished results of the instructor. In 6 languages, they electronically evaluated the impact in percentage value (%) of motor and functional skills (MOTFS), morphological characteristics (MORF), psychological profile of an athlete (PSIH), athletic intelligence (INTE) and technical and tactical competence - (TE-TA) on success in taekwondo. The analysis of metric characteristics of the constructed instrument showed a satisfactory degree of agreement (IHr) which is proportional to the level of respondent quality, i.e. it grows along with the increase in instructor quality in all analysed disciplines of both systems. Top instructors assigned the highest portion of impact on success to the motor and functional skills (MOTFS) variable: WTF-SPB=29.1, ITF-SPB=29.2, WTF-THN=35.0, ITF-THN=32.0). Statistically significant differences in opinions of instructors of different styles and disciplines were not recorded in any of the analysed variables. The only exception is the psychological profile of an athlete variable, which WTF instructors of sparring (AM=23.7%), on a significance level of p<0.01, evaluate as having a statistically significantly higher impact on success in tackwondo than WTF instructors of the technical discipline of patterns (15.4%).', "The goal of the research is to come up with an order, and importance, of impacts of particular anthropology, technical and tactical competence, and success in taekwondo. Two competition systems and two disciplines are not the same, and there are differences in the way they work. To fulfill the aims, the research was done on a sample of 730 taekwondo teachers from six continents and 69 countries who were all at different levels of success in taekwondo competition systems. The respondents were divided into 3 qualitative subsamples based on the instructor's accomplished results. In 6 languages they electronically evaluated the impact of percentage value of motor and functional skills, morphological characteristics, psychological profiling of an athlete, athletic intelligence, and technical and tactical competence. The analysis of the metric characteristics showed a satisfactory degree of agreement, which is proportional to the level of respondents' quality. It was e The increase in instructor quality in all disciplines of both systems increases. The instructors assigned the most impact to the motor and functional skills variables. There were not significant differences of opinions of instructors of different styles in the analysed variables. The psychological profile of an athlete variable, which was evaluated by the instructors of sparring as having a significantly higher impact on success than the instructors of pattern-based training, is the only exception."]

['Artificial intelligence (AI), a computer system aiming to mimic human intelligence, is gaining increasing interest and is being incorporated into many fields, including medicine. Stroke medicine is one such area of application of AI, for improving the accuracy of diagnosis and the quality of patient care. For stroke management, adequate analysis of stroke imaging is crucial. Recently, AI techniques have been applied to decipher the data from stroke imaging and have demonstrated some promising results. In the very near future, such AI techniques may play a pivotal role in determining the therapeutic methods and predicting the prognosis for stroke patients in an individualized manner. In this review, we offer a glimpse at the use of AI in stroke imaging, specifically focusing on its technical principles, clinical application, and future perspectives.', "Artificial intelligence is gaining increased interest and is being implemented into many fields, including medicine. The quality of patient care in stroke medicine is one area that is being improved by the use ofArtificial Intelligence. For stroke management, it's crucial that the analysis of the stroke is good. Some promising results have been achieved by using different Artificial Intelligence techniques to decipher the data from strokes. In the very near future, such techniques may be used to determine the therapeutic methods and predict the outcomes for stroke patients. In this review we offer a glimpse at how Artificial Intelligence can be used in stroke diagnostics, focusing on its technical principles, clinical application and future perspectives."]

['Cognitive Application Program Interface (API) is an API of emerging artificial intelligence (AI)-based cloud services, which extracts various contextual information from non-numerical multimedia data including image and audio. Our interest is to apply image-based cognitive APIs to implement flexible and efficient context sensing services in a smart home. In the existing approach with machine learning by us, with the complexity of recognition object and the number of the defined contexts increases by users, it still requires directly manually labeling a moderate scale of data for training and continually try to calling multiple cognitive APIs for feature extraction. In this paper, we propose a novel method that uses a small scale of labeled data to evaluate the capability of cognitive APIs in advance, before training features of the APIs with machine learning, for the flexible and efficient home context sensing. In the proposed method, we exploit document similarity measures and the concepts (i.e., internal cohesion and external isolation) integrate into clustering results, to see how the capability of different cognitive APIs for recognizing each context. By selecting the cognitive APIs that relatively adapt to the defined contexts and data based on the evaluation results, we have achieved the flexible integration and efficient process of cognitive APIs for home context sensing.', 'The Cognitive Application Program Interface is a framework for cloud services that use Artificial Intelligence to retrieve information from multimedia data. We are interested in using cognitive tools to build smart home services. Machine learning with complexity of recognition object and increasing number of defined contexts requires the manually labeling a moderate amount of data for training and continually calling multiple cognitive APIs for features. A novel method is proposed that uses a small scale of labeled data to test the capability of a cognitive application before training features of it with machine learning. We exploit document similarity measures and concepts in the proposed method. Is that e? To see how different cognitive apps can recognize each context, internal cohesion and external isolation integrate into the clustering results. By selecting the cognitiveAPI that relatively adapt to the defined contexts and data, we have achieved flexible integration and efficient process of cognitiveAPI.']

["Advances in artificial intelligence (AI) have significantly improved the abilities of machines. Human-unique abilities, such as art creation, are now being challenged by AI. Recent studies have investigated and compared people's attitudes toward human-made and AI-generated artworks. These results suggest that a negative bias may exist toward the latter. However, none of these previous studies has examined the extent of this bias. In this study, we investigate whether a bias against AI art can be found at an implicit level. Viewers' attitudes toward AI art were assessed using eye-tracking measures and subjective aesthetic evaluations. Visual attention and aesthetic judgments were compared between artworks categorized as human-made and AI-made. The results showed that although it was difficult for individuals to identify AI-generated artwork, they exhibited an implicit prejudice against AI art. Participants looked longer at paintings that they thought were made by humans. No significant effect of categorization of paintings was found in subjective evaluations. These findings suggest that although human and AI art may be perceived as having similar aesthetic values, an implicit negative bias toward AI art exists. Although AI can now perform creative tasks, artistic creativity is still considered a human prerogative.", "The machines have improved in their abilities because of advances in machine learning. Art creation, like other human unique abilities, are being challenged by artificial intelligence. Recent studies looked at the opinions of the public on human made and artificial intelligence-generated artworks. The results suggest a negative bias in favor of the latter. None of the previous studies have examined the extent of this bias. We investigate whether bias against the art can be found at an implicit level. Eye-tracing measures and subjective aesthetic evaluations were used to assess viewers' attitudes against Artificial Intelligence. The artworks were compared to look at and to think about. The results showed that even though it was difficult for individuals to see the artwork, they held an implicit prejudice against it. People looking at paintings that they thought were made by humans. There was no significant effect of categorization on the results of the evaluations. The findings suggest that a negative bias towards artificial intelligence may exist. Creative tasks are performed by humans and artistic creativity is still considered a human prerogative."]

['Utilizing artificial intelligence (AI) in drug design represents an advanced approach for identifying targets and developing new drugs. Integrating AI techniques significantly reduces the workload involved in drug development and enhances the efficiency of early-stage drug discovery. This review aims to present a comprehensive overview of the utilization of AI methods in the field of small drug design, with a specific focus on four key areas: protein structure prediction, molecular virtual screening, molecular design, and absorption, distribution, metabolism, excretion, and toxicity (ADMET) prediction. Additionally, the role and limitations of AI in drug development are explored, and the impact of AI on decision-making processes is studied. It is important to note that while AI can bring numerous benefits to the early stage of drug development, the direction and quality of decision-making should still be emphasized, as AI should be considered as a tool rather than a decisive factor.', 'Artificial Intelligence is used in drug designs to identify targets and develop new drugs. The workload involved in the drug development process is greatly reduced by integrating artificial intelligence techniques. The main focus of this review is on the use of artificial Intelligence in small drug design with a focus on four areas. The impact of Artificial Intelligence on decision-making processes is studied. The direction and quality of decision- making should be emphasized, as the direction and quality of decisions can still be improved by artificial intelligence.']

['Studies of executive function and its relationship with brain T2-weighted hyperintensities in children with neurofibromatosis type 1 (NF1) have yielded inconsistent results. We examined 16 children with NF1 aged 8 to 15 years, of normal intelligence, and compared their findings to those of 16 siblings and 16 typically developing children using the Behavioural Assessment of the Dysexecutive Syndrome in Children (BADS-C). NF1 patients had an adequate overall score at BADS-C, but showed significantly lower performance than typical peers in the Key Search subtest. This is a task that must be solved without any given rules, in which subjects must devise a strategy and an efficient search pattern transferable to other similar real situations. The Key Search scores were not correlated with number and signal characteristics of T2-weighted hyperintensities. Planning without external indications is impaired in children with NF1 because they have to rely entirely on self-organization and monitoring; this study provides information for remediation programs designed to improve functioning in daily life.', 'The results of recent studies of executive function and brain T 2-weighted hyperintensities in children with neuro Fibromatosis type 1 have been mixed. We examined 16 children who were normal intelligence and found similarities with those 16 siblings and 16 typically developing children using the Behavioral Assessment of the Dysexecutive Syndrome in Children. The patients had a score that was adequate but had lower performance than their peers in the Key Search subtest. This task needs to be solved without rules and it needs to be done in a way that is easy to repeat in other situations. The number and signal characteristics of the T2 were not correlated with the Key Search scores. The study showed that for children with NF1, planing without external indications is impaired because of their reliance on self-organization and monitoring.']

['We aimed to develop an ovarian cancer-specific predictive framework for clinical use platinum-sensitivity and prognosis using machine learning methods based on multiple biomarkers, including circulating tumor cells (CTCs). We enrolled 156 epithelial ovarian cancer (EOC) patients, randomly assigned into the training and validation cohorts. Eight machine learning classifiers, including Random Forest (RF), Support Vector Machine, Gradient Boosting Machine, Conditional RF, Neural Network, Naive Bayes, Elastic Net, and Logistic Regression, were used to derive predictive information from 11 peripheral blood parameters, including CTCs. Through the advanced CanPatrol CTC-enrichment technique, we detect CTCs and classify them into subpopulations: epithelial, mesenchymal, and hybrids. Survival curves were generated by Kaplan-Meier method and calculated through the Log rank test. Machine learning techniques, especially the Random Forest classifier, were superior to conventional regression-based analyses in predicting multiple clinical parameters related to EOC. The values for the receiver operating characteristic (ROC) curve for segregating EOC with advanced clinical stages and platinum-sensitivity were 0.796 (95% CI, 0.727-0.866) and 0.809 (95% CI, 0.742-0.876), respectively. Stepwise, we used the unsupervised clustering analysis to identify EOC subgroups with significantly worse overall survival (OS), especially in the advanced-stage group with the p-value of 0.0018 (HR, 2.716; 95% CI, 1.602-4.605) for progression-free survival (PFS) and 0.0037 (HR, 2.359; 95% CI, 1.752-6.390) for overall survival (OS). Machine learning systems could provide risk stratification for EOC patients before initial intervention through blood variables, including circulating tumor cells. The predictive algorithms could facilitate personalized treatment options through promising pre-treatment stratification of EOC patients. ChiCTR-DDD-16009601 Registered 25 October 2016.', 'We wanted to develop a ovarian cancer-specific framework with machine learning methods to understand ovarian cancer better. 156 ovarian cancer patients were randomly assigned into the training cohort. A group of eight machine learning engines, including Random Forest, Support Vector Machine, Neural Network, Naive Bayes, and Logistic Regression, were used to generate information from 11 peripheral blood parameters. We can detect CTCs through the advanced CanPatrol method and we can classify them into different populations. Kaplan-Meier method and Log rank test were used to calculate survival curves. Machine learning techniques were better than regression-based analyses when it came to predicting clinical parameters related to EOC. The receiver operating characteristic curves are used to segregating EOC with advanced clinical stages and platinum-sensitivity. The advanced stage group with the p-value of 0.0018 was identified by clustering analysis to be the subgroup with significantly worse overall survival. Risk is given by machine learning systems before initial intervention for EOC patients. The EOC patients would benefit from the promising pre-treatment stratification given by the Predictor-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873-8873']

['The increasing implementation of and reliance on machine-learning (ML) algorithms to perform tasks, deliver services and make decisions in health and healthcare have made the need for fairness in ML, and more specifically in healthcare ML algorithms (HMLA), a very important and urgent task. However, while the debate on fairness in the ethics of artificial intelligence (AI) and in HMLA has grown significantly over the last decade, the very concept of fairness as an ethical value has not yet been sufficiently explored. Our paper aims to fill this gap and address the AI ethics principle of fairness from a conceptual standpoint, drawing insights from accounts of fairness elaborated in moral philosophy and using them to conceptualise fairness as an ethical value and to redefine fairness in HMLA accordingly. To achieve our goal, following a first section aimed at clarifying the background, methodology and structure of the paper, in the second section, we provide an overview of the discussion of the AI ethics principle of fairness in HMLA and show that the concept of fairness underlying this debate is framed in purely distributive terms and overlaps with non-discrimination, which is defined in turn as the absence of biases. After showing that this framing is inadequate, in the third section, we pursue an ethical inquiry into the concept of fairness and argue that fairness ought to be conceived of as an ethical value. Following a clarification of the relationship between fairness and non-discrimination, we show that the two do not overlap and that fairness requires much more than just non-discrimination. Moreover, we highlight that fairness not only has a distributive but also a socio-relational dimension. Finally, we pinpoint the constitutive components of fairness. In doing so, we base our arguments on a renewed reflection on the concept of respect, which goes beyond the idea of equal respect to include respect for individual persons. In the fourth section, we analyse the implications of our conceptual redefinition of fairness as an ethical value in the discussion of fairness in HMLA. Here, we claim that fairness requires more than non-discrimination and the absence of biases as well as more than just distribution; it needs to ensure that HMLA respects persons both as persons and as particular individuals. Finally, in the fifth section, we sketch some broader implications and show how our inquiry can contribute to making HMLA and, more generally, AI promote the social good and a fairer society.', 'There is an urgent need for fairness in machine-learning software that is used in healthcare. Over the last decade, the debate on fairness in the ethics of artificial intelligence has grown but the concept of fairness is still being considered. From a conceptual perspective, our paper seeks to fill the gap by drawing insights from an account of fairness elaborated in moral philosophy, and using it to conceptualise fairness as an ethical value and to redefine fairness in HMLA accordingly. To accomplish our goal, we provide an overview of the discussion of the fairness principle of fairness in the first section and show how the concept of fairness is framed in purely distributive fashion. In the third section, we argue that fairness ought to be conceived of as an ethical value after we show that this framing is inadequate. Following a clarification of the relationship between fairness and non-discrimination, we show that it does not overlap and that fairness requires much more than just non- discrimination. We show that fairness has more than one aspect, namely it has a socio-relational one. We have finally located the components of fairness. We use a renewed reflection to argue for equal respect to include respect for individual persons. The implications of our conceptual redefinition of fairness as an ethical value is discussed in the fourth part of the section. There is a claim that fairness requires more than simply non- discrimination, as well as the ability to discriminate and the presence of biases, and it needs to be ensured that H In the fifth section, we show how our inquiry can be used to benefit the social good and a equitable society.']

