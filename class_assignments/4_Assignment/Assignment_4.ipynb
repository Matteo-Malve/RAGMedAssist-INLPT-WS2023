{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive/NLPT_Assignment4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji1GFISwErMn",
        "outputId": "281a6b84-f840-4eaa-fe2a-02809ca186e1"
      },
      "id": "ji1GFISwErMn",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive/NLPT_Assignment4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be9f7653"
      },
      "source": [
        "**Heidelberg University**\n",
        "\n",
        "**Data Science  Group**\n",
        "    \n",
        "Prof. Dr. Michael Gertz  \n",
        "\n",
        "Ashish Chouhan, Satya Almasian, John Ziegler, Jayson Salazar, Nicolas Reuter\n",
        "    \n",
        "January 16, 2024\n",
        "    \n",
        "Natural Language Processing with Transformers\n",
        "\n",
        "Winter Semster 2023/2024     \n",
        "***"
      ],
      "id": "be9f7653"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "258e9648"
      },
      "source": [
        "# **Assignment 4: Question Answering**\n",
        "**Due**: Monday, January 29, 2024, 2pm, via [Moodle](https://moodle.uni-heidelberg.de/course/view.php?id=19251)\n",
        "\n"
      ],
      "id": "258e9648"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc27ad9e"
      },
      "source": [
        "### **Submission Guidelines**\n",
        "\n",
        "- Solutions need to be uploaded as a **single** Jupyter notebook. You will find several pre-filled code segments in the notebook, your task is to fill in the missing cells.\n",
        "- For the written solution, use LaTeX in markdown inside the same notebook. Do **not** hand in a separate file for it.\n",
        "- Download the .zip file containing the dataset but do **not** upload it with your solution.\n",
        "- It is sufficient if one person per group uploads the solution to Moodle, but make sure that the full names of all team members are given in the notebook.\n",
        "\n",
        "***"
      ],
      "id": "fc27ad9e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HETm7VsBkmLq"
      },
      "source": [
        "## **Task 1: Retrieval Augmented Generation (RAG)** ( 4.5 + 3 + 4 + 3 + 1.5 = 16 points)"
      ],
      "id": "HETm7VsBkmLq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ODkKBIRkrfe"
      },
      "source": [
        "In this task, we look at using the open source `Llama-13b-chat` model for creating a RAG system. You must first apply for access to Llama 2 models via [this](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) form (access is typically granted within a few hours). etrieval augmented generation you also need to request to use the model on Hugging Face by going to the [model](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) card. ***Note that the emails you provide for your Hugging Face account must match the email you used to request Llama 2.***\n",
        "\n",
        "The final piece that you need is a Hugging Face authentication token. You can find such a token by going to the `setting` in your Hugging Face profile, under the `Access Token` menu you can generate a new token.\n",
        "\n",
        "To store the document you will need a free Pinecone [API key](https://app.pinecone.io/).\n",
        "Make sure you have these pieces ready before starting to work on this task.\n",
        "\n",
        "----\n",
        "When ready, let's start by downloading the necessary packages.\n",
        "\n",
        "It is advised to proceed with this notebook with a GPU (if you are on Colab make sure that a GPU environment is activated.)\n"
      ],
      "id": "0ODkKBIRkrfe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO1OwuHTMo75"
      },
      "source": [
        "Place all the access tokens in the `.env` file and upload it to the working directory (if you are running this notebook locally, you can change the path to fit your working directory). Please use the following format:\n",
        "\n",
        "\n",
        "```\n",
        "HF_AUTH= \"Hugging Face Authentication Key\"\n",
        "PINECONE_API_KEY=\"Pincone API Key\"\n",
        "PINECONE_ENVIRONMENT=\"Pinecone Environment\"\n",
        "```\n",
        "\n",
        "Run the cell below to load the access tokens into the environment variables."
      ],
      "id": "yO1OwuHTMo75"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1Dxt-e1bg73b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f43cb627-6493-49af-f331-a8d09403dcef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.3 in /usr/local/lib/python3.10/dist-packages (1.26.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy==1.26.3\n",
        "%pip install python-dotenv"
      ],
      "id": "1Dxt-e1bg73b"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install langchain\n",
        "from langchain.document_loaders import ReadTheDocsLoader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8fAL_4OyJHC",
        "outputId": "bf04214c-e301-4d55-99ed-8942379e6c53"
      },
      "id": "Y8fAL_4OyJHC",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.13 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.13)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.9 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.11)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.81)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.3)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.5.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.9->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.9->langchain) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.14.6)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EqmnxgpALbVV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# load environment variables from .env file\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())"
      ],
      "id": "EqmnxgpALbVV"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "has_gpu = torch.cuda.is_available()\n",
        "has_mps = torch.backends.mps.is_built()\n",
        "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "nnHqWB68QIo0"
      },
      "id": "nnHqWB68QIo0",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yE3EOlhTrKo6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cbc320a3-5ac7-41a4-a630-41c4b355c05b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: litellm in /usr/local/lib/python3.10/dist-packages (1.17.13)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from litellm) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from litellm) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (6.11.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from litellm) (3.1.3)\n",
            "Requirement already satisfied: openai>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (1.8.0)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (2.31.0)\n",
            "Requirement already satisfied: tiktoken>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from litellm) (0.5.2)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from litellm) (0.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm) (3.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm) (2.1.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->litellm) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.0.0->litellm) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->litellm) (0.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->litellm) (2.5.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->litellm) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->litellm) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->litellm) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->litellm) (2023.11.17)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.4.0->litellm) (2023.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm) (4.0.3)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers->litellm) (0.20.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.0.0->litellm) (1.2.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.0.0->litellm) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0.0->litellm) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->litellm) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->litellm) (2023.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->litellm) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0.0->litellm) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0.0->litellm) (2.14.6)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.26.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers) (12.3.101)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Collecting torch>=1.6.0 (from sentence-transformers)\n",
            "  Using cached torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.2\n",
            "    Uninstalling torch-2.1.2:\n",
            "      Successfully uninstalled torch-2.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xformers 0.0.23.post1 requires torch==2.1.2, but you have torch 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2023.11.17)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.9.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.7)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.26.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.13 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.13)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.9 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.11)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.81)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.3)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.5.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.9->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.9->langchain) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.14.6)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.9->langchain) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: xformers in /usr/local/lib/python3.10/dist-packages (0.0.23.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.26.3)\n",
            "Collecting torch==2.1.2 (from xformers)\n",
            "  Using cached torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->xformers) (12.3.101)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.2->xformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.2->xformers) (1.3.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0\n",
            "    Uninstalling torch-2.1.0:\n",
            "      Successfully uninstalled torch-2.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.1.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->bitsandbytes) (1.26.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.5.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.4.22)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.0.3)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.5.3)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.109.0)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.26.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.3)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.3.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.9.0)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.4.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.16.3)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.43b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.22.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.4.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.60.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.1.2)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (29.0.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.1.0)\n",
            "Requirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (23.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Requirement already satisfied: starlette<0.36.0,>=0.35.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.35.1)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2023.11.17)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.17.3)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (6.11.0)\n",
            "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.22.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.22.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.43b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.43b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.43b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.7.2)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.14.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.6)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.20.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.36.0,>=0.35.0->fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.36.0,>=0.35.0->fastapi>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.36.0,>=0.35.0->fastapi>=0.95.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.3)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.1)\n",
            "Requirement already satisfied: bert_score in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.1.2)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.5.3)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.35.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.26.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.66.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (23.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2023.3.post1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert_score) (12.3.101)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.20.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2023.11.17)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install litellm\n",
        "%pip install -qU trulens_eval pydantic fastapi kaleido python-multipart uvicorn cohere openai tiktoken \"llama-index\"\n",
        "%pip install transformers\n",
        "%pip install sentence-transformers\n",
        "%pip install pinecone-client\n",
        "%pip install datasets\n",
        "%pip install accelerate\n",
        "%pip install einops\n",
        "%pip install langchain\n",
        "%pip install xformers\n",
        "%pip install bitsandbytes\n",
        "%pip install matplotlib seaborn tqdm\n",
        "%pip install chromadb\n",
        "%pip install evaluate\n",
        "%pip install rouge_score\n",
        "%pip install bert_score"
      ],
      "id": "yE3EOlhTrKo6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLDAPMVST2Ds"
      },
      "source": [
        "\n",
        "\n",
        "## Subtask 1.1: Data Preparation\n",
        "\n"
      ],
      "id": "dLDAPMVST2Ds"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b9odZyFLDin"
      },
      "source": [
        "We need a collection of documents to perform our retrieval on. To make it closer to your final project, you will be downloading and using a subset of the LangChain documentation. We get some of the `.html` files located on the site. The code below will download all HTML files from the links on the webpage into a `docs` directory. `-l1` limits the download to only the first level of depth.\n"
      ],
      "id": "9b9odZyFLDin"
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "JXe86ZKVFfrm"
      },
      "id": "JXe86ZKVFfrm",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qiodFLkaLUsF"
      },
      "outputs": [],
      "source": [
        "#!wget -r -l1 -A.html -P docs https://api.python.langchain.com/en/stable/langchain_api_reference.html"
      ],
      "id": "qiodFLkaLUsF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DWa9M6JLb8O"
      },
      "source": [
        " The docs are going to be used as input text for answering questions that a normal language model might not be aware of (LangChain docs is not necessarily part of its training data of Llama2). We can use LangChain itself to process these docs. Use the [ReadTheDocsLoader](https://python.langchain.com/docs/integrations/document_loaders/readthedocs_documentation) to load the docs from the `docs` folder.\n",
        "\n",
        " At the time of creating this notebook, there  `423` documents were downloaded. However, since the documentation is being updated regularly this number might be different for you."
      ],
      "id": "5DWa9M6JLb8O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nd8ufORKLVy0"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import ReadTheDocsLoader\n",
        "#### your code ####\n",
        "loader = ReadTheDocsLoader(\"docs\")\n",
        "docs = loader.load()\n",
        "#### your code ####\n",
        "len(docs)"
      ],
      "id": "Nd8ufORKLVy0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKs0_OrBMQ7M"
      },
      "source": [
        "Let's take a look at one of the documents. You see that LangChain has created a `Document` object. Look at the example below and fill in the cells to print out the text content and URL of the page (the URL of the page should starts with `https://`)."
      ],
      "id": "yKs0_OrBMQ7M"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "vcpuudNRLV7k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fe6b287-d859-43e5-b0d2-79365963baf2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\"langchain.agents.agent.Agent¶\\nclass langchain.agents.agent.Agent[source]¶\\nBases: BaseSingleActionAgent\\nAgent that calls the language model and deciding the action.\\nThis is driven by an LLMChain. The prompt in the LLMChain MUST include\\na variable called “agent_scratchpad” where the agent can put its\\nintermediary work.\\nCreate a new model by parsing and validating input data from keyword arguments.\\nRaises ValidationError if the input data cannot be parsed to form a valid model.\\nparam allowed_tools: Optional[List[str]] = None¶\\nparam llm_chain: langchain.chains.llm.LLMChain [Required]¶\\nparam output_parser: langchain.agents.agent.AgentOutputParser [Required]¶\\nasync aplan(intermediate_steps: List[Tuple[AgentAction, str]], callbacks: Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]] = None, **kwargs: Any) → Union[AgentAction, AgentFinish][source]¶\\nGiven input, decided what to do.\\nParameters\\nintermediate_steps – Steps the LLM has taken to date,\\nalong with observations\\ncallbacks – Callbacks to run.\\n**kwargs – User inputs.\\nReturns\\nAction specifying what tool to use.\\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model¶\\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\\nDefault values are respected, but no other validation is performed.\\nBehaves as if Config.extra = ‘allow’ was set since it adds all passed values\\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model¶\\nDuplicate a model, optionally choose which fields to include, exclude and change.\\nParameters\\ninclude – fields to include in new model\\nexclude – fields to exclude from new model, as with values this takes precedence over include\\nupdate – values to change/add in the new model. Note: the data is not validated before creating\\nthe new model: you should trust this data\\ndeep – set to True to make a deep copy of the model\\nReturns\\nnew model instance\\nabstract classmethod create_prompt(tools: Sequence[BaseTool]) → BasePromptTemplate[source]¶\\nCreate a prompt for this class.\\ndict(**kwargs: Any) → Dict[source]¶\\nReturn dictionary representation of agent.\\nclassmethod from_llm_and_tools(llm: BaseLanguageModel, tools: Sequence[BaseTool], callback_manager: Optional[BaseCallbackManager] = None, output_parser: Optional[AgentOutputParser] = None, **kwargs: Any) → Agent[source]¶\\nConstruct an agent from an LLM and tools.\\nclassmethod from_orm(obj: Any) → Model¶\\nget_allowed_tools() → Optional[List[str]][source]¶\\nget_full_inputs(intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any) → Dict[str, Any][source]¶\\nCreate the full inputs for the LLMChain from intermediate steps.\\njson(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode¶\\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\\nclassmethod parse_file(path: Union[str, Path], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False) → Model¶\\nclassmethod parse_obj(obj: Any) → Model¶\\nclassmethod parse_raw(b: Union[str, bytes], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False) → Model¶\\nplan(intermediate_steps: List[Tuple[AgentAction, str]], callbacks: Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]] = None, **kwargs: Any) → Union[AgentAction, AgentFinish][source]¶\\nGiven input, decided what to do.\\nParameters\\nintermediate_steps – Steps the LLM has taken to date,\\nalong with observations\\ncallbacks – Callbacks to run.\\n**kwargs – User inputs.\\nReturns\\nAction specifying what tool to use.\\nreturn_stopped_response(early_stopping_method: str, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any) → AgentFinish[source]¶\\nReturn response when agent has been stopped due to max iterations.\\nsave(file_path: Union[Path, str]) → None¶\\nSave the agent.\\nParameters\\nfile_path – Path to file to save the agent to.\\nExample:\\n.. code-block:: python\\n# If working with agent executor\\nagent.agent.save(file_path=”path/agent.yaml”)\\nclassmethod schema(by_alias: bool = True, ref_template: unicode = '#/definitions/{model}') → DictStrAny¶\\nclassmethod schema_json(*, by_alias: bool = True, ref_template: unicode = '#/definitions/{model}', **dumps_kwargs: Any) → unicode¶\\ntool_run_logging_kwargs() → Dict[source]¶\\nclassmethod update_forward_refs(**localns: Any) → None¶\\nTry to update ForwardRefs on fields based on this Model, globalns and localns.\\nclassmethod validate(value: Any) → Model¶\\nabstract property llm_prefix: str¶\\nPrefix to append the LLM call with.\\nabstract property observation_prefix: str¶\\nPrefix to append the observation with.\\nproperty return_values: List[str]¶\\nReturn values of the agent.\", metadata={'source': 'docs/api.python.langchain.com/en/stable/agents/langchain.agents.agent.Agent.html'})"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "docs[10]"
      ],
      "id": "vcpuudNRLV7k"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "5Q24oWkGM3IP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbeed8d9-7d97-4445-f1f2-87da33d7b5a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "langchain.agents.agent.Agent¶\n",
            "class langchain.agents.agent.Agent[source]¶\n",
            "Bases: BaseSingleActionAgent\n",
            "Agent that calls the language model and deciding the action.\n",
            "This is driven by an LLMChain. The prompt in the LLMChain MUST include\n",
            "a variable called “agent_scratchpad” where the agent can put its\n",
            "intermediary work.\n",
            "Create a new model by parsing and validating input data from keyword arguments.\n",
            "Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
            "param allowed_tools: Optional[List[str]] = None¶\n",
            "param llm_chain: langchain.chains.llm.LLMChain [Required]¶\n",
            "param output_parser: langchain.agents.agent.AgentOutputParser [Required]¶\n",
            "async aplan(intermediate_steps: List[Tuple[AgentAction, str]], callbacks: Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]] = None, **kwargs: Any) → Union[AgentAction, AgentFinish][source]¶\n",
            "Given input, decided what to do.\n",
            "Parameters\n",
            "intermediate_steps – Steps the LLM has taken to date,\n",
            "along with observations\n",
            "callbacks – Callbacks to run.\n",
            "**kwargs – User inputs.\n",
            "Returns\n",
            "Action specifying what tool to use.\n",
            "classmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) → Model¶\n",
            "Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
            "Default values are respected, but no other validation is performed.\n",
            "Behaves as if Config.extra = ‘allow’ was set since it adds all passed values\n",
            "copy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) → Model¶\n",
            "Duplicate a model, optionally choose which fields to include, exclude and change.\n",
            "Parameters\n",
            "include – fields to include in new model\n",
            "exclude – fields to exclude from new model, as with values this takes precedence over include\n",
            "update – values to change/add in the new model. Note: the data is not validated before creating\n",
            "the new model: you should trust this data\n",
            "deep – set to True to make a deep copy of the model\n",
            "Returns\n",
            "new model instance\n",
            "abstract classmethod create_prompt(tools: Sequence[BaseTool]) → BasePromptTemplate[source]¶\n",
            "Create a prompt for this class.\n",
            "dict(**kwargs: Any) → Dict[source]¶\n",
            "Return dictionary representation of agent.\n",
            "classmethod from_llm_and_tools(llm: BaseLanguageModel, tools: Sequence[BaseTool], callback_manager: Optional[BaseCallbackManager] = None, output_parser: Optional[AgentOutputParser] = None, **kwargs: Any) → Agent[source]¶\n",
            "Construct an agent from an LLM and tools.\n",
            "classmethod from_orm(obj: Any) → Model¶\n",
            "get_allowed_tools() → Optional[List[str]][source]¶\n",
            "get_full_inputs(intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any) → Dict[str, Any][source]¶\n",
            "Create the full inputs for the LLMChain from intermediate steps.\n",
            "json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) → unicode¶\n",
            "Generate a JSON representation of the model, include and exclude arguments as per dict().\n",
            "encoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\n",
            "classmethod parse_file(path: Union[str, Path], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False) → Model¶\n",
            "classmethod parse_obj(obj: Any) → Model¶\n",
            "classmethod parse_raw(b: Union[str, bytes], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False) → Model¶\n",
            "plan(intermediate_steps: List[Tuple[AgentAction, str]], callbacks: Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]] = None, **kwargs: Any) → Union[AgentAction, AgentFinish][source]¶\n",
            "Given input, decided what to do.\n",
            "Parameters\n",
            "intermediate_steps – Steps the LLM has taken to date,\n",
            "along with observations\n",
            "callbacks – Callbacks to run.\n",
            "**kwargs – User inputs.\n",
            "Returns\n",
            "Action specifying what tool to use.\n",
            "return_stopped_response(early_stopping_method: str, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any) → AgentFinish[source]¶\n",
            "Return response when agent has been stopped due to max iterations.\n",
            "save(file_path: Union[Path, str]) → None¶\n",
            "Save the agent.\n",
            "Parameters\n",
            "file_path – Path to file to save the agent to.\n",
            "Example:\n",
            ".. code-block:: python\n",
            "# If working with agent executor\n",
            "agent.agent.save(file_path=”path/agent.yaml”)\n",
            "classmethod schema(by_alias: bool = True, ref_template: unicode = '#/definitions/{model}') → DictStrAny¶\n",
            "classmethod schema_json(*, by_alias: bool = True, ref_template: unicode = '#/definitions/{model}', **dumps_kwargs: Any) → unicode¶\n",
            "tool_run_logging_kwargs() → Dict[source]¶\n",
            "classmethod update_forward_refs(**localns: Any) → None¶\n",
            "Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
            "classmethod validate(value: Any) → Model¶\n",
            "abstract property llm_prefix: str¶\n",
            "Prefix to append the LLM call with.\n",
            "abstract property observation_prefix: str¶\n",
            "Prefix to append the observation with.\n",
            "property return_values: List[str]¶\n",
            "Return values of the agent.\n",
            "https://api.python.langchain.com/en/latest/agents/langchain.agents.agent.Agent.html\n"
          ]
        }
      ],
      "source": [
        "#### your code ####\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def retrieve_url(doc):\n",
        "  with open(doc.dict()['metadata']['source']) as original_file:\n",
        "    soup = BeautifulSoup(original_file, 'html.parser')\n",
        "    # Find the canonical link tag\n",
        "    canonical_tag = soup.find('link', {'rel': 'canonical'})\n",
        "\n",
        "    # Extract the URL from the href attribute\n",
        "    if canonical_tag:\n",
        "      url = canonical_tag.get('href')\n",
        "    else:\n",
        "      print(\"No canonical link found.\")\n",
        "  return url\n",
        "\n",
        "page_content=docs[10].page_content\n",
        "page_url=retrieve_url(docs[10])\n",
        "#### your code ####\n",
        "\n",
        "print(page_content)\n",
        "print(page_url)"
      ],
      "id": "5Q24oWkGM3IP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfZKdZJsMeyj"
      },
      "source": [
        "As you can imagine the documents can be long and if multiple of them are required as context to answer questions, we need to take the document lengths into account.\n",
        "This is due to the fact that language models do not have unlimited context span. In our case, we plan to use Llama2 for this project, where the maximum token limit is 4096. This limit is not only the input but also takes the generated output into account, moreover, you need to leave room for the query and instructions as well. Therefore, it is important to chunk the longer documents into smaller-sized fragments.\n",
        "\n",
        "Based on your use case and how many contexts you plan to feed into the model the length of these fragments will differ.\n",
        "In this case, we choose to assign 2000 tokens to context and choose to generate the answer from 5 context fragments, which leaves us with 400 tokens per context fragment as the maximum chunk size.\n",
        "\n",
        "To count the number of tokens in a chunk, we need to load the correct tokenizer for Llama2. Fill the code cell below to load the correct tokenizer and use it to complete the function that counts the number of tokens per given chunk.\n",
        "\n",
        "**Hint:** you need to use your Hugging Face authentication token to load the tokenizer."
      ],
      "id": "OfZKdZJsMeyj"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "WjOHBqXLLWDA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199,
          "referenced_widgets": [
            "b3aaea2f2d6e49f79a77f6c3b6a95c22",
            "c160783d70f84b43bcadd13bb260a3d3",
            "ae0fc8dc93fe4641a0d842d49ae1ce81",
            "c5ca275595bf4ab48c670baa43871453",
            "d04d3a3649a04eb6ad34b7bfdcdcbc67",
            "33534a682ca043e69c908166075850f1",
            "e1a2b3b7dc124cae9c79a56b68d6d7c0",
            "74f8bc48fcfc40ceb853010e517bbd2c",
            "a74f9c115a774fe4af5e597bcc0ba170",
            "6ecc873ae4314522878249cdc07370cb",
            "92ba58dca267413f9658d65a1a765171",
            "518b8a9fbfac42aab645663178c35c17",
            "ebae8b7f31a24cb4ad50846224b429c0",
            "adcebbf08ae249b9aef36f62fcc88e35",
            "cd3c6c3b9c7e4687ba6eb2ce220f5967",
            "d693e7629084410ea18fbfeab5a7970d",
            "e0b327cadb6a4c76ac7012a4658b82bd",
            "b0477e1db4a647a6b855e9dbfcbe6391",
            "a7b30ff0a9e941b0ac01efb903a8dbbc",
            "9e149ef92db94b96a4d1612670f0b066",
            "37957f8a2a9f4b8690896d6b9844353c",
            "0a23313075f04af6a48bf56296387d20",
            "8f48c81e9b8c49eb995da8568a6c892b",
            "f146548c7c1a488aa330f1dc13c7c971",
            "c32b7261bef7440abdf7dac3c14f81db",
            "de3f70165ccd457fa6df876cfad93dc0",
            "17f9e6e7af704eb48499462d6b62cc79",
            "50b5e9541014411bbf13a2c3c033125c",
            "b15a1df1910c4cbab879a329acf2de8a",
            "7d9a8f9bf47345208f72e80b6451d9b5",
            "6f168e94497e49f8b66963c62a98cc83",
            "ad089994c71147fd801cee3d3c79f766",
            "f25a9182d4764f20a458db3046f35e45",
            "8a5f5c8b4d844f34a14d8a2a0ad308e2",
            "9f7e44bcebc2419eb47a2ad3805ab212",
            "560d97da90b14b8189aebe85ca39e3f4",
            "0d1dfd462b0c438589ad253405135e06",
            "e688f429b5484283bacf6616c80dd58f",
            "12095966ada14a9591b709aea144ff64",
            "70dc32be1bcb4cdc81bca755261990f7",
            "c055418913144ee1ae30770caf93135a",
            "7216aa3e2adb479db509bb8b0bd0c482",
            "82041c5e64c44eaea530eb7819c5ea8b",
            "8ed2803fbdb44812bc7e6c13de80d0cd"
          ]
        },
        "outputId": "db2c4a9f-b1dc-4c6c-b3dc-162087539de0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:671: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3aaea2f2d6e49f79a77f6c3b6a95c22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "518b8a9fbfac42aab645663178c35c17"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f48c81e9b8c49eb995da8568a6c892b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a5f5c8b4d844f34a14d8a2a0ad308e2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#If you get an error here during the first import from the `transformers` package, restart the kernel and try again.\n",
        "#### your code ####\n",
        "import transformers\n",
        "\n",
        "model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=os.environ.get('HF_AUTH')\n",
        "  )\n",
        "#### your code ####"
      ],
      "id": "WjOHBqXLLWDA"
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "G5GutpFdLWFu"
      },
      "outputs": [],
      "source": [
        "def token_len(text):\n",
        "  #### your code ####\n",
        "\n",
        "  # Tokenize the input text\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "\n",
        "  # Calculate the length of the tokenized sequence\n",
        "  tokens_length = len(tokens)\n",
        "\n",
        "  return tokens_length\n",
        "\n",
        "  #### your code ####"
      ],
      "id": "G5GutpFdLWFu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pdG2JeGPfcS"
      },
      "source": [
        "Count the number of tokens for all documents and use it to compute minimum, maximum, and average token count statistics across all documents. Depending on how the documentation is updated by the time you run the cell below the numbers might slightly differ."
      ],
      "id": "2pdG2JeGPfcS"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "HdAdFPYyLWIM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b79222f8-d961-4b6c-e658-a67db659194c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min: 47\n",
            "Avg: 2662.9763593380617\n",
            "Max: 36799\n"
          ]
        }
      ],
      "source": [
        "#### your code ####\n",
        "import numpy as np\n",
        "\n",
        "token_counts = [token_len(doc.page_content) for doc in docs]\n",
        "min_tokens= np.min(token_counts)\n",
        "avg_tokens= np.average(token_counts)\n",
        "max_tokens= np.max(token_counts)\n",
        "#### your code ####\n",
        "print(f\"\"\"Min: {min_tokens}\n",
        "Avg: {avg_tokens}\n",
        "Max: {max_tokens}\"\"\")"
      ],
      "id": "HdAdFPYyLWIM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz9rYYRtMo2N"
      },
      "source": [
        "Now we will use LangChain's built-in chunking functionality to split the text into smaller chunks. LangChain offers a variety of text splitters that you can check out [here](https://api.python.langchain.com/en/latest/langchain_api_reference.html#module-langchain.text_splitter).\n",
        "Use the general-purpose splitter that splits text by recursively looking at characters. Use this class to split the text into 400 token-sized chunks, where the length of each chunk is computed based on the `token_len` function. The length is not the only criterion for splitting, if any of these separators `'\\n\\n', '\\n', ' ', ''` is encountered, we will have a new chunk.\n",
        "Since splitting only based on maximum length might result in incoherent chunks for every consecutive chunk, let the chunk overlap by 50 tokens. This way,  we preserve some of the previous context while chunking."
      ],
      "id": "iz9rYYRtMo2N"
    },
    {
      "cell_type": "code",
      "source": [
        "#### your code ####\n",
        "\n",
        "from langchain.text_splitter import TextSplitter\n",
        "\n",
        "class TokenSplitter(TextSplitter):\n",
        "  def split_text(self, text):\n",
        "    chunks = []\n",
        "    start_idx = 0\n",
        "\n",
        "    while start_idx < len(text):\n",
        "      end_idx = start_idx + self._chunk_size\n",
        "      # Adjust the end index if it exceeds the length of the text\n",
        "      if end_idx > len(text):\n",
        "        end_idx = len(text)\n",
        "      # Get the chunk of text\n",
        "      chunk = text[start_idx:end_idx]\n",
        "      # Your tokenization logic using token_len function\n",
        "      chunk_length = self._length_function(chunk)\n",
        "      # Adjust the end index based on chunk overlap\n",
        "      end_idx -= max(0, chunk_length - self._chunk_size + self._chunk_overlap)\n",
        "      # Append the chunk to the list\n",
        "      chunks.append(text[start_idx:end_idx])\n",
        "      # Move to the next chunk\n",
        "      start_idx = end_idx\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Now you can create an instance of TokenSplitter\n",
        "text_splitter = TokenSplitter(\n",
        "  chunk_size=400,\n",
        "  chunk_overlap=50,\n",
        "  length_function=token_len\n",
        ")\n",
        "\n",
        "#### your code ####"
      ],
      "metadata": {
        "id": "L4m-UZH3RGc2"
      },
      "id": "L4m-UZH3RGc2",
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "kS6uNIzpOD-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01ad782c-0871-40fd-a0f5-cde6c4d874ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "chunks = text_splitter.split_text(docs[100].page_content)\n",
        "len(chunks)"
      ],
      "id": "kS6uNIzpOD-I"
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "-r29RUJ7PUg6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ee1ea02-0c4f-4d5d-adfc-cfed0ce8ed5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "112"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "token_len(chunks[0])"
      ],
      "id": "-r29RUJ7PUg6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "${\\color{red}{Wrong: \\ we\\ sould\\ get\\ something\\ closer\\ to\\ 400}}$"
      ],
      "metadata": {
        "id": "MQDKlTtMTK-G"
      },
      "id": "MQDKlTtMTK-G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGaeBXo8ONLj"
      },
      "source": [
        "The next step is to apply the splitting function to all the documents in our corpus and to save our chunks in a logical way. We also want to assign a unique ID to each chunk so we know which part of the documentation they come from. In the end, the corpus should be transformed into a list of dictionaries of the following format:\n",
        "\n",
        "\n",
        "```\n",
        "[\n",
        "    {\n",
        "        \"id\": \"glossary-0\",\n",
        "        \"text\": \"first chunk of the document glossary\",\n",
        "        \"source\": \"https://langchain.readthedocs.io/en/latest/glossary.html\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"glossary-1\",\n",
        "        \"text\": \"second chunk of glossary\",\n",
        "        \"source\": \"https://langchain.readthedocs.io/en/latest/glossary.html\"\n",
        "    }\n",
        "    ...\n",
        "]\n",
        "```\n",
        "\n",
        "Construct the IDs by taking the name of the page before the suffix `.html` and appending a chronological number indicating which chunk it is.\n"
      ],
      "id": "SGaeBXo8ONLj"
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "pBsNJBUNPUjC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "200b4c3075ca4cd2bff970c7862f5eac",
            "b6a613cb8d0b40568444843df7c1cf76",
            "367bd36257694cf68b40305f2c28ed8b",
            "f7f369d306094ea8b400384a44a56e44",
            "b3596131853d4bf5a3eda2bbb749da15",
            "e551552ba1344852a043ba16a05f04f8",
            "8b287ce1f86849d6a2ff30f1a6217f81",
            "c103e0a3f4be40d890e6c296b9622a5b",
            "0e3ef0c6bda944f780b445a4686af003",
            "43dfe9d26bf844a6b3b5a146787f3e57",
            "9254e4ceef14469aaf8e9dec3585be6f"
          ]
        },
        "outputId": "4abd6be5-b583-4e50-ea55-e4b821e81035"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/423 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "200b4c3075ca4cd2bff970c7862f5eac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10458"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "from urllib.parse import urlparse\n",
        "import os\n",
        "\n",
        "documents = []\n",
        "\n",
        "id=0\n",
        "for doc in tqdm(docs):\n",
        "  #### your code ####\n",
        "    url = retrieve_url(doc) # Defined before\n",
        "    uid_basename = os.path.splitext(os.path.basename(urlparse(url).path))[0]\n",
        "    chunks = text_splitter.split_text(doc.page_content)\n",
        "    id=0\n",
        "    for chunk in chunks:\n",
        "      uid = \"glossary-\"+str(id)\n",
        "      documents.append({\"id\":uid,\"text\":chunk,\"source\":url})\n",
        "      id+=1\n",
        "\n",
        "  #### your code ####\n",
        "len(documents) # once again this value might differ based on how the LangChain documentation is updated"
      ],
      "id": "pBsNJBUNPUjC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hnx_s1bBQFQM"
      },
      "source": [
        "For the next steps, we require a `DataFrame`."
      ],
      "id": "Hnx_s1bBQFQM"
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "V64EmH9bLWNO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a9028716-ecc3-4a25-c51c-63e52b9bf48e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           id                                               text  \\\n",
              "0  glossary-0  langchain 0.0.353¶\\nlangchain.agents¶\\nAgent i...   \n",
              "1  glossary-1                  OpenAIFunctionsAgent\\n        ...   \n",
              "2  glossary-2  tion.\\nagents.agent.AgentExecutor\\nAgent that ...   \n",
              "3  glossary-3  e action agents.\\nagents.agent.MultiActionAgen...   \n",
              "4  glossary-4  ctorStore.\\nagents.agent_toolkits.vectorstore....   \n",
              "\n",
              "                                              source  \n",
              "0  https://api.python.langchain.com/en/latest/lan...  \n",
              "1  https://api.python.langchain.com/en/latest/lan...  \n",
              "2  https://api.python.langchain.com/en/latest/lan...  \n",
              "3  https://api.python.langchain.com/en/latest/lan...  \n",
              "4  https://api.python.langchain.com/en/latest/lan...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9843dac9-befa-4bd9-a420-6d01297ef868\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>glossary-0</td>\n",
              "      <td>langchain 0.0.353¶\\nlangchain.agents¶\\nAgent i...</td>\n",
              "      <td>https://api.python.langchain.com/en/latest/lan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>glossary-1</td>\n",
              "      <td>OpenAIFunctionsAgent\\n        ...</td>\n",
              "      <td>https://api.python.langchain.com/en/latest/lan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>glossary-2</td>\n",
              "      <td>tion.\\nagents.agent.AgentExecutor\\nAgent that ...</td>\n",
              "      <td>https://api.python.langchain.com/en/latest/lan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>glossary-3</td>\n",
              "      <td>e action agents.\\nagents.agent.MultiActionAgen...</td>\n",
              "      <td>https://api.python.langchain.com/en/latest/lan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>glossary-4</td>\n",
              "      <td>ctorStore.\\nagents.agent_toolkits.vectorstore....</td>\n",
              "      <td>https://api.python.langchain.com/en/latest/lan...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9843dac9-befa-4bd9-a420-6d01297ef868')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9843dac9-befa-4bd9-a420-6d01297ef868 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9843dac9-befa-4bd9-a420-6d01297ef868');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7d724ab0-8274-4371-b6be-aff43eaee687\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7d724ab0-8274-4371-b6be-aff43eaee687')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7d724ab0-8274-4371-b6be-aff43eaee687 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "import pandas as pd\n",
        "data = pd.DataFrame(documents)\n",
        "data.head()"
      ],
      "id": "V64EmH9bLWNO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGbju_bkOQoL"
      },
      "source": [
        "#### ${\\color{red}{Comments\\ 1.1}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ],
      "id": "CGbju_bkOQoL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTrg2hMrO0vF"
      },
      "source": [
        "## Subtask 1.2: Document Embedding Pipeline\n"
      ],
      "id": "VTrg2hMrO0vF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_DSEsWIT9eI"
      },
      "source": [
        "In this task, we initialize the embedding pipeline to transform the chunks into vector embeddings using Hugging Face and LangChain. These embeddings are used for similarity search between the query and the chunks to retrieve the most relevant chunks.\n",
        "  We will use the `sentence-transformers/all-MiniLM-L6-v2` model for embedding, which is a rather small model that you can easily run on Colab. Initialize the model using `HuggingFaceEmbeddings` to use Hugging Face via Langchain. The encoding batch size should be 32, and make sure that the model is placed on the correct device, otherwise, this can take a long time."
      ],
      "id": "q_DSEsWIT9eI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYpEw-IKUTqK"
      },
      "outputs": [],
      "source": [
        "from torch import cuda\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "import os\n",
        "import pinecone\n",
        "from tqdm import tqdm"
      ],
      "id": "DYpEw-IKUTqK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI3CJ5bbUTsT"
      },
      "outputs": [],
      "source": [
        "embedding_model = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "device = 'cuda:0' # make sure you are on gpu\n",
        "docs = [\n",
        "    \"An example document\",\n",
        "    \"A second document as an example\"\n",
        "]\n",
        "### your code ###\n",
        "embed_model =\n",
        "### your code ###"
      ],
      "id": "HI3CJ5bbUTsT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U73Hj93V0_-"
      },
      "source": [
        "Embed the example documents using the model you created and check the output.\n",
        "The output should be a list of lists, containing the embeddings."
      ],
      "id": "_U73Hj93V0_-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Qt3SDKoUvKx"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "embeddings =\n",
        "### your code ###\n",
        "print(\"number of docs:\",len(embeddings))\n",
        "print(\"dimension of docs:\",len(embeddings[0]))"
      ],
      "id": "9Qt3SDKoUvKx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D8sr-g9W8cw"
      },
      "source": [
        "Now we use the embedding pipeline created above to store the embeddings in a Pinecone vector index. First, lets setup the Pinecone environment, collect your API key and environment name from the environment variables, and initiate Pinecone with them."
      ],
      "id": "2D8sr-g9W8cw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tycv_q6RW8tO"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "\n",
        "### your code ###"
      ],
      "id": "tycv_q6RW8tO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9tvVVQ7X2pV"
      },
      "source": [
        "Initialize the index `rag-assignment` inside Pinecone. Use the cosine similarity as similarity metric. Keep in mind that if you run this multiple times on a free tier, where only one index is allowed, you need to remove the index created to make room for a new one (Pinecone index gets archived automatically after 14 days of inactivity)."
      ],
      "id": "_9tvVVQ7X2pV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_ZA_2J5XyTD"
      },
      "outputs": [],
      "source": [
        "index_name = 'rag-assignment'\n",
        "### your code ###\n",
        "\n",
        "### your code ###"
      ],
      "id": "g_ZA_2J5XyTD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxfs9gshYfKx"
      },
      "source": [
        "Lets take a look at the index you created. As of now the index should be empty but have the correct embedding dimension."
      ],
      "id": "rxfs9gshYfKx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNnN9mEyYWHB"
      },
      "outputs": [],
      "source": [
        "index_name = 'rag-assignment'\n",
        "index = pinecone.Index(index_name)\n",
        "index.describe_index_stats()"
      ],
      "id": "gNnN9mEyYWHB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woQKBXfobZMg"
      },
      "source": [
        "Process the dataset in batches of `32` and push the vectors to the Pinecone index. Your index should include the IDs and embeddings for each chunk. As metadata, pass the original text as `text` and the URL as `source` (no need to add the `https`). We use this metadata later to retrieve the original text."
      ],
      "id": "woQKBXfobZMg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8vjCSVKa8Y6"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "for i in tqdm(range(0, len(data), batch_size)):\n",
        "  ### your code ###\n",
        "\n",
        "    ids =\n",
        "    texts =\n",
        "    embeds =\n",
        "\n",
        "    ### your code ###\n"
      ],
      "id": "K8vjCSVKa8Y6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz2UZVj8ff_L"
      },
      "source": [
        "Now if we look at the index statistics we should have vectors of dimension `384`."
      ],
      "id": "kz2UZVj8ff_L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cH9Zq6azfgJm"
      },
      "outputs": [],
      "source": [
        "index.describe_index_stats()"
      ],
      "id": "cH9Zq6azfgJm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK-2Scv8qqYV"
      },
      "source": [
        "#### ${\\color{red}{Comments\\ 1.2}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ],
      "id": "NK-2Scv8qqYV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bVrpkiJhHAF"
      },
      "source": [
        "## Subtask 1.3: Text Generation Pipeline\n"
      ],
      "id": "5bVrpkiJhHAF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPykfVaHkIkO"
      },
      "source": [
        "So far we have our index ready and a way to find the most similar chunks to our query. Now, we need a way to generate the answer from the retrieved chunks. For this purpose, we use the `text-generation` pipeline from Hugging Face (refer to the Hugging Face [tutorial](https://moodle.uni-heidelberg.de/pluginfile.php/1286642/mod_resource/content/1/HuggingFace.ipynb)) and load it into LangChain using a wrapper."
      ],
      "id": "ZPykfVaHkIkO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL_PhWM3lEnj"
      },
      "outputs": [],
      "source": [
        "from torch import cuda, bfloat16\n",
        "import os\n",
        "import transformers\n",
        "model_id = 'meta-llama/Llama-2-13b-chat-hf'"
      ],
      "id": "CL_PhWM3lEnj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3l34T5T3eEN"
      },
      "source": [
        "Quantization techniques reduce memory and computational costs by representing weights and activations with lower-precision data types like 8-bit integers (int8). This enables loading larger models you normally wouldn’t be able to fit into memory, and thus speeds up inference.\n",
        "To make the process of model quantization more accessible, Hugging Face has seamlessly integrated with the [Bitsandbytes](https://huggingface.co/docs/accelerate/usage_guides/quantization) library.\n",
        "\n",
        "Define a config from `Bitsandbytes` that enables 4-bit quantization and set the nested quantization to `true`. This changes the datatype from float 32 (default) to normalized float 4 datatype to contain 4 bits of information.\n",
        "Additionally, add a compute type to store weights in 4-bits, but the computation to happen in 16-bit (bfloat16).\n",
        "Moreover, set the `bnb_4bit_use_double_quant` to true, which uses a second quantization after the first one to save an additional 0.4 bits per parameter.\n",
        "Refer to [here](https://huggingface.co/docs/transformers/main_classes/quantization) for more information."
      ],
      "id": "A3l34T5T3eEN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXNTODoOlFjX"
      },
      "outputs": [],
      "source": [
        "  ### your code ###\n",
        "bitsAndBites_config =\n",
        "  ### your code ###"
      ],
      "id": "mXNTODoOlFjX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wea-kVMF4Kvf"
      },
      "source": [
        "Use your Hugging Face token to load the correct model configuration using the `transformers` library."
      ],
      "id": "Wea-kVMF4Kvf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7U47CyIk4EUz"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "\n",
        "model_config =\n",
        "### your code ###\n"
      ],
      "id": "7U47CyIk4EUz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8CNl7G1SsUC"
      },
      "source": [
        "Load the model for text generation (pay attention to the model type) using the configuration file you have defined, with the specified quantization, and set the `trust_remote_code` flag to `true`. Another flag that is useful for large mode is  `device_map=\"auto\"`. By setting this flag, Accelerate will determine where to put each layer to maximize the use of GPUs and offload the rest on the CPU, or even the hard drive if you don’t have enough GPU RAM (or CPU RAM).\n",
        "\n",
        "It will take a while for the model to download."
      ],
      "id": "V8CNl7G1SsUC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUzuGsv61cBA"
      },
      "outputs": [],
      "source": [
        "#Loading the model will take some time, (roughly 5 min)\n",
        "### your code ###\n",
        "model =\n",
        "### your code ###\n",
        "model.eval()# we only use the model for inference\n",
        "print(f\"Model loaded \")"
      ],
      "id": "TUzuGsv61cBA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-brElhysTZVZ"
      },
      "source": [
        "You can even check the memory footprint of your model using the `get_memory_footprint` method.\n"
      ],
      "id": "-brElhysTZVZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWDTgyKhlLLQ"
      },
      "outputs": [],
      "source": [
        "model.get_memory_footprint()"
      ],
      "id": "zWDTgyKhlLLQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViFoo0vlSFMp"
      },
      "source": [
        "The next thing we need to do is initialize a `text-generation` pipeline with Hugging Face that uses the Llama2 model to generate some text, given some input. We will then use this pipeline inside LangChain to build our question-answering system.\n",
        "`text-generation` pipeline generates text from a language model conditioned on a given input. The pipeline is similar to other Hugging Face pipelines and requires two things that we must initialize:\n",
        "\n",
        "1.   A language model, in this case, it will be `meta-llama/Llama-2-13b-chat-hf`.\n",
        "2.   A tokenizer for the language model.\n",
        "\n",
        "LangChain expects the full-text outputs, therefore set the `return_full_text` to true. You can also pass additional generation parameters to the model.\n",
        "Since we want the questions to be answered mainly based on the retrieved chunks, let's set the model temperature to a low value of 0.01 to reduce randomness. Additionally, add a repetition penalty of 1.1 to stop the model from repeating itself and the maximum number of generation tokens to 512."
      ],
      "id": "ViFoo0vlSFMp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aFv-9-lPCJO"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "generate_text =\n",
        "### your code ###"
      ],
      "id": "1aFv-9-lPCJO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvdSEw9PZ-n9"
      },
      "source": [
        "We provide the language model a general question to make sure our pipeline is working correctly."
      ],
      "id": "ZvdSEw9PZ-n9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpuCkc77RF53"
      },
      "outputs": [],
      "source": [
        "sample_input=\"Explain to me the difference between alligator and crocodile.\"\n",
        "### your code ###\n",
        "\n",
        "generated_text=\n",
        "### your code ###\n",
        "print(generated_text)"
      ],
      "id": "FpuCkc77RF53"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QtViqTBSsPS"
      },
      "source": [
        "Use the LangChain Hugging Face wrapper, as subset of [LLM chain](https://python.langchain.com/docs/modules/chains/foundational/llm_chain) to create an interface for the text generation pipeline."
      ],
      "id": "2QtViqTBSsPS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-spabNmRMM2"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "\n",
        "llm =\n",
        "### your code ###"
      ],
      "id": "6-spabNmRMM2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EFZBGNgTM20"
      },
      "source": [
        "To confirm that it works the same way, use the sample input to generate text using the llm chain. The input should be passed as the `prompt` to the language model."
      ],
      "id": "9EFZBGNgTM20"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDToa1YOTFEX"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "\n",
        "### your code ###"
      ],
      "id": "rDToa1YOTFEX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn7-zQxeVhWF"
      },
      "source": [
        "#### ${\\color{red}{Comments\\ 1.3}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ],
      "id": "mn7-zQxeVhWF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRwTtnQEWq5h"
      },
      "source": [
        "## Subtask 1.4: Question Answering Chain\n"
      ],
      "id": "VRwTtnQEWq5h"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr9QzQvYXDBS"
      },
      "source": [
        "For Retrieval Augmented Generation (RAG) in LangChain, we need to initialize either a `RetrievalQA` or `RetrievalQAWithSourcesChain` object.\n",
        "\n",
        "`RetrievalQA` is a method for question-answering tasks, utilizing an index to retrieve relevant documents or text chunks, it is suitable for straightforward Q&A applications.\n",
        "\n",
        "`RetrievalQAWithSourcesChain` is an extension of RetrievalQA that chains together multiple sources of information, providing context and the source for answers.\n",
        "\n",
        " For both of these, we need an LLM and a Pinecone index. For LangChain to be able to use the Pinecone index, we need to initialize it through the LangChain vector store.\n",
        "\n",
        " **Hint**: You need to explicitly tell the vector storage where to find the original text."
      ],
      "id": "wr9QzQvYXDBS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQIR_Gb6Wrfs"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "### your code ###\n",
        "vectorstore =\n",
        "### your code ###"
      ],
      "id": "RQIR_Gb6Wrfs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS9d51dyptJd"
      },
      "source": [
        "Let's try a query that is specific to the LangChain documentation and see which chunks are relevant. Use the vector storage defined above to find the top-3 chunks related to the given query."
      ],
      "id": "mS9d51dyptJd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQeWNPDqXdJe"
      },
      "outputs": [],
      "source": [
        "query = 'what is a LangChain Agent?'\n",
        "### your code ###\n",
        "\n",
        "### your code ###"
      ],
      "id": "sQeWNPDqXdJe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEMooqALXqOt"
      },
      "source": [
        "Now use the `vectorstore` and `llm` to initialize the `RetrievalQA` object, which showcases question answering over an index. `RetrievalQA` is a document chain, these are useful for summarizing documents, answering questions about documents, extracting information from documents, and more. All such chains operate with 4 different chain types:\n",
        "\n",
        "\n",
        "1.   `stuff`: it takes a list of documents, inserts them all into a prompt, and passes that prompt to an LLM.\n",
        "2.   `refine`: it constructs a response by looping over the input documents and iteratively updating its answer. It is well-suited for tasks that require analyzing more documents than can fit in the model’s context.\n",
        "3. `map_reduce`:  it first applies an LLM chain to each document individually (the Map step), treating the chain output as a new document. It then passes all the new documents to a separate combined documents chain to get a single output (the Reduce step).\n",
        "4. `map_re_rank`: it runs an initial prompt on each document that not only tries to complete a task but also gives a score for how certain it is in its answer. The highest-scoring response is returned.\n",
        "\n",
        "For this assignment, we focus only on the first type. Make sure to set the `verbose` to `true`, so we can see the different stages of processing that happens while answering a question (you might need to set this parameter more than once). As mentioned before, we want our retrieve to input top-5 most similiar chunks to the query to generate an answer."
      ],
      "id": "gEMooqALXqOt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYnUrFnaXqU0"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "### your code ###\n",
        "\n",
        "rag_pipeline =\n",
        "\n",
        "### your code ###\n",
        "query='what is a LangChain Agent?'"
      ],
      "id": "lYnUrFnaXqU0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4Mo0JNMqSC7"
      },
      "source": [
        "First, we try to answer the question only using Llama2. As you see the answer is not convincing as it does not have access to the LangChain documentation."
      ],
      "id": "O4Mo0JNMqSC7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGk2ZsUVYiJ0"
      },
      "outputs": [],
      "source": [
        "llm(query)"
      ],
      "id": "jGk2ZsUVYiJ0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8U5Vce0qdch"
      },
      "source": [
        "Now use the Pipeline from above and see how the answer changes."
      ],
      "id": "W8U5Vce0qdch"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KaQyXKQYn57"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "\n",
        "### your code ###\n"
      ],
      "id": "1KaQyXKQYn57"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ2JxOL4aqM2"
      },
      "source": [
        "#### ${\\color{red}{Comments\\ 1.4}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ],
      "id": "pJ2JxOL4aqM2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inIFkOziTce3"
      },
      "source": [
        "## Subtask 1.5: Conversational Retrieval Chain\n",
        "\n",
        "\n"
      ],
      "id": "inIFkOziTce3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btxcJlEITiDt"
      },
      "source": [
        "We can also extend our retrieval chain to be able to remember the previous questions and answer the current question by looking at the previous context.\n",
        "The important part of a conversational model is conversation memory, which transforms the stateless language model to be able to remember previous interactions, e.g., similiar to ChatGPT. In this subtask, we will use LangChain to create a conversational memory.\n"
      ],
      "id": "btxcJlEITiDt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYHPVo-9TiGU"
      },
      "source": [
        "To implement the memory we use `ConversationalRetrievalChain`.\n",
        "This chain takes in chat history (a list of messages) and new questions and then returns an answer to that question. The algorithm for this chain consists of three parts:\n",
        "\n",
        "1. Use the chat history and the new question to create a new question that contains the information from the previous context.\n",
        "\n",
        "2. This new question is passed to the retriever and relevant documents are returned.\n",
        "\n",
        "3. The retrieved documents are passed to an LLM to generate a final response."
      ],
      "id": "DYHPVo-9TiGU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLv_d-RYyNYL"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "chat_history = []\n",
        "\n",
        "### your code ###\n",
        "qa_conversation =\n",
        "result =\n",
        "### your code ###\n"
      ],
      "id": "GLv_d-RYyNYL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6HxivGwyv2x"
      },
      "outputs": [],
      "source": [
        "result[\"answer\"]"
      ],
      "id": "S6HxivGwyv2x"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd6tqKhg24dL"
      },
      "source": [
        "Change the chat history to contain the previous question and answer pair and ask a follow-up question.  "
      ],
      "id": "yd6tqKhg24dL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36vj0R9Fyyd1"
      },
      "outputs": [],
      "source": [
        "follow_up=\"What are tools and toolkits?\"\n",
        "\n",
        "### your code ###\n",
        "chat_history =\n",
        "result =\n",
        "### your code ###"
      ],
      "id": "36vj0R9Fyyd1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvlY2MtH0xo3"
      },
      "source": [
        "This is the previous context that was fed in alongside the new question."
      ],
      "id": "EvlY2MtH0xo3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09zxWnojzKEu"
      },
      "outputs": [],
      "source": [
        "chat_history"
      ],
      "id": "09zxWnojzKEu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSlQYeWG04Gj"
      },
      "source": [
        "The current question is answered by knowing that the tools and toolkits are referring to a LangChain Agent, which was part of the previous question."
      ],
      "id": "DSlQYeWG04Gj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FP4_WAbzMSQ"
      },
      "outputs": [],
      "source": [
        "result['answer']"
      ],
      "id": "5FP4_WAbzMSQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQUW_iFs3q3m"
      },
      "source": [
        "#### ${\\color{red}{Comments\\ 1.5}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ],
      "id": "LQUW_iFs3q3m"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KDwOSuNPBe-"
      },
      "source": [
        "## **Task 2: Advanced RAG Techniques and Evaluation (4 + 5 = 9 points)**"
      ],
      "id": "0KDwOSuNPBe-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN6pjlW7PBhQ"
      },
      "source": [
        "Now that you have successfully implemented your first RAG system, we dive into more advanced techniques and learn how to evaluate your methods using metrics you learned during the lecture. We focus on evaluation with an already annotated dataset. To this end, we load a small subset of [NarrativeQA](https://huggingface.co/datasets/narrativeqa), which is an English-language dataset of stories and corresponding questions designed to test reading comprehension, especially on long documents. We only load 30 samples from the data, as you will see in the upcoming sections, answer generation takes quite some time. In actual setting, it is advised to use a much larger set to obtain statistically significant results."
      ],
      "id": "cN6pjlW7PBhQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM2dodC2PAlr"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"satyaalmasian/narrativeqa_subset\",split=\"train[:30]\")\n",
        "len(dataset)"
      ],
      "id": "wM2dodC2PAlr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we already used our free index in Pinecone for the previous task, we use Chroma, an open-source vector database, instead. As opposed to Pinecone, Chroma creates a collection on your machine."
      ],
      "metadata": {
        "id": "N0HDF2M1fz4D"
      },
      "id": "N0HDF2M1fz4D"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ8kqj95hqvo"
      },
      "outputs": [],
      "source": [
        "from langchain.docstore.document import Document\n",
        "documents=[ doc[\"text\"] for doc in dataset[\"document\"]]\n",
        "questions=[quest for quest in dataset[\"question\"]]\n",
        "answers=[ans for ans in dataset[\"answers\"]]\n",
        "documents=list(set(documents))"
      ],
      "id": "hZ8kqj95hqvo"
    },
    {
      "cell_type": "code",
      "source": [
        "docs= [Document(page_content=doc, metadata={\"source\": \"local\"}) for doc in documents]"
      ],
      "metadata": {
        "id": "fj0X3PqHIFpQ"
      },
      "id": "fj0X3PqHIFpQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of documents is smaller  than the number of questions and answers and each document is used as a reference for multiple questions:"
      ],
      "metadata": {
        "id": "3BDIuYQkJB0U"
      },
      "id": "3BDIuYQkJB0U"
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(docs))\n",
        "print(len(questions))"
      ],
      "metadata": {
        "id": "FbWqRg12JLJn"
      },
      "id": "FbWqRg12JLJn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Subtask 2.1: Build Contextual Compression in LangChain"
      ],
      "metadata": {
        "id": "cD69GYOjjJuk"
      },
      "id": "cD69GYOjjJuk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's split our documents using the TextSplitter from Task 1 and embed them inside the Chroma database with the embedding model of the previous task."
      ],
      "metadata": {
        "id": "D-OyjUQGhGPq"
      },
      "id": "D-OyjUQGhGPq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7NGs-RzhQa7"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "all_splits =\n",
        "### your code ###"
      ],
      "id": "M7NGs-RzhQa7"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "### your code ###\n",
        "vectordb =\n",
        "retriever =\n",
        "### your code ###"
      ],
      "metadata": {
        "id": "hHknAFVBf5ia"
      },
      "id": "hHknAFVBf5ia",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Fist question in the set:\",questions[2]['text'])\n",
        "r_docs = retriever.get_relevant_documents(questions[2]['text'])\n",
        "r_docs"
      ],
      "metadata": {
        "id": "RbSBLyg364Ie"
      },
      "id": "RbSBLyg364Ie",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, make a simple RAG pipeline that works on top of the Chroma retriever. This retriever should be similar to the previous task. However, since we want to use it for a large number of questions, remove the `verbose` parameters."
      ],
      "metadata": {
        "id": "5YIitJ47Km7p"
      },
      "id": "5YIitJ47Km7p"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "### your code ###\n",
        "rag_simple=\n",
        "### your code ###"
      ],
      "metadata": {
        "id": "CTp5BOapKami"
      },
      "id": "CTp5BOapKami",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We look at an example question and compare the answer by RAG to the gold answer from the dataset. Note that the answers can contain multiple lines."
      ],
      "metadata": {
        "id": "I8EzQFiNRja9"
      },
      "id": "I8EzQFiNRja9"
    },
    {
      "cell_type": "code",
      "source": [
        "rag_simple(questions[2]['text']) #ignore the warning"
      ],
      "metadata": {
        "id": "afGjILYMKfCK"
      },
      "id": "afGjILYMKfCK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers[2]"
      ],
      "metadata": {
        "id": "jvjAzg42R2WE"
      },
      "id": "jvjAzg42R2WE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the `rag_simple` pipeline to all the question in your corpus and accumulate the answers. **It should take around 10 minutes on a T4 GPU on Colab**."
      ],
      "metadata": {
        "id": "mNXD6PaUSlLE"
      },
      "id": "mNXD6PaUSlLE"
    },
    {
      "cell_type": "code",
      "source": [
        "simple_answers=[]\n",
        "### your code ###\n",
        "\n",
        "### your code ###"
      ],
      "metadata": {
        "id": "uyTM0QD2KzwW"
      },
      "id": "uyTM0QD2KzwW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries such as LangChain and [Llamaindex](https://www.llamaindex.ai/) provide a variety of retrieval strategies for building a RAG system. In this subtask, you will use one of these variations called **contextual compression**. This method aims to extract only the relevant information from documents, reducing the need for expensive language model calls and improving response quality. Contextual compression consists of two parts:\n",
        "\n",
        "\n",
        "1.  **Base retriever:** retrieves the initial set of documents based on the query. This is similar to the retriever from the previous task.\n",
        "2.   **Document compressor:** processes these documents to extract the relevant content. We use `LLMChainExtractor`, which will iterate over the initially returned documents and extract from each only the content that is relevant to the query.\n"
      ],
      "metadata": {
        "id": "0q3EbAw_hV5h"
      },
      "id": "0q3EbAw_hV5h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ0UzPIVPAoc"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor,LLMChainFilter\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "### your code ###\n",
        "\n",
        "compression_retriever =\n",
        "### your code ###"
      ],
      "id": "RJ0UzPIVPAoc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at an example of compression retriever works."
      ],
      "metadata": {
        "id": "qMPaRwoeuvDX"
      },
      "id": "qMPaRwoeuvDX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AK9xhW8JPAqo"
      },
      "outputs": [],
      "source": [
        "print(\"Fist question in the set:\",questions[2]['text'])\n",
        "compressed_docs = compression_retriever.get_relevant_documents(questions[2]['text'])\n",
        "compressed_docs"
      ],
      "id": "AK9xhW8JPAqo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the output and try out several different questions by yourself. Does the compressed output make sense?\n",
        "\n",
        "Compare this to the previous **simple** approach. Which one, in your opinion, is better?"
      ],
      "metadata": {
        "id": "eWAPRPqCS3WP"
      },
      "id": "eWAPRPqCS3WP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we use the new retriever with the Llama2 model from the previous task to create the context compressor RAG pipeline. The code below should be similiar to what you did in the previous task. Once again, make sure to turn off the `verbose` argument."
      ],
      "metadata": {
        "id": "1GVulJfSu3lt"
      },
      "id": "1GVulJfSu3lt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKu2Dhr1PAtM"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "rag_compressor =\n",
        "### your code ###\n"
      ],
      "id": "PKu2Dhr1PAtM"
    },
    {
      "cell_type": "code",
      "source": [
        "rag_compressor(questions[2]['text'])"
      ],
      "metadata": {
        "id": "DDtrO4z2pAD9"
      },
      "id": "DDtrO4z2pAD9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use the pipeline to generate answers for all the questions in our dataset. **It should take around 20 minutes on a T4 GPU on Colab.**"
      ],
      "metadata": {
        "id": "8PfY6ya8z-1T"
      },
      "id": "8PfY6ya8z-1T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eofiL90APAvl"
      },
      "outputs": [],
      "source": [
        "compressor_answers=[]\n",
        "### your code ###\n",
        "\n",
        "### your code ###\n"
      ],
      "id": "eofiL90APAvl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ${\\color{red}{Comments\\ 2.1}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ],
      "metadata": {
        "id": "6ck8Ktt6Zns5"
      },
      "id": "6ck8Ktt6Zns5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Subtask 2.2. Evaluate"
      ],
      "metadata": {
        "id": "RlA3IDsn0Oxa"
      },
      "id": "RlA3IDsn0Oxa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we have access to ground truth answers, we can use various evaluation metrics from the literature. In this task, we explore three metrics:\n",
        "\n",
        "\n",
        "1.   **BLEU:** BLEU score stands for Bilingual Evaluation Understudy and is a precision-based metric developed\n",
        "for evaluating machine translation. BLEU scores a candidate by computing the\n",
        "number of n-grams in the candidate that also appear\n",
        "in a reference. The n can vary, in this task we compute for n=4.\n",
        "2.   **ROUGE:** ROUGE score stands for Recall-Oriented Understudy for Gisting Evaluation and is an F-measure metric designed for\n",
        "evaluating translation and summarization. There are a number of variants of ROUGE.\n",
        "3. **BERTScore:** BERTScore first obtains BERT representation of each word in the candidate and reference by feeding the candidate\n",
        "and reference through a BERT model separately.\n",
        "An alignment is then computed between candidate\n",
        "and reference words by computing pairwise cosine\n",
        "similarity. This alignment is then aggregated in to\n",
        "precision and recall scores before being aggregated\n",
        "into a (modified) F1 score that is weighted using\n",
        "inverse-document-frequency values.\n",
        "\n",
        "Luckily, Hugging Face has an implementation for all these metrics. Use the `evaluate` library to load the metrics."
      ],
      "metadata": {
        "id": "E274c-Yc0gAr"
      },
      "id": "E274c-Yc0gAr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the loaded metrics to compare the RAG pipelines from the previous subtask."
      ],
      "metadata": {
        "id": "VT57xstfUtAL"
      },
      "id": "VT57xstfUtAL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4SU_Z4vPAxy"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "### your code ###\n",
        "bleu =\n",
        "rouge =\n",
        "bertscore =\n",
        "### your code ###"
      ],
      "id": "M4SU_Z4vPAxy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As seen in the previous subtask, the answers can contain multiple lines. To be able to compare the output of our systems to the gold answers, merge the multiple answers into a single string."
      ],
      "metadata": {
        "id": "ET3Ns8NSQ73F"
      },
      "id": "ET3Ns8NSQ73F"
    },
    {
      "cell_type": "code",
      "source": [
        "answers_merged=[]\n",
        "### your code ###\n",
        "\n",
        "### your code ###\n",
        "print(len(answers_merged))"
      ],
      "metadata": {
        "id": "6ITKpzWkQ_hJ"
      },
      "id": "6ITKpzWkQ_hJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the BLUE score for the simple RAG and compressor RAG."
      ],
      "metadata": {
        "id": "IOlhI3iV1p9M"
      },
      "id": "IOlhI3iV1p9M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9PgmMfLPA0a"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "bleu_simple =\n",
        "bleu_compressor =\n",
        "### your code ###\n",
        "print(\"Simple system:\")\n",
        "print(bleu_simple)\n",
        "print(\"Compressor:\")\n",
        "print(bleu_compressor)"
      ],
      "id": "Z9PgmMfLPA0a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does the elements below in the output of the BLEU impelementation in Hugging Face mean? (do not copy and paste the documentation but write the implications behind each element!).\n",
        "\n",
        "\n",
        "\n",
        "1.   **precisions:** `your answer`\n",
        "2.   **brevity_penalty:** `your answer`\n",
        "3.   **translation_length:** `your answer`\n",
        "4.   **reference_length:** `your answer`\n",
        "5.   **length_ratio:** `your answer`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BZI2_Jfqrgrc"
      },
      "id": "BZI2_Jfqrgrc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "\n",
        "1.   **precisions:** precision of n-grams, which is calculated as the number of n-grams that appear in both the machine-generated translation and the reference translations divided by the total number of n-grams in the machine-generated translation.\n",
        "2.   **brevity_penalty:** is a penalty term that adjusts the score for translations that are shorter than the reference translations. It is calculated as min(1, (reference_length / translation_length)). It essentially penalizes generated translations that are too short compared to the closest reference length with an exponential decay.\n",
        "3.   **translation_length:**   is the total number of words in the machine-generated translation.\n",
        "4.   **reference_length:**  is the total number of words in the reference translations.\n",
        "5. **length_ratio:** ratio of the 3 and 4."
      ],
      "metadata": {
        "id": "G-usgncEr7i9"
      },
      "id": "G-usgncEr7i9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ot-jQzvqPA3J"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "rouge_simple =\n",
        "rouge_compressor =\n",
        "### your code ###\n",
        "print(\"Simple system:\")\n",
        "print(rouge_simple)\n",
        "print(\"Compressor:\")\n",
        "print(rouge_compressor)"
      ],
      "id": "Ot-jQzvqPA3J"
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the difference in variants of ROUGE (ROUGE-N, ROUGE-L, ROUGE-SUM)?\n",
        "\n",
        "`your answer`\n"
      ],
      "metadata": {
        "id": "IFE7RI-HsSGq"
      },
      "id": "IFE7RI-HsSGq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "ROUGE measures the similarity between the machine-generated summary and the reference summaries using overlapping n-grams, word sequences that appear in both the machine-generated summary and the reference summaries. The most common n-grams used are unigrams, bigrams, and trigrams. ROUGE score calculates the recall of n-grams in the machine-generated summary by comparing them to the reference summaries.\n",
        "\n",
        "**ROUGE-N:** ROUGE-N measures the overlap of n-grams (contiguous sequences of n words) between the candidate text and the reference text. It computes the precision, recall, and F1-score based on the n-gram overlap. For example, ROUGE-1 (unigram) measures the overlap of single words, ROUGE-2 (bigram) measures the overlap of two-word sequences, and so on. ROUGE-N is often used to evaluate the grammatical correctness and fluency of generated text.\n",
        "\n",
        "**ROUGE-L:** ROUGE-L measures the longest common subsequence (LCS) between the candidate text and the reference text. It computes the precision, recall, and F1-score based on the length of the LCS. ROUGE-L is often used to evaluate the semantic similarity and content coverage of generated text, as it considers the common subsequence regardless of word order.\n",
        "\n",
        "**ROUGE-S:** ROUGE-S measures the skip-bigram (bi-gram with at most one intervening word) overlap between the candidate text and the reference text. It computes the precision, recall, and F1-score based on the skip-bigram overlap. ROUGE-S is often used to evaluate the coherence and local cohesion of generated text, as it captures the semantic similarity between adjacent words.\n",
        "\n"
      ],
      "metadata": {
        "id": "IJmntC2_sfkx"
      },
      "id": "IJmntC2_sfkx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSF5zMOyPA5I"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "bertscore_simple_averaged={}\n",
        "bertscore_compressor_averaged={}\n",
        "### your code ###\n",
        "\n",
        "\n",
        "### your code ###\n",
        "print(\"Simple system:\")\n",
        "print(bertscore_simple_averaged)\n",
        "print(\"Compressor:\")\n",
        "print(bertscore_compressor_averaged)"
      ],
      "id": "zSF5zMOyPA5I"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which model works better?"
      ],
      "metadata": {
        "id": "LfiuFxqQVrHa"
      },
      "id": "LfiuFxqQVrHa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ${\\color{red}{Comments\\ 2.2}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ],
      "metadata": {
        "id": "bbIA8Cg3Zq6g"
      },
      "id": "bbIA8Cg3Zq6g"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b3aaea2f2d6e49f79a77f6c3b6a95c22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c160783d70f84b43bcadd13bb260a3d3",
              "IPY_MODEL_ae0fc8dc93fe4641a0d842d49ae1ce81",
              "IPY_MODEL_c5ca275595bf4ab48c670baa43871453"
            ],
            "layout": "IPY_MODEL_d04d3a3649a04eb6ad34b7bfdcdcbc67"
          }
        },
        "c160783d70f84b43bcadd13bb260a3d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33534a682ca043e69c908166075850f1",
            "placeholder": "​",
            "style": "IPY_MODEL_e1a2b3b7dc124cae9c79a56b68d6d7c0",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "ae0fc8dc93fe4641a0d842d49ae1ce81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74f8bc48fcfc40ceb853010e517bbd2c",
            "max": 1618,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a74f9c115a774fe4af5e597bcc0ba170",
            "value": 1618
          }
        },
        "c5ca275595bf4ab48c670baa43871453": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ecc873ae4314522878249cdc07370cb",
            "placeholder": "​",
            "style": "IPY_MODEL_92ba58dca267413f9658d65a1a765171",
            "value": " 1.62k/1.62k [00:00&lt;00:00, 79.8kB/s]"
          }
        },
        "d04d3a3649a04eb6ad34b7bfdcdcbc67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33534a682ca043e69c908166075850f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1a2b3b7dc124cae9c79a56b68d6d7c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74f8bc48fcfc40ceb853010e517bbd2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a74f9c115a774fe4af5e597bcc0ba170": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ecc873ae4314522878249cdc07370cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92ba58dca267413f9658d65a1a765171": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "518b8a9fbfac42aab645663178c35c17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ebae8b7f31a24cb4ad50846224b429c0",
              "IPY_MODEL_adcebbf08ae249b9aef36f62fcc88e35",
              "IPY_MODEL_cd3c6c3b9c7e4687ba6eb2ce220f5967"
            ],
            "layout": "IPY_MODEL_d693e7629084410ea18fbfeab5a7970d"
          }
        },
        "ebae8b7f31a24cb4ad50846224b429c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0b327cadb6a4c76ac7012a4658b82bd",
            "placeholder": "​",
            "style": "IPY_MODEL_b0477e1db4a647a6b855e9dbfcbe6391",
            "value": "tokenizer.model: 100%"
          }
        },
        "adcebbf08ae249b9aef36f62fcc88e35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7b30ff0a9e941b0ac01efb903a8dbbc",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e149ef92db94b96a4d1612670f0b066",
            "value": 499723
          }
        },
        "cd3c6c3b9c7e4687ba6eb2ce220f5967": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37957f8a2a9f4b8690896d6b9844353c",
            "placeholder": "​",
            "style": "IPY_MODEL_0a23313075f04af6a48bf56296387d20",
            "value": " 500k/500k [00:00&lt;00:00, 9.17MB/s]"
          }
        },
        "d693e7629084410ea18fbfeab5a7970d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0b327cadb6a4c76ac7012a4658b82bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0477e1db4a647a6b855e9dbfcbe6391": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7b30ff0a9e941b0ac01efb903a8dbbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e149ef92db94b96a4d1612670f0b066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37957f8a2a9f4b8690896d6b9844353c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a23313075f04af6a48bf56296387d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f48c81e9b8c49eb995da8568a6c892b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f146548c7c1a488aa330f1dc13c7c971",
              "IPY_MODEL_c32b7261bef7440abdf7dac3c14f81db",
              "IPY_MODEL_de3f70165ccd457fa6df876cfad93dc0"
            ],
            "layout": "IPY_MODEL_17f9e6e7af704eb48499462d6b62cc79"
          }
        },
        "f146548c7c1a488aa330f1dc13c7c971": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50b5e9541014411bbf13a2c3c033125c",
            "placeholder": "​",
            "style": "IPY_MODEL_b15a1df1910c4cbab879a329acf2de8a",
            "value": "tokenizer.json: 100%"
          }
        },
        "c32b7261bef7440abdf7dac3c14f81db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d9a8f9bf47345208f72e80b6451d9b5",
            "max": 1842767,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f168e94497e49f8b66963c62a98cc83",
            "value": 1842767
          }
        },
        "de3f70165ccd457fa6df876cfad93dc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad089994c71147fd801cee3d3c79f766",
            "placeholder": "​",
            "style": "IPY_MODEL_f25a9182d4764f20a458db3046f35e45",
            "value": " 1.84M/1.84M [00:00&lt;00:00, 26.1MB/s]"
          }
        },
        "17f9e6e7af704eb48499462d6b62cc79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50b5e9541014411bbf13a2c3c033125c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b15a1df1910c4cbab879a329acf2de8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d9a8f9bf47345208f72e80b6451d9b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f168e94497e49f8b66963c62a98cc83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad089994c71147fd801cee3d3c79f766": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f25a9182d4764f20a458db3046f35e45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a5f5c8b4d844f34a14d8a2a0ad308e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f7e44bcebc2419eb47a2ad3805ab212",
              "IPY_MODEL_560d97da90b14b8189aebe85ca39e3f4",
              "IPY_MODEL_0d1dfd462b0c438589ad253405135e06"
            ],
            "layout": "IPY_MODEL_e688f429b5484283bacf6616c80dd58f"
          }
        },
        "9f7e44bcebc2419eb47a2ad3805ab212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12095966ada14a9591b709aea144ff64",
            "placeholder": "​",
            "style": "IPY_MODEL_70dc32be1bcb4cdc81bca755261990f7",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "560d97da90b14b8189aebe85ca39e3f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c055418913144ee1ae30770caf93135a",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7216aa3e2adb479db509bb8b0bd0c482",
            "value": 414
          }
        },
        "0d1dfd462b0c438589ad253405135e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82041c5e64c44eaea530eb7819c5ea8b",
            "placeholder": "​",
            "style": "IPY_MODEL_8ed2803fbdb44812bc7e6c13de80d0cd",
            "value": " 414/414 [00:00&lt;00:00, 27.8kB/s]"
          }
        },
        "e688f429b5484283bacf6616c80dd58f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12095966ada14a9591b709aea144ff64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70dc32be1bcb4cdc81bca755261990f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c055418913144ee1ae30770caf93135a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7216aa3e2adb479db509bb8b0bd0c482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82041c5e64c44eaea530eb7819c5ea8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ed2803fbdb44812bc7e6c13de80d0cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "200b4c3075ca4cd2bff970c7862f5eac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6a613cb8d0b40568444843df7c1cf76",
              "IPY_MODEL_367bd36257694cf68b40305f2c28ed8b",
              "IPY_MODEL_f7f369d306094ea8b400384a44a56e44"
            ],
            "layout": "IPY_MODEL_b3596131853d4bf5a3eda2bbb749da15"
          }
        },
        "b6a613cb8d0b40568444843df7c1cf76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e551552ba1344852a043ba16a05f04f8",
            "placeholder": "​",
            "style": "IPY_MODEL_8b287ce1f86849d6a2ff30f1a6217f81",
            "value": "100%"
          }
        },
        "367bd36257694cf68b40305f2c28ed8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c103e0a3f4be40d890e6c296b9622a5b",
            "max": 423,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e3ef0c6bda944f780b445a4686af003",
            "value": 423
          }
        },
        "f7f369d306094ea8b400384a44a56e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43dfe9d26bf844a6b3b5a146787f3e57",
            "placeholder": "​",
            "style": "IPY_MODEL_9254e4ceef14469aaf8e9dec3585be6f",
            "value": " 423/423 [00:52&lt;00:00, 23.50it/s]"
          }
        },
        "b3596131853d4bf5a3eda2bbb749da15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e551552ba1344852a043ba16a05f04f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b287ce1f86849d6a2ff30f1a6217f81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c103e0a3f4be40d890e6c296b9622a5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e3ef0c6bda944f780b445a4686af003": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43dfe9d26bf844a6b3b5a146787f3e57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9254e4ceef14469aaf8e9dec3585be6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}