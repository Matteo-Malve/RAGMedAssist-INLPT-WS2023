{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be9f7653",
   "metadata": {
    "id": "be9f7653"
   },
   "source": [
    "**Heidelberg University**\n",
    "\n",
    "**Data Science  Group**\n",
    "    \n",
    "Prof. Dr. Michael Gertz  \n",
    "\n",
    "Ashish Chouhan, Satya Almasian, John Ziegler, Jayson Salazar, Nicolas Reuter\n",
    "    \n",
    "December 4, 2023\n",
    "    \n",
    "Natural Language Processing with Transformers\n",
    "\n",
    "Winter Semster 2023/2024     \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e9648",
   "metadata": {
    "id": "258e9648"
   },
   "source": [
    "# **Assignment 3: “Transformers”**\n",
    "**Due**: Monday, January 8, 2024, 2pm, via [Moodle](https://moodle.uni-heidelberg.de/course/view.php?id=19251)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc27ad9e",
   "metadata": {
    "id": "fc27ad9e"
   },
   "source": [
    "### **Submission Guidelines**\n",
    "\n",
    "- Solutions need to be uploaded as a **single** Jupyter notebook. You will find several pre-filled code segments in the notebook, your task is to fill in the missing cells.\n",
    "- For the written solution, use LaTeX in markdown inside the same notebook. Do **not** hand in a separate file for it.\n",
    "- Download the .zip file containing the dataset but do **not** upload it with your solution.\n",
    "- It is sufficient if one person per group uploads the solution to Moodle, but make sure that the full names of all team members are given in the notebook.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HETm7VsBkmLq",
   "metadata": {
    "id": "HETm7VsBkmLq"
   },
   "source": [
    "## **Task 1: Diving into Attention** (3 + 4 + 4 + 1 = 12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ODkKBIRkrfe",
   "metadata": {
    "id": "0ODkKBIRkrfe"
   },
   "source": [
    "In this task, you work with self-attention equations and find out why multi-head attention is preferable to single-head attention.\n",
    "\n",
    "Recall the equation of attention on slide 5-9 to compute self-attention on a series of input tokens. We simplify the formula by focusing on a single query vector $q \\in R^d$, value vectors ($\\{ v_1,v_2,...,v_i \\},v_i \\in R^d$), and key vectors ($\\{ k_1,k_2,...,k_i \\},k_i \\in R^d$). We then have\n",
    "\n",
    "$$\n",
    "a_i=\\frac{exp(q^Tk_i)}{\\Sigma^n_{j=1}exp(q^Tk_j)}\n",
    "$$\n",
    "\n",
    "$$\n",
    " o= \\Sigma^n_{i=1} a_i v_i\n",
    "$$\n",
    "\n",
    "with $a_i$ being the attention weight for query $q$ with respect to key $k_i$. Then the output $o$ is the new representation for the query token as a weighted average of value vectors with weights $a=\\{ a_1,a_2,...,a_i \\},a_i \\in R^d$.\n",
    "Answer the following questions with the help of the equations and the intuition behind attention that you learned in the class:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MRwxqsMVodDt",
   "metadata": {
    "id": "MRwxqsMVodDt"
   },
   "source": [
    "### Subtask 1: Copying  \n",
    "\n",
    "1.   Explain why $a$ can be interpreted as a categorical distribution.\n",
    "2.   This distribution is typically diffuse, where the mass is spread out between different values of $a_i$. Describe a scenario in which the categorical distribution puts all the weight on a single element, e.g., $a_j \\gg \\Sigma_{j\\neq i}a_i$. What are the conditions on key and/or query for this to happen?\n",
    "3. In this case of a single large $a$, what would the output $c$ look like and what it means intuitively?\n",
    "\n",
    "In attention, it is easy to **copy** a value vector $v_i$ to the output $o$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Km0X1VPoqgrt",
   "metadata": {
    "id": "Km0X1VPoqgrt"
   },
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZcLTljmmqhIM",
   "metadata": {
    "id": "ZcLTljmmqhIM"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "1.1\n",
    "\n",
    "As described in slide 5-9, the values of the $a_i$'s refer to the similarity of the query and the different keys. When applying softmax to the values of $q^Tk_i$, the respective values are mapped to a probability distribution from 0 to 1 due to the properties of the softmax function. Consequently, each value $a_i$ refers to a probability value of a categorical distribution among all keys, which makes the entire $a$ vector a categorical probability distribution among itself.\n",
    "\n",
    "1.2\n",
    "\n",
    "Whenever only a single key attends to a given query $q_i^*$ and all others only to a small extent, the described scenario can occur. The softmax function will then raise the value $a_i^*$ to almost 1 and all others to $\\approx 0$.\n",
    "\n",
    "1.3\n",
    "\n",
    "In the case of a single large $a$, the output $o$ will be approximately equal to the value of the corresponding key that most closely matches the given query. In the above notation, this would be $v_i^*$. This happens because all $a_i$ except $a_i^*$ make only a small contribution to the summation and only the summand $a_i^* \\cdot v_i^*$ remains for the summation. Since $a_i$ tends to be equal to 1, the total sum is approximately $v_i^*$.\n",
    "\n",
    "In practical terms, this means that all the attention of the given query is focused on a single token, namely the best matching token.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NK-2Scv8qqYV",
   "metadata": {
    "id": "NK-2Scv8qqYV"
   },
   "source": [
    "#### ${\\color{red}{Comments\\ 1.1}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "cross-feedback comment section\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YmWxycPF15kM",
   "metadata": {
    "id": "YmWxycPF15kM"
   },
   "source": [
    "### Subtask 2: Averaging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WFKra56Q15mq",
   "metadata": {
    "id": "WFKra56Q15mq"
   },
   "source": [
    "Instead of focusing on just one value vector $v_j$, the Transformer model can incorporate information from multiple inputs. Consider the situation where we want to incorporate information from two value vectors $v_b$ and $v_c$ with keys $k_b$ and $k_c$. In machine learning one of the ways to combine this information is through averaging of vectors $o= \\frac{1}{2}(v_b+v_c)$.  It might seem hard to extract information about the original vectors $v_b$ and $v_c$ from the resulting average. But under certain conditions, one can do so. In this subtask, we look at the following cases:\n",
    "\n",
    "1. Suppose we know the following:\n",
    "\n",
    "\n",
    "* $v_b$ lies in a subspace $B$ formed by the $m$ basis vectors $\\{b_1, b_2, .. , b_m\\}$, while $v_c$ lies in a subspace $C$ formed by the $p$ basis vectors $\\{c_1, c_2, . . . , c_p\\}$ (This means that any $v_b$ and $v_c$ can be expressed as a linear combination of their basis vectors).\n",
    "*   All basis vectors have the norm 1 and are orthogonal to each other.\n",
    "*   The two subspaces $B$ and $C$ are orthogonal, meaning $b_j^Tc_k=0$ for all $j$ and $k$.\n",
    "* Given that $\\{b_1, b_2, .. , b_m\\}$ are both orthogonal and form a basis for $v_b$, we know that there exists some $d_1, ..., d_m$ such that $v_b=d_1 b_1+d_2 b_2+...+d_m b_m$. Use these $d\\text{s}$ to solve this task.\n",
    "\n",
    "Using the basis vectors $\\{b_1, b_2, .. , b_m\\}$, construct a matrix $M$ such that for arbitrary vectors $v_b$ and $v_c$ with the given conditions, we can use $M$ to extract $v_b$ from the sum of the vector $s = v_b + v_c$. In other words, construct an $M$ such that  $ Ms = v_b$ holds.\n",
    "\n",
    "\n",
    "2. If we assume that\n",
    "* all key vectors are orthogonal, i.e., $k_i^Tk_j=0$ for all $i \\neq j$, and\n",
    "* all key vectors have the norm 1.\n",
    "\n",
    "Find an expression for the query vector $q$ such that $o \\approx \\frac{1}{2}(v_b+v_c)$. Justify your answer.\n",
    "\n",
    "**Hint:** Use your finding in subtask 1 to solve part 2.\n",
    "\n",
    "**Hint:** If the norm of a vector $x$ is 1, then $x^Tx=1$\n",
    "\n",
    "**Hint:** Start with writing $v_b$ and $v_c$ as the linear combination of the bases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UdPaWhCzTxKT",
   "metadata": {
    "id": "UdPaWhCzTxKT"
   },
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uDB4aKn0Us4H",
   "metadata": {
    "id": "uDB4aKn0Us4H"
   },
   "source": [
    "1. We want to solve $Ms = v_b$ for $s = v_b + v_c$ by using the rules of orthonormal projections matrices. To do so, we introduce matrix $X$, which is constructed by the basis $b_1, \\dots, b_m$ as its columns.\n",
    "$$\n",
    "X = \\begin{pmatrix} b_1 & \\dots & b_m \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "  We now define the matrix $M$ based on $X$ as follows:\n",
    "\n",
    "  $$\n",
    "  M = X(X^TX)^{-1}X^T\n",
    "  $$\n",
    "\n",
    "  Now one notices a few things regarding the definition of $X$, respectively $X^T$\n",
    "  - $X^Tv_c = 0$, since $X^Tv_c = \\begin{pmatrix}\n",
    "  b^T_1 \\\\\n",
    "  \\dots \\\\\n",
    "  b^T_m\n",
    "  \\end{pmatrix}\n",
    "  (e_1c_1 + \\dots e_pc_p) = c_1\\begin{pmatrix}\n",
    "  b^T_1 \\\\\n",
    "  \\dots \\\\\n",
    "  b^T_m\n",
    "  \\end{pmatrix}d_1 + \\dots + c_p\\begin{pmatrix}\n",
    "  b^T_1 \\\\\n",
    "  \\dots \\\\\n",
    "  b^T_m\n",
    "  \\end{pmatrix}d_p =\n",
    "  c_1\\begin{pmatrix}\n",
    "  0 \\\\\n",
    "  \\dots \\\\\n",
    "  0\n",
    "  \\end{pmatrix} + \\dots + c_p\\begin{pmatrix}\n",
    "  0 \\\\\n",
    "  \\dots \\\\\n",
    "  0\n",
    "  \\end{pmatrix}\n",
    "  =\n",
    "  0\n",
    "  $, due to the orthogonality of $b_j$ and $c_k$.\n",
    "\n",
    "  - $v_b = X\\begin{pmatrix}\n",
    "  d_1 \\\\\n",
    "  \\dots \\\\\n",
    "  d_m\n",
    "  \\end{pmatrix}$ since $X = \\begin{pmatrix} b_1 & \\dots & b_m \\end{pmatrix}\\begin{pmatrix}\n",
    "  d_1\\\\\n",
    "  \\dots \\\\\n",
    "  d_m\n",
    "  \\end{pmatrix} = d_1b_1 + \\dots + d_mb_m$.\n",
    "\n",
    "  Combining these two observations, we get:\n",
    "\n",
    "  \\begin{align}\n",
    "  Ms &= M(v_b + v_c) \\\\\n",
    "      &= X(X^TX)^{-1}X^T(v_b + v_c) \\\\\n",
    "      &= X(X^TX)^{-1}X^Tv_b + X(X^TX)^{-1}\\underbrace{X^Tv_c}_{= 0} \\\\\n",
    "      &= X\\underbrace{(X^TX)^{-1}X^TX}_{= I}\\begin{pmatrix}\n",
    "      d_1 \\\\\n",
    "      \\cdots \\\\\n",
    "      d_m\n",
    "      \\end{pmatrix} + 0 \\\\\n",
    "      &= X\\begin{pmatrix}\n",
    "      d_1 \\\\\n",
    "      \\cdots \\\\\n",
    "      d_m\n",
    "      \\end{pmatrix} \\\\\n",
    "      &= v_b\n",
    "  \\end{align}\n",
    "\n",
    "2. We propose $q = \\alpha (k_b + k_c)$ with $0 << \\alpha$ as solution to be $c \\approx \\frac{1}{2} (v_b + v_c)$.\n",
    "Given this $q$, due to the orthogonality of $k_j$ and $k_i$ and $||k_j|| = 1$, it yields for each $k_j$:\n",
    "\\begin{align}\n",
    "    q^Tk_j&=\\alpha(k_b+k_c)^Tk_j \\\\\n",
    "        &= \\begin{cases}\n",
    "    \\alpha,& \\text{if } j = b,c\\\\\n",
    "    0,              & \\text{otherwise}\n",
    "    \\end{cases} \\\\\n",
    "\\end{align}\n",
    "\n",
    "It follows:\n",
    "\n",
    "\\begin{align}\n",
    "    \\exp(q^Tk_j)&=\\exp((k_b+k_c)^Tk_j) \\\\\n",
    "        &= \\begin{cases}\n",
    "    \\exp(\\alpha),& \\text{if } j = b,c\\\\\n",
    "    1,              & \\text{otherwise}\n",
    "    \\end{cases} \\\\\n",
    "\\end{align}\n",
    "\n",
    "And therefore it yields: $\\Sigma^n_{j=1}exp(q^Tk_j) = 2\\exp(\\alpha)+(n-2)$.\n",
    "\n",
    "It follows:\n",
    "\\begin{align}\n",
    "    \\Rightarrow o&= \\Sigma^n_{i=1} a_i v_i \\\\\n",
    "    &= \\Sigma^n_{i=1} \\frac{exp(q^Tk_i)}{\\Sigma^n_{j=1}exp(q^Tk_j)} v_i \\\\\n",
    "    &= \\frac{1}{2\\exp(\\alpha)+(n-2)}\\Sigma^n_{i=1} \\exp(q^Tk_i) v_i \\\\\n",
    "    &= \\frac{1}{2\\exp(\\alpha)+(n-2)}\\left(\\exp(\\alpha)v_b + \\exp(\\alpha)v_c + \\Sigma^n_{i=1, i \\neq b,c} v_i\\right) \\\\\n",
    "    &= \\frac{1}{2\\exp(\\alpha)+(n-2)}\\left(\\exp(\\alpha)(v_b + v_c) + \\Sigma^n_{i=1, i \\neq b,c} v_i\\right)\n",
    "\\end{align}\n",
    "\n",
    "We now assume two things based on the fact that we choose a huge $\\alpha$:\n",
    "- $\\exp(\\alpha)(v_b + v_c) + \\Sigma^n_{i=1, i \\neq b,c} v_i \\approx \\exp(\\alpha)(v_b + v_c)$\n",
    "- $2\\exp(\\alpha)+(n-2) \\approx 2\\exp(\\alpha)$\n",
    "\n",
    "In other words: The impact of the keys and values which aren't $b$ or $c$ becomes nearly 0 and therefore we finally get:\n",
    "\n",
    "\\begin{align}\n",
    "    o&= \\Sigma^n_{i=1} a_i v_i \\\\\n",
    "    &= \\dots \\\\\n",
    "    &= \\frac{1}{2\\exp(\\alpha)+(n-2)}\\left(\\exp(\\alpha)(v_b + v_c) + \\Sigma^n_{i=1, i \\neq b,c} v_i\\right) \\\\\n",
    "    &\\approx \\frac{1}{2\\exp(\\alpha)}(\\exp(\\alpha)(v_b + v_c)) \\\\\n",
    "    &= \\frac{1}{2}(v_b + v_c)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5poTWvSxUoc3",
   "metadata": {
    "id": "5poTWvSxUoc3"
   },
   "source": [
    "#### ${\\color{red}{Comments\\ 1.2}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "cross-feedback comment section\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imqOTai0bxuA",
   "metadata": {
    "id": "imqOTai0bxuA"
   },
   "source": [
    "### Subtask 3: Drawbacks of Single-head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0lObCVx7bx3q",
   "metadata": {
    "id": "0lObCVx7bx3q"
   },
   "source": [
    "You might have wondered why we need multi-heads for attention. In this subtask, we look at some of the drawbacks of having a single head attention. As shown in the previous subtask, it is possible for single head attention to focus equally on two values. The same can apply to any subset of values, which therefor can become problematic.\n",
    "\n",
    "Consider a set of key vectors $\\{ k_1,k_2,...,k_n \\}$, randomly sampled from a normal distribution with a known mean value of $\\mu_i \\in R^d$ and unknown covariance $Σ_i, i \\in \\{1, \\ldots, n\\}$, where\n",
    "\n",
    "\n",
    "*   $\\mu_i\\text{s}$ are all orthogonal $\\mu_i^T\\mu_j=0$ if $i \\neq j$.\n",
    "*   $\\mu_i\\text{s}$ all have unit norm $||\\mu_i||=1$.\n",
    "\n",
    "1. For a vanishingly small $\\alpha$ (not to be confused with attention weights), the covariance matrices are  $Σ_i=\\alpha I, \\forall i  \\in \\{1,2,..,n\\}$, design a query $q$ in terms of the $\\mu_i$ such that as before, $o= \\frac{1}{2}(v_b+v_c)$ and describe why it works.\n",
    "\n",
    "2.  Large perturbations in key value might cause problems for single head attention.  Specifically, in some cases, one key vector $k_b$ may be larger or smaller in norm than the others, while still pointing in the same direction as $\\mu_b$. As an example of such a case,\n",
    "consider a covariance matrix for item $b$ for vanishingly small $\\alpha$ as $Σ_b=\\alpha I + \\frac{1}{2}(\\mu_b^T\\mu_b)$. This causes $k_a$ to point to roughly the same direction as $\\mu_b$ but with large differences in magnitude, while for other items. Further, let $Σ_i=\\alpha I\\  \\forall_i i \\neq b$. When you sample multiple keys from the distribution $\\{ k_1,k_2,...,k_n \\}$ and use the $q$ vector from the pervious part, what do you expect vector $o$ to look like? Explain why this shows the drawback of single-head attention.\n",
    "\n",
    "**Hint:**\n",
    "Think about how it differs from pervious part and how $o$'s variance would be affected by the change in $Σ_b$.\n",
    "\n",
    "**Hint:** Considering that $\\mu_b^T\\mu_b=1$, think of what are the ranges $Σ_b$ can take and how does that effect a sampled $k_b$ value.\n",
    "\n",
    "**Hint:** $\\frac{exp(b)}{exp(b)+exp(c)}=\\frac{exp(b)}{exp(b)+exp(c)}\\frac{exp(-b)}{exp(-b)}= \\frac{1}{1+exp(c-b)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PFkzZv9NcCpZ",
   "metadata": {
    "id": "PFkzZv9NcCpZ"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1yhrRv0rTww2",
   "metadata": {
    "id": "1yhrRv0rTww2"
   },
   "source": [
    "1. We propose $q = \\beta (\\mu_b + \\mu_c)$ with $0 << \\beta << \\frac{1}{\\alpha}$ as solution to be $c \\approx \\frac{1}{2} (v_b + v_c)$.\n",
    "Before deriving the result of $o$ we want to make note of an important result based on the given assumptions.\n",
    "\n",
    "  Since each $k_j \\sim N(\\mu_j, \\alpha I)$ and each $\\mu_j$ is fixed and known it follows based on the rules of Gaussian distribution and the orthonormality of all $\\mu_j$:\n",
    "\n",
    "  \\begin{align}\n",
    "    \\beta\\mu_b^Tk_j &\\sim N(\\beta\\mu_b^T\\mu_j, \\mu_b^T\\Sigma_j\\mu_b) \\\\\n",
    "        &\\sim N(\\beta\\mu_b^T\\mu_j, \\beta^2\\mu_b^T\\alpha I\\mu_b) \\\\\n",
    "        &\\sim N(\\beta\\mu_b^T\\mu_j, \\beta^2\\alpha \\cdot 1) \\\\\n",
    "        &\\sim \\begin{cases}\n",
    "        N(\\beta, \\beta^2\\alpha),& \\text{if } j = b,c\\\\\n",
    "        N(0, \\beta^2\\alpha),& \\text{otherwise }\\\\\n",
    "        \\end{cases}\n",
    "  \\end{align}\n",
    "\n",
    "  Since $\\alpha$ is per definition vanishing small and $0 << \\beta << \\frac{1}{\\alpha}$, it follows that the variance of $\\mu_b^Tk_j \\approx 0$. Therefore we have with probability $\\approx 1$ the value of\n",
    "  $$\n",
    "  \\beta\\mu_b^Tk_j \\approx\n",
    "  \\begin{cases}\n",
    "      \\beta,& \\text{if } j = b\\\\\n",
    "      0,& \\text{otherwise }\\\\\n",
    "  \\end{cases}\n",
    "  $$\n",
    "\n",
    "  The same applies for $\\mu_c$ yielding\n",
    "  $$\n",
    "  \\beta\\mu_c^Tk_j \\approx\n",
    "  \\begin{cases}\n",
    "      \\beta,& \\text{if } j = c\\\\\n",
    "      0,& \\text{otherwise }\\\\\n",
    "  \\end{cases}\n",
    "  $$\n",
    "\n",
    "  For the proposed query $q = \\beta (\\mu_b + \\mu_c)$ we get:\n",
    "  \n",
    "  \\begin{align}\n",
    "      q^Tk_j &= \\beta(\\mu_b + \\mu_c)^Tk_j \\\\\n",
    "      &= \\beta\\mu_b^Tk_j + \\beta\\mu_b^Tk_j \\\\\n",
    "      &\\approx\n",
    "  \\begin{cases}\n",
    "      \\beta,& \\text{if } j = b,c\\\\\n",
    "      0,& \\text{otherwise }\\\\\n",
    "  \\end{cases}\n",
    "  \\end{align}\n",
    "\n",
    "  Now we are in the same setting as before in the first equation of Subtask 2.2 which leads to the desired result $o = \\frac{1}{2}(v_b + v_c)$ as derived above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c4665",
   "metadata": {
    "id": "511c4665"
   },
   "source": [
    "2. If we now assume that $\\Sigma_b = \\alpha I + \\frac{1}{2}(\\mu_b\\mu_b^T)$, we get the following based on the fact that $||\\mu_j|| = 1$:\n",
    "\n",
    "    \\begin{align}\n",
    "    \\beta\\mu_b^Tk_b &\\sim N(\\beta\\mu_b^T\\mu_b, \\mu_b^T\\Sigma_b\\mu_b)\\\\\n",
    "        &\\sim N(\\beta, \\beta^2\\mu_b^T(\\alpha I + \\frac{1}{2}(\\mu_b\\mu_b^T))\\mu_b) \\\\\n",
    "        &\\sim N(\\beta, \\beta^2\\mu_b^T\\alpha I\\mu_b + \\frac{1}{2}\\beta^2\\mu_b^T\\mu_b\\mu_b^T\\mu_b) \\\\\n",
    "        &\\sim N(\\beta, \\beta^2\\alpha + \\frac{1}{2}\\beta^2) \\\\\n",
    "    \\end{align}\n",
    "\n",
    "    Further if one multiplies $\\beta\\mu_c^Tk_b$ one gets due to the orthonormality of all $\\mu_j$:\n",
    "\n",
    "    \\begin{align}\n",
    "    \\beta\\mu_c^Tk_b &\\sim N(\\beta\\mu_c^T\\mu_b, \\mu_c^T\\Sigma_b\\mu_c)\\\\\n",
    "        &\\sim N(0, \\beta^2\\mu_c^T(\\alpha I + \\frac{1}{2}(\\mu_b\\mu_b^T))\\mu_c) \\\\\n",
    "        &\\sim N(0, \\beta^2\\mu_c^T\\alpha I\\mu_c + \\frac{1}{2}\\beta^2\\mu_c^T\\mu_b\\mu_b^T\\mu_c) \\\\\n",
    "        &\\sim N(0, \\beta^2\\alpha) \\\\\n",
    "    \\end{align}\n",
    "\n",
    "    For the query $q = \\beta (\\mu_b + \\mu_c)$ we get:\n",
    "    \n",
    "    \\begin{align}\n",
    "        q^Tk_b &= \\beta(\\mu_b + \\mu_c)^Tk_b \\\\\n",
    "        &= \\beta\\mu_b^Tk_b + \\beta\\mu_b^Tk_b \\\\\n",
    "        &\\sim N(\\beta, \\beta^2(2 + \\alpha)) \\\\\n",
    "    \\end{align}\n",
    "\n",
    "    One notices, that the variance of $q^Tk_b$ is not reaching zero due to the factor $2+ \\alpha$ and therefore might be quite high. Especially applying exponential function within softmax might yield to large variations for $\\exp(q^Tk_b)$ compared to proposed value in the setting of subtask 3.1 beeing $\\approx \\exp(\\beta)$.\n",
    "\n",
    "    As a conseqeunce $o = \\Sigma^n_{i=1} a_i v_i = \\Sigma^n_{i=1} \\frac{exp(q^Tk_i)}{\\Sigma^n_{j=1}exp(q^Tk_j)} v_i$ varies also a lot from the derived solution in subtask 3.1 and 2.2. Specifially, a large value of $\\exp(q^Tk_b)$ leads to a bias towards $v_b$ and a low value of $\\exp(q^Tk_b)$ to bias towards $v_c$ in contrast to the average result.\n",
    "\n",
    "    This problem may be resolved in the multi-head setting, as multiple sampling might compensate for the variation. A higher value of $exp(q^Tk_b)$ of a single head would be evened out by other heads that have a lower value in $exp(q^Tk_b)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j0r37mi5cC1B",
   "metadata": {
    "id": "j0r37mi5cC1B"
   },
   "source": [
    "#### ${\\color{red}{Comments\\ 1.3}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "cross-feedback comment section\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_qxdcrolx48r",
   "metadata": {
    "id": "_qxdcrolx48r"
   },
   "source": [
    "### Subtask 4: Model Size  \n",
    "1. Imagine you have an input sequence of  $l$ tokens, how much memory is required and what time complexity do we have for a single self-attention layer? (give your answer in terms of $l$)\n",
    "2. If you have $N$ layers of self-attention, how  would the memory requirements and the time complexity change? (give your answer in terms of $l$ and $N$)\n",
    "3. If you have $l=10,000$ and $10$ layers, with the ability to perform $10M$ operations per second, how long would it take to compute the attention output?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s35qnPU4yoYD",
   "metadata": {
    "id": "s35qnPU4yoYD"
   },
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de31135c",
   "metadata": {
    "id": "de31135c"
   },
   "source": [
    "\n",
    "1. - Memory: Assuming the model utilizes scaled dot-product attention, the memory requirement for a single self-attention layer can be calculated as approximately $O(l^2)$ for storing the attention weights and $O(l \\times d)$ for storing the intermediate representations, where $d$ is the dimension of the embeddings.\n",
    "   - Time Complexity: The time taken for a single self-attention layer depends on matrix multiplications, leading to a time complexity roughly proportional to the square of the sequence length multiplied by the embedding dimension, e.g. $O(l^2 \\times d)$.\n",
    "\n",
    "2. - Memory: With $N$ layers of self-attention, the overall memory demand becomes a sum of the memory requirements for each layer.\n",
    "   - Time Complexity: The time complexity for $N$ layers grows linearly with $N$, and it is proportional to the product of the sequence length squared and the embedding dimension.\n",
    "\n",
    "3. Given:\n",
    "   - $l = 10,000$ tokens (input sequence length)\n",
    "   - $N = 10$ layers\n",
    "   - Computational capacity = $10,000,000$ ops/sec\n",
    "   - Let's assume a typical embedding dimension, $d = 512$\n",
    "\n",
    "   Calculations:\n",
    "      - Time complexity per layer is approximately $O(l^2 \\times d)$. So, for one layer: $10,000^2 \\times 512$ operations.\n",
    "      - Since there are $10$ layers, the total operations are $10 \\times (10,000^2 \\times 512) = 512,000,000,000$.\n",
    "      - Total time = $512,000,000,000$ ops / $10,000,000$ ops/sec = $51,200 \\text{ seconds} \\approx 14.22 \\text{ hours}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n2UFNSuozvNd",
   "metadata": {
    "id": "n2UFNSuozvNd"
   },
   "source": [
    "#### ${\\color{red}{Comments\\ 1.4}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "cross-feedback comment section\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Av_GjYokHmGo",
   "metadata": {
    "id": "Av_GjYokHmGo"
   },
   "source": [
    "## **Task 2: Multiple Choice Question Answering** (4 + 3 + 5 + 2 = 14 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hypnmNp2yeNz",
   "metadata": {
    "id": "hypnmNp2yeNz"
   },
   "source": [
    "In this task, you will fine-tune a transformer model on a multiple-choice task, which is the task of selecting the most plausible inputs in a given selection. The dataset used here is [SWAG](https://www.aclweb.org/anthology/D18-1009/), which is available via the Hugging Face [hub](https://huggingface.co/datasets/swag). Check the link for an overview of the dataset. SWAG is a dataset about commonsense reasoning, where each example describes a situation and then proposes four options that could apply for it.\n",
    "Let's start by installing the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "RYb57u4CHnZ7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RYb57u4CHnZ7",
    "outputId": "b9863cf7-052b-4a92-8c7e-573ad80f8576"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: pyarrow-hotfix, dill, multiprocess, datasets\n",
      "Successfully installed datasets-2.16.1 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m933.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
      "Collecting responses<0.19 (from evaluate)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (10.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "Installing collected packages: responses, evaluate\n",
      "Successfully installed evaluate-0.4.1 responses-0.18.0\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.25.0\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install datasets\n",
    "%pip install evaluate\n",
    "%pip install accelerate -U\n",
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oLwC5naF8KzV",
   "metadata": {
    "id": "oLwC5naF8KzV"
   },
   "source": [
    "In this task, you will use a BERT model with a `MultipleChoice` head from the Hugging Face library and then create your custom model.   Recall from the class that the BERT model has an auxiliary next sentence prediction task, in which two sentences are given to BERT separated by a `[SEP]` token and a classifier head decides if the second sentence logically follows the first one. Hugging Face has\n",
    " a `*ForMultipleChoice` architecture that uses the representation of the `[CLS]` token and a linear layer to classify if one sentence follows the other. We first start with this default architecture and then build a more complicated one in a later subtask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FJ7H6SSk1DqI",
   "metadata": {
    "id": "FJ7H6SSk1DqI"
   },
   "source": [
    "### Subtask 1: Loading and Processing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KvvYhCtW1kI3",
   "metadata": {
    "id": "KvvYhCtW1kI3"
   },
   "source": [
    "We use the `dataset` library to download the SWAG dataset, which already contains train, validation, and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "-cLxQBhq0Rmo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 821,
     "referenced_widgets": [
      "57a1fb272c954640ab20ce5b6f758f0c",
      "b7e0d6ea05b14f65b32824b284851694",
      "6a3c3dafc825432d9c2286492ff16ef8",
      "d399cde830fa47b5a57effc04f3ff1c2",
      "d6ec2dd479cc4396a673ab237d58174d",
      "c54e4e6e3d6347ce88b4141b3c392f7e",
      "23fb82edf193442aa5d1f8ecbb8d9395",
      "73e738410bd34a3ba6f2269582904c98",
      "a502a25ae6894f54ae78150c10f89d7d",
      "cb4a0bf5d2d14cbda9129a2a3b7f5bce",
      "a8370a85793040f180594e3aa8d1fcd6",
      "828b61e745174f7db6db389c02ae4ae5",
      "a013d9e35af5404c81c35352c2a9eda8",
      "356ba877c3b140458d14f2e1ea6a6158",
      "72c650076946494d9f650d566cbed1e7",
      "a28dc265b0e5412185525a27fc79e8d8",
      "7b7d9abcdce74ae0bb9363071c888389",
      "538e529431f94577b246a5e09a673bde",
      "6d029fa40e484a04a5663ed8b2d12595",
      "2d2b71d7290642f497e9d8c089b884e5",
      "ba929fb09d994f84b8b6b606f1ee3cd8",
      "940eb0ab7d044da8adaf3b4d43acdaca",
      "a6a80247917d4f9a9066ae8ff370db4c",
      "5f1ae48b488b4cf3a04197d17b1c3d93",
      "54ea95a91999465292e204ba103ccb2c",
      "e19474492bd249df851d4ae28eb53809",
      "e33caaaa2b7b4f198ff9db47452a2b61",
      "bb7e289f74d44b0f8bcb9e240165c761",
      "41bf3eba668f48dba600a23d63acee36",
      "c1e231e79a444135a3b542476cd903d6",
      "0b5e342dffad42169aa5832cf868c1b1",
      "db65fe7f62704e2f97811eb010ef083a",
      "d21382fd71cc46b79242b0dae16ae4ce",
      "458e7eac7f08494e89ec552ce062579d",
      "0cef5c21968e414d94276353973988fd",
      "93a615b1d7ef46acbca2e154a1b640fe",
      "3d1ad8c1404845aeba6191db24b0c4cd",
      "a30d7f9e70fe4be4aa1a114aed9b3ab8",
      "ae24a68203304b7eb6191fda58794f5b",
      "d0abb01fd7fc44558aeda57c007ca0a0",
      "9f387a48ea244bd6aed0bf94acbff7b4",
      "70e234b0f62441a584ef69f638e2eb82",
      "f9bccf048f1545cfb2facc74efe13edb",
      "96a412b19d4c4bd9b692cac753d4cf85",
      "81cc3e3762664ae0b9ed533e96d1dac5",
      "e4181752e01c4cb8b7b5ed7359631774",
      "79cf53689b884f8d9bea30d2e9251593",
      "ade13eb5e9b943fb97f021c392daefe6",
      "3829c33c797c481a904003edbcdbea3a",
      "4f3cba1decc543ad845f2d07deb1db89",
      "f647162f69944610a2fd270657c677e9",
      "777d64955e0641a6912b315329f2e723",
      "ce65fe5e3dc04b9d8e493fe780ee76d7",
      "64eb6474bb0547f28f1bc9dff8a5c8bf",
      "4cbfd253472446579ba8d539c2ee285c",
      "0435d0b27c494d23853e48fe8f32364e",
      "65fca82f6cab4387b4a9ee094f05e630",
      "06b58078556b41c68e4aa3ab42824f8f",
      "be72e99f0aa24439a283c65d3e5f563f",
      "247abc453da641349aad5816847f5a86",
      "81220679e4974027badda9187ee4326f",
      "fbc61bd955f7487692b819fa5dd50995",
      "5dfda28f95b74a5da54b27496907abcf",
      "34e128a9cb444bc59c635194159b0c37",
      "57834ab0a05048caa55f11173588890c",
      "699d3cc4b0fe4ce19aa6f6e7d3fb0610",
      "f1c431adee704bf2ac12e638cff64047",
      "7204b1fe03194216a46b06549acab5e6",
      "010a6381c9384be4a2952a5dc78dfdc8",
      "593f2d2469494b849391b2826b010a7e",
      "e401d9167c194c409ca396bc0dbb470a",
      "be519380274a4a6a864718247f88e6bb",
      "fd603deac00f468fadbdfd4a7470ec54",
      "71ccaacdd4804e018abea2bf0406e66b",
      "36b40711686b4f1f97c9a63b1146bf58",
      "2d5b3575e96646e3996df7e6c7b75a23",
      "916c7bafacfc451aa24f9d6db6cd54cb",
      "3d54a6946c7d4b92892e585ae83140f4",
      "11c4715a4de643e994bf9088f30b414b",
      "b9bb29a192e34f2f95ea4cd72a079323",
      "0d38a7833a8e4391bfe8d08931ba7a54",
      "26b7c5bdcf6a4cb1bd86a02b714f0c60",
      "33bdb329a75d421f84ca9edf0e7d0b2a",
      "98c8ee3905cb41babdcc5c0f2f61a7d2",
      "3da62bee5f9d47dabf887e60f8636b81",
      "0267f47bde5246728342b83f531febae",
      "5aaec0b27bf040e59c7dcadce5e20e0d",
      "f6ce69cb3114489bb63e1dc0d5a85cdb",
      "f31b37cc5dfe4cf18973c8952f97c404",
      "f3388909be17494d9c5a3246be849d5d",
      "bcb2ffd39c704c0190670c11eb260cf5",
      "3b073488be804fc58cbd02b1d54b8568",
      "408346c70dcc4a1680b4ba37e3acaaff",
      "f84164ed5f67460d8fd642f26d277cbb",
      "bca936f917774d31815f6a992081b845",
      "e4b1e240127f4a0e852641676a928d2d",
      "e206bc78e90c4f8d8543d636580137d9",
      "70e98b1cc88f498b83fb327d118d0da2",
      "eeb4e1b99292488dac0357d058d19e49"
     ]
    },
    "id": "-cLxQBhq0Rmo",
    "outputId": "f2dd11cc-c70d-4006-9d9c-6b51f0f359d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/l/.cache/huggingface/modules/datasets_modules/datasets/swag/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c (last modified on Mon Dec 11 12:45:22 2023) since it couldn't be found locally at swag., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],\n",
       "        num_rows: 73546\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],\n",
       "        num_rows: 20006\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],\n",
       "        num_rows: 20005\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "from datasets import load_dataset, load_metric\n",
    "datasets = load_dataset(\"swag\", \"regular\")\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DHkkMD3v4bs3",
   "metadata": {
    "id": "DHkkMD3v4bs3"
   },
   "source": [
    "Lets look at the first item to see how the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "vN6p1Mt84ZW_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vN6p1Mt84ZW_",
    "outputId": "00a02b55-3742-493e-dd73-b7064487a1c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video-id': 'anetv_jkn6uvmqwh4',\n",
       " 'fold-ind': '3416',\n",
       " 'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',\n",
       " 'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',\n",
       " 'sent2': 'A drum line',\n",
       " 'gold-source': 'gold',\n",
       " 'ending0': 'passes by walking down the street playing their instruments.',\n",
       " 'ending1': 'has heard approaching them.',\n",
       " 'ending2': \"arrives and they're outside dancing and asleep.\",\n",
       " 'ending3': 'turns the lead singer watches the performance.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xOxxXEdbTF8l",
   "metadata": {
    "id": "xOxxXEdbTF8l"
   },
   "source": [
    "**Question:**\n",
    "Look at the dataset card on the Hugging Face hub and define what each of these fields means, with respect to the task:\n",
    "\n",
    "*   `sent1`:\n",
    "*   `sent2`:\n",
    "*    `ending0`, `ending1`, `ending2` and `ending3`:\n",
    "*   `label`:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2FfOTODlURAY",
   "metadata": {
    "id": "2FfOTODlURAY"
   },
   "source": [
    "**Answer**\n",
    "\n",
    "\n",
    "*   `sent1`: the first sentence\n",
    "*   `sent2`: the beginning of the second sentence\n",
    "*    `ending0`, `ending1`, `ending2` and `ending3`: candidate endings of the second sentence\n",
    "*   `label`: the index (0, 1, 2 or 3) of the correct ending\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D9TcSWfiU2m-",
   "metadata": {
    "id": "D9TcSWfiU2m-"
   },
   "source": [
    "Write a function that displays the context and each of the four choices, following the format\n",
    "\n",
    "\n",
    "```\n",
    "Context:...\n",
    "A-\n",
    "B-\n",
    "C-\n",
    "D-\n",
    "Ground truth: option ...\n",
    "```\n",
    "\n",
    "How you display the results is not important. You should be able to extract different parts of the data correctly and know what each field represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "jVOf4b2r4gDq",
   "metadata": {
    "id": "jVOf4b2r4gDq"
   },
   "outputs": [],
   "source": [
    "def explain_example(example):\n",
    "  ### your code ###\n",
    "  alphabet = \"ABCD\"\n",
    "  print(\"Context:\", example[\"startphrase\"], \"...\")\n",
    "  for i, letter in enumerate(alphabet):\n",
    "    print(f\"{letter}- ... {example['ending' + str(i)]}\")\n",
    "  print(\"Ground truth: option\", alphabet[example[\"label\"]])\n",
    "  ### your code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "BlTFSjmR5QrP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BlTFSjmR5QrP",
    "outputId": "dccaad1d-1493-4654-d48b-7948b9d3e549"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Members of the procession walk down the street holding small horn brass instruments. A drum line ...\n",
      "A- ... passes by walking down the street playing their instruments.\n",
      "B- ... has heard approaching them.\n",
      "C- ... arrives and they're outside dancing and asleep.\n",
      "D- ... turns the lead singer watches the performance.\n",
      "Ground truth: option A\n"
     ]
    }
   ],
   "source": [
    "explain_example(datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lXnX0E915qbb",
   "metadata": {
    "id": "lXnX0E915qbb"
   },
   "source": [
    "Before feeding the data into the model, we need to preprocess the text using `Tokenizer` to tokenize the inputs into tokens and put it in a format that the model expects. The tokenizer specific to the model we want to use for this task is `distilbert-base-uncased`. Complete the code below to load a fast tokenizer for this model. DistilBERT is similar to the BERT model, and we only use this particular architecture for faster training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd4BkVDc5RdO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275,
     "referenced_widgets": [
      "9a08472b47434fb195f062d2911d6c76",
      "2948fd5fd78c4feba9a760e409c6f6f4",
      "f635ebae98f54bf8a9fe4c15101952bc",
      "46bd30eff7c14a25a0746de2fbc8c29a",
      "b803b0fe2d584786a39de1f14b1c1381",
      "0b817a9d883e424a8f670c75b81a5aa6",
      "2366a2606cd541b38b68da52bbce193a",
      "0c6ce1e7e30b4ceeb9785152e02d4350",
      "cfd8e10ae72443e5be42b8d1658e6ade",
      "0b4d58ea15ed49248726123ca8799d28",
      "ed30f215d43447eba06579ce72f12b96",
      "5baa58ea4480446ca10ea2f6cd088af9",
      "b84d07304c6c47e988e08907a1a3ff2e",
      "e5456b96c4de487986a5a8ab57fc98c7",
      "118021eacfb44e0abbf87d0515a4c163",
      "69b96184395e48168245958bbaeb7d57",
      "021f264f3dfe44ddb6c88d18ff2a0254",
      "c9135ce61f074aca8086a83ca8cfedea",
      "917f189db0574e2f9ccbe829878d8955",
      "64f9c83eff18400494ef958da63eb70e",
      "6cf282dfb38a42c68ce466df72c23512",
      "d53622519a28468e9cf01d43a4d6359f",
      "a945c62c41e74d2a9bda8a9f2cf0e732",
      "e6711f0dcac647efbd826dc77fbb3dee",
      "97f8f1df6dc74654832180936fe4ebc9",
      "51afabc05924441d833880a5ff000266",
      "0452811cbd1f4c80b2342864b592d7bd",
      "ae6b31ffe127446ba324d8556ca538d1",
      "3934d28144874d4e8380180d5cef9811",
      "b95c669229c6469d92664d4e14b5500e",
      "75c80ece31f94678b04929c31245fabe",
      "c2183af0e2134bd793c6bae94c7f3e59",
      "d8798df78b4842baa89666531d5c4a27",
      "25db7ed706784fcc9c5a74f695bead9d",
      "b1ebcfdf19a34e1bbc15ee7c1ef9b364",
      "9245a253e88845b1a9637e58192c1593",
      "21adc6f5607b4996a5871dc6705a66d8",
      "77dbf198aeda4a8693ea4c55fdd60c30",
      "d051c1c76bd24072a4af604861346b22",
      "f30382301e354a2c9c1fd312ae093356",
      "c83537aaa4874309a059e21e2fce8602",
      "86a1cdf187224d9da30a815570382432",
      "7350f7be82444d25a1fd180b085dc4e6",
      "0f41dd429f7d4a688a18c42d97685dc0"
     ]
    },
    "id": "fd4BkVDc5RdO",
    "outputId": "6523e917-bd4e-4daa-c705-e87c974f321f"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "###your code###\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "###your code###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "BbtOfQ4T7AQV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BbtOfQ4T7AQV",
    "outputId": "7228c5d7-53b3-4223-8bdf-7a4e2fa919b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 999, 102, 1998, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"This is the first sentence!\", \"And this is the second one.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8ba870d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8ba870d",
    "outputId": "ffca574f-91b4-4698-8679-739f72be5da3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 1012, 102, 2023, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demonstration: There is a [SEP] token at the end of each sentence (102)\n",
    "tokenizer(\"This.\", \"This.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb82ecaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb82ecaf",
    "outputId": "7d038ff4-6dae-4e52-b7a8-23a8910f22ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 1012, 2023, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demonstration: [SEP] token only at the end of overall input (102), not between two sentences\n",
    "tokenizer(\"This. This.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfe021f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfe021f7",
    "outputId": "80365179-9d68-4358-e083-a4ad5d2db041"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 1012, 102, 2023, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demonstration: [SEP] token at the end of overall input (102) and where manually inserted\n",
    "# - equivalent to passing the two sentences as separate arguments (see above)\n",
    "tokenizer(\"This. [SEP] This.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77b0e590",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77b0e590",
    "outputId": "c4afc282-0b6a-4ab1-ed3a-c157a199118b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 1012, 101, 2023, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demonstration: [CLS] is represented as 101 and is automatically inserted ad the beginning of the input\n",
    "tokenizer(\"This. [CLS] This.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05c1e0d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05c1e0d1",
    "outputId": "97438e05-120e-4145-87f2-1101ceb0685e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1037, 102, 1038, 102], [101, 1037, 102, 1039, 102], [101, 1040, 102, 1041, 102]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demonstration: you can pass the list of first sentences as the first argument\n",
    "# and the list of second sentences as the second; they are then merged\n",
    "# and glued with [SEP] (102) automatically\n",
    "tokenizer([\"A\", \"A\", \"D\"], [\"B\", \"C\", \"E\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4LMdG45b7Yop",
   "metadata": {
    "id": "4LMdG45b7Yop"
   },
   "source": [
    "Write a function that preprocesses the samples.\n",
    "The tricky part is to put all the possible pairs of sentences in two big lists before passing them to the tokenizer.\n",
    "Each **first** sentence has to be repeated 4 times to go with different ending options.\n",
    "There should be a separator token between the first and second sentence, to follow the BERT input logic.\n",
    "The final output is a list of 4 elements, one for each choice, where the input is transformed by the tokenizer.\n",
    "For example, with a list of 2 training examples, the output includes 2 lists, where each contains 4 elements. Each of those elements is the converted input ID of the first sentence followed by the second sentence with different endings.\n",
    "When calling the `tokenizer`, we use the argument `truncation=True`. This will ensure that an input longer than what the model selected can handle will be truncated to the maximum length accepted by the model.\n",
    "\n",
    "**Hint:** Flatten the lists (all choices are flattened into a single list) before feeding them into the tokenizer and unflatten them once again for the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "uEBvuUSm7FD_",
   "metadata": {
    "id": "uEBvuUSm7FD_"
   },
   "outputs": [],
   "source": [
    "### your code ###\n",
    "def unflatten(data: list, n_endings: int) -> list[list]:\n",
    "    assert len(data) % n_endings == 0\n",
    "    n_sets = len(data) // n_endings\n",
    "    return [[data[i * n_endings + j] for j in range(n_endings)]\n",
    "            for i in range(n_sets)]\n",
    "\n",
    "n_endings = 4\n",
    "ending_names = [f\"ending{i}\" for i in range(n_endings)]\n",
    "### your code ###\n",
    "def preprocess_function(examples):\n",
    "  ### your code ###\n",
    "    # repeat each first sentence four times\n",
    "    first_sentences = [[ex]*4 for ex in examples[\"sent1\"]]\n",
    "    # second sentences possible are combination of header and ending\n",
    "    question_headers = [[ex]*4 for ex in examples[\"sent2\"]]  # ← why do we need this?\n",
    "    second_sentences = [[\" \".join([header, examples[en][i]]) for en in ending_names]\n",
    "                        for i, header in enumerate(examples[\"sent2\"])]\n",
    "\n",
    "    # flatten everything\n",
    "    # ↑ It would have been easier to define the sentences flattened from the beginning,\n",
    "    #   but your wish is my command ;)\n",
    "    first_sentences_flat = [sent for four in first_sentences for sent in four]\n",
    "    second_sentences_flat = [sent for four in second_sentences for sent in four]\n",
    "\n",
    "    # tokenize\n",
    "    tokenized = tokenizer(first_sentences_flat, second_sentences_flat, truncation=True)\n",
    "\n",
    "    # un-flatten\n",
    "    # demonstration: attention_mask is garbage, as always ones:\n",
    "    assert all(el == 1 for mask in tokenized[\"attention_mask\"] for el in mask)\n",
    "\n",
    "    n_sets = len(first_sentences)\n",
    "    assert n_sets == len(second_sentences)\n",
    "    assert n_sets * n_endings == len(second_sentences_flat)\n",
    "\n",
    "    for key, value in tokenized.items():\n",
    "      tokenized[key] = unflatten(value, n_endings=n_endings)\n",
    "\n",
    "    return tokenized\n",
    "    ### your code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bcf941d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bcf941d6",
    "outputId": "7c39368c-3aca-43cc-ae85-1bc57ca4c9bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],\n",
       "    num_rows: 73546\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9Q25z-jQ-rMu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Q25z-jQ-rMu",
    "outputId": "b63b2593-8d59-4c85-afd9-9b2bbe5058f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4 [30, 25, 30, 28]\n"
     ]
    }
   ],
   "source": [
    "examples = datasets[\"train\"][:2]\n",
    "features = preprocess_function(examples)\n",
    "print(len(features[\"input_ids\"]), len(features[\"input_ids\"][0]), [len(x) for x in features[\"input_ids\"][0]])# output should be 2 4 [30, 25, 30, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ca71ef1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ca71ef1",
    "outputId": "c92869af-7c45-4af4-a3fa-ca6f629109c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[[101, 2372, 1997, 1996, 14385, 3328, 2091, 1996, 2395, 3173, 2235, 7109, 8782, 5693, 1012, 102, 1037, 6943, 2240, 5235, 2011, 3788, 2091, 1996, 2395, 2652, 2037, 5693, 1012, 102], [101, 2372, 1997, 1996, 14385, 3328, 2091, 1996, 2395, 3173, 2235, 7109, 8782, 5693, 1012, 102, 1037, 6943, 2240, 2038, 2657, 8455, 2068, 1012, 102], [101, 2372, 1997, 1996, 14385, 3328, 2091, 1996, 2395, 3173, 2235, 7109, 8782, 5693, 1012, 102, 1037, 6943, 2240, 8480, 1998, 2027, 1005, 2128, 2648, 5613, 1998, 6680, 1012, 102], [101, 2372, 1997, 1996, 14385, 3328, 2091, 1996, 2395, 3173, 2235, 7109, 8782, 5693, 1012, 102, 1037, 6943, 2240, 4332, 1996, 2599, 3220, 12197, 1996, 2836, 1012, 102]], [[101, 1037, 6943, 2240, 5235, 2011, 3788, 2091, 1996, 2395, 2652, 2037, 5693, 1012, 102, 2372, 1997, 1996, 14385, 2024, 2652, 17852, 13433, 3070, 1998, 12964, 2028, 2187, 2169, 1999, 4248, 1012, 102], [101, 1037, 6943, 2240, 5235, 2011, 3788, 2091, 1996, 2395, 2652, 2037, 5693, 1012, 102, 2372, 1997, 1996, 14385, 3524, 3254, 2875, 1996, 15724, 1012, 102], [101, 1037, 6943, 2240, 5235, 2011, 3788, 2091, 1996, 2395, 2652, 2037, 5693, 1012, 102, 2372, 1997, 1996, 14385, 4247, 2000, 2377, 2004, 2092, 2247, 1996, 4306, 2247, 2007, 1996, 2316, 2108, 10263, 1012, 102], [101, 1037, 6943, 2240, 5235, 2011, 3788, 2091, 1996, 2395, 2652, 2037, 5693, 1012, 102, 2372, 1997, 1996, 14385, 3613, 2000, 2377, 10998, 1010, 25338, 1012, 102]]], 'attention_mask': [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]}\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hKizHUiGXgj-",
   "metadata": {
    "id": "hKizHUiGXgj-"
   },
   "source": [
    "We can now apply our function to all the examples in the dataset. We use the `map` method to apply the function on all the elements of all the splits in the dataset (training, validation, and testing).\n",
    "Note that we passed `batched=True` to leverage the fast tokenizer and use multi-threading to process the texts in batches concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9Wau4GZsXcvE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "2e91419d8e9e44a88af9db79c8cee9b3",
      "c0828b8615704f46ae7acb22e096e317",
      "c1241d9b17684226991c2c10bcdf7204",
      "985903c02de64308ad68608e0041864b",
      "755870ce8b3547fea948786faebe09ea",
      "238b17ce39d9400c82118698c584920d",
      "9ee9b5a037904a738cb7f38f1160f996",
      "cdb98ca7743347eabb43320813a79499",
      "3cf54bb66c004459a2a360b2176fc725",
      "79b60235330f495a98d7b7cc4b43ee50",
      "6077f8732e0e4d769126fd8674398ee6",
      "ba7de05b49714b5fb3c8138be9f885f3",
      "c8ed9d29416e44de98a89a4a126913ea",
      "c330256a88b348ff94b71b14ba75ab2c",
      "5d0d3aca86fb47a3879fab1e9658c270",
      "8810d3542c8a4a3bb47ccf71a10911e7",
      "3cf5075659ee4a698b1a83fe3e09a7de",
      "832b01d35cd74cd4bf9ff613c58119f2",
      "54213d85b79b475a96eda14c9b703872",
      "b136bc57560a44768aa3b78e38e6b96d",
      "aa53b93e979a4b5d94822f9637d1d44a",
      "121e618c099f4edbb7c486b4eac0050c",
      "b92e9a909f954d7b974c8b993bae125d",
      "8ddb9bf99d8e4def86ba68192cee970d",
      "542f4b3cda6245d4b2787d6840154ff2",
      "6535f6687fd74a74aa3c79eb38212212",
      "97c242dc7bab49a987a20310562120f2",
      "ef6fbca63a654cb29e36c9a65376fae2",
      "b06194c2573e4f769b0ad607bda0371d",
      "328054cd92a54fccaf33fb543e7af402",
      "11c9b2ed276f4578b9a58b0c7a714747",
      "8b48951e479542e5a6d257b18d3731c0",
      "bbeffaba54cc44df82da2283f8dbbf4a"
     ]
    },
    "id": "9Wau4GZsXcvE",
    "outputId": "552f9baa-af38-4419-9c75-cd81f9b2880c"
   },
   "outputs": [],
   "source": [
    "encoded_datasets = datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G69L2vtPR8uV",
   "metadata": {
    "id": "G69L2vtPR8uV"
   },
   "source": [
    "Our dataset is still not converted to tensors and not padded. This is the job of the `data collator`. A data collator takes a list of examples and converts them to a batch.\n",
    "There is no data collator in the Hugging Face default library that works on our specific problem. We thus need to write our own one. In this collator:\n",
    "\n",
    "*  All the inputs/attention masks are flattened.\n",
    "* A flattened list is passed to the `tokenizer.pad ` method to apply dynamic padding to pad inputs to the maximum length in the batch. Output will be the size of `(batch_size * 4) x seq_length`.\n",
    "* Everything needs to be unflattened for the output of the data collator.\n",
    "* `input_ids` and `labels` should be returned as tensors.\n",
    "* The output is a dictionary called `batch` that contains features needed for training (`input_ids`, `attention_mask`, `label`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "r_3SvkBOSgZx",
   "metadata": {
    "id": "r_3SvkBOSgZx"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class MultipleChoiceDataCollator:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: list[dict]):\n",
    "        accepted_keys = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "        if len(features[0])>len(accepted_keys):\n",
    "          features = [{k: v for k, v in i.items() if k in accepted_keys} for i in features]\n",
    "        ### your code ###\n",
    "\n",
    "        # flatten\n",
    "        keys_flatten = {\"input_ids\", \"attention_mask\"}\n",
    "        flattened_features = []\n",
    "        for row in features:\n",
    "          for key in keys_flatten:\n",
    "            assert len(row[key]) == n_endings\n",
    "          for j in range(n_endings):\n",
    "            flattened_features.append({\n",
    "              key: row[key][j] if key in keys_flatten else row[key]\n",
    "              for key in accepted_keys\n",
    "            })\n",
    "\n",
    "\n",
    "        # pad\n",
    "        batch = self.tokenizer.pad(flattened_features,\n",
    "                          padding=self.padding,\n",
    "                          max_length=self.max_length,\n",
    "                          pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "\n",
    "\n",
    "        # un-flatten\n",
    "        for key, flat in batch.items():\n",
    "          values = []\n",
    "          for i in range(len(features)):\n",
    "            unflattened = flat[i*n_endings:(i+1)*n_endings]\n",
    "            if key not in keys_flatten:\n",
    "              value = unflattened[0]\n",
    "              for j in range(n_endings):\n",
    "                assert unflattened[j] == value\n",
    "              unflattened = value\n",
    "            values.append(unflattened)\n",
    "\n",
    "          batch[key] = torch.tensor(values)\n",
    "\n",
    "        # very ugly, but it seems to be necessary to rename this to \"labels\", as\n",
    "        # otherwise we encounter an unexpected keyword argument error on training\n",
    "        batch[\"labels\"] = batch[\"label\"]\n",
    "        del batch[\"label\"]\n",
    "\n",
    "        ### your code ###\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "-oKeJUr_UBUV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-oKeJUr_UBUV",
    "outputId": "745f396c-23eb-466f-d2cb-1fddcd65500f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 35])\n",
      "torch.Size([2, 4, 35])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "accepted_keys = [\"input_ids\", \"attention_mask\", \"label\"]  # note \"label\" without \"s\"\n",
    "features = [{k: v for k, v in encoded_datasets[\"train\"][i].items() if k in accepted_keys} for i in range(2)]\n",
    "batch=MultipleChoiceDataCollator(tokenizer)(features)\n",
    "print(batch[\"input_ids\"].shape)\n",
    "print(batch[\"attention_mask\"].shape)\n",
    "print(batch[\"labels\"].shape)  # note the \"s\" here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "glDO1KfaVXoX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "glDO1KfaVXoX",
    "outputId": "0532ef7a-7027-4d11-a0c0-227b14e81d57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  1037,  6943,  2240,  5235,  2011,  3788,  2091,  1996,  2395,\n",
      "         2652,  2037,  5693,  1012,   102,  2372,  1997,  1996, 14385,  2024,\n",
      "         2652, 17852, 13433,  3070,  1998, 12964,  2028,  2187,  2169,  1999,\n",
      "         4248,  1012,   102,     0,     0])\n",
      "[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession are playing ping pong and celebrating one left each in quick. [SEP] [PAD] [PAD]\n",
      "tensor([  101,  1037,  6943,  2240,  5235,  2011,  3788,  2091,  1996,  2395,\n",
      "         2652,  2037,  5693,  1012,   102,  2372,  1997,  1996, 14385,  3524,\n",
      "         3254,  2875,  1996, 15724,  1012,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0])\n",
      "[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession wait slowly towards the cadets. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "tensor([  101,  1037,  6943,  2240,  5235,  2011,  3788,  2091,  1996,  2395,\n",
      "         2652,  2037,  5693,  1012,   102,  2372,  1997,  1996, 14385,  4247,\n",
      "         2000,  2377,  2004,  2092,  2247,  1996,  4306,  2247,  2007,  1996,\n",
      "         2316,  2108, 10263,  1012,   102])\n",
      "[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession continues to play as well along the crowd along with the band being interviewed. [SEP]\n",
      "tensor([  101,  1037,  6943,  2240,  5235,  2011,  3788,  2091,  1996,  2395,\n",
      "         2652,  2037,  5693,  1012,   102,  2372,  1997,  1996, 14385,  3613,\n",
      "         2000,  2377, 10998,  1010, 25338,  1012,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0])\n",
      "[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession continue to play marching, interspersed. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "  print(batch[\"input_ids\"][1][i])\n",
    "  print(tokenizer.decode(batch[\"input_ids\"][1][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WD4IjgtLYCLa",
   "metadata": {
    "id": "WD4IjgtLYCLa"
   },
   "source": [
    "#### ${\\color{red}{Comments\\ 2.1}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "cross-feedback comment section\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mZZDaB9LXSB4",
   "metadata": {
    "id": "mZZDaB9LXSB4"
   },
   "source": [
    "### Subtask 2: Fine-tuning a Hugging Face Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toXHZtX7XSEs",
   "metadata": {
    "id": "toXHZtX7XSEs"
   },
   "source": [
    "To fine-tune our model, we first need to download the correct architecture from Hugging Face. Import the correct class for this task and download the pre-trained checkpoint for the base class from `distilbert-base-uncased`. Note that the weights in the classification head are initialized at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "NONtYIrE-w6p",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NONtYIrE-w6p",
    "outputId": "d0e843c8-da04-4151-d04c-2da39d29461c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForMultipleChoice were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "### your code ###\n",
    "from transformers import DistilBertForMultipleChoice\n",
    "model_hf = DistilBertForMultipleChoice.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "### your code ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4C8rBosXSVi",
   "metadata": {
    "id": "f4C8rBosXSVi"
   },
   "source": [
    "Next, we need to define our `Trainer` and pass in the correct `TrainingArguments` (a class that contains all the attributes to customize the training). Define a `TrainingArguments` that\n",
    "\n",
    "\n",
    "* creates an output directory `distilbert-base-uncased-swag` to save the checkpoints and logs.\n",
    "*   evaluates the model on the validation set after the `300` steps.\n",
    "* a checkpoint should be saved after each `600` step and no more than 2 checkpoints should be saved in total.\n",
    "* the random seed for training is `77`.\n",
    "* batch size for training and evaluation: `48` (if you are running out of memory, feel free to change this setting but indicate it as a comment in your notebook, on a T4 GPU from google colab this takes about `13.2GB` of `15.0GB`).\n",
    "* train for `1800` steps with a learning rate of `5e-5`, and add weight decay of `0.01` to the optimizer.\n",
    "* the trainer should remove the columns from the data that are not used by the model.\n",
    "* The final checkpoint should be the checkpoint that had the best overall validation metric not necessarily the last checkpoint.\n",
    "\n",
    "**Note:** Please use GPU for to train your model. If on colab, you can use T4 GPU for free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "DXLTm2Ed-1CI",
   "metadata": {
    "id": "DXLTm2Ed-1CI"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(\n",
    "    ### your code ###\n",
    "    output_dir=\"distilbert-base-uncased-swag\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=300,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=600,\n",
    "    save_total_limit=2,\n",
    "    seed=77,\n",
    "    # reducing batch size to half of the suggested value due to GPU memory pressure\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    "    max_steps=1800,\n",
    "    learning_rate=5e-5,  # also the default\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,  # undocumented, but used in tutorial\n",
    ")\n",
    "    ### your code ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8w1tE975ZGS9",
   "metadata": {
    "id": "8w1tE975ZGS9"
   },
   "source": [
    "Before we initialize the `Trainer`, we create a function that tells the trainer how to compute the metrics from the predictions. Fill the `compute_metrics` function to compute the accuracy based on the `predictions`. This object contains the prediction of the model, as well as the ground truth labels.\n",
    "\n",
    "**Hint 1:** Keep in mind that the output of this function should be a dictionary containing the metric name and value.\n",
    "\n",
    "**Hint 2:** Consider the shape of the example input. This is similar to the logits produced by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "gilWMRNJXsBq",
   "metadata": {
    "id": "gilWMRNJXsBq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "def compute_metrics(predictions: EvalPrediction):\n",
    "  ### your code ###\n",
    "   preds, label_ids = predictions\n",
    "\n",
    "   choices = np.argmax(preds, axis=-1)\n",
    "   return_dict = {\"accuracy\": np.mean(choices == label_ids)}\n",
    "  ### your code ###\n",
    "   return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "SAC4DktAAt8y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SAC4DktAAt8y",
    "outputId": "c9d0cc60-e804-423f-a08d-e778a33b8fc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds=np.array([[0.9,0.2,0,0],\n",
    "                [0.2,0.2,0.9,0.1],\n",
    "                [0.2,0.9,0,0],\n",
    "                [0.2,0.1,0.8,0],\n",
    "                [0.9,0.1,0.8,0],\n",
    "                [0.2,1,0.4,0],\n",
    "                [0.2,1,0.4,0.9],\n",
    "                [1,0.1,0.4,0.3],\n",
    "                [0.1,0.1,0.9,0.3],\n",
    "                [0.1,0.1,0.2,1]])\n",
    "label_ids=np.array([0,3,1,2,0,1,3,0,2,3])\n",
    "compute_metrics((preds,label_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "koPEX6UmAvTp",
   "metadata": {
    "id": "koPEX6UmAvTp"
   },
   "source": [
    "Now it's time to pass everything to a `Trainer` object to start the training process. Initialize a `Trainer` object and pass all the necessary information, keep in mind that we also have the optional metric computation and that we tend to run an evaluation on the validation set during training. The training should take around 30 min on Google Colab T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38Xrwx4OSUIe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38Xrwx4OSUIe",
    "outputId": "2413123e-f3ee-4793-e5e3-01606e18302f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ybmjWr1gFoR9",
   "metadata": {
    "id": "ybmjWr1gFoR9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "### your code ###\n",
    "trainer = Trainer(\n",
    "    model=model_hf,\n",
    "    args=training_args,\n",
    "    # data collator removes columns from the data not used by model\n",
    "    data_collator=MultipleChoiceDataCollator(tokenizer),\n",
    "    train_dataset=encoded_datasets[\"train\"],\n",
    "    eval_dataset=encoded_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "### your code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "UFcB9_hCF8Kx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "UFcB9_hCF8Kx",
    "outputId": "7b2b7b28-da81-4a7b-9314-fef8d9f244ae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1800' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1800/1800 17:54, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.978314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.110400</td>\n",
       "      <td>0.914558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.110400</td>\n",
       "      <td>0.868163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.994700</td>\n",
       "      <td>0.847908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.932300</td>\n",
       "      <td>0.814828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.932300</td>\n",
       "      <td>0.789878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1800, training_loss=0.9945607503255208, metrics={'train_runtime': 1076.1998, 'train_samples_per_second': 40.141, 'train_steps_per_second': 1.673, 'total_flos': 2564887381385472.0, 'train_loss': 0.9945607503255208, 'epoch': 0.59})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()# should take around 30 min on Google Colab T4 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nLXcPADKi73j",
   "metadata": {
    "id": "nLXcPADKi73j"
   },
   "source": [
    "Save the model in `distilbert-base-uncased-swag/final_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "tlHd67J2qBdA",
   "metadata": {
    "id": "tlHd67J2qBdA"
   },
   "outputs": [],
   "source": [
    "### your code ###\n",
    "trainer.save_model(\"distilbert-base-uncased-swag/final_model\")\n",
    "### your code ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "of7b-SkFne_J",
   "metadata": {
    "id": "of7b-SkFne_J"
   },
   "source": [
    "Look at the saved files and answer the following questions (it is possible to answer these questions by writing some code, but we want you to explore the saved files):\n",
    "\n",
    "**Question:**\n",
    "\n",
    "\n",
    "1.   What is the vocabulary id for the `[CLS]` and `[MASK]` tokens?\n",
    "2.   What is the dropout probability for the attention layer?\n",
    "\n",
    "**Dropout:** With dropout, certain nodes are set to the value zero in a training run, i.e. removed from the network. Thus, they have no influence on the prediction and also in the backpropagation. Thus, a new, slightly modified network architecture is built in each run and the network learns to produce good predictions without certain inputs. Read more [here](https://databasecamp.de/en/ml/dropout-layer-en).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eZ74Uaouno6L",
   "metadata": {
    "id": "eZ74Uaouno6L"
   },
   "source": [
    "**Answer**\n",
    "\n",
    "```\n",
    "1. [CLS]: 101\n",
    "   [MASK]: 103\n",
    "   (see tokenizer_config.json, added_tokens_decoder)\n",
    "2. attention_dropout: 0.1\n",
    "   (see config.json)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Auvva-EkGcRR",
   "metadata": {
    "id": "Auvva-EkGcRR"
   },
   "source": [
    "#### ${\\color{red}{Comments\\ 2.2}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "cross-feedback comment section\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vAcJQfpHAvbR",
   "metadata": {
    "id": "vAcJQfpHAvbR"
   },
   "source": [
    "### Subtask 3: Fine-tune a Custom Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QT3cbQO-wnio",
   "metadata": {
    "id": "QT3cbQO-wnio"
   },
   "source": [
    "In this case, we were lucky that Hugging Face had a pre-implemented architecture available for us to use. However, that is not always the case. Moreover, we might want to experiment beyond the default architectures to find a suitable one for a task. Therefore, it is important to learn to extend the Hugging Face models and train a custom model. The good news is that except for the model architecture the rest of the code can remain as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G2RtNQewxG2O",
   "metadata": {
    "id": "G2RtNQewxG2O"
   },
   "source": [
    "Design a model for multiple choice model as follows:\n",
    "\n",
    "\n",
    "1.   the config file for a feature extractor (must be a distilbert type) is  passed during initialization. The config file determines which model is used for feature extraction.\n",
    "2.   From the `last_hidden_state` of the feature extractor, choose the `[CLS]` embedding (first one). This embedding is used as the compressed representation of first and second sentences. During pre-training it is used  for classifying whether these two sentences follow one another, making it a good candidate for our task.\n",
    "3. `[CLS]` embedding is passed through a linear layer **that does not change the size of the embedding** and is passed through a tanh nonlinearity.\n",
    "4. The output of tanh is passed through a dropout layer, where the dropout probability is the same as the dropout probability used for the `distilbert` model used as feature extractor.\n",
    "5. The output of the previous stage is fed into another linear layer that shrinks the size of the embedding dimension to a quarter of the original size, e.g., if the embedding size is 12, the new embedding dimension is 3.\n",
    "6. The output is followed by another dropout layer (you can use the one from stage 4).\n",
    "7. Finally, a binary classifier is applied to determine the probability of sentence 1 being followed by sentence 2.\n",
    "8. the cross-entropy loss is used to compute the loss.\n",
    "\n",
    "**Hint:** Keep in mind that for a 4 choice system, you classify each of the four solutions independently. However, the final output should group the four logits together. For example, if input ids have the shape `[2, 4, 35]` (batch size=2, num choices=4, seq len=35), then the logits have the `[2, 4]` and labels have the dimension `[2, 1]`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "KUZOp8o1Pb9X",
   "metadata": {
    "id": "KUZOp8o1Pb9X"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel,BertConfig,DistilBertConfig,PretrainedConfig,PreTrainedModel,DistilBertPreTrainedModel\n",
    "from torch import nn\n",
    "\n",
    "class CustomMultipleChoice(DistilBertPreTrainedModel):\n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__(config)\n",
    "        ###your code ###\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.dense = nn.Linear(config.dim, config.dim)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
    "        self.dense2 = nn.Linear(config.dim, config.dim//4)\n",
    "        self.classifier = nn.Linear(config.dim//4, 1)\n",
    "        ###your code ###\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        input_ids: input sentences converted to ids\n",
    "        attention_mask: the attention mask\n",
    "        labels:  Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors.\n",
    "        \"\"\"\n",
    "\n",
    "        num_choices = input_ids.shape[1]\n",
    "\n",
    "        ###your code ###\n",
    "        batch_size = input_ids.shape[0]\n",
    "        seq_len = input_ids.shape[2]\n",
    "        input_ids = input_ids.view(batch_size*num_choices, seq_len) if input_ids is not None else None\n",
    "        attention_mask = attention_mask.view(batch_size*num_choices, seq_len) if attention_mask is not None else None\n",
    "\n",
    "\n",
    "        distilbert_out = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = distilbert_out.last_hidden_state.view(batch_size, num_choices, seq_len, -1)[:, :, 0]\n",
    "\n",
    "        x = self.dense(cls_embedding)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.classifier(x).squeeze(-1)\n",
    "        reshaped_logits = logits.view(batch_size, num_choices)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(reshaped_logits, labels.view(-1))\n",
    "\n",
    "        ###your code ###\n",
    "        return {\"loss\":loss, \"logits\":reshaped_logits}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IXFcGJQPgjil",
   "metadata": {
    "id": "IXFcGJQPgjil"
   },
   "source": [
    "Initialize the feature extractor with `distilbert-base-uncased` and create your custome model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "GHCT0HdYKk5p",
   "metadata": {
    "id": "GHCT0HdYKk5p"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "###your code ###\n",
    "config = AutoConfig.from_pretrained(\"distilbert-base-uncased\")\n",
    "model_custom = CustomMultipleChoice(config)\n",
    "###your code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "_genDEZbgsJd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_genDEZbgsJd",
    "outputId": "f00505cc-5a63-4d78-b925-c30cf0b651f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense.weight torch.Size([768, 768])\n",
      "dense.bias torch.Size([768])\n",
      "dense2.weight torch.Size([192, 768])\n",
      "dense2.bias torch.Size([192])\n",
      "classifier.weight torch.Size([1, 192])\n",
      "classifier.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_custom.named_parameters():\n",
    "    if param.requires_grad and not name.startswith(\"distilbert.\"):\n",
    "      print(name, param.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yyD-ivMzh5P7",
   "metadata": {
    "id": "yyD-ivMzh5P7"
   },
   "source": [
    "We keep the same training arguments but change the directory in which we save the model logs, the directory in which we save the model output and the name of the run, to `custom_model`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aX5Jw-Eubo56",
   "metadata": {
    "id": "aX5Jw-Eubo56"
   },
   "outputs": [],
   "source": [
    "###your code ###\n",
    "training_args = TrainingArguments(\n",
    "    ### your code ###\n",
    "    output_dir=\"custom_model\",  # only kwarg changed compared to subtask 2\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=300,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=600,\n",
    "    save_total_limit=2,\n",
    "    seed=77,\n",
    "    # reducing batch size to half of the suggested value due to GPU memory pressure\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    "    max_steps=1800,\n",
    "    learning_rate=5e-5,  # also the default\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,  # undocumented, but used in tutorial\n",
    ")\n",
    "\n",
    "###your code ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0sCoDrINiO0j",
   "metadata": {
    "id": "0sCoDrINiO0j"
   },
   "source": [
    "Initialize the trainer for training the custom model.The training should take around 30 min on Google Colab T4 GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "s1_2hwKabo8p",
   "metadata": {
    "id": "s1_2hwKabo8p"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "###your code ###\n",
    "trainer = Trainer(\n",
    "    model=model_custom,  # only kwarg changed\n",
    "    args=training_args,\n",
    "    # data collator removes columns from the data not used by model\n",
    "    data_collator=MultipleChoiceDataCollator(tokenizer),\n",
    "    train_dataset=encoded_datasets[\"train\"],\n",
    "    eval_dataset=encoded_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "###your code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "btobBRSZbo_O",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "btobBRSZbo_O",
    "outputId": "6fd8fd6d-151e-4cac-8794-2a9deb7df9df"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1800' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1800/1800 17:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.392269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.391200</td>\n",
       "      <td>1.383739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.391200</td>\n",
       "      <td>1.386545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.381300</td>\n",
       "      <td>1.389654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.372600</td>\n",
       "      <td>1.389665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.372600</td>\n",
       "      <td>1.388726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1800, training_loss=1.379572516547309, metrics={'train_runtime': 1070.6342, 'train_samples_per_second': 40.35, 'train_steps_per_second': 1.681, 'total_flos': 2573635572211968.0, 'train_loss': 1.379572516547309, 'epoch': 0.59})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()# should take around 30 min on Colab T4 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B9Fi0ZZYjIdJ",
   "metadata": {
    "id": "B9Fi0ZZYjIdJ"
   },
   "source": [
    "Save the model in `custom_model/final_model`. Note that with the custom model, you need to save it without the help of the trainer. The trainer would save the configuration but since this model is not a registered Hugging Face model only the base model would be saved. Loading the model weights is also effected by this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "rvb3xo-ibpB9",
   "metadata": {
    "id": "rvb3xo-ibpB9"
   },
   "outputs": [],
   "source": [
    "###your code ###\n",
    "model_custom.save_pretrained(\"custom_model/final_model\")\n",
    "###your code ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UxU3fivlihOX",
   "metadata": {
    "id": "UxU3fivlihOX"
   },
   "source": [
    "#### ${\\color{red}{Comments\\ 2.3}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "cross-feedback comment section\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A_M4MSb0kcBP",
   "metadata": {
    "id": "A_M4MSb0kcBP"
   },
   "source": [
    "### Subtask 4: Evaluation and Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4oO0XKbbmzzW",
   "metadata": {
    "id": "4oO0XKbbmzzW"
   },
   "source": [
    "Many times you do not perform the final evaluation right after training, but load the checkpoints and evaluate them on the fly. To this end, load the two models from  disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ptE1gxKwa843",
   "metadata": {
    "id": "ptE1gxKwa843"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForMultipleChoice were not initialized from the model checkpoint at custom_model/final_model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForMultipleChoice were not initialized from the model checkpoint at custom_model/final_model and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1, 192]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMultipleChoice,AutoConfig\n",
    "### your code ###\n",
    "model_hf = AutoModelForMultipleChoice.from_pretrained(\"distilbert-base-uncased-swag/final_model\")\n",
    "model_custom = AutoModelForMultipleChoice.from_pretrained(\"custom_model/final_model\", ignore_mismatched_sizes=True)\n",
    "### your code ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Am9mUOXHbwC9",
   "metadata": {
    "id": "Am9mUOXHbwC9"
   },
   "source": [
    "To evaluate the data we load the validation split using a data loader and our previously defined data collator. Note that although we had a test split we cannot use it, since there are no labels available for this split (you can check the data to confirm this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "jhBq38xadGvA",
   "metadata": {
    "id": "jhBq38xadGvA"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "\n",
    "eval_dataloader = DataLoader(encoded_datasets[\"validation\"], batch_size=64, collate_fn=MultipleChoiceDataCollator(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ctcpVXXrl8Wd",
   "metadata": {
    "id": "ctcpVXXrl8Wd"
   },
   "source": [
    "To make things easier, let's use the `evaluate` library from Hugging Face to compute the accuracy metric. Here we load `accuracy` from the `evaluate` library two times, one for the custom model and one for the Hugging Face model. Further, we put the models on eval mode. Complete the code for evaluation using the capabilities of the `evaluate` library to simultaneously compute the metric for both models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1AZcv5RQbfwq",
   "metadata": {
    "id": "1AZcv5RQbfwq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/313 [00:25<1:07:05, 12.94s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "metric_dict={\"custom\":evaluate.load(\"accuracy\"),\"hf\":evaluate.load(\"accuracy\")} #use to compute accuracy\n",
    "models_dict= {\"custom\":model_custom,\"hf\":model_hf}# use to access models\n",
    "\n",
    "for name, model in models_dict.items():\n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "\n",
    "for i,batch in tqdm(enumerate(eval_dataloader), total=len(eval_dataloader)):\n",
    "  ### your code ###\n",
    "  #evaluate on both model on each batch\n",
    "  for name,model in models_dict.items():\n",
    "    with torch.no_grad():\n",
    "      outputs = model(**batch)\n",
    "      preds = torch.argmax(outputs.logits, axis=-1)\n",
    "      metric_dict[name].add_batch(predictions=preds, references=batch[\"labels\"])\n",
    "acc_hf= metric_dict[\"hf\"].compute()\n",
    "acc_custom= metric_dict[\"custom\"].compute()\n",
    "  ### your code ###\n",
    "print(\"Hugging Face Model :\",acc_hf)\n",
    "print(\"Custom Model :\",acc_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HQC6grKnuSgZ",
   "metadata": {
    "id": "HQC6grKnuSgZ"
   },
   "source": [
    "#### ${\\color{red}{Comments\\ 2.4}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "cross-feedback comment section\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_sO-H1PVMSsp",
   "metadata": {
    "id": "_sO-H1PVMSsp"
   },
   "source": [
    "## **Task 3: Encoder-Decoder Architecture** (5 + 2 + 2 + 5 = 14 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W42O89DrMZNj",
   "metadata": {
    "id": "W42O89DrMZNj"
   },
   "source": [
    "We explored an encoder-based model (BERT) in the previous exercise. In this task, we look at another family of transformer architectures, the encoder-decoder. We use the [T5](https://arxiv.org/pdf/1910.10683.pdf) model, presented by Raffel et al.  T5 is an encoder-decoder architecture pre-trained on a multi-task mixture of unsupervised and supervised tasks. In this task, we set up a fine-tuning example for question answering using the [SQUAD](https://huggingface.co/datasets/squad) dataset. Since the actual fine-tuning is time-consuming and computational intensive for inference, we use an already pre-trained model. The main goal is to introduce you to the structure of the fine-tuning and its simplicity with the Hugging Face framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-oSCvS6wwef",
   "metadata": {
    "id": "c-oSCvS6wwef"
   },
   "source": [
    "To fine-tune the BERT-based models, we usually add a task-specific head. On the other hand, T5 converts all NLP problems into a text-to-text format.  \n",
    "It is trained using teacher forcing, meaning that we require an input sequence and a corresponding target sequence.\n",
    "\n",
    "\n",
    "1.   The input sequence is fed to the model using `input_ids` from the tokenizer.\n",
    "2.   The target sequence is shifted to the right, i.e., prepended by a start-sequence token and fed to the decoder using the `decoder_input_ids` (input_ids of the encoded target sequence). The target sequence is appended by EOS (end of the sentence) to denote the end of a generation and corresponds to the `labels`.\n",
    "3. The task prefix defines what task is expected of T5. For example, we prepend the input sequence with `translate English to German: ` before encoding the input to tell the model to translate. T5 already has a set of pre-defined task prefixes, and it is best to stick to those since they were used during pre-training. With enough training data, you can also introduce your own custom task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sg6jO_SeznX3",
   "metadata": {
    "id": "sg6jO_SeznX3"
   },
   "source": [
    "In contrast to the encoder model, where only a single `max_length` is required, for encoder-decoder architectures, one typically defines a `max_source_length` and `max_target_length`, which determine the maximum length of the input and output sequences, respectively. We must also ensure that the padding ID of the `labels` is not taken into account by the loss function. This can be done by replacing them with `-100`, which is the `ignore_index` of the `CrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0zLQjAxlNzJ_",
   "metadata": {
    "id": "0zLQjAxlNzJ_"
   },
   "source": [
    "### Subtask 1: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7hx8LCNjBlvi",
   "metadata": {
    "id": "7hx8LCNjBlvi"
   },
   "source": [
    "We first start by loading the dataset from Hugging Face hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FjujP1xsuQGX",
   "metadata": {
    "id": "FjujP1xsuQGX",
    "outputId": "20723a74-1d9a-4527-a6e6-92fce1ebf139"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|██████████| 5.27k/5.27k [00:00<00:00, 12.1MB/s]\n",
      "Downloading metadata: 100%|██████████| 2.36k/2.36k [00:00<00:00, 16.4MB/s]\n",
      "Downloading readme: 100%|██████████| 7.67k/7.67k [00:00<00:00, 16.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset squad/plain_text to /Users/konradgoldenbaum/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 30.3MB [00:00, 35.8MB/s]/2 [00:00<?, ?it/s]\n",
      "Downloading data: 4.85MB [00:00, 54.3MB/s]                   .07s/it]\n",
      "Downloading data files: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 1456.36it/s]\n",
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset squad downloaded and prepared to /Users/konradgoldenbaum/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 302.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "datasets_squad = load_dataset(\"squad\")\n",
    "datasets_squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x7VEY66uEEeY",
   "metadata": {
    "id": "x7VEY66uEEeY",
    "outputId": "7471f76e-7ac2-44b3-cb46-151bc9917a35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context ----> Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "question ----> To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "answers ----> {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n"
     ]
    }
   ],
   "source": [
    "print(\"context ---->\" ,datasets_squad[\"train\"][0][\"context\"])\n",
    "print(\"question ---->\",datasets_squad[\"train\"][0][\"question\"])\n",
    "print(\"answers ---->\",datasets_squad[\"train\"][0][\"answers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5OrYB9xBByvj",
   "metadata": {
    "id": "5OrYB9xBByvj"
   },
   "source": [
    "Now let's load the needed pre-trained tokenizer for `t5-small`, which is the smallest T5 model. Set the maximum sequence length to `512`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WwYjYXASMg_z",
   "metadata": {
    "id": "WwYjYXASMg_z"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "### your code ###\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\", model_max_length=512)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "### your code ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FvN50codiJXR",
   "metadata": {
    "id": "FvN50codiJXR"
   },
   "source": [
    "The next step is to pre-process the dataset using the tokenizer to convert the sequences to IDs and add the special tokens.\n",
    "T5 is based on the SentencePiece tokenizer, and the end of sentence token is denoted by `</s>`.\n",
    "Complete the function `add_eos_to_examples` to format the input and target sequence. Your input as `input_text` should have the format `question:{question_text} context:{context_text} <EOS_Token>` and your target as `target_text` should have the format `{answer_text} <EOS_Token>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xbisNi2-MYC2",
   "metadata": {
    "id": "xbisNi2-MYC2"
   },
   "outputs": [],
   "source": [
    "def add_eos_to_examples(example):\n",
    "    #print(example)\n",
    "    ### your code ###\n",
    "    example['input_text'] = \"question:\" + example[\"question\"] + \" context:\" + example[\"context\"] + \"</s>\"\n",
    "    example['target_text'] = example[\"answers\"][\"text\"][0] + \"</s>\"\n",
    "    ### your code ###\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l1cI8qcCIcXM",
   "metadata": {
    "id": "l1cI8qcCIcXM"
   },
   "source": [
    "Use the `map` function to process the data, and do not set the `batched` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z7Q2TH_AIJDp",
   "metadata": {
    "id": "Z7Q2TH_AIJDp",
    "outputId": "a7e32765-4f9a-4287-da1d-e0e904a2b0ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/konradgoldenbaum/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-03f42b974da75c48.arrow\n",
      "Loading cached processed dataset at /Users/konradgoldenbaum/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-4644bfce6c5753de.arrow\n"
     ]
    }
   ],
   "source": [
    "### your code ###\n",
    "encoded_squad = datasets_squad.map(add_eos_to_examples)\n",
    "### your code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FgqckdGNDWNp",
   "metadata": {
    "id": "FgqckdGNDWNp",
    "outputId": "88c44b24-d85d-4170-8e31-5915ee98c32b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? context:Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.</s>\n",
      "Saint Bernadette Soubirous</s>\n"
     ]
    }
   ],
   "source": [
    "print(encoded_squad[\"train\"][0][\"input_text\"])\n",
    "print(encoded_squad[\"train\"][0][\"target_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UqK9_TtLFZrU",
   "metadata": {
    "id": "UqK9_TtLFZrU"
   },
   "source": [
    "Complete the function `convert_to_features` that takes in the examples from the dataset and tokenizes them using the T5 tokenizer. However, our answers in this dataset are relatively short and do not require `512` tokens, in contrast to the input sequence which is a combination of question and context paragraphs and is usually long. To this end, we want to truncate the input sequence at `512` and the target sequence at `16`. If any input or target is smaller than the specified length, make sure you pad them. Finally, convert everything to PyTorch tensors to be easily used by the data collator and place them in the dictionary `encodings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4yPglRvTEg_p",
   "metadata": {
    "id": "4yPglRvTEg_p"
   },
   "outputs": [],
   "source": [
    "def convert_to_features(examples):\n",
    "    ### your code ###\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-small\", model_max_length=512, use_fast=True)\n",
    "\n",
    "    encodings = {\n",
    "        'input_ids': tokenizer(examples['input_text'], truncation=True, padding='max_length', max_length=512, return_tensors=\"pt\").input_ids,\n",
    "        'target_ids': tokenizer(examples['target_text'], truncation=True, padding='max_length', max_length=16, return_tensors=\"pt\").input_ids\n",
    "    }\n",
    "    ### your code ###\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UI_tQfeNIgi7",
   "metadata": {
    "id": "UI_tQfeNIgi7"
   },
   "source": [
    "Use the `map` function to process the data.\n",
    "\n",
    "**Takes long (>10min)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gYm6RZGF19xY",
   "metadata": {
    "id": "gYm6RZGF19xY",
    "outputId": "549445b9-7b3e-47f3-e9b5-1d06066a2397"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 1/87599 [00:00<4:46:59,  5.09 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '5733be284776f41900661182', 'title': 'University_of_Notre_Dame', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}, 'input_text': 'question:To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? context:Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.</s>', 'target_text': 'Saint Bernadette Soubirous</s>'}\n",
      "{'id': '5733be284776f4190066117f', 'title': 'University_of_Notre_Dame', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'question': 'What is in front of the Notre Dame Main Building?', 'answers': {'text': ['a copper statue of Christ'], 'answer_start': [188]}, 'input_text': 'question:What is in front of the Notre Dame Main Building? context:Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.</s>', 'target_text': 'a copper statue of Christ</s>'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 3/87599 [00:00<4:03:57,  5.98 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '5733be284776f41900661180', 'title': 'University_of_Notre_Dame', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'question': 'The Basilica of the Sacred heart at Notre Dame is beside to which structure?', 'answers': {'text': ['the Main Building'], 'answer_start': [279]}, 'input_text': 'question:The Basilica of the Sacred heart at Notre Dame is beside to which structure? context:Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.</s>', 'target_text': 'the Main Building</s>'}\n",
      "{'id': '5733be284776f41900661181', 'title': 'University_of_Notre_Dame', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'question': 'What is the Grotto at Notre Dame?', 'answers': {'text': ['a Marian place of prayer and reflection'], 'answer_start': [381]}, 'input_text': 'question:What is the Grotto at Notre Dame? context:Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.</s>', 'target_text': 'a Marian place of prayer and reflection</s>'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 5/87599 [00:00<3:57:50,  6.14 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '5733be284776f4190066117e', 'title': 'University_of_Notre_Dame', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'question': 'What sits on top of the Main Building at Notre Dame?', 'answers': {'text': ['a golden statue of the Virgin Mary'], 'answer_start': [92]}, 'input_text': 'question:What sits on top of the Main Building at Notre Dame? context:Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.</s>', 'target_text': 'a golden statue of the Virgin Mary</s>'}\n",
      "{'id': '5733bf84d058e614000b61be', 'title': 'University_of_Notre_Dame', 'context': \"As at most other universities, Notre Dame's students run a number of news media outlets. The nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals. Begun as a one-page journal in September 1876, the Scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the United States. The other magazine, The Juggler, is released twice a year and focuses on student literature and artwork. The Dome yearbook is published annually. The newspapers have varying publication interests, with The Observer published daily and mainly reporting university and other news, and staffed by students from both Notre Dame and Saint Mary's College. Unlike Scholastic and The Dome, The Observer is an independent publication and does not have a faculty advisor or any editorial oversight from the University. In 1987, when some students believed that The Observer began to show a conservative bias, a liberal newspaper, Common Sense was published. Likewise, in 2003, when other students believed that the paper showed a liberal bias, the conservative paper Irish Rover went into production. Neither paper is published as often as The Observer; however, all three are distributed to all students. Finally, in Spring 2008 an undergraduate journal for political science research, Beyond Politics, made its debut.\", 'question': 'When did the Scholastic Magazine of Notre dame begin publishing?', 'answers': {'text': ['September 1876'], 'answer_start': [248]}, 'input_text': \"question:When did the Scholastic Magazine of Notre dame begin publishing? context:As at most other universities, Notre Dame's students run a number of news media outlets. The nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals. Begun as a one-page journal in September 1876, the Scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the United States. The other magazine, The Juggler, is released twice a year and focuses on student literature and artwork. The Dome yearbook is published annually. The newspapers have varying publication interests, with The Observer published daily and mainly reporting university and other news, and staffed by students from both Notre Dame and Saint Mary's College. Unlike Scholastic and The Dome, The Observer is an independent publication and does not have a faculty advisor or any editorial oversight from the University. In 1987, when some students believed that The Observer began to show a conservative bias, a liberal newspaper, Common Sense was published. Likewise, in 2003, when other students believed that the paper showed a liberal bias, the conservative paper Irish Rover went into production. Neither paper is published as often as The Observer; however, all three are distributed to all students. Finally, in Spring 2008 an undergraduate journal for political science research, Beyond Politics, made its debut.</s>\", 'target_text': 'September 1876</s>'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 7/87599 [00:01<4:01:10,  6.05 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '5733bf84d058e614000b61bf', 'title': 'University_of_Notre_Dame', 'context': \"As at most other universities, Notre Dame's students run a number of news media outlets. The nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals. Begun as a one-page journal in September 1876, the Scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the United States. The other magazine, The Juggler, is released twice a year and focuses on student literature and artwork. The Dome yearbook is published annually. The newspapers have varying publication interests, with The Observer published daily and mainly reporting university and other news, and staffed by students from both Notre Dame and Saint Mary's College. Unlike Scholastic and The Dome, The Observer is an independent publication and does not have a faculty advisor or any editorial oversight from the University. In 1987, when some students believed that The Observer began to show a conservative bias, a liberal newspaper, Common Sense was published. Likewise, in 2003, when other students believed that the paper showed a liberal bias, the conservative paper Irish Rover went into production. Neither paper is published as often as The Observer; however, all three are distributed to all students. Finally, in Spring 2008 an undergraduate journal for political science research, Beyond Politics, made its debut.\", 'question': \"How often is Notre Dame's the Juggler published?\", 'answers': {'text': ['twice'], 'answer_start': [441]}, 'input_text': \"question:How often is Notre Dame's the Juggler published? context:As at most other universities, Notre Dame's students run a number of news media outlets. The nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals. Begun as a one-page journal in September 1876, the Scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the United States. The other magazine, The Juggler, is released twice a year and focuses on student literature and artwork. The Dome yearbook is published annually. The newspapers have varying publication interests, with The Observer published daily and mainly reporting university and other news, and staffed by students from both Notre Dame and Saint Mary's College. Unlike Scholastic and The Dome, The Observer is an independent publication and does not have a faculty advisor or any editorial oversight from the University. In 1987, when some students believed that The Observer began to show a conservative bias, a liberal newspaper, Common Sense was published. Likewise, in 2003, when other students believed that the paper showed a liberal bias, the conservative paper Irish Rover went into production. Neither paper is published as often as The Observer; however, all three are distributed to all students. Finally, in Spring 2008 an undergraduate journal for political science research, Beyond Politics, made its debut.</s>\", 'target_text': 'twice</s>'}\n",
      "{'id': '5733bf84d058e614000b61c0', 'title': 'University_of_Notre_Dame', 'context': \"As at most other universities, Notre Dame's students run a number of news media outlets. The nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals. Begun as a one-page journal in September 1876, the Scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the United States. The other magazine, The Juggler, is released twice a year and focuses on student literature and artwork. The Dome yearbook is published annually. The newspapers have varying publication interests, with The Observer published daily and mainly reporting university and other news, and staffed by students from both Notre Dame and Saint Mary's College. Unlike Scholastic and The Dome, The Observer is an independent publication and does not have a faculty advisor or any editorial oversight from the University. In 1987, when some students believed that The Observer began to show a conservative bias, a liberal newspaper, Common Sense was published. Likewise, in 2003, when other students believed that the paper showed a liberal bias, the conservative paper Irish Rover went into production. Neither paper is published as often as The Observer; however, all three are distributed to all students. Finally, in Spring 2008 an undergraduate journal for political science research, Beyond Politics, made its debut.\", 'question': 'What is the daily student paper at Notre Dame called?', 'answers': {'text': ['The Observer'], 'answer_start': [598]}, 'input_text': \"question:What is the daily student paper at Notre Dame called? context:As at most other universities, Notre Dame's students run a number of news media outlets. The nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals. Begun as a one-page journal in September 1876, the Scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the United States. The other magazine, The Juggler, is released twice a year and focuses on student literature and artwork. The Dome yearbook is published annually. The newspapers have varying publication interests, with The Observer published daily and mainly reporting university and other news, and staffed by students from both Notre Dame and Saint Mary's College. Unlike Scholastic and The Dome, The Observer is an independent publication and does not have a faculty advisor or any editorial oversight from the University. In 1987, when some students believed that The Observer began to show a conservative bias, a liberal newspaper, Common Sense was published. Likewise, in 2003, when other students believed that the paper showed a liberal bias, the conservative paper Irish Rover went into production. Neither paper is published as often as The Observer; however, all three are distributed to all students. Finally, in Spring 2008 an undergraduate journal for political science research, Beyond Politics, made its debut.</s>\", 'target_text': 'The Observer</s>'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/konradgoldenbaum/Documents/WiSe2324/assignments/Assignment_3/Assignment_3.ipynb Cell 109\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/konradgoldenbaum/Documents/WiSe2324/assignments/Assignment_3/Assignment_3.ipynb#Y213sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m### your code ###\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/konradgoldenbaum/Documents/WiSe2324/assignments/Assignment_3/Assignment_3.ipynb#Y213sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m encoded_squad \u001b[39m=\u001b[39m encoded_squad\u001b[39m.\u001b[39;49mmap(convert_to_features)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/konradgoldenbaum/Documents/WiSe2324/assignments/Assignment_3/Assignment_3.ipynb#Y213sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m### your code ###\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/datasets/dataset_dict.py:851\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    849\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    850\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 851\u001b[0m     {\n\u001b[1;32m    852\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m    853\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m    854\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m    855\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m    856\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m    857\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m    858\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    859\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m    860\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m    861\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m    862\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m    863\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[1;32m    864\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m    865\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m    866\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m    867\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m    868\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m    869\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m    870\u001b[0m         )\n\u001b[1;32m    871\u001b[0m         \u001b[39mfor\u001b[39;49;00m k, dataset \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitems()\n\u001b[1;32m    872\u001b[0m     }\n\u001b[1;32m    873\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/datasets/dataset_dict.py:852\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    849\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    850\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    851\u001b[0m     {\n\u001b[0;32m--> 852\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m    853\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m    854\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m    855\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m    856\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m    857\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m    858\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    859\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m    860\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m    861\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m    862\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m    863\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[1;32m    864\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m    865\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m    866\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m    867\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m    868\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m    869\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m    870\u001b[0m         )\n\u001b[1;32m    871\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    872\u001b[0m     }\n\u001b[1;32m    873\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/datasets/arrow_dataset.py:580\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    579\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    581\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    582\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    583\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/datasets/arrow_dataset.py:545\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    539\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    540\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    541\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    542\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    543\u001b[0m }\n\u001b[1;32m    544\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    546\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    547\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/datasets/arrow_dataset.py:3087\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3079\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3080\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   3081\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3082\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3085\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3086\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3087\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3088\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3089\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/datasets/arrow_dataset.py:3441\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3439\u001b[0m _time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   3440\u001b[0m \u001b[39mfor\u001b[39;00m i, example \u001b[39min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3441\u001b[0m     example \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[39m=\u001b[39;49moffset)\n\u001b[1;32m   3442\u001b[0m     \u001b[39mif\u001b[39;00m update_data:\n\u001b[1;32m   3443\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/datasets/arrow_dataset.py:3344\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3342\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   3343\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 3344\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   3345\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3346\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m   3347\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[1;32m   3348\u001b[0m     }\n",
      "\u001b[1;32m/Users/konradgoldenbaum/Documents/WiSe2324/assignments/Assignment_3/Assignment_3.ipynb Cell 109\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/konradgoldenbaum/Documents/WiSe2324/assignments/Assignment_3/Assignment_3.ipynb#Y213sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_to_features\u001b[39m(examples):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/konradgoldenbaum/Documents/WiSe2324/assignments/Assignment_3/Assignment_3.ipynb#Y213sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m### your code ###\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/konradgoldenbaum/Documents/WiSe2324/assignments/Assignment_3/Assignment_3.ipynb#Y213sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(examples)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/konradgoldenbaum/Documents/WiSe2324/assignments/Assignment_3/Assignment_3.ipynb#Y213sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mt5-small\u001b[39;49m\u001b[39m\"\u001b[39;49m, model_max_length\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m, use_fast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/konradgoldenbaum/Documents/WiSe2324/assignments/Assignment_3/Assignment_3.ipynb#Y213sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     encodings \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/konradgoldenbaum/Documents/WiSe2324/assignments/Assignment_3/Assignment_3.ipynb#Y213sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m: tokenizer(examples[\u001b[39m'\u001b[39m\u001b[39minput_text\u001b[39m\u001b[39m'\u001b[39m], truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m, max_length\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minput_ids,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/konradgoldenbaum/Documents/WiSe2324/assignments/Assignment_3/Assignment_3.ipynb#Y213sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mtarget_ids\u001b[39m\u001b[39m'\u001b[39m: tokenizer(examples[\u001b[39m'\u001b[39m\u001b[39mtarget_text\u001b[39m\u001b[39m'\u001b[39m], truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m, max_length\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minput_ids\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/konradgoldenbaum/Documents/WiSe2324/assignments/Assignment_3/Assignment_3.ipynb#Y213sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/konradgoldenbaum/Documents/WiSe2324/assignments/Assignment_3/Assignment_3.ipynb#Y213sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m### your code ###\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:643\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    642\u001b[0m \u001b[39m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 643\u001b[0m tokenizer_config \u001b[39m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    644\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    645\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m tokenizer_config[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:487\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[39mLoads the tokenizer configuration from a pretrained model tokenizer configuration.\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39mtokenizer_config = get_tokenizer_config(\"tokenizer-test\")\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[1;32m    486\u001b[0m commit_hash \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 487\u001b[0m resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    488\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    489\u001b[0m     TOKENIZER_CONFIG_FILE,\n\u001b[1;32m    490\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    491\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    492\u001b[0m     resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    493\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    494\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    495\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    496\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    497\u001b[0m     subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    498\u001b[0m     _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    499\u001b[0m     _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    500\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    501\u001b[0m )\n\u001b[1;32m    502\u001b[0m \u001b[39mif\u001b[39;00m resolved_config_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    503\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/transformers/utils/hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    414\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    418\u001b[0m         path_or_repo_id,\n\u001b[1;32m    419\u001b[0m         filename,\n\u001b[1;32m    420\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    421\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    422\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    423\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    424\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    425\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    426\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    427\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    428\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    429\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    430\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    433\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/huggingface_hub/file_download.py:1195\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1194\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m         metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1196\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1197\u001b[0m             token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1198\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1199\u001b[0m             timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1200\u001b[0m         )\n\u001b[1;32m   1201\u001b[0m     \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1202\u001b[0m         \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m         commit_hash \u001b[39m=\u001b[39m http_error\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(HUGGINGFACE_HEADER_X_REPO_COMMIT)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/huggingface_hub/file_download.py:1532\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1529\u001b[0m headers[\u001b[39m\"\u001b[39m\u001b[39mAccept-Encoding\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39midentity\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1532\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1533\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1534\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1535\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1536\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1537\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1538\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1539\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1540\u001b[0m )\n\u001b[1;32m   1541\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1543\u001b[0m \u001b[39m# Return\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/huggingface_hub/file_download.py:407\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39m# 2. Force relative redirection\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 407\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m    408\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    409\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    410\u001b[0m         max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    411\u001b[0m         base_wait_time\u001b[39m=\u001b[39;49mbase_wait_time,\n\u001b[1;32m    412\u001b[0m         max_wait_time\u001b[39m=\u001b[39;49mmax_wait_time,\n\u001b[1;32m    413\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    414\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    415\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    416\u001b[0m     )\n\u001b[1;32m    418\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m300\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus_code \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m399\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/huggingface_hub/file_download.py:442\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n\u001b[1;32m    441\u001b[0m \u001b[39m# 3. Exponential backoff\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m \u001b[39mreturn\u001b[39;00m http_backoff(\n\u001b[1;32m    443\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    444\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    445\u001b[0m     max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    446\u001b[0m     base_wait_time\u001b[39m=\u001b[39;49mbase_wait_time,\n\u001b[1;32m    447\u001b[0m     max_wait_time\u001b[39m=\u001b[39;49mmax_wait_time,\n\u001b[1;32m    448\u001b[0m     retry_on_exceptions\u001b[39m=\u001b[39;49m(ConnectTimeout, ProxyError),\n\u001b[1;32m    449\u001b[0m     retry_on_status_codes\u001b[39m=\u001b[39;49m(),\n\u001b[1;32m    450\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    451\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    452\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:212\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[1;32m    211\u001b[0m \u001b[39m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m response \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    213\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m retry_on_status_codes:\n\u001b[1;32m    214\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    460\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    463\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1376\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/Content_Crawler/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### your code ###\n",
    "encoded_squad = encoded_squad.map(convert_to_features)\n",
    "### your code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gaIppB0A2sxX",
   "metadata": {
    "id": "gaIppB0A2sxX",
    "outputId": "dbcba616-295e-4f80-8688-65491ba0292c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x28c32eef0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_squad #new columns are added"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iMhQ6_vpIlUC",
   "metadata": {
    "id": "iMhQ6_vpIlUC"
   },
   "source": [
    "Interestingly, although we specified PyTorch tensors as output, the type of the `input_ids` is still a list. To remedy this problem, you need to explicitly set the type of the column that contains PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gaU31GYeIlsH",
   "metadata": {
    "id": "gaU31GYeIlsH",
    "outputId": "ebcc4392-fe10-43bd-b687-215c7e6f3e2d"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'map' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/konradgoldenbaum/Documents/WiSe2324/assignments/Assignment_3/Assignment_3.ipynb Cell 112\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/konradgoldenbaum/Documents/WiSe2324/assignments/Assignment_3/Assignment_3.ipynb#Y216sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mtype\u001b[39m(encoded_squad[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'map' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "type(encoded_squad[\"train\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "depIPwgSIntY",
   "metadata": {
    "id": "depIPwgSIntY"
   },
   "outputs": [],
   "source": [
    "### your code ###\n",
    "encoded_squad.set_format(type='torch', columns=['input_ids', 'target_ids'])\n",
    "### your code ###\n",
    "type(encoded_squad[\"train\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kcZ1Jg8kI9UN",
   "metadata": {
    "id": "kcZ1Jg8kI9UN"
   },
   "outputs": [],
   "source": [
    "print(\"Shape of the input_ids:\",encoded_squad[\"train\"][0][\"input_ids\"].shape)\n",
    "print(\"Shape of the target_ids:\",encoded_squad[\"train\"][0][\"target_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XpYHaNlpOCMd",
   "metadata": {
    "id": "XpYHaNlpOCMd"
   },
   "source": [
    "The final step in the data processing is the creation of the data collator to\n",
    "prepare `labels` from `target_ids` and return examples with keys as expected by the forward method of T5.\n",
    "This is necessary because the trainer directly passes this dict as argument to the model so you need to check the input of T5 and rename the column based on that.\n",
    "`input_ids`, `target_ids`, `attention_mask`, and `target_attention_mask` need to be stacked in a batch and the pad tokens in the target need to be set to `-100` to avoid loss computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RQzQ50R326cL",
   "metadata": {
    "id": "RQzQ50R326cL"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers import DataCollator\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "@dataclass\n",
    "class T2TDataCollator:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = -100\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    def __call__(self, batch):\n",
    "\n",
    "      ### your code ###\n",
    "\n",
    "\n",
    "        feature_dict = DataCollator(tokenizer=self.tokenizer, padding=self.padding, max_length=self.max_length, pad_to_multiple_of=self.pad_to_multiple_of)(batch)\n",
    "        return feature_dict\n",
    "      ### your code ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zXOwBe1bPCRJ",
   "metadata": {
    "id": "zXOwBe1bPCRJ",
    "outputId": "efd65332-839e-4923-9a4d-95c7609074d3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't5_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/konradgoldenbaum/Documents/WiSe2324/assignments/Assignment_3/Assignment_3.ipynb Cell 117\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/konradgoldenbaum/Documents/WiSe2324/assignments/Assignment_3/Assignment_3.ipynb#Y224sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m accepted_keys \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39minput_text\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtarget_text\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtarget_ids\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtarget_attention_mask\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/konradgoldenbaum/Documents/WiSe2324/assignments/Assignment_3/Assignment_3.ipynb#Y224sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m features \u001b[39m=\u001b[39m [{k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m encoded_squad[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][i]\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m accepted_keys} \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m)]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/konradgoldenbaum/Documents/WiSe2324/assignments/Assignment_3/Assignment_3.ipynb#Y224sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m batch\u001b[39m=\u001b[39mT2TDataCollator(t5_tokenizer)(features)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/konradgoldenbaum/Documents/WiSe2324/assignments/Assignment_3/Assignment_3.ipynb#Y224sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(batch[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/konradgoldenbaum/Documents/WiSe2324/assignments/Assignment_3/Assignment_3.ipynb#Y224sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(batch[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 't5_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "accepted_keys = ['input_text', 'target_text', 'input_ids', 'attention_mask', 'target_ids', 'target_attention_mask']\n",
    "features = [{k: v for k, v in encoded_squad[\"train\"][i].items() if k in accepted_keys} for i in range(2)]\n",
    "batch=T2TDataCollator(t5_tokenizer)(features)\n",
    "print(batch[\"input_ids\"].shape)\n",
    "print(batch[\"attention_mask\"].shape)\n",
    "print(batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gb9KZjhsu_5N",
   "metadata": {
    "id": "gb9KZjhsu_5N"
   },
   "source": [
    "#### ${\\color{red}{Comments\\ 3.1}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "Point distribution \\\n",
    "✅ 0.5 point if the tokenizer is loaded correctly. \\\n",
    "Note: Slightly different sintax (with Autotokenizer) but virtually the same. \\\n",
    "✅ 0.5 if the EOS sentence and prefixes are added correctly in `add_eos_to_examples`. \\\n",
    "✅ 0.5 if the map function is used correctly for `add_eos_to_examples`. \\\n",
    "✅ 1 point if the input and target are tokenized and padded correctly in `convert_to_features`. \\\n",
    "Note: Slightly different sintax but virtually the same. \\\n",
    "❌ 0.5 point for use of the map function for `convert_to_features`. \\\n",
    "Note: They didn't set batched=True, which by default is False \\\n",
    "❌✅ (0.25) 0.5 point for conversion of the columns to tensors. \\\n",
    "Note: They called the right method \"set_format\" but they didn't set up the correct columns.\\ \n",
    "\n",
    "\n",
    "✅ 0.5 point if the tensors are stacked properly, look at the output shapes for hints. \\\n",
    "Note: Their code is not running for previous errors in cascade, so we cannot check the shapes.\\ \n",
    "      Moreover, their implementation in substatially different from ours and the one present in the solutions. \\\n",
    "      Anyway, I checked the data_collator.py script, and read through the code in which the DataCollator class is defined and it seems to me that the stacking is done properly by the call of that function! \\\n",
    "      Their implementation is less transparent than the one presented in the solutions, but it's also much more compact. \\\n",
    "      I suggest to read the definition of the method \"torch_default_data_collator(...)\", which is invoked since they did not pass any \"return_tensor\" parameter and that's defaulted to call the quoted method. \\\n",
    "❌ 0.5 points if the names are correctly defined. \\\n",
    "Note: Using the DataCollator, they had no control over this. \\\n",
    "✅ 0.5 point if the label pads are set to `-100`. \\\n",
    "Note: They did it straight away in the beginning.\n",
    "\n",
    "Points: \\\n",
    "3.75/5.0\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ktvtcVh1N6l0",
   "metadata": {
    "id": "ktvtcVh1N6l0"
   },
   "source": [
    "### Subtask 2: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mnMvSwtbV2GC",
   "metadata": {
    "id": "mnMvSwtbV2GC"
   },
   "source": [
    "For training and inference, we can use `T5ForConditionalGeneration`, which includes the language modeling head on top of the decoder. Load the `t5-small` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aO4NNyEFKXap",
   "metadata": {
    "id": "aO4NNyEFKXap"
   },
   "outputs": [],
   "source": [
    "### your code ###\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "t5 = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "### your code ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rE9j5o_iBLOL",
   "metadata": {
    "id": "rE9j5o_iBLOL"
   },
   "source": [
    "Next, similar to the previous task we initiate training arguments. Note that this time we are using a `Seq2SeqTrainingArguments` for a `Seq2SeqTrainer`. Set the parameters for training as follows:\n",
    "\n",
    "\n",
    "*   T5 doesn't support GPU and TPU evaluation for now, so we only focus on training. You do not need to pass any parameters for evaluation setup.\n",
    "*   The output directory should be named `t5-squad`.\n",
    "* The T5 models need a slightly higher learning rate than the default one set in the `Trainer` when using the `AdamW` optimizer. Set the learning rate to `1e-4` and the regularization parameter to `0.01`.\n",
    "* Random seed should be `77`, and we train for a maximum of `200` steps and save a checkpoint every `100` steps. A complete training of the T5 model requires far more than `200` steps, however, that is beyond the scope of this assignment.\n",
    "* T5 models require a large batch size. The default model was trained with a batch size of `128`. However, we cannot fit that into a single GPU, therefore we use gradient accumulation. Set the batch size to `32` and choose the gradient accumulation step to reach the effective batch size of `128`.\n",
    "* Make sure that your trainer does not remove unused columns during training, as this will cause a runtime error later on.\n",
    "\n",
    "\n",
    "**Gradient accumulation:** is a technique that simulates a larger batch size by accumulating gradients from multiple small batches before performing a weight update.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UiWbhXjk5Mr0",
   "metadata": {
    "id": "UiWbhXjk5Mr0"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "### your code ###\n",
    "training_args = TrainingArguments(output_dir=\"./t5_squad\",\n",
    "                                  learning_rate=1e-4,\n",
    "                                  weight_decay=0.01,\n",
    "                                  per_device_train_batch_size=32,\n",
    "                                  gradient_accumulation_steps=4,\n",
    "                                  num_train_epochs=1,\n",
    "                                  max_steps=200,\n",
    "                                  save_steps=100,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  metric_for_best_model=\"eval_loss\",\n",
    "                                  remove_unused_columns=False,\n",
    "                                  greater_is_better=False,\n",
    "                                  random_seed=77)\n",
    "\n",
    "\n",
    "    ### your code ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AEOvujcduYjG",
   "metadata": {
    "id": "AEOvujcduYjG"
   },
   "source": [
    "Once again make sure that you are using GPU before running the cell below.\n",
    "Initilize your `Seq2SeqTrainer` with inputs necessary for training. The training should take around 15 min on Google Colab T4 GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J_Vb3za_4kBo",
   "metadata": {
    "id": "J_Vb3za_4kBo"
   },
   "outputs": [],
   "source": [
    "# Initialize our Trainer\n",
    "from transformers import Trainer\n",
    "### your code ###\n",
    "trainer = Trainer(TrainingArguments=training_args,\n",
    "                  model=t5,\n",
    "                  data_collator=T2TDataCollator(t5_tokenizer),\n",
    "                  train_dataset=encoded_squad[\"train\"],\n",
    "                  eval_dataset=encoded_squad[\"validation\"],\n",
    "                  device=torch.device(\"mps\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "\n",
    "\n",
    "    ### your code ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8GDPeyPiBIEX",
   "metadata": {
    "id": "8GDPeyPiBIEX"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1-gKhqSpu7Uz",
   "metadata": {
    "id": "1-gKhqSpu7Uz"
   },
   "source": [
    "#### ${\\color{red}{Comments\\ 3.2}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "Point distribution \\\n",
    "✅ 0.5 point for initializing the correct model \\\n",
    "Note: Different call, but AutoModelForSeq2SeqLM loads the same underlying model class as T5ForConditionalGeneration. \\\n",
    "❌✅ (-0.25 points) 1 points if all the training parameters are set correctly. \\\n",
    "Note: Using TrainingArguments instead of Seq2SeqTrainingArguments isn't entirely wrong, but it lacks all the additional functionalities that Seq2SeqTrainingArguments specifically has to tackle seq2seq tasks. \\\n",
    "❌✅ (-0.25 points) 0.5 point if the trainer is correctly initiated. \\\n",
    "Note: Same as for previous point. They used Trainer instead of Seq2SeqTrainer. \\\n",
    "\n",
    "1.50/2.00 points\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AQ9jj65UvER1",
   "metadata": {
    "id": "AQ9jj65UvER1"
   },
   "source": [
    "### Subtask 3: Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bkDgQponWOs4",
   "metadata": {
    "id": "bkDgQponWOs4"
   },
   "source": [
    "Our trained model has seen far too few instances to make a coherent prediction. To this end, we load an already trained checkpoint from Hugging Face and perform inference. Load this [model](https://huggingface.co/mrm8488/t5-base-finetuned-squadv2) and the respective tokenizer. Note that we are loading a `base` model that is slightly larger than `t5-small`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lW8Wss36vV20",
   "metadata": {
    "id": "lW8Wss36vV20"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "### your code ###\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\", model_max_length=512)\n",
    "t5_model = AutoModelForSeq2SeqLM.from_pretrained(\"./t5_squad/t5-small\")\n",
    "\n",
    "### your code ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "azC95xxzxj84",
   "metadata": {
    "id": "azC95xxzxj84"
   },
   "source": [
    "At inference time for T5, it is recommended to use the `generate()` function. This auto-regressively generates the decoder output. Complete the code for the `get_answer` function, which gives a model, a tokenizer, and a question and context pair, and generates the answer from the context given. The output should be the answer to the given question in natural text (without the special tokens).\n",
    "\n",
    "**Hint:** Many of the steps are similar to how you prepared your input data for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_Run50fvWPcv",
   "metadata": {
    "id": "_Run50fvWPcv"
   },
   "outputs": [],
   "source": [
    "def get_answer(tokenizer,model, question, context):\n",
    "  ### your code ###\n",
    "  t5_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "  answer = t5_pipeline(question, context)[0]['generated_text']\n",
    "  ### your code ###\n",
    "  return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F6Fq37I3yMeQ",
   "metadata": {
    "id": "F6Fq37I3yMeQ"
   },
   "source": [
    "Let's try it with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09znSykkdkk0",
   "metadata": {
    "id": "09znSykkdkk0"
   },
   "outputs": [],
   "source": [
    "context = \"Sarah has joined NLP for transformers class and is working on her research project with the support of Harry.\"\n",
    "question = \"Who is supporting Sarah?\"\n",
    "\n",
    "get_answer(t5_tokenizer,t5_model,question, context) ###your answer should be \"Harry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LQNpgTyazjge",
   "metadata": {
    "id": "LQNpgTyazjge"
   },
   "outputs": [],
   "source": [
    "context = \"TPUs are more power efficient in comparison to GPUs making them a better choice for machine learning projects.\"\n",
    "question = \"What is better for machine learning projects?\"\n",
    "\n",
    "get_answer(t5_tokenizer,t5_model,question, context)###your answer should be \"TPUs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sIz0JuwzGEFN",
   "metadata": {
    "id": "sIz0JuwzGEFN"
   },
   "source": [
    "#### ${\\color{red}{Comments\\ 3.3}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "Point distribution \\\n",
    "❌ 0.5 points for initializing the correct model and tokenizer.\n",
    "Note: The loaded tokenizer is wrong. They might have downloaded on local and loaded the right model, but we don't have access to that. Indeed they do: \\\n",
    "\\>\\>\\> t5_model = AutoModelForSeq2SeqLM.from_pretrained(\"./t5_squad/t5-small\")\n",
    "\n",
    "✅ 0.5 points for preparing the input correctly.\n",
    "Note: Nice approach with the pipeline, very elegant. We checked and it should accomplish the exact same results as the provided solution.  \\\n",
    "❌ 1 point generation and decoding using the tokenizer.    \\\n",
    "Note: The notebook we received contains no output on the corresponding cells. Since they didn't even provide a working initialization of the model we will unfiortunately not run the code for them.\n",
    "\n",
    "0.5/2.0 Points\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9LfBp2eKqYc",
   "metadata": {
    "id": "i9LfBp2eKqYc"
   },
   "source": [
    "### Subtask 4: T5 Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NRVp_0KaKvN4",
   "metadata": {
    "id": "NRVp_0KaKvN4"
   },
   "source": [
    "To answer questions of the final subtask you need to have a general overview of the [T5 paper](https://arxiv.org/pdf/1910.10683.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EwoEHJ1eMSVr",
   "metadata": {
    "id": "EwoEHJ1eMSVr"
   },
   "source": [
    "\n",
    "\n",
    "1.   Describe what a “text-to-text format\" is and how T5 processes input and output for text classification tasks? What are the possible complications with a predefined set of classes?\n",
    "2.   Describe the \"masked language modeling\" and \"word dropout\" unsupervised objective with sentinel tokens. Give an example of how this would look in a single sentence.\n",
    "3. Explain \"fully-visible\", \"causal\" and \"causal masking with prefix\" masking.\n",
    "4. Briefly describe \"adapter layers\" and \"gradual unfreezing\" as methods for fine-tuning on fewer parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gIi_WZuvMWUb",
   "metadata": {
    "id": "gIi_WZuvMWUb"
   },
   "source": [
    "**Answer**\n",
    "\n",
    "1. **Text-to-Text Format**: This format refers to a method in natural language processing (NLP) where both the input and output are in text form. Unlike other formats where input might be text and output could be a label, category, or action, in text-to-text, everything is converted into a textual representation. This approach simplifies the processing model since it deals with only one type of data - text.\n",
    "\n",
    "2. **T5 (Text-to-Text Transfer Transformer) Processing for Text Classification Tasks**: T5 treats every NLP problem as a text-to-text problem. For text classification tasks, both the input and the desired output (e.g., a class label) are formulated as text.\n",
    "\n",
    "   - Input Processing: The input text is prefixed with a task-specific identifier (like \"classify:\") to provide context to the model about what task it needs to perform. This text is then tokenized and fed into the model.\n",
    "   \n",
    "   - Output Processing: T5 generates text as output, which in the case of classification, would be the name of the class. This output is interpreted as the class to which the input text belongs.\n",
    "\n",
    "3. **Complications with a Predefined Set of Classes**:\n",
    "\n",
    "   - **Limited Flexibility**: The model can only classify inputs into the predefined classes, which might not cover all possible or relevant categories, leading to misclassification or oversimplification.\n",
    "   \n",
    "   - **Bias and Imbalance**: If the predefined classes are not representative of the diversity in the real world, the model could be biased. This is especially problematic in datasets where some classes are overrepresented compared to others.\n",
    "   \n",
    "   - **Adaptability**: In dynamic domains where new categories might emerge over time (like in news topics), a fixed set of classes can render the model outdated or less effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rIf7G01tkc4u",
   "metadata": {
    "id": "rIf7G01tkc4u"
   },
   "source": [
    "**Answer**\n",
    "\n",
    "**Masked Language Modeling (MLM):** This is a technique used in language model training where some of the words in a sentence are randomly masked or hidden, and the model's objective is to predict these masked words. This approach helps the model learn context, grammar, and word relationships.\n",
    "\n",
    "**Word Dropout:** This involves randomly dropping words from the input during training, forcing the model to make predictions or understand context without relying on all available information. It's a form of regularization that prevents overfitting and encourages the model to learn more robust features.\n",
    "\n",
    "**Sentinel Tokens:** These are special tokens used to represent masked words. In MLM, original words are replaced with sentinel tokens, and the model predicts the original word based on the context provided by the surrounding words.\n",
    "\n",
    "**Example:**\n",
    "- Original Sentence: \"The quick brown fox jumps over the lazy dog.\"\n",
    "- With MLM and Sentinel Tokens: \"The quick [MASK] fox jumps over the [MASK] dog.\"\n",
    "- With Word Dropout: \"The quick brown jumps over the lazy.\"\n",
    "\n",
    "In the MLM example, the model would aim to predict the words \"brown\" and \"lazy\" based on the context provided by the rest of the sentence. The sentinel tokens (e.g., [MASK]) indicate the positions of the masked words. In the Word Dropout example, the model sees a sentence with missing words (\"fox\" and \"dog\") and must understand or process the sentence without them.\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iC-WBUKQkf6r",
   "metadata": {
    "id": "iC-WBUKQkf6r"
   },
   "source": [
    "**Answer**\n",
    "\n",
    "1. **Fully-Visible Masking**: In fully-visible masking, every token in the input sequence can attend to every other token in the sequence. This means there's no restriction on what each token can see during the processing.\n",
    "\n",
    "2. **Causal Masking**: Causal masking, also known as autoregressive masking, is employed for text generation tasks. Here, a token can only attend to previous tokens in the sequence, not the future ones. This is akin to reading or generating text left-to-right; each word only \"knows\" about the words that came before it, not the ones that follow. This type of masking ensures that the model generates text in a forward direction, predicting one word at a time based on the preceding context.\n",
    "\n",
    "3. **Causal Masking with Prefix**: This is a variation of causal masking where the model is provided with a prefixed context or initial sequence of tokens. The model then continues generating text based on this prefixed input. The masking still ensures that each token can only attend to the prefix and the previously generated tokens, not to any of the subsequent tokens. This approach is useful in tasks where you want the model to start with a specific context or theme and then generate content that logically follows from that starting point.\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L2Mo5YElkhtI",
   "metadata": {
    "id": "L2Mo5YElkhtI"
   },
   "source": [
    "**Answer**\n",
    "\n",
    "1. **Adapter Layers**: Adapter layers are small neural network modules inserted between the layers of a pre-trained model. When fine-tuning on a specific task, only these adapter layers are trained, while the original pre-trained layers are kept frozen. This method significantly reduces the number of parameters that need to be trained, making the fine-tuning process more efficient and requiring less computational resources. Adapters enable the model to adapt to new tasks or datasets with minimal changes to the overall network architecture.\n",
    "\n",
    "2. **Gradual Unfreezing**: Gradual unfreezing is a technique where layers of a pre-trained model are unfrozen and fine-tuned incrementally, rather than all at once. Initially, only the top layers (the ones closest to the output) are unfrozen and trained. As training progresses, more layers are gradually unfrozen. This approach allows the model to retain much of its pre-trained knowledge while adapting to new data in a controlled manner. It helps in preventing catastrophic forgetting and ensures that the fine-tuning process is more stable and efficient, especially when dealing with a limited dataset or fewer parameters.\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WiYns91wr2P5",
   "metadata": {
    "id": "WiYns91wr2P5"
   },
   "source": [
    "#### ${\\color{red}{Comments\\ 3.4}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "Point distribution\n",
    "\n",
    "✅ 1.5 point for part 1.\n",
    "\n",
    "✅ 1 point for part 2.\n",
    "\n",
    "✅ 1.5 point for part 3.\n",
    "\n",
    "✅ 1 point for part 4.\n",
    "\n",
    "Note: All of them are very extensive answers.\n",
    "\n",
    "\n",
    "5.0/5.0 points\n",
    "\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
