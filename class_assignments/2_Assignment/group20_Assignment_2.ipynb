{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "93036b5344e3432db954b65809b32dfa",
    "deepnote_cell_type": "markdown",
    "id": "be9f7653"
   },
   "source": [
    "**Heidelberg University**\n",
    "\n",
    "**Data Science  Group**\n",
    "    \n",
    "Prof. Dr. Michael Gertz  \n",
    "\n",
    "Ashish Chouhan, Satya Almasian, John Ziegler, Jayson Salazar, Nicolas Reuter\n",
    "    \n",
    "November 13, 2023\n",
    "    \n",
    "Natural Language Processing with Transformers\n",
    "\n",
    "Winter Semster 2023/2024     \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b4834406a4e847f29929138fbd0a0a3b",
    "deepnote_cell_type": "markdown",
    "id": "258e9648"
   },
   "source": [
    "# **Assignment 2: “Sequence Models”**\n",
    "**Due**: Monday, November 27, 2pm, via [Moodle](https://moodle.uni-heidelberg.de/course/view.php?id=19251)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 20\n",
    "\n",
    "| Nachname     | Vorname | Matrikelnummer     |\n",
    "|----------|-----|----------------|\n",
    "| Mathew Biju     | Ibin  | 3770662 |\n",
    "| Reddy    | Anu  | 3768482  |\n",
    "| Yilmaz      | Burhan Akin  | 4114082    |\n",
    "| Rijhwani      | Nilesh Parshotam  | 3771253    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a15672d1e5fd440c9c0d245e7e3709fc",
    "deepnote_cell_type": "markdown",
    "id": "fc27ad9e"
   },
   "source": [
    "### **Submission Guidelines**\n",
    "\n",
    "- Solutions need to be uploaded as a **single** Jupyter notebook. You will find several pre-filled code segments in the notebook, your task is to fill in the missing cells.\n",
    "- For the written solution, use LaTeX in markdown inside the same notebook. Do **not** hand in a separate file for it.\n",
    "- Download the .zip file containing the dataset but do **not** upload it with your solution.\n",
    "- It is sufficient if one person per group uploads the solution to Moodle, but make sure that the complete names of all team members are given in the notebook.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "769c265f477b4814a9fcd612d7dbdbaf",
    "deepnote_cell_type": "markdown",
    "id": "e322e8b0"
   },
   "source": [
    "## **Task 1: Part-of-Speech Tagging with a Bidirectional LSTM**  (2+4+5=11 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c4bb61765ff74caabe1c44bf67633d23",
    "deepnote_cell_type": "markdown",
    "id": "b4ca26ac"
   },
   "source": [
    "In this task we will be building a sequence tagger that produces an output for every element in an input sequence, using `PyTorch` and `TorchText`, where `TorchText` consists of data processing utilities and popular datasets for natural language.\n",
    "\n",
    "\n",
    "*   **input:** a sequence of text\n",
    "*   **output:** part-of-speech (POS) tag for each token in the input text\n",
    "\n",
    "We tackle this task using a multi-layer bi-directional LSTM (BiLSTM) to predict POS tags using the [Universal Dependencies](https://universaldependencies.org/) English Web Treebank (UDPOS) dataset. This dataset is contained in the `TorchText` library and we do not require an external file for it. The dataset in  `TorchText`  has two different sets of tags, universal dependency (UD) tags and Penn Treebank (PTB) tags. We only focus on the UD tags for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "34d25f7e88ea42bc8009a68b832ab8e1",
    "deepnote_cell_type": "code",
    "id": "nnBfLC1P5QXw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting portalocker\n",
      "  Obtaining dependency information for portalocker from https://files.pythonhosted.org/packages/17/9e/87671efcca80ba6203811540ed1f9c0462c1609d2281d7b7f53cef05da3d/portalocker-2.8.2-py3-none-any.whl.metadata\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\rijhw\\appdata\\roaming\\python\\python39\\site-packages (from portalocker) (305)\n",
      "Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: portalocker\n",
      "Successfully installed portalocker-2.8.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install portalocker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "2e1e4e360290456c987b8e7ca6d93fda",
    "deepnote_cell_type": "code",
    "id": "nT5geerO7d1P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchdata\n",
      "  Obtaining dependency information for torchdata from https://files.pythonhosted.org/packages/08/05/d717b62841b32c29aabfb834d7fe606fdeb0420953b0391da1cde7804577/torchdata-0.7.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading torchdata-0.7.1-cp39-cp39-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\rijhw\\appdata\\roaming\\python\\python39\\site-packages (from torchdata) (1.26.13)\n",
      "Requirement already satisfied: requests in c:\\program files\\python39\\lib\\site-packages (from torchdata) (2.28.2)\n",
      "Collecting torch>=2 (from torchdata)\n",
      "  Obtaining dependency information for torch>=2 from https://files.pythonhosted.org/packages/c7/1d/de42543b890ecbfc74786fdfdf80207f52c157fae4ac4c76612a8bb27a4a/torch-2.1.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading torch-2.1.1-cp39-cp39-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\rijhw\\appdata\\roaming\\python\\python39\\site-packages (from torch>=2->torchdata) (3.8.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\rijhw\\appdata\\roaming\\python\\python39\\site-packages (from torch>=2->torchdata) (4.4.0)\n",
      "Collecting sympy (from torch>=2->torchdata)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "     ---------------------------------------- 0.0/5.7 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.2/5.7 MB 5.9 MB/s eta 0:00:01\n",
      "     -- ------------------------------------- 0.4/5.7 MB 4.9 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 0.7/5.7 MB 5.7 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 1.1/5.7 MB 5.7 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 1.5/5.7 MB 6.2 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 1.8/5.7 MB 6.5 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 2.2/5.7 MB 6.4 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 2.6/5.7 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 3.0/5.7 MB 6.8 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 3.4/5.7 MB 6.9 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 3.7/5.7 MB 6.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 4.1/5.7 MB 6.8 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 4.4/5.7 MB 6.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 4.8/5.7 MB 7.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 5.2/5.7 MB 7.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 5.4/5.7 MB 7.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.7/5.7 MB 7.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 5.7/5.7 MB 6.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: networkx in c:\\program files\\python39\\lib\\site-packages (from torch>=2->torchdata) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rijhw\\appdata\\roaming\\python\\python39\\site-packages (from torch>=2->torchdata) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rijhw\\appdata\\roaming\\python\\python39\\site-packages (from torch>=2->torchdata) (2022.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rijhw\\appdata\\roaming\\python\\python39\\site-packages (from requests->torchdata) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python39\\lib\\site-packages (from requests->torchdata) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rijhw\\appdata\\roaming\\python\\python39\\site-packages (from requests->torchdata) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rijhw\\appdata\\roaming\\python\\python39\\site-packages (from jinja2->torch>=2->torchdata) (2.1.1)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=2->torchdata)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "     ------------------------- ----------- 368.6/536.2 kB 11.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 536.2/536.2 kB 6.7 MB/s eta 0:00:00\n",
      "Downloading torchdata-0.7.1-cp39-cp39-win_amd64.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.3/1.3 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.7/1.3 MB 7.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.1/1.3 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 7.1 MB/s eta 0:00:00\n",
      "Downloading torch-2.1.1-cp39-cp39-win_amd64.whl (192.2 MB)\n",
      "   ---------------------------------------- 0.0/192.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/192.2 MB 18.6 MB/s eta 0:00:11\n",
      "   ---------------------------------------- 0.7/192.2 MB 8.4 MB/s eta 0:00:23\n",
      "   ---------------------------------------- 1.1/192.2 MB 7.6 MB/s eta 0:00:26\n",
      "   ---------------------------------------- 1.5/192.2 MB 7.8 MB/s eta 0:00:25\n",
      "   ---------------------------------------- 1.8/192.2 MB 8.2 MB/s eta 0:00:24\n",
      "   ---------------------------------------- 2.1/192.2 MB 8.0 MB/s eta 0:00:24\n",
      "    --------------------------------------- 2.4/192.2 MB 7.8 MB/s eta 0:00:25\n",
      "    --------------------------------------- 2.8/192.2 MB 7.8 MB/s eta 0:00:25\n",
      "    --------------------------------------- 3.2/192.2 MB 7.9 MB/s eta 0:00:25\n",
      "    --------------------------------------- 3.6/192.2 MB 7.9 MB/s eta 0:00:24\n",
      "    --------------------------------------- 4.0/192.2 MB 8.0 MB/s eta 0:00:24\n",
      "    --------------------------------------- 4.3/192.2 MB 7.9 MB/s eta 0:00:24\n",
      "    --------------------------------------- 4.8/192.2 MB 7.8 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 5.3/192.2 MB 7.9 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 5.6/192.2 MB 8.0 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 6.0/192.2 MB 7.9 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 6.2/192.2 MB 7.8 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 6.7/192.2 MB 7.8 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 7.2/192.2 MB 8.0 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 7.5/192.2 MB 8.0 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 7.9/192.2 MB 7.9 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 8.3/192.2 MB 7.9 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 8.6/192.2 MB 7.9 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 9.0/192.2 MB 7.9 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 9.3/192.2 MB 7.9 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 9.6/192.2 MB 7.7 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 9.9/192.2 MB 7.8 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 10.2/192.2 MB 7.8 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 10.7/192.2 MB 7.7 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 11.0/192.2 MB 7.8 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 11.3/192.2 MB 7.8 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 11.7/192.2 MB 7.6 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 11.9/192.2 MB 7.7 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 12.3/192.2 MB 7.8 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 12.6/192.2 MB 7.6 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 13.1/192.2 MB 7.6 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 13.4/192.2 MB 7.6 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 13.7/192.2 MB 7.6 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 14.0/192.2 MB 7.6 MB/s eta 0:00:24\n",
      "   --- ------------------------------------ 14.4/192.2 MB 7.5 MB/s eta 0:00:24\n",
      "   --- ------------------------------------ 14.9/192.2 MB 7.7 MB/s eta 0:00:24\n",
      "   --- ------------------------------------ 15.2/192.2 MB 7.6 MB/s eta 0:00:24\n",
      "   --- ------------------------------------ 15.7/192.2 MB 7.7 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 16.0/192.2 MB 7.7 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 16.4/192.2 MB 7.7 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 16.8/192.2 MB 7.7 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 17.2/192.2 MB 7.6 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 17.5/192.2 MB 7.6 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 17.9/192.2 MB 7.6 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 18.3/192.2 MB 7.6 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 18.7/192.2 MB 7.7 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 19.1/192.2 MB 7.8 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 19.4/192.2 MB 7.6 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 19.7/192.2 MB 7.7 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 19.9/192.2 MB 7.7 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 20.4/192.2 MB 7.7 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 20.8/192.2 MB 7.7 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 21.1/192.2 MB 7.7 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 21.4/192.2 MB 7.6 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 21.6/192.2 MB 7.6 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 21.8/192.2 MB 7.4 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 22.2/192.2 MB 7.5 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 22.5/192.2 MB 7.4 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 22.7/192.2 MB 7.4 MB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 23.0/192.2 MB 7.4 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 23.3/192.2 MB 7.4 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 23.7/192.2 MB 7.4 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 24.1/192.2 MB 7.4 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 24.4/192.2 MB 7.4 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 24.7/192.2 MB 7.4 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 25.0/192.2 MB 7.4 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 25.3/192.2 MB 7.4 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 25.7/192.2 MB 7.4 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 26.1/192.2 MB 7.4 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 26.4/192.2 MB 7.4 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 26.7/192.2 MB 7.4 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 27.1/192.2 MB 7.3 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 27.3/192.2 MB 7.3 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 27.6/192.2 MB 7.2 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 28.0/192.2 MB 7.3 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 28.3/192.2 MB 7.2 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 28.7/192.2 MB 7.2 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 29.1/192.2 MB 7.1 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 29.4/192.2 MB 7.2 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 29.8/192.2 MB 7.2 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 30.2/192.2 MB 7.2 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 30.5/192.2 MB 7.1 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 30.9/192.2 MB 7.2 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 31.2/192.2 MB 7.2 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 31.3/192.2 MB 7.0 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 31.6/192.2 MB 7.0 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 31.8/192.2 MB 7.0 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 32.1/192.2 MB 7.0 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 32.3/192.2 MB 7.0 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 32.6/192.2 MB 7.0 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 33.0/192.2 MB 7.0 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 33.3/192.2 MB 7.0 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 33.6/192.2 MB 7.0 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 33.9/192.2 MB 7.0 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 34.4/192.2 MB 7.0 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 34.7/192.2 MB 6.9 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 35.1/192.2 MB 7.0 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 35.5/192.2 MB 7.0 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 35.9/192.2 MB 7.0 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 36.3/192.2 MB 7.0 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 36.7/192.2 MB 7.0 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 37.1/192.2 MB 7.0 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 37.4/192.2 MB 7.0 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 37.7/192.2 MB 7.2 MB/s eta 0:00:22\n",
      "   ------- -------------------------------- 38.1/192.2 MB 7.3 MB/s eta 0:00:22\n",
      "   ------- -------------------------------- 38.4/192.2 MB 7.2 MB/s eta 0:00:22\n",
      "   -------- ------------------------------- 38.8/192.2 MB 7.2 MB/s eta 0:00:22\n",
      "   -------- ------------------------------- 39.1/192.2 MB 7.3 MB/s eta 0:00:22\n",
      "   -------- ------------------------------- 39.5/192.2 MB 7.4 MB/s eta 0:00:21\n",
      "   -------- ------------------------------- 39.8/192.2 MB 7.1 MB/s eta 0:00:22\n",
      "   -------- ------------------------------- 40.1/192.2 MB 7.3 MB/s eta 0:00:21\n",
      "   -------- ------------------------------- 40.5/192.2 MB 7.3 MB/s eta 0:00:21\n",
      "   -------- ------------------------------- 40.9/192.2 MB 7.3 MB/s eta 0:00:21\n",
      "   -------- ------------------------------- 41.3/192.2 MB 7.3 MB/s eta 0:00:21\n",
      "   -------- ------------------------------- 41.4/192.2 MB 7.2 MB/s eta 0:00:21\n",
      "   -------- ------------------------------- 41.8/192.2 MB 7.4 MB/s eta 0:00:21\n",
      "   -------- ------------------------------- 42.2/192.2 MB 7.6 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 42.6/192.2 MB 7.7 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 43.0/192.2 MB 7.7 MB/s eta 0:00:20\n",
      "   --------- ------------------------------ 43.3/192.2 MB 7.6 MB/s eta 0:00:20\n",
      "   --------- ------------------------------ 43.6/192.2 MB 7.7 MB/s eta 0:00:20\n",
      "   --------- ------------------------------ 44.1/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 44.5/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 44.9/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 45.2/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 45.6/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 46.0/192.2 MB 7.8 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 46.4/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 46.6/192.2 MB 7.8 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 47.0/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 47.3/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 47.7/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 48.0/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 48.4/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 48.5/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 48.5/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 48.5/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 48.5/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 48.5/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 48.5/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 48.5/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 48.5/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 48.5/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 48.5/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 48.5/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 48.5/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 48.5/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 48.5/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 48.5/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 48.5/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 48.5/192.2 MB 7.9 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 48.6/192.2 MB 5.0 MB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 49.0/192.2 MB 5.0 MB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 49.4/192.2 MB 5.0 MB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 49.8/192.2 MB 4.9 MB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 50.0/192.2 MB 5.0 MB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 50.4/192.2 MB 5.0 MB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 50.9/192.2 MB 5.0 MB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 51.3/192.2 MB 5.0 MB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 51.7/192.2 MB 5.1 MB/s eta 0:00:28\n",
      "   ---------- ----------------------------- 52.2/192.2 MB 5.1 MB/s eta 0:00:28\n",
      "   ---------- ----------------------------- 52.5/192.2 MB 5.1 MB/s eta 0:00:28\n",
      "   ---------- ----------------------------- 52.8/192.2 MB 5.0 MB/s eta 0:00:28\n",
      "   ----------- ---------------------------- 53.2/192.2 MB 5.1 MB/s eta 0:00:28\n",
      "   ----------- ---------------------------- 53.4/192.2 MB 5.0 MB/s eta 0:00:28\n",
      "   ----------- ---------------------------- 53.7/192.2 MB 5.0 MB/s eta 0:00:28\n",
      "   ----------- ---------------------------- 53.9/192.2 MB 5.0 MB/s eta 0:00:28\n",
      "   ----------- ---------------------------- 54.2/192.2 MB 5.0 MB/s eta 0:00:28\n",
      "   ----------- ---------------------------- 54.5/192.2 MB 4.9 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 54.7/192.2 MB 4.9 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 54.9/192.2 MB 4.8 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 55.2/192.2 MB 4.8 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 55.5/192.2 MB 4.8 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 55.9/192.2 MB 4.8 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 56.2/192.2 MB 4.8 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 56.7/192.2 MB 4.8 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 57.0/192.2 MB 4.8 MB/s eta 0:00:29\n",
      "   ----------- ---------------------------- 57.4/192.2 MB 4.9 MB/s eta 0:00:28\n",
      "   ------------ --------------------------- 57.7/192.2 MB 4.9 MB/s eta 0:00:28\n",
      "   ------------ --------------------------- 57.9/192.2 MB 4.9 MB/s eta 0:00:28\n",
      "   ------------ --------------------------- 58.2/192.2 MB 4.8 MB/s eta 0:00:28\n",
      "   ------------ --------------------------- 58.7/192.2 MB 4.8 MB/s eta 0:00:28\n",
      "   ------------ --------------------------- 59.0/192.2 MB 7.4 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 59.3/192.2 MB 7.4 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 59.4/192.2 MB 7.3 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 59.8/192.2 MB 7.3 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 60.1/192.2 MB 7.2 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 60.5/192.2 MB 7.2 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 60.8/192.2 MB 7.2 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 61.1/192.2 MB 7.2 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 61.5/192.2 MB 7.2 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 61.7/192.2 MB 7.1 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 62.1/192.2 MB 7.1 MB/s eta 0:00:19\n",
      "   ------------- -------------------------- 62.5/192.2 MB 7.1 MB/s eta 0:00:19\n",
      "   ------------- -------------------------- 62.9/192.2 MB 7.1 MB/s eta 0:00:19\n",
      "   ------------- -------------------------- 63.2/192.2 MB 7.1 MB/s eta 0:00:19\n",
      "   ------------- -------------------------- 63.6/192.2 MB 7.3 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 63.9/192.2 MB 7.2 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 64.3/192.2 MB 7.4 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 64.7/192.2 MB 7.5 MB/s eta 0:00:17\n",
      "   ------------- -------------------------- 65.0/192.2 MB 7.6 MB/s eta 0:00:17\n",
      "   ------------- -------------------------- 65.3/192.2 MB 7.8 MB/s eta 0:00:17\n",
      "   ------------- -------------------------- 65.6/192.2 MB 7.8 MB/s eta 0:00:17\n",
      "   ------------- -------------------------- 66.0/192.2 MB 7.7 MB/s eta 0:00:17\n",
      "   ------------- -------------------------- 66.4/192.2 MB 7.5 MB/s eta 0:00:17\n",
      "   ------------- -------------------------- 66.9/192.2 MB 7.7 MB/s eta 0:00:17\n",
      "   -------------- ------------------------- 67.3/192.2 MB 7.6 MB/s eta 0:00:17\n",
      "   -------------- ------------------------- 67.6/192.2 MB 7.6 MB/s eta 0:00:17\n",
      "   -------------- ------------------------- 67.9/192.2 MB 7.6 MB/s eta 0:00:17\n",
      "   -------------- ------------------------- 68.3/192.2 MB 7.6 MB/s eta 0:00:17\n",
      "   -------------- ------------------------- 68.7/192.2 MB 7.8 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 69.2/192.2 MB 7.7 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 69.6/192.2 MB 7.8 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 69.9/192.2 MB 7.8 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 70.3/192.2 MB 7.9 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 70.6/192.2 MB 7.9 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 70.9/192.2 MB 7.8 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 71.4/192.2 MB 7.9 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 71.8/192.2 MB 7.9 MB/s eta 0:00:16\n",
      "   --------------- ------------------------ 72.2/192.2 MB 7.9 MB/s eta 0:00:16\n",
      "   --------------- ------------------------ 72.6/192.2 MB 8.1 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 72.9/192.2 MB 8.1 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 73.3/192.2 MB 8.0 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 73.6/192.2 MB 8.0 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 73.9/192.2 MB 7.9 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 74.4/192.2 MB 8.0 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 74.7/192.2 MB 8.0 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 75.1/192.2 MB 8.0 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 75.5/192.2 MB 8.0 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 75.8/192.2 MB 7.9 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 76.2/192.2 MB 8.0 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 76.5/192.2 MB 7.9 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 76.8/192.2 MB 8.0 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 77.2/192.2 MB 7.9 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 77.6/192.2 MB 7.9 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 77.9/192.2 MB 8.0 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 78.2/192.2 MB 8.0 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 78.6/192.2 MB 8.1 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 79.0/192.2 MB 8.1 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 79.5/192.2 MB 8.1 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 79.8/192.2 MB 8.2 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 80.2/192.2 MB 8.1 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 80.5/192.2 MB 8.1 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 80.9/192.2 MB 8.1 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 81.2/192.2 MB 8.2 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 81.6/192.2 MB 8.2 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 81.9/192.2 MB 8.3 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 82.3/192.2 MB 8.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 82.6/192.2 MB 8.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 83.0/192.2 MB 8.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 83.3/192.2 MB 8.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 83.5/192.2 MB 8.0 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 83.8/192.2 MB 8.0 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 83.9/192.2 MB 7.8 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 84.0/192.2 MB 7.6 MB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 84.4/192.2 MB 7.6 MB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 84.8/192.2 MB 7.6 MB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 85.2/192.2 MB 7.6 MB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 85.5/192.2 MB 7.6 MB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 85.9/192.2 MB 7.6 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 86.2/192.2 MB 7.4 MB/s eta 0:00:15\n",
      "   ------------------ --------------------- 86.5/192.2 MB 7.4 MB/s eta 0:00:15\n",
      "   ------------------ --------------------- 86.9/192.2 MB 7.5 MB/s eta 0:00:15\n",
      "   ------------------ --------------------- 87.2/192.2 MB 7.4 MB/s eta 0:00:15\n",
      "   ------------------ --------------------- 87.6/192.2 MB 7.4 MB/s eta 0:00:15\n",
      "   ------------------ --------------------- 87.9/192.2 MB 7.4 MB/s eta 0:00:15\n",
      "   ------------------ --------------------- 88.2/192.2 MB 7.4 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 88.5/192.2 MB 7.5 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 88.9/192.2 MB 7.5 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 89.3/192.2 MB 7.5 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 89.7/192.2 MB 7.4 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 90.1/192.2 MB 7.5 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 90.5/192.2 MB 7.5 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 90.9/192.2 MB 7.5 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 91.3/192.2 MB 7.4 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 91.8/192.2 MB 7.4 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 92.1/192.2 MB 7.4 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 92.4/192.2 MB 7.4 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 92.8/192.2 MB 7.6 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 93.1/192.2 MB 7.4 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 93.5/192.2 MB 7.6 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 93.9/192.2 MB 7.6 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 94.2/192.2 MB 8.0 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 94.5/192.2 MB 8.1 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 94.8/192.2 MB 8.1 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 95.1/192.2 MB 8.1 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 95.5/192.2 MB 8.3 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 95.8/192.2 MB 8.1 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 96.3/192.2 MB 8.3 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 96.6/192.2 MB 8.2 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 96.9/192.2 MB 8.3 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 97.3/192.2 MB 8.2 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 97.6/192.2 MB 8.1 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 98.0/192.2 MB 8.3 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 98.4/192.2 MB 8.3 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 98.7/192.2 MB 8.3 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 99.1/192.2 MB 8.3 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 99.4/192.2 MB 8.1 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 99.8/192.2 MB 8.2 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 100.3/192.2 MB 8.2 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 100.7/192.2 MB 8.3 MB/s eta 0:00:12\n",
      "   --------------------- ------------------ 101.1/192.2 MB 8.3 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 101.4/192.2 MB 8.4 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 101.7/192.2 MB 8.4 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 102.1/192.2 MB 8.3 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 102.3/192.2 MB 8.3 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 102.7/192.2 MB 8.3 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 102.9/192.2 MB 8.2 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 103.3/192.2 MB 8.3 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 103.7/192.2 MB 8.3 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 104.0/192.2 MB 8.2 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 104.4/192.2 MB 8.2 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 104.8/192.2 MB 8.2 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 105.2/192.2 MB 8.1 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 105.5/192.2 MB 8.1 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 105.9/192.2 MB 8.0 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 106.2/192.2 MB 8.0 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 106.5/192.2 MB 7.9 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 106.8/192.2 MB 8.1 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 107.3/192.2 MB 7.9 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 107.6/192.2 MB 7.9 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 107.9/192.2 MB 7.9 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 108.3/192.2 MB 8.0 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 108.6/192.2 MB 7.9 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 109.0/192.2 MB 8.0 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 109.4/192.2 MB 8.0 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 109.9/192.2 MB 8.1 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 110.2/192.2 MB 8.0 MB/s eta 0:00:11\n",
      "   ----------------------- ---------------- 110.6/192.2 MB 7.9 MB/s eta 0:00:11\n",
      "   ----------------------- ---------------- 110.9/192.2 MB 7.9 MB/s eta 0:00:11\n",
      "   ----------------------- ---------------- 111.2/192.2 MB 7.8 MB/s eta 0:00:11\n",
      "   ----------------------- ---------------- 111.6/192.2 MB 7.7 MB/s eta 0:00:11\n",
      "   ----------------------- ---------------- 112.0/192.2 MB 7.8 MB/s eta 0:00:11\n",
      "   ----------------------- ---------------- 112.4/192.2 MB 7.8 MB/s eta 0:00:11\n",
      "   ----------------------- ---------------- 112.8/192.2 MB 7.8 MB/s eta 0:00:11\n",
      "   ----------------------- ---------------- 113.2/192.2 MB 8.0 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 113.5/192.2 MB 7.9 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 113.9/192.2 MB 7.9 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 114.3/192.2 MB 8.1 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 114.7/192.2 MB 7.9 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 115.1/192.2 MB 8.0 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 115.3/192.2 MB 7.8 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 115.6/192.2 MB 7.9 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 116.0/192.2 MB 7.8 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 116.3/192.2 MB 8.0 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 116.7/192.2 MB 7.9 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 117.1/192.2 MB 7.8 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 117.4/192.2 MB 7.9 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 117.6/192.2 MB 7.8 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 117.9/192.2 MB 7.8 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 118.3/192.2 MB 7.8 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 118.7/192.2 MB 7.7 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 119.0/192.2 MB 7.7 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 119.4/192.2 MB 7.7 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 119.7/192.2 MB 7.8 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 120.0/192.2 MB 7.7 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 120.4/192.2 MB 7.7 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 120.7/192.2 MB 7.7 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 121.0/192.2 MB 7.6 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 121.3/192.2 MB 7.6 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 121.7/192.2 MB 7.5 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 122.1/192.2 MB 7.6 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 122.5/192.2 MB 7.8 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 122.8/192.2 MB 7.6 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 123.1/192.2 MB 7.5 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 123.6/192.2 MB 7.5 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 123.8/192.2 MB 7.5 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 124.2/192.2 MB 7.4 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 124.5/192.2 MB 7.4 MB/s eta 0:00:10\n",
      "   -------------------------- ------------- 125.0/192.2 MB 7.4 MB/s eta 0:00:10\n",
      "   -------------------------- ------------- 125.3/192.2 MB 7.4 MB/s eta 0:00:10\n",
      "   -------------------------- ------------- 125.7/192.2 MB 7.5 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 125.9/192.2 MB 7.5 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 126.3/192.2 MB 7.4 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 126.6/192.2 MB 7.5 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 127.0/192.2 MB 7.5 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 127.3/192.2 MB 7.5 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 127.6/192.2 MB 7.6 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 128.1/192.2 MB 7.7 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 128.5/192.2 MB 7.6 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 128.8/192.2 MB 7.6 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 129.1/192.2 MB 7.6 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 129.6/192.2 MB 7.7 MB/s eta 0:00:09\n",
      "   --------------------------- ------------ 130.0/192.2 MB 7.7 MB/s eta 0:00:09\n",
      "   --------------------------- ------------ 130.4/192.2 MB 7.8 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 130.6/192.2 MB 7.6 MB/s eta 0:00:09\n",
      "   --------------------------- ------------ 131.0/192.2 MB 7.7 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 131.4/192.2 MB 7.8 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 131.6/192.2 MB 7.8 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 132.1/192.2 MB 7.7 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 132.4/192.2 MB 7.8 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 132.8/192.2 MB 7.8 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 133.1/192.2 MB 7.9 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 133.4/192.2 MB 7.9 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 133.8/192.2 MB 7.9 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 134.1/192.2 MB 7.9 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 134.5/192.2 MB 8.1 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 134.8/192.2 MB 8.0 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 135.1/192.2 MB 8.0 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 135.5/192.2 MB 8.0 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 135.9/192.2 MB 7.9 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 136.3/192.2 MB 8.0 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 136.7/192.2 MB 8.1 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 137.1/192.2 MB 8.1 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 137.5/192.2 MB 8.0 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 137.8/192.2 MB 8.1 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 138.1/192.2 MB 8.0 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 138.5/192.2 MB 8.0 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 138.8/192.2 MB 8.1 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 139.1/192.2 MB 8.0 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 139.4/192.2 MB 8.0 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 139.8/192.2 MB 8.0 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 140.0/192.2 MB 7.8 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 140.1/192.2 MB 7.7 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 140.3/192.2 MB 7.4 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 140.6/192.2 MB 7.4 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 140.8/192.2 MB 7.3 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 141.0/192.2 MB 7.2 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 141.2/192.2 MB 7.1 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 141.4/192.2 MB 7.0 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 141.7/192.2 MB 7.0 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 142.0/192.2 MB 6.9 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 142.0/192.2 MB 6.8 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 142.3/192.2 MB 6.8 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 142.6/192.2 MB 6.7 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 142.9/192.2 MB 6.7 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 143.2/192.2 MB 6.6 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 143.4/192.2 MB 6.5 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 143.9/192.2 MB 6.5 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 144.2/192.2 MB 6.6 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 144.6/192.2 MB 6.5 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 145.0/192.2 MB 6.6 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 145.4/192.2 MB 6.6 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 145.8/192.2 MB 6.6 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 146.2/192.2 MB 6.7 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 146.5/192.2 MB 6.5 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 146.9/192.2 MB 6.5 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 147.3/192.2 MB 6.6 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 147.6/192.2 MB 6.6 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 148.0/192.2 MB 6.6 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 148.4/192.2 MB 6.6 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 148.8/192.2 MB 6.6 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 149.1/192.2 MB 6.5 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 149.4/192.2 MB 6.7 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 149.8/192.2 MB 6.7 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 150.3/192.2 MB 6.9 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 150.7/192.2 MB 7.2 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 151.0/192.2 MB 7.1 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 151.3/192.2 MB 7.5 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 151.6/192.2 MB 7.6 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 151.9/192.2 MB 7.7 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 152.2/192.2 MB 7.7 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 152.5/192.2 MB 7.8 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 152.9/192.2 MB 8.1 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 153.2/192.2 MB 8.1 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 153.6/192.2 MB 8.2 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 153.9/192.2 MB 8.0 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 154.4/192.2 MB 8.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 154.8/192.2 MB 8.2 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 155.1/192.2 MB 8.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 155.5/192.2 MB 8.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 155.7/192.2 MB 8.0 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 156.1/192.2 MB 8.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 156.4/192.2 MB 8.0 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 156.8/192.2 MB 8.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 157.2/192.2 MB 8.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 157.5/192.2 MB 8.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 158.0/192.2 MB 8.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 158.4/192.2 MB 8.2 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 158.8/192.2 MB 8.0 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 159.2/192.2 MB 8.0 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 159.5/192.2 MB 8.0 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 159.9/192.2 MB 8.0 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 160.2/192.2 MB 7.9 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 160.6/192.2 MB 7.8 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 160.9/192.2 MB 7.8 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 161.3/192.2 MB 7.9 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 161.7/192.2 MB 7.9 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 162.0/192.2 MB 7.9 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 162.4/192.2 MB 7.9 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 162.8/192.2 MB 8.0 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 163.2/192.2 MB 8.0 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 163.4/192.2 MB 7.8 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 163.8/192.2 MB 7.8 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 164.1/192.2 MB 7.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 164.5/192.2 MB 7.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 164.8/192.2 MB 7.8 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 165.2/192.2 MB 7.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 165.5/192.2 MB 7.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 165.8/192.2 MB 7.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 166.3/192.2 MB 7.8 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 166.6/192.2 MB 7.8 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 166.9/192.2 MB 7.8 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 167.3/192.2 MB 7.8 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 167.7/192.2 MB 7.8 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 168.1/192.2 MB 7.8 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 168.4/192.2 MB 7.7 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 168.8/192.2 MB 7.8 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 169.2/192.2 MB 7.8 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 169.5/192.2 MB 7.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 169.8/192.2 MB 7.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 170.2/192.2 MB 7.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 170.6/192.2 MB 7.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 170.9/192.2 MB 7.8 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 171.3/192.2 MB 7.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 171.6/192.2 MB 7.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 172.0/192.2 MB 7.9 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 172.4/192.2 MB 7.8 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 172.8/192.2 MB 7.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 173.2/192.2 MB 7.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 173.5/192.2 MB 7.6 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 173.9/192.2 MB 7.7 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 174.3/192.2 MB 7.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 174.7/192.2 MB 7.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 175.1/192.2 MB 7.7 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 175.5/192.2 MB 7.7 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 175.8/192.2 MB 7.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 176.2/192.2 MB 7.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 176.5/192.2 MB 7.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 176.8/192.2 MB 7.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 177.0/192.2 MB 7.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 177.3/192.2 MB 7.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 177.5/192.2 MB 7.6 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 177.6/192.2 MB 7.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 177.9/192.2 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 178.2/192.2 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 178.5/192.2 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 178.8/192.2 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 179.2/192.2 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 179.6/192.2 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 180.0/192.2 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 180.3/192.2 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 180.8/192.2 MB 7.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 181.1/192.2 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 181.5/192.2 MB 7.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 181.8/192.2 MB 7.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 182.1/192.2 MB 7.2 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 182.6/192.2 MB 7.3 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 182.9/192.2 MB 7.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 183.2/192.2 MB 7.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 183.6/192.2 MB 7.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 183.9/192.2 MB 7.3 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 184.3/192.2 MB 7.3 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 184.7/192.2 MB 7.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 185.0/192.2 MB 7.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 185.4/192.2 MB 7.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 185.8/192.2 MB 7.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 186.1/192.2 MB 7.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 186.4/192.2 MB 7.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 186.8/192.2 MB 7.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 187.1/192.2 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  187.5/192.2 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  187.9/192.2 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  188.3/192.2 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  188.8/192.2 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  189.1/192.2 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  189.4/192.2 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  189.7/192.2 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  190.0/192.2 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  190.3/192.2 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  190.6/192.2 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  191.0/192.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  191.3/192.2 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  191.6/192.2 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  192.0/192.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  192.2/192.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  192.2/192.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  192.2/192.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  192.2/192.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  192.2/192.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  192.2/192.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  192.2/192.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  192.2/192.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  192.2/192.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  192.2/192.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  192.2/192.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  192.2/192.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 192.2/192.2 MB 4.5 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, torch, torchdata\n",
      "Successfully installed mpmath-1.3.0 sympy-1.12 torch-2.1.1 torchdata-0.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.8.1 requires torch==1.7.1, but you have torch 2.1.1 which is incompatible.\n",
      "torchvision 0.8.2+cu110 requires torch==1.7.1, but you have torch 2.1.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "a9821d7a2616424fb4b710afd85fb6bd",
    "deepnote_cell_type": "code",
    "id": "_pIBtTh3sOQe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchtext in c:\\program files\\python39\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rijhw\\appdata\\roaming\\python\\python39\\site-packages (from torchtext) (4.64.1)\n",
      "Requirement already satisfied: requests in c:\\program files\\python39\\lib\\site-packages (from torchtext) (2.28.2)\n",
      "Collecting torch==1.7.1 (from torchtext)\n",
      "  Downloading torch-1.7.1-cp39-cp39-win_amd64.whl (184.0 MB)\n",
      "     ---------------------------------------- 0.0/184.0 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.1/184.0 MB 2.8 MB/s eta 0:01:05\n",
      "     ---------------------------------------- 0.4/184.0 MB 4.2 MB/s eta 0:00:44\n",
      "     ---------------------------------------- 0.6/184.0 MB 4.3 MB/s eta 0:00:43\n",
      "     ---------------------------------------- 0.9/184.0 MB 5.0 MB/s eta 0:00:37\n",
      "     ---------------------------------------- 1.2/184.0 MB 5.4 MB/s eta 0:00:34\n",
      "     ---------------------------------------- 1.4/184.0 MB 5.1 MB/s eta 0:00:36\n",
      "     ---------------------------------------- 1.8/184.0 MB 5.8 MB/s eta 0:00:32\n",
      "     ---------------------------------------- 2.1/184.0 MB 5.8 MB/s eta 0:00:32\n",
      "      --------------------------------------- 2.4/184.0 MB 5.9 MB/s eta 0:00:31\n",
      "      --------------------------------------- 2.8/184.0 MB 6.2 MB/s eta 0:00:30\n",
      "      --------------------------------------- 3.1/184.0 MB 6.0 MB/s eta 0:00:31\n",
      "      --------------------------------------- 3.5/184.0 MB 6.3 MB/s eta 0:00:29\n",
      "      --------------------------------------- 3.8/184.0 MB 6.2 MB/s eta 0:00:30\n",
      "      --------------------------------------- 4.1/184.0 MB 6.4 MB/s eta 0:00:28\n",
      "      --------------------------------------- 4.5/184.0 MB 6.5 MB/s eta 0:00:28\n",
      "     - -------------------------------------- 4.9/184.0 MB 6.7 MB/s eta 0:00:27\n",
      "     - -------------------------------------- 5.3/184.0 MB 6.6 MB/s eta 0:00:28\n",
      "     - -------------------------------------- 5.6/184.0 MB 6.6 MB/s eta 0:00:28\n",
      "     - -------------------------------------- 6.0/184.0 MB 6.6 MB/s eta 0:00:28\n",
      "     - -------------------------------------- 6.4/184.0 MB 6.8 MB/s eta 0:00:27\n",
      "     - -------------------------------------- 6.7/184.0 MB 6.8 MB/s eta 0:00:26\n",
      "     - -------------------------------------- 7.1/184.0 MB 6.7 MB/s eta 0:00:27\n",
      "     - -------------------------------------- 7.5/184.0 MB 6.8 MB/s eta 0:00:26\n",
      "     - -------------------------------------- 7.9/184.0 MB 6.9 MB/s eta 0:00:26\n",
      "     - -------------------------------------- 8.3/184.0 MB 7.0 MB/s eta 0:00:26\n",
      "     - -------------------------------------- 8.7/184.0 MB 7.0 MB/s eta 0:00:26\n",
      "     - -------------------------------------- 9.0/184.0 MB 6.9 MB/s eta 0:00:26\n",
      "     -- ------------------------------------- 9.4/184.0 MB 7.0 MB/s eta 0:00:26\n",
      "     -- ------------------------------------- 9.7/184.0 MB 6.9 MB/s eta 0:00:26\n",
      "     -- ------------------------------------ 10.0/184.0 MB 6.9 MB/s eta 0:00:26\n",
      "     -- ------------------------------------ 10.3/184.0 MB 7.1 MB/s eta 0:00:25\n",
      "     -- ------------------------------------ 10.6/184.0 MB 7.1 MB/s eta 0:00:25\n",
      "     -- ------------------------------------ 10.9/184.0 MB 7.2 MB/s eta 0:00:25\n",
      "     -- ------------------------------------ 11.4/184.0 MB 7.3 MB/s eta 0:00:24\n",
      "     -- ------------------------------------ 11.7/184.0 MB 7.4 MB/s eta 0:00:24\n",
      "     -- ------------------------------------ 12.1/184.0 MB 7.4 MB/s eta 0:00:24\n",
      "     -- ------------------------------------ 12.3/184.0 MB 7.2 MB/s eta 0:00:24\n",
      "     -- ------------------------------------ 12.7/184.0 MB 7.4 MB/s eta 0:00:24\n",
      "     -- ------------------------------------ 13.1/184.0 MB 7.4 MB/s eta 0:00:24\n",
      "     -- ------------------------------------ 13.4/184.0 MB 7.4 MB/s eta 0:00:24\n",
      "     -- ------------------------------------ 13.9/184.0 MB 7.5 MB/s eta 0:00:23\n",
      "     --- ----------------------------------- 14.2/184.0 MB 7.4 MB/s eta 0:00:24\n",
      "     --- ----------------------------------- 14.7/184.0 MB 7.4 MB/s eta 0:00:23\n",
      "     --- ----------------------------------- 15.0/184.0 MB 7.4 MB/s eta 0:00:23\n",
      "     --- ----------------------------------- 15.5/184.0 MB 7.4 MB/s eta 0:00:23\n",
      "     --- ----------------------------------- 15.8/184.0 MB 7.5 MB/s eta 0:00:23\n",
      "     --- ----------------------------------- 16.2/184.0 MB 7.6 MB/s eta 0:00:23\n",
      "     --- ----------------------------------- 16.5/184.0 MB 7.4 MB/s eta 0:00:23\n",
      "     --- ----------------------------------- 16.9/184.0 MB 7.4 MB/s eta 0:00:23\n",
      "     --- ----------------------------------- 17.3/184.0 MB 7.6 MB/s eta 0:00:22\n",
      "     --- ----------------------------------- 17.7/184.0 MB 7.6 MB/s eta 0:00:22\n",
      "     --- ----------------------------------- 18.1/184.0 MB 7.6 MB/s eta 0:00:22\n",
      "     --- ----------------------------------- 18.5/184.0 MB 7.5 MB/s eta 0:00:23\n",
      "     ---- ---------------------------------- 18.9/184.0 MB 7.6 MB/s eta 0:00:22\n",
      "     ---- ---------------------------------- 19.2/184.0 MB 7.6 MB/s eta 0:00:22\n",
      "     ---- ---------------------------------- 19.6/184.0 MB 7.6 MB/s eta 0:00:22\n",
      "     ---- ---------------------------------- 19.9/184.0 MB 7.6 MB/s eta 0:00:22\n",
      "     ---- ---------------------------------- 20.3/184.0 MB 7.6 MB/s eta 0:00:22\n",
      "     ---- ---------------------------------- 20.7/184.0 MB 7.7 MB/s eta 0:00:22\n",
      "     ---- ---------------------------------- 21.1/184.0 MB 7.9 MB/s eta 0:00:21\n",
      "     ---- ---------------------------------- 21.4/184.0 MB 7.8 MB/s eta 0:00:21\n",
      "     ---- ---------------------------------- 21.8/184.0 MB 7.9 MB/s eta 0:00:21\n",
      "     ---- ---------------------------------- 22.2/184.0 MB 7.8 MB/s eta 0:00:21\n",
      "     ---- ---------------------------------- 22.7/184.0 MB 7.9 MB/s eta 0:00:21\n",
      "     ---- ---------------------------------- 23.0/184.0 MB 7.9 MB/s eta 0:00:21\n",
      "     ---- ---------------------------------- 23.3/184.0 MB 7.8 MB/s eta 0:00:21\n",
      "     ----- --------------------------------- 23.7/184.0 MB 7.9 MB/s eta 0:00:21\n",
      "     ----- --------------------------------- 24.1/184.0 MB 7.9 MB/s eta 0:00:21\n",
      "     ----- --------------------------------- 24.5/184.0 MB 8.0 MB/s eta 0:00:20\n",
      "     ----- --------------------------------- 24.9/184.0 MB 7.9 MB/s eta 0:00:21\n",
      "     ----- --------------------------------- 25.3/184.0 MB 7.9 MB/s eta 0:00:21\n",
      "     ----- --------------------------------- 25.7/184.0 MB 8.0 MB/s eta 0:00:20\n",
      "     ----- --------------------------------- 26.1/184.0 MB 7.9 MB/s eta 0:00:21\n",
      "     ----- --------------------------------- 26.5/184.0 MB 8.0 MB/s eta 0:00:20\n",
      "     ----- --------------------------------- 26.9/184.0 MB 8.0 MB/s eta 0:00:20\n",
      "     ----- --------------------------------- 27.3/184.0 MB 7.9 MB/s eta 0:00:20\n",
      "     ----- --------------------------------- 27.6/184.0 MB 7.9 MB/s eta 0:00:20\n",
      "     ----- --------------------------------- 27.9/184.0 MB 7.9 MB/s eta 0:00:20\n",
      "     ----- --------------------------------- 28.3/184.0 MB 7.8 MB/s eta 0:00:20\n",
      "     ------ -------------------------------- 28.7/184.0 MB 7.9 MB/s eta 0:00:20\n",
      "     ------ -------------------------------- 29.1/184.0 MB 7.9 MB/s eta 0:00:20\n",
      "     ------ -------------------------------- 29.4/184.0 MB 7.9 MB/s eta 0:00:20\n",
      "     ------ -------------------------------- 29.8/184.0 MB 7.9 MB/s eta 0:00:20\n",
      "     ------ -------------------------------- 30.2/184.0 MB 7.9 MB/s eta 0:00:20\n",
      "     ------ -------------------------------- 30.6/184.0 MB 7.9 MB/s eta 0:00:20\n",
      "     ------ -------------------------------- 30.9/184.0 MB 8.0 MB/s eta 0:00:20\n",
      "     ------ -------------------------------- 31.3/184.0 MB 7.9 MB/s eta 0:00:20\n",
      "     ------ -------------------------------- 31.7/184.0 MB 8.0 MB/s eta 0:00:20\n",
      "     ------ -------------------------------- 32.0/184.0 MB 7.9 MB/s eta 0:00:20\n",
      "     ------ -------------------------------- 32.4/184.0 MB 8.0 MB/s eta 0:00:19\n",
      "     ------ -------------------------------- 32.8/184.0 MB 8.0 MB/s eta 0:00:19\n",
      "     ------- ------------------------------- 33.1/184.0 MB 8.0 MB/s eta 0:00:19\n",
      "     ------- ------------------------------- 33.5/184.0 MB 8.0 MB/s eta 0:00:19\n",
      "     ------- ------------------------------- 34.0/184.0 MB 7.9 MB/s eta 0:00:20\n",
      "     ------- ------------------------------- 34.3/184.0 MB 7.8 MB/s eta 0:00:20\n",
      "     ------- ------------------------------- 34.6/184.0 MB 8.0 MB/s eta 0:00:19\n",
      "     ------- ------------------------------- 35.1/184.0 MB 8.0 MB/s eta 0:00:19\n",
      "     ------- ------------------------------- 35.3/184.0 MB 7.9 MB/s eta 0:00:19\n",
      "     ------- ------------------------------- 35.5/184.0 MB 7.8 MB/s eta 0:00:20\n",
      "     ------- ------------------------------- 35.6/184.0 MB 7.4 MB/s eta 0:00:20\n",
      "     ------- ------------------------------- 35.8/184.0 MB 7.4 MB/s eta 0:00:20\n",
      "     ------- ------------------------------- 36.0/184.0 MB 7.3 MB/s eta 0:00:21\n",
      "     ------- ------------------------------- 36.1/184.0 MB 7.2 MB/s eta 0:00:21\n",
      "     ------- ------------------------------- 36.3/184.0 MB 7.0 MB/s eta 0:00:21\n",
      "     ------- ------------------------------- 36.6/184.0 MB 6.8 MB/s eta 0:00:22\n",
      "     ------- ------------------------------- 36.7/184.0 MB 6.7 MB/s eta 0:00:22\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     ------- ------------------------------- 36.9/184.0 MB 6.6 MB/s eta 0:00:23\n",
      "     -------- ------------------------------ 39.0/184.0 MB 3.0 MB/s eta 0:00:49\n",
      "     -------- ------------------------------ 39.0/184.0 MB 2.9 MB/s eta 0:00:50\n",
      "     -------- ------------------------------ 39.0/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.1/184.0 MB 2.9 MB/s eta 0:00:51\n",
      "     -------- ------------------------------ 39.8/184.0 MB 2.0 MB/s eta 0:01:12\n",
      "     -------- ------------------------------ 39.8/184.0 MB 2.0 MB/s eta 0:01:12\n",
      "     -------- ------------------------------ 39.8/184.0 MB 2.0 MB/s eta 0:01:13\n",
      "     -------- ------------------------------ 39.9/184.0 MB 2.0 MB/s eta 0:01:14\n",
      "     -------- ------------------------------ 39.9/184.0 MB 1.9 MB/s eta 0:01:14\n",
      "     -------- ------------------------------ 40.0/184.0 MB 1.9 MB/s eta 0:01:15\n",
      "     -------- ------------------------------ 40.0/184.0 MB 1.9 MB/s eta 0:01:15\n",
      "     -------- ------------------------------ 40.1/184.0 MB 1.9 MB/s eta 0:01:16\n",
      "     -------- ------------------------------ 40.1/184.0 MB 1.9 MB/s eta 0:01:17\n",
      "     -------- ------------------------------ 40.1/184.0 MB 1.9 MB/s eta 0:01:17\n",
      "     -------- ------------------------------ 40.2/184.0 MB 1.9 MB/s eta 0:01:18\n",
      "     -------- ------------------------------ 40.2/184.0 MB 1.8 MB/s eta 0:01:18\n",
      "     -------- ------------------------------ 40.3/184.0 MB 1.8 MB/s eta 0:01:19\n",
      "     -------- ------------------------------ 40.3/184.0 MB 1.8 MB/s eta 0:01:20\n",
      "     -------- ------------------------------ 40.4/184.0 MB 1.8 MB/s eta 0:01:20\n",
      "     -------- ------------------------------ 40.4/184.0 MB 1.8 MB/s eta 0:01:21\n",
      "     -------- ------------------------------ 40.5/184.0 MB 1.8 MB/s eta 0:01:21\n",
      "     -------- ------------------------------ 40.6/184.0 MB 1.8 MB/s eta 0:01:22\n",
      "     -------- ------------------------------ 40.7/184.0 MB 1.8 MB/s eta 0:01:22\n",
      "     -------- ------------------------------ 40.8/184.0 MB 1.7 MB/s eta 0:01:23\n",
      "     -------- ------------------------------ 40.9/184.0 MB 1.7 MB/s eta 0:01:23\n",
      "     -------- ------------------------------ 41.0/184.0 MB 1.7 MB/s eta 0:01:23\n",
      "     -------- ------------------------------ 41.1/184.0 MB 1.7 MB/s eta 0:01:23\n",
      "     -------- ------------------------------ 41.3/184.0 MB 1.7 MB/s eta 0:01:24\n",
      "     -------- ------------------------------ 41.5/184.0 MB 1.7 MB/s eta 0:01:24\n",
      "     -------- ------------------------------ 41.6/184.0 MB 1.7 MB/s eta 0:01:24\n",
      "     -------- ------------------------------ 41.7/184.0 MB 1.7 MB/s eta 0:01:25\n",
      "     -------- ------------------------------ 41.9/184.0 MB 1.7 MB/s eta 0:01:25\n",
      "     -------- ------------------------------ 42.1/184.0 MB 1.7 MB/s eta 0:01:25\n",
      "     -------- ------------------------------ 42.3/184.0 MB 1.7 MB/s eta 0:01:25\n",
      "     --------- ----------------------------- 42.5/184.0 MB 1.7 MB/s eta 0:01:25\n",
      "     --------- ----------------------------- 42.7/184.0 MB 1.7 MB/s eta 0:01:25\n",
      "     --------- ----------------------------- 42.8/184.0 MB 1.7 MB/s eta 0:01:25\n",
      "     --------- ----------------------------- 43.0/184.0 MB 1.7 MB/s eta 0:01:26\n",
      "     --------- ----------------------------- 43.2/184.0 MB 1.6 MB/s eta 0:01:26\n",
      "     --------- ----------------------------- 43.4/184.0 MB 1.6 MB/s eta 0:01:26\n",
      "     --------- ----------------------------- 43.7/184.0 MB 1.6 MB/s eta 0:01:26\n",
      "     --------- ----------------------------- 43.9/184.0 MB 1.6 MB/s eta 0:01:26\n",
      "     --------- ----------------------------- 44.0/184.0 MB 1.6 MB/s eta 0:01:26\n",
      "     --------- ----------------------------- 44.2/184.0 MB 1.6 MB/s eta 0:01:26\n",
      "     --------- ----------------------------- 44.4/184.0 MB 1.6 MB/s eta 0:01:27\n",
      "     --------- ----------------------------- 44.5/184.0 MB 1.6 MB/s eta 0:01:27\n",
      "     --------- ----------------------------- 44.7/184.0 MB 1.6 MB/s eta 0:01:27\n",
      "     --------- ----------------------------- 45.0/184.0 MB 1.6 MB/s eta 0:01:27\n",
      "     --------- ----------------------------- 45.1/184.0 MB 1.6 MB/s eta 0:01:27\n",
      "     --------- ----------------------------- 45.3/184.0 MB 1.6 MB/s eta 0:01:28\n",
      "     --------- ----------------------------- 45.5/184.0 MB 1.6 MB/s eta 0:01:28\n",
      "     --------- ----------------------------- 45.7/184.0 MB 1.6 MB/s eta 0:01:28\n",
      "     --------- ----------------------------- 45.9/184.0 MB 1.6 MB/s eta 0:01:27\n",
      "     --------- ----------------------------- 46.1/184.0 MB 1.6 MB/s eta 0:01:27\n",
      "     --------- ----------------------------- 46.4/184.0 MB 1.6 MB/s eta 0:01:26\n",
      "     --------- ----------------------------- 46.6/184.0 MB 1.6 MB/s eta 0:01:26\n",
      "     --------- ----------------------------- 46.7/184.0 MB 1.6 MB/s eta 0:01:26\n",
      "     --------- ----------------------------- 46.9/184.0 MB 1.6 MB/s eta 0:01:26\n",
      "     --------- ----------------------------- 47.1/184.0 MB 1.6 MB/s eta 0:01:26\n",
      "     ---------- ---------------------------- 47.3/184.0 MB 2.4 MB/s eta 0:00:57\n",
      "     ---------- ---------------------------- 47.6/184.0 MB 2.4 MB/s eta 0:00:57\n",
      "     ---------- ---------------------------- 47.8/184.0 MB 2.4 MB/s eta 0:00:58\n",
      "     ---------- ---------------------------- 48.0/184.0 MB 2.3 MB/s eta 0:00:58\n",
      "     ---------- ---------------------------- 48.3/184.0 MB 2.3 MB/s eta 0:00:59\n",
      "     ---------- ---------------------------- 48.6/184.0 MB 2.3 MB/s eta 0:01:00\n",
      "     ---------- ---------------------------- 48.8/184.0 MB 2.2 MB/s eta 0:01:01\n",
      "     ---------- ---------------------------- 49.0/184.0 MB 2.2 MB/s eta 0:01:01\n",
      "     ---------- ---------------------------- 49.2/184.0 MB 2.2 MB/s eta 0:01:01\n",
      "     ---------- ---------------------------- 49.4/184.0 MB 3.5 MB/s eta 0:00:39\n",
      "     ---------- ---------------------------- 49.7/184.0 MB 3.4 MB/s eta 0:00:40\n",
      "     ---------- ---------------------------- 49.9/184.0 MB 3.3 MB/s eta 0:00:41\n",
      "     ---------- ---------------------------- 50.1/184.0 MB 3.4 MB/s eta 0:00:40\n",
      "     ---------- ---------------------------- 50.3/184.0 MB 3.6 MB/s eta 0:00:37\n",
      "     ---------- ---------------------------- 50.4/184.0 MB 3.8 MB/s eta 0:00:36\n",
      "     ---------- ---------------------------- 50.6/184.0 MB 4.1 MB/s eta 0:00:33\n",
      "     ---------- ---------------------------- 50.8/184.0 MB 4.1 MB/s eta 0:00:33\n",
      "     ---------- ---------------------------- 51.1/184.0 MB 4.3 MB/s eta 0:00:32\n",
      "     ---------- ---------------------------- 51.2/184.0 MB 4.3 MB/s eta 0:00:32\n",
      "     ---------- ---------------------------- 51.5/184.0 MB 4.3 MB/s eta 0:00:31\n",
      "     ---------- ---------------------------- 51.7/184.0 MB 4.3 MB/s eta 0:00:31\n",
      "     ---------- ---------------------------- 51.8/184.0 MB 4.3 MB/s eta 0:00:31\n",
      "     ----------- --------------------------- 52.1/184.0 MB 4.4 MB/s eta 0:00:31\n",
      "     ----------- --------------------------- 52.4/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 52.5/184.0 MB 4.4 MB/s eta 0:00:30\n",
      "     ----------- --------------------------- 53.7/184.0 MB 3.3 MB/s eta 0:00:41\n",
      "     ----------- --------------------------- 53.7/184.0 MB 3.3 MB/s eta 0:00:41\n",
      "     ----------- --------------------------- 53.7/184.0 MB 3.3 MB/s eta 0:00:41\n",
      "     ----------- --------------------------- 53.7/184.0 MB 3.3 MB/s eta 0:00:41\n",
      "     ----------- --------------------------- 53.7/184.0 MB 3.3 MB/s eta 0:00:41\n",
      "     ----------- --------------------------- 53.7/184.0 MB 3.3 MB/s eta 0:00:41\n",
      "     ----------- --------------------------- 53.7/184.0 MB 3.3 MB/s eta 0:00:41\n",
      "     ----------- --------------------------- 53.7/184.0 MB 3.3 MB/s eta 0:00:41\n",
      "     ----------- --------------------------- 53.7/184.0 MB 3.3 MB/s eta 0:00:41\n",
      "     ----------- --------------------------- 53.7/184.0 MB 3.3 MB/s eta 0:00:41\n",
      "     ----------- --------------------------- 53.7/184.0 MB 3.3 MB/s eta 0:00:41\n",
      "     ----------- --------------------------- 53.7/184.0 MB 3.3 MB/s eta 0:00:41\n",
      "     ----------- --------------------------- 53.7/184.0 MB 2.8 MB/s eta 0:00:47\n",
      "     ----------- --------------------------- 53.7/184.0 MB 2.8 MB/s eta 0:00:47\n",
      "     ----------- --------------------------- 53.7/184.0 MB 2.8 MB/s eta 0:00:47\n",
      "     ----------- --------------------------- 53.7/184.0 MB 2.8 MB/s eta 0:00:47\n",
      "     ----------- --------------------------- 53.7/184.0 MB 2.8 MB/s eta 0:00:47\n",
      "     ----------- --------------------------- 53.7/184.0 MB 2.8 MB/s eta 0:00:47\n",
      "     ----------- --------------------------- 53.7/184.0 MB 2.8 MB/s eta 0:00:47\n",
      "     ----------- --------------------------- 53.7/184.0 MB 2.8 MB/s eta 0:00:47\n",
      "     ----------- --------------------------- 53.7/184.0 MB 2.8 MB/s eta 0:00:47\n",
      "     ----------- --------------------------- 53.7/184.0 MB 2.5 MB/s eta 0:00:53\n",
      "     ----------- --------------------------- 53.7/184.0 MB 2.5 MB/s eta 0:00:53\n",
      "     ----------- --------------------------- 53.7/184.0 MB 2.5 MB/s eta 0:00:53\n",
      "     ----------- --------------------------- 53.7/184.0 MB 2.4 MB/s eta 0:00:54\n",
      "     ----------- --------------------------- 53.7/184.0 MB 2.4 MB/s eta 0:00:55\n",
      "     ----------- --------------------------- 53.8/184.0 MB 2.4 MB/s eta 0:00:56\n",
      "     ----------- --------------------------- 53.9/184.0 MB 2.3 MB/s eta 0:00:56\n",
      "     ----------- --------------------------- 53.9/184.0 MB 2.3 MB/s eta 0:00:56\n",
      "     ----------- --------------------------- 54.0/184.0 MB 2.3 MB/s eta 0:00:57\n",
      "     ----------- --------------------------- 54.1/184.0 MB 2.3 MB/s eta 0:00:57\n",
      "     ----------- --------------------------- 54.2/184.0 MB 2.3 MB/s eta 0:00:57\n",
      "     ----------- --------------------------- 54.3/184.0 MB 2.3 MB/s eta 0:00:57\n",
      "     ----------- --------------------------- 54.4/184.0 MB 2.3 MB/s eta 0:00:58\n",
      "     ----------- --------------------------- 54.5/184.0 MB 2.3 MB/s eta 0:00:58\n",
      "     ----------- --------------------------- 54.5/184.0 MB 2.2 MB/s eta 0:00:58\n",
      "     ----------- --------------------------- 54.6/184.0 MB 2.2 MB/s eta 0:00:58\n",
      "     ----------- --------------------------- 54.7/184.0 MB 2.2 MB/s eta 0:00:59\n",
      "     ----------- --------------------------- 54.8/184.0 MB 2.2 MB/s eta 0:00:59\n",
      "     ----------- --------------------------- 54.9/184.0 MB 2.2 MB/s eta 0:00:59\n",
      "     ----------- --------------------------- 55.1/184.0 MB 2.2 MB/s eta 0:00:59\n",
      "     ----------- --------------------------- 55.3/184.0 MB 2.2 MB/s eta 0:00:58\n",
      "     ----------- --------------------------- 55.7/184.0 MB 2.2 MB/s eta 0:00:58\n",
      "     ----------- --------------------------- 55.9/184.0 MB 2.2 MB/s eta 0:00:58\n",
      "     ----------- --------------------------- 56.2/184.0 MB 2.2 MB/s eta 0:00:58\n",
      "     ----------- --------------------------- 56.4/184.0 MB 2.2 MB/s eta 0:00:57\n",
      "     ------------ -------------------------- 56.8/184.0 MB 2.3 MB/s eta 0:00:57\n",
      "     ------------ -------------------------- 57.1/184.0 MB 2.3 MB/s eta 0:00:57\n",
      "     ------------ -------------------------- 57.4/184.0 MB 2.3 MB/s eta 0:00:56\n",
      "     ------------ -------------------------- 57.8/184.0 MB 2.3 MB/s eta 0:00:56\n",
      "     ------------ -------------------------- 58.3/184.0 MB 2.3 MB/s eta 0:00:55\n",
      "     ------------ -------------------------- 58.6/184.0 MB 2.3 MB/s eta 0:00:55\n",
      "     ------------ -------------------------- 58.8/184.0 MB 2.3 MB/s eta 0:00:55\n",
      "     ------------ -------------------------- 59.4/184.0 MB 2.4 MB/s eta 0:00:53\n",
      "     ------------ -------------------------- 59.8/184.0 MB 2.4 MB/s eta 0:00:53\n",
      "     ------------ -------------------------- 60.0/184.0 MB 2.4 MB/s eta 0:00:53\n",
      "     ------------ -------------------------- 60.2/184.0 MB 2.4 MB/s eta 0:00:53\n",
      "     ------------ -------------------------- 60.5/184.0 MB 2.4 MB/s eta 0:00:52\n",
      "     ------------ -------------------------- 60.8/184.0 MB 2.4 MB/s eta 0:00:51\n",
      "     ------------ -------------------------- 61.2/184.0 MB 2.4 MB/s eta 0:00:51\n",
      "     ------------- ------------------------- 61.6/184.0 MB 2.5 MB/s eta 0:00:50\n",
      "     ------------- ------------------------- 62.0/184.0 MB 2.5 MB/s eta 0:00:50\n",
      "     ------------- ------------------------- 62.4/184.0 MB 2.5 MB/s eta 0:00:49\n",
      "     ------------- ------------------------- 62.7/184.0 MB 2.5 MB/s eta 0:00:49\n",
      "     ------------- ------------------------- 63.1/184.0 MB 3.4 MB/s eta 0:00:36\n",
      "     ------------- ------------------------- 63.4/184.0 MB 3.3 MB/s eta 0:00:37\n",
      "     ------------- ------------------------- 63.8/184.0 MB 3.3 MB/s eta 0:00:37\n",
      "     ------------- ------------------------- 64.1/184.0 MB 5.5 MB/s eta 0:00:22\n",
      "     ------------- ------------------------- 64.5/184.0 MB 5.9 MB/s eta 0:00:21\n",
      "     ------------- ------------------------- 64.9/184.0 MB 6.5 MB/s eta 0:00:19\n",
      "     ------------- ------------------------- 65.2/184.0 MB 7.0 MB/s eta 0:00:18\n",
      "     ------------- ------------------------- 65.6/184.0 MB 7.1 MB/s eta 0:00:17\n",
      "     ------------- ------------------------- 66.0/184.0 MB 7.2 MB/s eta 0:00:17\n",
      "     -------------- ------------------------ 66.3/184.0 MB 7.2 MB/s eta 0:00:17\n",
      "     -------------- ------------------------ 66.7/184.0 MB 7.4 MB/s eta 0:00:16\n",
      "     -------------- ------------------------ 67.1/184.0 MB 7.4 MB/s eta 0:00:16\n",
      "     -------------- ------------------------ 67.4/184.0 MB 7.4 MB/s eta 0:00:16\n",
      "     -------------- ------------------------ 67.7/184.0 MB 7.4 MB/s eta 0:00:16\n",
      "     -------------- ------------------------ 68.1/184.0 MB 7.5 MB/s eta 0:00:16\n",
      "     -------------- ------------------------ 68.4/184.0 MB 7.4 MB/s eta 0:00:16\n",
      "     -------------- ------------------------ 68.6/184.0 MB 7.4 MB/s eta 0:00:16\n",
      "     -------------- ------------------------ 69.0/184.0 MB 7.4 MB/s eta 0:00:16\n",
      "     -------------- ------------------------ 69.4/184.0 MB 7.5 MB/s eta 0:00:16\n",
      "     -------------- ------------------------ 69.8/184.0 MB 7.4 MB/s eta 0:00:16\n",
      "     -------------- ------------------------ 70.1/184.0 MB 7.5 MB/s eta 0:00:16\n",
      "     -------------- ------------------------ 70.4/184.0 MB 7.8 MB/s eta 0:00:15\n",
      "     --------------- ----------------------- 70.8/184.0 MB 7.8 MB/s eta 0:00:15\n",
      "     --------------- ----------------------- 71.1/184.0 MB 7.7 MB/s eta 0:00:15\n",
      "     --------------- ----------------------- 71.5/184.0 MB 7.7 MB/s eta 0:00:15\n",
      "     --------------- ----------------------- 71.8/184.0 MB 7.7 MB/s eta 0:00:15\n",
      "     --------------- ----------------------- 72.2/184.0 MB 7.7 MB/s eta 0:00:15\n",
      "     --------------- ----------------------- 72.6/184.0 MB 7.7 MB/s eta 0:00:15\n",
      "     --------------- ----------------------- 72.8/184.0 MB 7.6 MB/s eta 0:00:15\n",
      "     --------------- ----------------------- 73.2/184.0 MB 7.6 MB/s eta 0:00:15\n",
      "     --------------- ----------------------- 73.5/184.0 MB 7.6 MB/s eta 0:00:15\n",
      "     --------------- ----------------------- 73.9/184.0 MB 7.6 MB/s eta 0:00:15\n",
      "     --------------- ----------------------- 74.2/184.0 MB 7.6 MB/s eta 0:00:15\n",
      "     --------------- ----------------------- 74.7/184.0 MB 7.6 MB/s eta 0:00:15\n",
      "     --------------- ----------------------- 75.0/184.0 MB 7.6 MB/s eta 0:00:15\n",
      "     --------------- ----------------------- 75.3/184.0 MB 7.5 MB/s eta 0:00:15\n",
      "     ---------------- ---------------------- 75.8/184.0 MB 7.6 MB/s eta 0:00:15\n",
      "     ---------------- ---------------------- 76.1/184.0 MB 7.5 MB/s eta 0:00:15\n",
      "     ---------------- ---------------------- 76.4/184.0 MB 7.5 MB/s eta 0:00:15\n",
      "     ---------------- ---------------------- 76.5/184.0 MB 7.4 MB/s eta 0:00:15\n",
      "     ---------------- ---------------------- 76.7/184.0 MB 7.2 MB/s eta 0:00:15\n",
      "     ---------------- ---------------------- 76.8/184.0 MB 7.0 MB/s eta 0:00:16\n",
      "     ---------------- ---------------------- 77.1/184.0 MB 7.0 MB/s eta 0:00:16\n",
      "     ---------------- ---------------------- 77.2/184.0 MB 7.0 MB/s eta 0:00:16\n",
      "     ---------------- ---------------------- 77.4/184.0 MB 6.8 MB/s eta 0:00:16\n",
      "     ---------------- ---------------------- 77.5/184.0 MB 6.7 MB/s eta 0:00:16\n",
      "     ---------------- ---------------------- 77.8/184.0 MB 6.6 MB/s eta 0:00:17\n",
      "     ---------------- ---------------------- 78.1/184.0 MB 6.5 MB/s eta 0:00:17\n",
      "     ---------------- ---------------------- 78.3/184.0 MB 6.5 MB/s eta 0:00:17\n",
      "     ---------------- ---------------------- 78.6/184.0 MB 6.5 MB/s eta 0:00:17\n",
      "     ---------------- ---------------------- 78.7/184.0 MB 6.4 MB/s eta 0:00:17\n",
      "     ---------------- ---------------------- 78.9/184.0 MB 6.4 MB/s eta 0:00:17\n",
      "     ---------------- ---------------------- 79.1/184.0 MB 6.2 MB/s eta 0:00:17\n",
      "     ---------------- ---------------------- 79.3/184.0 MB 6.2 MB/s eta 0:00:17\n",
      "     ---------------- ---------------------- 79.6/184.0 MB 6.1 MB/s eta 0:00:18\n",
      "     ---------------- ---------------------- 79.9/184.0 MB 6.1 MB/s eta 0:00:18\n",
      "     ---------------- ---------------------- 80.1/184.0 MB 6.0 MB/s eta 0:00:18\n",
      "     ----------------- --------------------- 80.4/184.0 MB 6.0 MB/s eta 0:00:18\n",
      "     ----------------- --------------------- 80.7/184.0 MB 6.0 MB/s eta 0:00:18\n",
      "     ----------------- --------------------- 81.0/184.0 MB 5.9 MB/s eta 0:00:18\n",
      "     ----------------- --------------------- 81.3/184.0 MB 5.9 MB/s eta 0:00:18\n",
      "     ----------------- --------------------- 81.7/184.0 MB 5.9 MB/s eta 0:00:18\n",
      "     ----------------- --------------------- 82.0/184.0 MB 5.9 MB/s eta 0:00:18\n",
      "     ----------------- --------------------- 82.3/184.0 MB 5.8 MB/s eta 0:00:18\n",
      "     ----------------- --------------------- 82.6/184.0 MB 5.9 MB/s eta 0:00:18\n",
      "     ----------------- --------------------- 83.1/184.0 MB 6.0 MB/s eta 0:00:17\n",
      "     ----------------- --------------------- 83.4/184.0 MB 6.0 MB/s eta 0:00:17\n",
      "     ----------------- --------------------- 83.7/184.0 MB 6.0 MB/s eta 0:00:17\n",
      "     ----------------- --------------------- 84.1/184.0 MB 6.0 MB/s eta 0:00:17\n",
      "     ----------------- --------------------- 84.4/184.0 MB 5.9 MB/s eta 0:00:17\n",
      "     ----------------- --------------------- 84.8/184.0 MB 6.0 MB/s eta 0:00:17\n",
      "     ------------------ -------------------- 85.1/184.0 MB 5.9 MB/s eta 0:00:17\n",
      "     ------------------ -------------------- 85.5/184.0 MB 5.9 MB/s eta 0:00:17\n",
      "     ------------------ -------------------- 85.8/184.0 MB 6.0 MB/s eta 0:00:17\n",
      "     ------------------ -------------------- 86.2/184.0 MB 5.9 MB/s eta 0:00:17\n",
      "     ------------------ -------------------- 86.6/184.0 MB 6.0 MB/s eta 0:00:17\n",
      "     ------------------ -------------------- 87.0/184.0 MB 6.2 MB/s eta 0:00:16\n",
      "     ------------------ -------------------- 87.3/184.0 MB 6.4 MB/s eta 0:00:16\n",
      "     ------------------ -------------------- 87.7/184.0 MB 6.7 MB/s eta 0:00:15\n",
      "     ------------------ -------------------- 88.0/184.0 MB 6.8 MB/s eta 0:00:15\n",
      "     ------------------ -------------------- 88.3/184.0 MB 6.8 MB/s eta 0:00:15\n",
      "     ------------------ -------------------- 88.7/184.0 MB 6.9 MB/s eta 0:00:14\n",
      "     ------------------ -------------------- 89.0/184.0 MB 7.0 MB/s eta 0:00:14\n",
      "     ------------------ -------------------- 89.4/184.0 MB 7.3 MB/s eta 0:00:13\n",
      "     ------------------- ------------------- 89.8/184.0 MB 7.4 MB/s eta 0:00:13\n",
      "     ------------------- ------------------- 90.1/184.0 MB 7.5 MB/s eta 0:00:13\n",
      "     ------------------- ------------------- 90.5/184.0 MB 7.7 MB/s eta 0:00:13\n",
      "     ------------------- ------------------- 90.8/184.0 MB 7.7 MB/s eta 0:00:13\n",
      "     ------------------- ------------------- 91.2/184.0 MB 7.7 MB/s eta 0:00:13\n",
      "     ------------------- ------------------- 91.6/184.0 MB 7.7 MB/s eta 0:00:12\n",
      "     ------------------- ------------------- 91.9/184.0 MB 7.6 MB/s eta 0:00:13\n",
      "     ------------------- ------------------- 92.2/184.0 MB 7.7 MB/s eta 0:00:12\n",
      "     ------------------- ------------------- 92.7/184.0 MB 7.7 MB/s eta 0:00:12\n",
      "     ------------------- ------------------- 93.0/184.0 MB 7.7 MB/s eta 0:00:12\n",
      "     ------------------- ------------------- 93.4/184.0 MB 7.7 MB/s eta 0:00:12\n",
      "     ------------------- ------------------- 93.7/184.0 MB 7.8 MB/s eta 0:00:12\n",
      "     ------------------- ------------------- 94.0/184.0 MB 7.8 MB/s eta 0:00:12\n",
      "     -------------------- ------------------ 94.4/184.0 MB 7.9 MB/s eta 0:00:12\n",
      "     -------------------- ------------------ 94.7/184.0 MB 7.7 MB/s eta 0:00:12\n",
      "     -------------------- ------------------ 95.1/184.0 MB 7.7 MB/s eta 0:00:12\n",
      "     -------------------- ------------------ 95.4/184.0 MB 7.7 MB/s eta 0:00:12\n",
      "     -------------------- ------------------ 95.8/184.0 MB 7.7 MB/s eta 0:00:12\n",
      "     -------------------- ------------------ 96.2/184.0 MB 7.7 MB/s eta 0:00:12\n",
      "     -------------------- ------------------ 96.6/184.0 MB 7.7 MB/s eta 0:00:12\n",
      "     -------------------- ------------------ 96.9/184.0 MB 7.8 MB/s eta 0:00:12\n",
      "     -------------------- ------------------ 97.3/184.0 MB 7.8 MB/s eta 0:00:12\n",
      "     -------------------- ------------------ 97.6/184.0 MB 7.7 MB/s eta 0:00:12\n",
      "     -------------------- ------------------ 98.0/184.0 MB 7.7 MB/s eta 0:00:12\n",
      "     -------------------- ------------------ 98.3/184.0 MB 7.7 MB/s eta 0:00:12\n",
      "     -------------------- ------------------ 98.7/184.0 MB 7.7 MB/s eta 0:00:12\n",
      "     -------------------- ------------------ 99.0/184.0 MB 7.7 MB/s eta 0:00:12\n",
      "     --------------------- ----------------- 99.4/184.0 MB 7.8 MB/s eta 0:00:11\n",
      "     --------------------- ----------------- 99.8/184.0 MB 7.8 MB/s eta 0:00:11\n",
      "     -------------------- ----------------- 100.1/184.0 MB 7.8 MB/s eta 0:00:11\n",
      "     -------------------- ----------------- 100.5/184.0 MB 7.8 MB/s eta 0:00:11\n",
      "     -------------------- ----------------- 100.9/184.0 MB 7.8 MB/s eta 0:00:11\n",
      "     -------------------- ----------------- 101.2/184.0 MB 7.8 MB/s eta 0:00:11\n",
      "     -------------------- ----------------- 101.6/184.0 MB 7.8 MB/s eta 0:00:11\n",
      "     --------------------- ---------------- 102.0/184.0 MB 7.8 MB/s eta 0:00:11\n",
      "     --------------------- ---------------- 102.3/184.0 MB 7.8 MB/s eta 0:00:11\n",
      "     --------------------- ---------------- 102.6/184.0 MB 7.8 MB/s eta 0:00:11\n",
      "     --------------------- ---------------- 103.0/184.0 MB 7.8 MB/s eta 0:00:11\n",
      "     --------------------- ---------------- 103.3/184.0 MB 7.7 MB/s eta 0:00:11\n",
      "     --------------------- ---------------- 103.6/184.0 MB 7.8 MB/s eta 0:00:11\n",
      "     --------------------- ---------------- 104.0/184.0 MB 7.7 MB/s eta 0:00:11\n",
      "     --------------------- ---------------- 104.4/184.0 MB 7.9 MB/s eta 0:00:11\n",
      "     --------------------- ---------------- 104.7/184.0 MB 7.8 MB/s eta 0:00:11\n",
      "     --------------------- ---------------- 105.0/184.0 MB 7.8 MB/s eta 0:00:11\n",
      "     --------------------- ---------------- 105.5/184.0 MB 7.9 MB/s eta 0:00:10\n",
      "     --------------------- ---------------- 105.9/184.0 MB 7.9 MB/s eta 0:00:10\n",
      "     --------------------- ---------------- 106.2/184.0 MB 7.8 MB/s eta 0:00:10\n",
      "     ---------------------- --------------- 106.6/184.0 MB 7.8 MB/s eta 0:00:10\n",
      "     ---------------------- --------------- 106.9/184.0 MB 7.8 MB/s eta 0:00:10\n",
      "     ---------------------- --------------- 107.3/184.0 MB 7.8 MB/s eta 0:00:10\n",
      "     ---------------------- --------------- 107.6/184.0 MB 7.9 MB/s eta 0:00:10\n",
      "     ---------------------- --------------- 107.9/184.0 MB 7.9 MB/s eta 0:00:10\n",
      "     ---------------------- --------------- 108.2/184.0 MB 7.8 MB/s eta 0:00:10\n",
      "     ---------------------- --------------- 108.6/184.0 MB 7.8 MB/s eta 0:00:10\n",
      "     ---------------------- --------------- 109.0/184.0 MB 7.8 MB/s eta 0:00:10\n",
      "     ---------------------- --------------- 109.3/184.0 MB 7.8 MB/s eta 0:00:10\n",
      "     ---------------------- --------------- 109.7/184.0 MB 7.8 MB/s eta 0:00:10\n",
      "     ---------------------- --------------- 110.1/184.0 MB 7.9 MB/s eta 0:00:10\n",
      "     ---------------------- --------------- 110.4/184.0 MB 7.8 MB/s eta 0:00:10\n",
      "     ---------------------- --------------- 110.8/184.0 MB 7.8 MB/s eta 0:00:10\n",
      "     ---------------------- --------------- 111.2/184.0 MB 7.8 MB/s eta 0:00:10\n",
      "     ----------------------- -------------- 111.5/184.0 MB 7.9 MB/s eta 0:00:10\n",
      "     ----------------------- -------------- 111.9/184.0 MB 7.8 MB/s eta 0:00:10\n",
      "     ----------------------- -------------- 112.2/184.0 MB 7.8 MB/s eta 0:00:10\n",
      "     ----------------------- -------------- 112.6/184.0 MB 7.8 MB/s eta 0:00:10\n",
      "     ----------------------- -------------- 112.9/184.0 MB 7.8 MB/s eta 0:00:10\n",
      "     ----------------------- -------------- 113.2/184.0 MB 7.8 MB/s eta 0:00:10\n",
      "     ----------------------- -------------- 113.6/184.0 MB 7.8 MB/s eta 0:00:10\n",
      "     ----------------------- -------------- 113.9/184.0 MB 7.8 MB/s eta 0:00:09\n",
      "     ----------------------- -------------- 114.3/184.0 MB 7.8 MB/s eta 0:00:09\n",
      "     ----------------------- -------------- 114.7/184.0 MB 7.8 MB/s eta 0:00:09\n",
      "     ----------------------- -------------- 115.0/184.0 MB 7.8 MB/s eta 0:00:09\n",
      "     ----------------------- -------------- 115.4/184.0 MB 7.7 MB/s eta 0:00:09\n",
      "     ----------------------- -------------- 115.7/184.0 MB 7.7 MB/s eta 0:00:09\n",
      "     ----------------------- -------------- 116.0/184.0 MB 7.6 MB/s eta 0:00:09\n",
      "     ------------------------ ------------- 116.4/184.0 MB 7.6 MB/s eta 0:00:09\n",
      "     ------------------------ ------------- 116.8/184.0 MB 7.7 MB/s eta 0:00:09\n",
      "     ------------------------ ------------- 117.1/184.0 MB 7.8 MB/s eta 0:00:09\n",
      "     ------------------------ ------------- 117.5/184.0 MB 7.7 MB/s eta 0:00:09\n",
      "     ------------------------ ------------- 117.9/184.0 MB 7.8 MB/s eta 0:00:09\n",
      "     ------------------------ ------------- 118.2/184.0 MB 7.8 MB/s eta 0:00:09\n",
      "     ------------------------ ------------- 118.5/184.0 MB 7.7 MB/s eta 0:00:09\n",
      "     ------------------------ ------------- 118.9/184.0 MB 7.8 MB/s eta 0:00:09\n",
      "     ------------------------ ------------- 119.2/184.0 MB 7.7 MB/s eta 0:00:09\n",
      "     ------------------------ ------------- 119.6/184.0 MB 7.8 MB/s eta 0:00:09\n",
      "     ------------------------ ------------- 120.0/184.0 MB 7.7 MB/s eta 0:00:09\n",
      "     ------------------------ ------------- 120.3/184.0 MB 7.7 MB/s eta 0:00:09\n",
      "     ------------------------ ------------- 120.6/184.0 MB 7.7 MB/s eta 0:00:09\n",
      "     ------------------------ ------------- 121.0/184.0 MB 7.7 MB/s eta 0:00:09\n",
      "     ------------------------- ------------ 121.4/184.0 MB 7.7 MB/s eta 0:00:09\n",
      "     ------------------------- ------------ 121.7/184.0 MB 7.7 MB/s eta 0:00:09\n",
      "     ------------------------- ------------ 122.1/184.0 MB 7.6 MB/s eta 0:00:09\n",
      "     ------------------------- ------------ 122.4/184.0 MB 7.6 MB/s eta 0:00:09\n",
      "     ------------------------- ------------ 122.7/184.0 MB 7.7 MB/s eta 0:00:08\n",
      "     ------------------------- ------------ 123.0/184.0 MB 7.6 MB/s eta 0:00:09\n",
      "     ------------------------- ------------ 123.0/184.0 MB 7.6 MB/s eta 0:00:09\n",
      "     ------------------------- ------------ 123.0/184.0 MB 7.6 MB/s eta 0:00:09\n",
      "     ------------------------- ------------ 123.0/184.0 MB 7.6 MB/s eta 0:00:09\n",
      "     ------------------------- ------------ 123.0/184.0 MB 7.6 MB/s eta 0:00:09\n",
      "     ------------------------- ------------ 123.0/184.0 MB 7.6 MB/s eta 0:00:09\n",
      "     ------------------------- ------------ 123.1/184.0 MB 6.4 MB/s eta 0:00:10\n",
      "     ------------------------- ------------ 125.2/184.0 MB 7.5 MB/s eta 0:00:08\n",
      "     ------------------------- ------------ 125.3/184.0 MB 7.4 MB/s eta 0:00:08\n",
      "     ------------------------- ------------ 125.4/184.0 MB 7.2 MB/s eta 0:00:09\n",
      "     ------------------------- ------------ 125.6/184.0 MB 7.0 MB/s eta 0:00:09\n",
      "     ------------------------- ------------ 125.8/184.0 MB 7.0 MB/s eta 0:00:09\n",
      "     -------------------------- ----------- 126.2/184.0 MB 7.0 MB/s eta 0:00:09\n",
      "     -------------------------- ----------- 126.6/184.0 MB 7.0 MB/s eta 0:00:09\n",
      "     -------------------------- ----------- 126.8/184.0 MB 6.9 MB/s eta 0:00:09\n",
      "     -------------------------- ----------- 126.9/184.0 MB 6.8 MB/s eta 0:00:09\n",
      "     -------------------------- ----------- 127.0/184.0 MB 6.6 MB/s eta 0:00:09\n",
      "     -------------------------- ----------- 127.2/184.0 MB 6.5 MB/s eta 0:00:09\n",
      "     -------------------------- ----------- 127.5/184.0 MB 6.5 MB/s eta 0:00:09\n",
      "     -------------------------- ----------- 127.9/184.0 MB 6.5 MB/s eta 0:00:09\n",
      "     -------------------------- ----------- 128.3/184.0 MB 6.5 MB/s eta 0:00:09\n",
      "     -------------------------- ----------- 128.7/184.0 MB 6.5 MB/s eta 0:00:09\n",
      "     -------------------------- ----------- 129.1/184.0 MB 6.5 MB/s eta 0:00:09\n",
      "     -------------------------- ----------- 129.4/184.0 MB 6.5 MB/s eta 0:00:09\n",
      "     -------------------------- ----------- 129.8/184.0 MB 6.5 MB/s eta 0:00:09\n",
      "     -------------------------- ----------- 130.2/184.0 MB 6.5 MB/s eta 0:00:09\n",
      "     -------------------------- ----------- 130.5/184.0 MB 6.5 MB/s eta 0:00:09\n",
      "     --------------------------- ---------- 130.8/184.0 MB 6.5 MB/s eta 0:00:09\n",
      "     --------------------------- ---------- 131.2/184.0 MB 6.5 MB/s eta 0:00:09\n",
      "     --------------------------- ---------- 131.6/184.0 MB 6.5 MB/s eta 0:00:09\n",
      "     --------------------------- ---------- 131.9/184.0 MB 6.5 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 132.3/184.0 MB 6.5 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 132.7/184.0 MB 6.5 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 133.0/184.0 MB 6.5 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 133.4/184.0 MB 7.9 MB/s eta 0:00:07\n",
      "     --------------------------- ---------- 133.7/184.0 MB 7.7 MB/s eta 0:00:07\n",
      "     --------------------------- ---------- 134.1/184.0 MB 7.4 MB/s eta 0:00:07\n",
      "     --------------------------- ---------- 134.5/184.0 MB 7.2 MB/s eta 0:00:07\n",
      "     --------------------------- ---------- 134.7/184.0 MB 7.0 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 134.7/184.0 MB 7.0 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 134.7/184.0 MB 7.0 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 134.7/184.0 MB 7.0 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 134.7/184.0 MB 7.0 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 134.7/184.0 MB 7.0 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 134.7/184.0 MB 7.0 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 134.7/184.0 MB 7.0 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 134.7/184.0 MB 7.0 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 134.7/184.0 MB 7.0 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 134.7/184.0 MB 7.0 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 134.7/184.0 MB 7.0 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 134.7/184.0 MB 7.0 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 134.7/184.0 MB 7.0 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 134.7/184.0 MB 7.0 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 134.7/184.0 MB 7.0 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 134.7/184.0 MB 7.0 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 134.7/184.0 MB 7.0 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 134.7/184.0 MB 7.0 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 134.7/184.0 MB 7.0 MB/s eta 0:00:08\n",
      "     --------------------------- ---------- 134.7/184.0 MB 7.0 MB/s eta 0:00:08\n",
      "     ---------------------------- --------- 135.9/184.0 MB 4.5 MB/s eta 0:00:11\n",
      "     ---------------------------- --------- 136.4/184.0 MB 4.7 MB/s eta 0:00:11\n",
      "     ---------------------------- --------- 136.4/184.0 MB 4.7 MB/s eta 0:00:11\n",
      "     ---------------------------- --------- 136.4/184.0 MB 4.7 MB/s eta 0:00:11\n",
      "     ---------------------------- --------- 136.4/184.0 MB 4.7 MB/s eta 0:00:11\n",
      "     ---------------------------- --------- 136.4/184.0 MB 4.7 MB/s eta 0:00:11\n",
      "     ---------------------------- --------- 136.4/184.0 MB 4.7 MB/s eta 0:00:11\n",
      "     ---------------------------- --------- 136.4/184.0 MB 4.7 MB/s eta 0:00:11\n",
      "     ---------------------------- --------- 136.4/184.0 MB 4.7 MB/s eta 0:00:11\n",
      "     ---------------------------- --------- 136.4/184.0 MB 4.7 MB/s eta 0:00:11\n",
      "     ---------------------------- --------- 136.4/184.0 MB 4.7 MB/s eta 0:00:11\n",
      "     ---------------------------- --------- 136.4/184.0 MB 4.7 MB/s eta 0:00:11\n",
      "     ---------------------------- --------- 136.4/184.0 MB 4.7 MB/s eta 0:00:11\n",
      "     ---------------------------- --------- 136.4/184.0 MB 4.7 MB/s eta 0:00:11\n",
      "     ---------------------------- --------- 136.4/184.0 MB 4.7 MB/s eta 0:00:11\n",
      "     ---------------------------- --------- 136.4/184.0 MB 4.7 MB/s eta 0:00:11\n",
      "     ---------------------------- --------- 136.4/184.0 MB 4.7 MB/s eta 0:00:11\n",
      "     ---------------------------- --------- 136.4/184.0 MB 4.7 MB/s eta 0:00:11\n",
      "     ---------------------------- --------- 136.4/184.0 MB 4.7 MB/s eta 0:00:11\n",
      "     ---------------------------- --------- 136.4/184.0 MB 4.7 MB/s eta 0:00:11\n",
      "     ---------------------------- --------- 136.4/184.0 MB 4.7 MB/s eta 0:00:11\n",
      "     ---------------------------- --------- 136.5/184.0 MB 3.3 MB/s eta 0:00:15\n",
      "     ---------------------------- --------- 136.5/184.0 MB 3.3 MB/s eta 0:00:15\n",
      "     ---------------------------- --------- 136.5/184.0 MB 3.3 MB/s eta 0:00:15\n",
      "     ---------------------------- --------- 136.5/184.0 MB 3.3 MB/s eta 0:00:15\n",
      "     ---------------------------- --------- 136.5/184.0 MB 3.3 MB/s eta 0:00:15\n",
      "     ---------------------------- --------- 136.5/184.0 MB 3.3 MB/s eta 0:00:15\n",
      "     ---------------------------- --------- 136.5/184.0 MB 3.0 MB/s eta 0:00:16\n",
      "     ---------------------------- --------- 136.5/184.0 MB 3.0 MB/s eta 0:00:16\n",
      "     ---------------------------- --------- 136.5/184.0 MB 2.9 MB/s eta 0:00:17\n",
      "     ---------------------------- --------- 136.5/184.0 MB 2.9 MB/s eta 0:00:17\n",
      "     ---------------------------- --------- 136.6/184.0 MB 2.9 MB/s eta 0:00:17\n",
      "     ---------------------------- --------- 136.8/184.0 MB 2.9 MB/s eta 0:00:17\n",
      "     ---------------------------- --------- 137.1/184.0 MB 2.9 MB/s eta 0:00:17\n",
      "     ---------------------------- --------- 137.4/184.0 MB 2.9 MB/s eta 0:00:16\n",
      "     ---------------------------- --------- 137.7/184.0 MB 2.9 MB/s eta 0:00:16\n",
      "     ---------------------------- --------- 138.1/184.0 MB 2.9 MB/s eta 0:00:16\n",
      "     ---------------------------- --------- 138.4/184.0 MB 2.9 MB/s eta 0:00:16\n",
      "     ---------------------------- --------- 138.8/184.0 MB 2.9 MB/s eta 0:00:16\n",
      "     ---------------------------- --------- 139.1/184.0 MB 2.9 MB/s eta 0:00:16\n",
      "     ---------------------------- --------- 139.5/184.0 MB 2.9 MB/s eta 0:00:16\n",
      "     ---------------------------- --------- 139.9/184.0 MB 2.9 MB/s eta 0:00:16\n",
      "     ---------------------------- --------- 140.3/184.0 MB 2.9 MB/s eta 0:00:15\n",
      "     ----------------------------- -------- 140.6/184.0 MB 2.9 MB/s eta 0:00:15\n",
      "     ----------------------------- -------- 140.9/184.0 MB 2.9 MB/s eta 0:00:15\n",
      "     ----------------------------- -------- 141.3/184.0 MB 2.9 MB/s eta 0:00:15\n",
      "     ----------------------------- -------- 141.6/184.0 MB 2.9 MB/s eta 0:00:15\n",
      "     ----------------------------- -------- 142.0/184.0 MB 2.9 MB/s eta 0:00:15\n",
      "     ----------------------------- -------- 142.3/184.0 MB 2.9 MB/s eta 0:00:15\n",
      "     ----------------------------- -------- 142.7/184.0 MB 2.9 MB/s eta 0:00:15\n",
      "     ----------------------------- -------- 143.0/184.0 MB 2.9 MB/s eta 0:00:15\n",
      "     ----------------------------- -------- 143.4/184.0 MB 2.9 MB/s eta 0:00:14\n",
      "     ----------------------------- -------- 143.7/184.0 MB 2.9 MB/s eta 0:00:14\n",
      "     ----------------------------- -------- 144.1/184.0 MB 2.9 MB/s eta 0:00:14\n",
      "     ----------------------------- -------- 144.5/184.0 MB 2.9 MB/s eta 0:00:14\n",
      "     ----------------------------- -------- 144.8/184.0 MB 2.9 MB/s eta 0:00:14\n",
      "     ----------------------------- -------- 145.2/184.0 MB 4.0 MB/s eta 0:00:10\n",
      "     ------------------------------ ------- 145.6/184.0 MB 3.9 MB/s eta 0:00:10\n",
      "     ------------------------------ ------- 146.0/184.0 MB 3.9 MB/s eta 0:00:10\n",
      "     ------------------------------ ------- 146.3/184.0 MB 3.8 MB/s eta 0:00:10\n",
      "     ------------------------------ ------- 146.7/184.0 MB 3.7 MB/s eta 0:00:10\n",
      "     ------------------------------ ------- 146.9/184.0 MB 7.5 MB/s eta 0:00:05\n",
      "     ------------------------------ ------- 147.3/184.0 MB 7.6 MB/s eta 0:00:05\n",
      "     ------------------------------ ------- 147.7/184.0 MB 7.7 MB/s eta 0:00:05\n",
      "     ------------------------------ ------- 148.0/184.0 MB 7.6 MB/s eta 0:00:05\n",
      "     ------------------------------ ------- 148.3/184.0 MB 7.6 MB/s eta 0:00:05\n",
      "     ------------------------------ ------- 148.7/184.0 MB 7.7 MB/s eta 0:00:05\n",
      "     ------------------------------ ------- 149.0/184.0 MB 7.7 MB/s eta 0:00:05\n",
      "     ------------------------------ ------- 149.4/184.0 MB 7.7 MB/s eta 0:00:05\n",
      "     ------------------------------ ------- 149.7/184.0 MB 7.6 MB/s eta 0:00:05\n",
      "     ------------------------------ ------- 150.1/184.0 MB 7.7 MB/s eta 0:00:05\n",
      "     ------------------------------- ------ 150.4/184.0 MB 7.7 MB/s eta 0:00:05\n",
      "     ------------------------------- ------ 150.7/184.0 MB 7.6 MB/s eta 0:00:05\n",
      "     ------------------------------- ------ 151.0/184.0 MB 7.6 MB/s eta 0:00:05\n",
      "     ------------------------------- ------ 151.4/184.0 MB 7.6 MB/s eta 0:00:05\n",
      "     ------------------------------- ------ 151.8/184.0 MB 7.7 MB/s eta 0:00:05\n",
      "     ------------------------------- ------ 152.2/184.0 MB 7.7 MB/s eta 0:00:05\n",
      "     ------------------------------- ------ 152.6/184.0 MB 7.7 MB/s eta 0:00:05\n",
      "     ------------------------------- ------ 153.0/184.0 MB 7.7 MB/s eta 0:00:05\n",
      "     ------------------------------- ------ 153.3/184.0 MB 7.7 MB/s eta 0:00:04\n",
      "     ------------------------------- ------ 153.6/184.0 MB 7.7 MB/s eta 0:00:04\n",
      "     ------------------------------- ------ 153.9/184.0 MB 7.7 MB/s eta 0:00:04\n",
      "     ------------------------------- ------ 154.2/184.0 MB 7.6 MB/s eta 0:00:04\n",
      "     ------------------------------- ------ 154.5/184.0 MB 7.6 MB/s eta 0:00:04\n",
      "     ------------------------------- ------ 154.9/184.0 MB 7.7 MB/s eta 0:00:04\n",
      "     -------------------------------- ----- 155.2/184.0 MB 7.6 MB/s eta 0:00:04\n",
      "     -------------------------------- ----- 155.5/184.0 MB 7.6 MB/s eta 0:00:04\n",
      "     -------------------------------- ----- 155.9/184.0 MB 7.6 MB/s eta 0:00:04\n",
      "     -------------------------------- ----- 156.3/184.0 MB 7.5 MB/s eta 0:00:04\n",
      "     -------------------------------- ----- 156.6/184.0 MB 7.6 MB/s eta 0:00:04\n",
      "     -------------------------------- ----- 157.0/184.0 MB 7.6 MB/s eta 0:00:04\n",
      "     -------------------------------- ----- 157.4/184.0 MB 7.7 MB/s eta 0:00:04\n",
      "     -------------------------------- ----- 157.8/184.0 MB 7.7 MB/s eta 0:00:04\n",
      "     -------------------------------- ----- 158.1/184.0 MB 7.6 MB/s eta 0:00:04\n",
      "     -------------------------------- ----- 158.5/184.0 MB 7.9 MB/s eta 0:00:04\n",
      "     -------------------------------- ----- 158.9/184.0 MB 7.8 MB/s eta 0:00:04\n",
      "     -------------------------------- ----- 159.3/184.0 MB 7.9 MB/s eta 0:00:04\n",
      "     -------------------------------- ----- 159.7/184.0 MB 7.9 MB/s eta 0:00:04\n",
      "     --------------------------------- ---- 160.1/184.0 MB 8.0 MB/s eta 0:00:03\n",
      "     --------------------------------- ---- 160.4/184.0 MB 7.9 MB/s eta 0:00:03\n",
      "     --------------------------------- ---- 160.8/184.0 MB 7.9 MB/s eta 0:00:03\n",
      "     --------------------------------- ---- 161.0/184.0 MB 7.9 MB/s eta 0:00:03\n",
      "     --------------------------------- ---- 161.4/184.0 MB 7.9 MB/s eta 0:00:03\n",
      "     --------------------------------- ---- 161.8/184.0 MB 7.9 MB/s eta 0:00:03\n",
      "     --------------------------------- ---- 162.1/184.0 MB 7.9 MB/s eta 0:00:03\n",
      "     --------------------------------- ---- 162.5/184.0 MB 7.9 MB/s eta 0:00:03\n",
      "     --------------------------------- ---- 162.9/184.0 MB 7.9 MB/s eta 0:00:03\n",
      "     --------------------------------- ---- 163.3/184.0 MB 7.8 MB/s eta 0:00:03\n",
      "     --------------------------------- ---- 163.6/184.0 MB 7.8 MB/s eta 0:00:03\n",
      "     --------------------------------- ---- 163.9/184.0 MB 7.9 MB/s eta 0:00:03\n",
      "     --------------------------------- ---- 164.3/184.0 MB 8.0 MB/s eta 0:00:03\n",
      "     ---------------------------------- --- 164.6/184.0 MB 8.0 MB/s eta 0:00:03\n",
      "     ---------------------------------- --- 165.0/184.0 MB 8.0 MB/s eta 0:00:03\n",
      "     ---------------------------------- --- 165.3/184.0 MB 7.9 MB/s eta 0:00:03\n",
      "     ---------------------------------- --- 165.7/184.0 MB 7.9 MB/s eta 0:00:03\n",
      "     ---------------------------------- --- 166.0/184.0 MB 7.9 MB/s eta 0:00:03\n",
      "     ---------------------------------- --- 166.4/184.0 MB 7.9 MB/s eta 0:00:03\n",
      "     ---------------------------------- --- 166.7/184.0 MB 7.9 MB/s eta 0:00:03\n",
      "     ---------------------------------- --- 167.1/184.0 MB 8.0 MB/s eta 0:00:03\n",
      "     ---------------------------------- --- 167.4/184.0 MB 7.9 MB/s eta 0:00:03\n",
      "     ---------------------------------- --- 167.8/184.0 MB 7.9 MB/s eta 0:00:03\n",
      "     ---------------------------------- --- 168.1/184.0 MB 7.9 MB/s eta 0:00:03\n",
      "     ---------------------------------- --- 168.4/184.0 MB 7.8 MB/s eta 0:00:02\n",
      "     ---------------------------------- --- 168.8/184.0 MB 7.8 MB/s eta 0:00:02\n",
      "     ---------------------------------- --- 169.2/184.0 MB 7.8 MB/s eta 0:00:02\n",
      "     ----------------------------------- -- 169.6/184.0 MB 7.8 MB/s eta 0:00:02\n",
      "     ----------------------------------- -- 169.9/184.0 MB 7.8 MB/s eta 0:00:02\n",
      "     ----------------------------------- -- 170.3/184.0 MB 7.7 MB/s eta 0:00:02\n",
      "     ----------------------------------- -- 170.6/184.0 MB 7.7 MB/s eta 0:00:02\n",
      "     ----------------------------------- -- 171.0/184.0 MB 7.7 MB/s eta 0:00:02\n",
      "     ----------------------------------- -- 171.3/184.0 MB 7.8 MB/s eta 0:00:02\n",
      "     ----------------------------------- -- 171.7/184.0 MB 7.7 MB/s eta 0:00:02\n",
      "     ----------------------------------- -- 172.1/184.0 MB 7.8 MB/s eta 0:00:02\n",
      "     ----------------------------------- -- 172.4/184.0 MB 7.8 MB/s eta 0:00:02\n",
      "     ----------------------------------- -- 172.8/184.0 MB 7.8 MB/s eta 0:00:02\n",
      "     ----------------------------------- -- 173.2/184.0 MB 7.8 MB/s eta 0:00:02\n",
      "     ----------------------------------- -- 173.6/184.0 MB 7.8 MB/s eta 0:00:02\n",
      "     ----------------------------------- -- 174.0/184.0 MB 7.9 MB/s eta 0:00:02\n",
      "     ------------------------------------ - 174.3/184.0 MB 7.9 MB/s eta 0:00:02\n",
      "     ------------------------------------ - 174.7/184.0 MB 7.9 MB/s eta 0:00:02\n",
      "     ------------------------------------ - 175.0/184.0 MB 7.8 MB/s eta 0:00:02\n",
      "     ------------------------------------ - 175.5/184.0 MB 8.0 MB/s eta 0:00:02\n",
      "     ------------------------------------ - 175.8/184.0 MB 7.9 MB/s eta 0:00:02\n",
      "     ------------------------------------ - 176.2/184.0 MB 7.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ - 176.4/184.0 MB 7.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ - 176.8/184.0 MB 7.8 MB/s eta 0:00:01\n",
      "     ------------------------------------ - 177.1/184.0 MB 7.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ - 177.3/184.0 MB 7.8 MB/s eta 0:00:01\n",
      "     ------------------------------------ - 177.6/184.0 MB 7.4 MB/s eta 0:00:01\n",
      "     ------------------------------------ - 177.8/184.0 MB 7.4 MB/s eta 0:00:01\n",
      "     ------------------------------------ - 178.2/184.0 MB 7.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ - 178.5/184.0 MB 7.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ - 178.7/184.0 MB 7.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ - 179.0/184.0 MB 7.1 MB/s eta 0:00:01\n",
      "     -------------------------------------  179.3/184.0 MB 7.0 MB/s eta 0:00:01\n",
      "     -------------------------------------  179.6/184.0 MB 7.0 MB/s eta 0:00:01\n",
      "     -------------------------------------  179.8/184.0 MB 6.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  180.1/184.0 MB 6.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  180.3/184.0 MB 6.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  180.7/184.0 MB 6.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  181.0/184.0 MB 6.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  181.3/184.0 MB 6.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  181.7/184.0 MB 6.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  182.1/184.0 MB 6.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  182.4/184.0 MB 6.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  182.7/184.0 MB 6.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  183.0/184.0 MB 6.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  183.3/184.0 MB 6.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  183.6/184.0 MB 6.7 MB/s eta 0:00:01\n",
      "     -------------------------------------  183.9/184.0 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------------------  184.0/184.0 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------------------  184.0/184.0 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------------------  184.0/184.0 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------------------  184.0/184.0 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------------------  184.0/184.0 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------------------  184.0/184.0 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------------------  184.0/184.0 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------------------  184.0/184.0 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------------------  184.0/184.0 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------------------  184.0/184.0 MB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 184.0/184.0 MB 4.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\program files\\python39\\lib\\site-packages (from torchtext) (1.24.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\rijhw\\appdata\\roaming\\python\\python39\\site-packages (from torch==1.7.1->torchtext) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rijhw\\appdata\\roaming\\python\\python39\\site-packages (from requests->torchtext) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python39\\lib\\site-packages (from requests->torchtext) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rijhw\\appdata\\roaming\\python\\python39\\site-packages (from requests->torchtext) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rijhw\\appdata\\roaming\\python\\python39\\site-packages (from requests->torchtext) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\rijhw\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->torchtext) (0.4.6)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.1\n",
      "    Uninstalling torch-2.1.1:\n",
      "      Successfully uninstalled torch-2.1.1\n",
      "Successfully installed torch-1.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchdata 0.7.1 requires torch>=2, but you have torch 1.7.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "19b404b570db48a5bca93c697c6b8d98",
    "deepnote_cell_type": "code",
    "id": "787e2059"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext.datasets import UDPOS\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import random\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# for reproducibility\n",
    "random.seed(77)\n",
    "np.random.seed(77)\n",
    "torch.manual_seed(77)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a34b8c77984a418ea6ff68af1a1594c2",
    "deepnote_cell_type": "markdown",
    "id": "fc29cb37"
   },
   "source": [
    "### Subtask 1: Data Analysis\n",
    "The very basic components of the torchtext library include `vocab`, `word vectors`, and `tokenizer`. Those are the basic data processing building blocks for the raw text string.\n",
    "In this case, we use the tokenizer and the vocabulary. Use the `build_vocab_from_iterator` to create the vocabulary for the text field and add the `<unk>` and `<pad>` tokens to it. Use a minimal frequency of `2`.\n",
    "Also create a vocabulary for the labels (tag field). However, since the tags are predefined elements, you will not need an `<unk>` token.\n",
    " This dataset actually has two different sets of tags, universal dependency (UD) tags and Penn Treebank (PTB) tags. We train our model on the UD tags, which is the second element on the list of outputs (example below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "ae554462df1b4c33a025de3bee3a2ff4",
    "deepnote_cell_type": "code",
    "id": "IlEFywat4Gva"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.'], ['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT'], ['NNP', 'HYPH', 'NNP', ':', 'JJ', 'NNS', 'VBD', 'NNP', 'NNP', 'NNP', 'HYPH', 'NNP', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', ',', 'IN', 'DT', 'JJ', 'NN', '.']]\n"
     ]
    }
   ],
   "source": [
    "train_iter = iter(UDPOS(split=\"train\"))\n",
    "print(next(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "0f676fc508d64b40b17ab0c2089b06d8",
    "deepnote_cell_type": "code",
    "id": "7HJWFvb3AgTI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 12543\n",
      "Number of validation examples: 2002\n",
      "Number of testing examples: 2077\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(list(UDPOS(split='train')))}\")\n",
    "print(f\"Number of validation examples: {len(list(UDPOS(split='valid')))}\")\n",
    "print(f\"Number of testing examples: {len(list(UDPOS(split='test')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f24e061070df4f54b2bbd3227cd1a988",
    "deepnote_cell_type": "markdown",
    "id": "RaAW09-n_5DC"
   },
   "source": [
    "Note that the data is already tokenized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "3a8551135da04076a29039ddb1dc328d",
    "deepnote_cell_type": "code",
    "id": "rcG-FcDu5DcA"
   },
   "outputs": [],
   "source": [
    "# Create vocabularies\n",
    "def yield_tokens(data_iter, field):\n",
    "    for item in data_iter:\n",
    "        if field == 'text':\n",
    "            yield item[0]\n",
    "        elif field == 'ud_tags':\n",
    "            yield item[1]\n",
    "\n",
    "# Reset the iterator for text\n",
    "train_iter = UDPOS(split=\"train\")\n",
    "\n",
    "# Create vocabulary for text\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter, 'text'), specials=[\"<pad>\", \"<unk>\"], min_freq=2, special_first=True)\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "# Reset the iterator for UD tags\n",
    "train_iter = UDPOS(split=\"train\")\n",
    "\n",
    "# Create vocabulary for UD tags, ensuring <pad> is first in the list of specials\n",
    "ud_vocab = build_vocab_from_iterator(yield_tokens(train_iter, 'ud_tags'), specials=[\"<pad>\"], special_first=True)\n",
    "ud_vocab.set_default_index(ud_vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "16aeedfd06104e26850b18aa43f5705f",
    "deepnote_cell_type": "markdown",
    "id": "nzalgc92j2mG"
   },
   "source": [
    "Prepare a text processing pipeline that takes raw input and labels and converts them to ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "359fc1df8aee4f228dece854dbeafca8",
    "deepnote_cell_type": "code",
    "id": "BDqKYQnRj2D8"
   },
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(x)\n",
    "label_pipeline = lambda x: ud_vocab(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "5c7855be24eb4c9c92f3320d639d9332",
    "deepnote_cell_type": "code",
    "id": "wg36B2Z_Hdi0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 9271, 35, 9097, 0, 1]\n",
      "[6, 8, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "print(text_pipeline(['the', 'preacher', 'at', 'mosque', \"<pad>\",\"pppp\"])) #should output [3, 9271, 35, 9097, 0, 1]\n",
    "print(label_pipeline(['DET', 'ADJ', 'NOUN', 'PUNCT'])) # should output [6, 8, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "c04a16be98f64863934cabd96d0feabe",
    "deepnote_cell_type": "code",
    "id": "cjn1qH2dHoaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in input vocabulary: 9875\n",
      "Unique tokens in UD vocabulary: 18\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in input vocabulary: {len(vocab)}\")\n",
    "print(f\"Unique tokens in UD vocabulary: {len(ud_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "49f8e6c033ff4c298dbde57b4a044a75",
    "deepnote_cell_type": "markdown",
    "id": "FUnDiAOW-N5S"
   },
   "source": [
    "Write a custom function for the dataloader that applies the text and label pipeline and pads the sequences to have equal lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "11efa07d3a5d44ae9f2be54429f26c70",
    "deepnote_cell_type": "code",
    "id": "0aN6s1UhoRhK"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def collate_batch(batch, text_pipeline, label_pipeline, pad_token_ix, pad_token_ix_ud):\n",
    "    text_list, label_list = [], []\n",
    "    for item in batch:\n",
    "        _text, _label, _ = item  # ignore PTB label\n",
    "        processed_text = text_pipeline(_text)\n",
    "        processed_label = label_pipeline(_label)\n",
    "        text_list.append(torch.tensor(processed_text, dtype=torch.int64))\n",
    "        label_list.append(torch.tensor(processed_label, dtype=torch.int64))\n",
    "\n",
    "    x_padded = pad_sequence(text_list, batch_first=True, padding_value=pad_token_ix)\n",
    "    y_padded = pad_sequence(label_list, batch_first=True, padding_value=pad_token_ix_ud)\n",
    "\n",
    "    return x_padded, y_padded\n",
    "\n",
    "\n",
    "collate_fn = partial(collate_batch, text_pipeline=text_pipeline, label_pipeline=label_pipeline, pad_token_ix=vocab['<pad>'], pad_token_ix_ud=ud_vocab['<pad>'])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    UDPOS(split=\"train\"), batch_size=128, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    UDPOS(split=\"valid\"), batch_size=128, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    UDPOS(split=\"test\"), batch_size=128, shuffle=True, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "5902b48d1a0b4663b4860a110c33f59e",
    "deepnote_cell_type": "code",
    "id": "2X8ZZ0J4oek_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 57])\n",
      "torch.Size([128, 57])\n",
      "tensor([1832,   13,   17,  488, 2837,    3, 3074,    1, 5532,  512,   13,   23,\n",
      "          56,  143,    5,  170,  154,  273,   39,  157,  971,   44, 2212,  648,\n",
      "           2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0])\n",
      "tensor([14,  4,  9, 12,  3,  6,  1,  1, 10, 17,  4,  3, 10, 13,  5, 13,  1, 10,\n",
      "        14,  9,  3,  5,  7,  7,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "for idx, (label, text) in enumerate(train_dataloader):\n",
    "      print(label.shape)\n",
    "      print(text.shape)\n",
    "      print(label[0])\n",
    "      print(text[0])\n",
    "      break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c3bc34b83db7485994ae623fe1aeb4b4",
    "deepnote_cell_type": "markdown",
    "id": "jDy_wvjR_1Sg"
   },
   "source": [
    "Let's take a closer look at the data and the distribution of tags.\n",
    "Implement `tag_percentage`:\n",
    "\n",
    "*   Use the `collection.counter` to count the unique instances of each tag.\n",
    "\n",
    "*    Compute the percentage of each tag in the entire set, by using the counted frequencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "443bf7dda5814b48a5959afa3486ac6f",
    "deepnote_cell_type": "code",
    "id": "273JqNZn_rRT"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def tag_percentage(training_iterator):\n",
    "    counter = Counter()\n",
    "    #### code to count the number of tags\n",
    "    for item in training_iterator:\n",
    "        text, ud_tags, ptb_tags = item \n",
    "        counter.update(ud_tags) \n",
    "\n",
    "    total_tags = sum(counter.values())\n",
    "\n",
    "    ### compute the tag percentages based on the counter object\n",
    "\n",
    "    tag_p = [(tag, count, count / total_tags) for tag, count in counter.items()]\n",
    "    ###\n",
    "    return tag_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "f434fd468769433e83a7c840f4dc10ee",
    "deepnote_cell_type": "code",
    "id": "fWzoEd7h_qRw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag\t\tCount\t\tPercentage\n",
      "\n",
      "PROPN\t\t12946\t\t 6.3%\n",
      "PUNCT\t\t23679\t\t11.6%\n",
      "ADJ\t\t12477\t\t 6.1%\n",
      "NOUN\t\t34781\t\t17.0%\n",
      "VERB\t\t23081\t\t11.3%\n",
      "DET\t\t16285\t\t 8.0%\n",
      "ADP\t\t17638\t\t 8.6%\n",
      "AUX\t\t12343\t\t 6.0%\n",
      "PRON\t\t18577\t\t 9.1%\n",
      "PART\t\t5567\t\t 2.7%\n",
      "SCONJ\t\t3843\t\t 1.9%\n",
      "NUM\t\t3999\t\t 2.0%\n",
      "ADV\t\t10548\t\t 5.2%\n",
      "CCONJ\t\t6707\t\t 3.3%\n",
      "X\t\t847\t\t 0.4%\n",
      "INTJ\t\t688\t\t 0.3%\n",
      "SYM\t\t599\t\t 0.3%\n"
     ]
    }
   ],
   "source": [
    "print(\"Tag\\t\\tCount\\t\\tPercentage\\n\")\n",
    "\n",
    "for tag, count, percent in tag_percentage(UDPOS(split='train')):\n",
    "    print(f\"{tag}\\t\\t{count}\\t\\t{percent*100:4.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "584094dc2c244a65b5ff242f81cf1931",
    "deepnote_cell_type": "markdown",
    "id": "GKTQL0EsxSUU"
   },
   "source": [
    "####${\\color{red}{Comments\\ 1.1}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "cross-feedback comment section\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ccd3f213af204b5793b493852b22621c",
    "deepnote_cell_type": "markdown",
    "id": "gYHy3vSELfaN"
   },
   "source": [
    "### Subtask 2: The Model\n",
    "\n",
    "We start by creating a simple model and then make it more complex in later subtasks. The class `BiLSTMTagger` must subclass the `nn.Module` class of `PyTorch`. Fill the blank in the class by following the notes described below.\n",
    "1.   The input is a sequence of tokens, $X = \\{x_1, x_2,...,x_T\\}$.\n",
    "2.   Each token passes through  an embeddings layer, $e(X) = \\{e(x_1), e(x_2), ..., e(x_T)\\}$. Use `nn.Embedding` for the embedding layer and make sure to pass in the index of the pad token.\n",
    "3. Embedding is processed by forward and backward LSTMs from left to right and right to left.  The first input to the forward LSTM is $x_1$ and the first input to the backward LSTM is $x_T$. The hidden state of LSTMs is dependent on\n",
    "the hidden, $h$, and cell, $c$, states from the previous time-steps:\n",
    "$$h^{\\rightarrow}_t = \\text{LSTM}^{\\rightarrow}(e(x^{\\rightarrow}_t), h^{\\rightarrow}_{t-1}, c^{\\rightarrow}_{t-1})$$\n",
    "$$h^{\\leftarrow}_t=\\text{LSTM}^{\\leftarrow}(e(x^{\\leftarrow}_t), h^{\\leftarrow}_{t-1}, c^{\\leftarrow}_{t-1})$$\n",
    "4. The hidden, $h$, and cell, $c$ of each layer is passed to the next layer, where the $h_0$ and $c_0$, for each direction and layer, are initialized to a tensor full of zeros. Use `nn.LSTM` for LSTM cells. How can you make it bidirectional?\n",
    "5. Final hidden state is the concatenation of forward and backward hidden states from the final layer of the LSTM, $H = \\{h_1, h_2, ... h_T\\}$, where $h_1 = [h^{\\rightarrow}_1;h^{\\leftarrow}_T]$, $h_2 = [h^{\\rightarrow}_2;h^{\\leftarrow}_{T-1}]$. Use `nn.Linear` here.\n",
    "6. The last layer is linear layer $f$, which is used to make the prediction of which tag applies to this token, $\\hat{y}_t = f(h_t)$.\n",
    "7. Define a `nn.Dropout` layer to apply to the embeddings and the outputs of the final layer of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "cell_id": "d0d03fc04c114710936d2c8778c8c904",
    "deepnote_cell_type": "code",
    "id": "qE4GQE6NJ1k8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, hyperparameters):\n",
    "        '''\n",
    "        hyperparameters: is a dictionary containing:\n",
    "            input_dim: dimension of the input\n",
    "            embedding_dim: dimension of the embedding layer\n",
    "            lstm_hidden_dim: dimension of the hidden state of lstms\n",
    "            output_dim: dimension of the output hidden layer\n",
    "            n_layers: number of layers to stack\n",
    "            bidirectional: is the lstm bi-directional\n",
    "            dropout: probability for the drop out layer\n",
    "            pad_idx: id of the pad token\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=hyperparameters['input_dim'],\n",
    "            embedding_dim=hyperparameters['embedding_dim'],\n",
    "            padding_idx=hyperparameters['pad_idx']\n",
    "        )\n",
    "\n",
    "        # Bi-LSTM, apply dropout if the number of layers is more than 1\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hyperparameters['embedding_dim'],\n",
    "            hidden_size=hyperparameters['lstm_hidden_dim'],\n",
    "            num_layers=hyperparameters['n_layers'],\n",
    "            bidirectional=hyperparameters['bidirectional'],\n",
    "            dropout=hyperparameters['dropout'] if hyperparameters['n_layers'] > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Linear layer for prediction\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=hyperparameters['lstm_hidden_dim'] * 2 if hyperparameters['bidirectional'] else hyperparameters['lstm_hidden_dim'],\n",
    "            out_features=hyperparameters['output_dim']\n",
    "        )\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(hyperparameters['dropout'])\n",
    "\n",
    "    def forward(self, text, debug=False):  # (B,S)\n",
    "        '''\n",
    "        S: sentence len\n",
    "        B: batch size\n",
    "        E: embedding size\n",
    "        H: hidden size\n",
    "        O: output size\n",
    "        L: number of layers\n",
    "        '''\n",
    "\n",
    "        # Pass text through the embedding layer and a dropout layer\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "\n",
    "        # Pass embeddings into Bi-LSTM\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        # Concatenate the final hidden states from both directions if bidirectional\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1, :, :]\n",
    "\n",
    "        # Compute prediction\n",
    "        predictions = self.fc(hidden)\n",
    "\n",
    "        if debug:\n",
    "            print(\"Input shape:\", text.shape)\n",
    "            print(\"Embedding shape:\", embedded.shape)\n",
    "            print(\"LSTM output shape:\", outputs.shape)\n",
    "            print(\"LSTM hidden shape:\", hidden.shape)\n",
    "            print(\"LSTM cell shape:\", cell.shape)\n",
    "            print(\"Output shape:\", predictions.shape)\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b53845ef311147448dae4573abb04cb3",
    "deepnote_cell_type": "markdown",
    "id": "4OcQbD9LUkD5"
   },
   "source": [
    "Response in plain text:\n",
    "1. Based on the notation defined in the forward function. What is the dimension of `outputs`, `hidden`, and `cell`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f58aad93965b42bdac802c7fab521907",
    "deepnote_cell_type": "markdown",
    "id": "ml4KUd1BU3op"
   },
   "source": [
    "The dimension of outputs, hidden, and cell are defined as follows:\n",
    "\n",
    "outputs: The output at each time step, where outputs is a tensor of shape (sequence_length, batch_size, hidden_size).\n",
    "\n",
    "hidden: The final hidden state at each layer for each time step. hidden is a tensor of shape (num_layers, batch_size, hidden_size).\n",
    "\n",
    "cell: The final cell state at each layer for each time step. cell is a tensor of shape (num_layers, batch_size, hidden_size).\n",
    "\n",
    "The dimension of these tensors can vary based on the batch_first argument. If batch_first=True, the input tensor should be of shape (batch_size, sequence_length, input_size). In this case, the outputs tensor would have shape (batch_size, sequence_length, hidden_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "cell_id": "7c57a4b9909a4c79b7a1607402ec40be",
    "deepnote_cell_type": "code",
    "id": "ivB8Iv4yScVW"
   },
   "outputs": [],
   "source": [
    "hyper_parameters={\n",
    "  'input_dim':  len(vocab),\n",
    "  'embedding_dim': 100,\n",
    "  'lstm_hidden_dim': 128,\n",
    "  'output_dim':len(ud_vocab),\n",
    "  'n_layers': 2 ,\n",
    "  'bidirectional':True,\n",
    "  'dropout': 0.25,\n",
    "  'pad_idx': vocab['<pad>']\n",
    "}\n",
    "model = BiLSTMTagger(hyper_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "cell_id": "cee3865b5dbe4876b44c6991e7c06894",
    "deepnote_cell_type": "code",
    "id": "1XvNFfJBV7ya"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 6])\n",
      "Embedding shape: torch.Size([2, 6, 100])\n",
      "LSTM output shape: torch.Size([2, 6, 256])\n",
      "LSTM hidden shape: torch.Size([6, 256])\n",
      "LSTM cell shape: torch.Size([4, 6, 128])\n",
      "Output shape: torch.Size([6, 18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 18])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input=torch.tensor([[3, 9271, 35, 9097, 0, 1],\n",
    "                    [3, 9271, 35, 9097, 0, 1]])\n",
    "model(input,debug=True).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fd716785248d4b0687d85f8d23c6bbd0",
    "deepnote_cell_type": "markdown",
    "id": "2XiJkG-NfrDV"
   },
   "source": [
    "Weights of the network are initialized randomly, so let's make a more systematic initialization to help us with the optimization. For example,  Xavier Initialization creates weights such that the variance of the activations is the same across every layer. This constant variance helps prevent the gradient from exploding or vanishing. However, it does not apply to bias terms.\n",
    "\n",
    "Create a function that uses Xavier Initialization to initialize the weights of the network, for biases use a normal distribution with a mean of 0 and a standard deviation of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_id": "8be9698f70fe4c3b961b362092a30374",
    "deepnote_cell_type": "code",
    "id": "PB78pCHpa758"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMTagger(\n",
       "  (embedding): Embedding(9875, 100, padding_idx=0)\n",
       "  (lstm): LSTM(100, 128, num_layers=2, dropout=0.25, bidirectional=True)\n",
       "  (fc): Linear(in_features=256, out_features=18, bias=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias, mean=0, std=0.1)\n",
    "    elif type(m) == nn.Embedding:\n",
    "        init.xavier_uniform_(m.weight)\n",
    "\n",
    "# Applying the initialization to the model\n",
    "model.apply(init_weights)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ad51f9d8761c47ec9106e93b43781e8c",
    "deepnote_cell_type": "markdown",
    "id": "sdPx1mSOgy0V"
   },
   "source": [
    "Let's count the number of trainable parameters in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "7a6bc753aa2241b8864de9934399854f",
    "deepnote_cell_type": "code",
    "id": "P5738nU2ew_O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable parameters: 1622910\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"number of trainable parameters:\",count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "64eb82862a7c4e38b91db6146ba4e413",
    "deepnote_cell_type": "markdown",
    "id": "91uNA-vnyoUW"
   },
   "source": [
    "####${\\color{red}{Comments\\ 1.2}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "cross-feedback comment section\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4c16f8f5c69c4d2c9cc17925a52157ff",
    "deepnote_cell_type": "markdown",
    "id": "-lVg08iTd0XN"
   },
   "source": [
    "### Subtask 3: Training\n",
    "\n",
    "We start by defining a loss function and an optimizer.\n",
    "\n",
    "\n",
    "*   **optimizer:** We use Adam with the learning rate=0.0001.\n",
    "*   **loss:** We use cross-entropy loss.\n",
    "\n",
    "Even though we have no `<unk>` tokens within our tag vocab, we still have `<pad>` tokens to create batches of the same size. However, we do not want to calculate loss on those tokens, so make sure you define your loss function in such a way that ignores the `<pad>` tokens.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_id": "969e8689e5df4c8aa0b9d02c852d14db",
    "deepnote_cell_type": "code",
    "id": "8LEyNeKhmBvK"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "# Loss function (ignores <pad> tokens)\n",
    "pad_idx = hyper_parameters['pad_idx']\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6252090606c34e999c0a885d6a11517a",
    "deepnote_cell_type": "markdown",
    "id": "95Cf7ZcVgLHK"
   },
   "source": [
    "Watching the loss go down as you train a model is a good indication of the correct training procedure, but does not tell us how well we are doing on a given task.\n",
    "To this end, we also implement a categorical accuracy measure to keep track of how well our model is doing on a given task.\n",
    "Same as before: we don't want to calculate accuracy over the `<pad>` tokens as we aren't interested in predicting them.\n",
    "Implement the function `categorical_acc` to compare the prediction of non-pad tokens with labels count the correct ones and calculate the accuracy over a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cell_id": "e1a8317728054f4dbab53fd453f73ca8",
    "deepnote_cell_type": "code",
    "id": "J5OpDKuBitk1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def categorical_acc(preds, gt, pad_idx):\n",
    "    \"\"\"\n",
    "    Returns categorical accuracy per batch\n",
    "    \"\"\"\n",
    "    # Get the index of the max probability\n",
    "    max_preds = preds.argmax(dim=1)\n",
    "\n",
    "    # Create a mask for non-padding positions\n",
    "    non_pad_mask = gt != pad_idx\n",
    "\n",
    "    # Count the correct predictions where the ground truth is not <pad>\n",
    "    correct = torch.sum(max_preds[non_pad_mask] == gt[non_pad_mask]).item()\n",
    "\n",
    "    # Count the total number of non-pad tokens\n",
    "    total_non_pad = non_pad_mask.sum().item()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct / total_non_pad if total_non_pad > 0 else 0\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cell_id": "f671ab7332ea47f38982d7ad6d3dada8",
    "deepnote_cell_type": "code",
    "id": "WANOhA56kjY_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummpy_input=torch.tensor([\n",
    "    [0.9,0,0,0],\n",
    "    [0.1,0.9,0,0],\n",
    "    [0.1,0,0,0.9],\n",
    "    [0.9,0.1,0,0],\n",
    "    [0.1,0.8,0,0]\n",
    "\n",
    "])\n",
    "categorical_acc(dummpy_input, torch.tensor([0,2,3,0,1]), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "580e06e3975b4293abf00fd2ba76b29a",
    "deepnote_cell_type": "markdown",
    "id": "y_wngIs7jFrT"
   },
   "source": [
    "Define the `train` model that performs one epoch of training. You can refer to the Tutorial 2 of the course to get a sample workflow. The only difference to the tutorial is that we keep track of the batch-wise accuracy as well as the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "cell_id": "3075911a5f7744929752bdf08e969624",
    "deepnote_cell_type": "code",
    "id": "kyMs0FJNfGCp"
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, loss_function, pad_idx):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    for batch_idx, (text, tags) in enumerate(dataloader):\n",
    "        # Check for empty batches\n",
    "        if text.size(0) == 0:\n",
    "            continue\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(text)\n",
    "\n",
    "        # Ensure predictions and tags have the same size\n",
    "        if predictions.size(1) != tags.size(1):\n",
    "            print(f\"Predictions shape {predictions.size()} does not match tags shape {tags.size()}.\")\n",
    "            continue\n",
    "\n",
    "        # compute the loss, ignoring <pad> token\n",
    "        loss = loss_function(predictions.view(-1, predictions.shape[-1]), tags.view(-1))\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute categorical accuracy\n",
    "        acc = categorical_acc(predictions, tags, pad_idx)\n",
    "\n",
    "        # Update epoch loss and accuracy\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc\n",
    "\n",
    "    end_time = time.time() - epoch_start_time\n",
    "\n",
    "    # Ensure division by zero is avoided\n",
    "    if len(dataloader) == 0:\n",
    "        length_dataloader = 1\n",
    "    else:\n",
    "        length_dataloader = len(dataloader)\n",
    "\n",
    "    return epoch_loss / length_dataloader, epoch_acc / length_dataloader, end_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "56f3175c5e7440708c5e1e6ac198ed7c",
    "deepnote_cell_type": "markdown",
    "id": "W0ytUn7vkdcR"
   },
   "source": [
    "It is not enough to only look at the training loss and accuracy, since with more training, we can always do better on the training set, but lose the generalizability to unseen data, a phenomenon known as **overfitting**. Therefore, it is important to check the loss and accuracy on the validation set after each epoch and stop before  overfitting occurs. Moreover, we can use the validation metric as an indication of which checkpoint of our model is the best.\n",
    "\n",
    "Define an `evaluate` function that runs once through the validation set and computes loss and accuracy. **Note:** You should not be updating gradients here and your model should be in evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cell_id": "b402c539afb34dbbbac9522060226344",
    "deepnote_cell_type": "code",
    "id": "0CKzEqyAfktd"
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, dataloader, loss_function, pad_idx):\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    length_dataloader = len(dataloader)\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients during evaluation\n",
    "        for batch_num, (text, tags) in enumerate(dataloader):\n",
    "            # Forward pass\n",
    "            predictions = model(text)\n",
    "\n",
    "            # Compute the loss, ignoring <pad> tokens\n",
    "            loss = loss_function(predictions.view(-1, predictions.shape[-1]), tags.view(-1))\n",
    "\n",
    "            # Compute categorical accuracy\n",
    "            acc = categorical_acc(predictions, tags, pad_idx)\n",
    "\n",
    "            # Update validation loss and accuracy\n",
    "            val_loss += loss.item()\n",
    "            val_acc += acc\n",
    "\n",
    "    return val_loss / length_dataloader, val_acc / length_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b14c4fd10bb042c682394e7c7a4a5d99",
    "deepnote_cell_type": "markdown",
    "id": "3AZJidLFnOUW"
   },
   "source": [
    "Let's use the functions defined so far and train our model for `30` epochs. We suggest using GPU for this task, as it is quite slow on the CPU. Run the training loop for the given number of epochs and calculate the validation metric at the end of each epoch. Based on the validation loss, save the best checkpoint of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "cell_id": "e8131cfb251e47d284530a09b4cedfdc",
    "deepnote_cell_type": "code",
    "id": "NblN-U0inVYc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_idx: 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not PackedSequence",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32mc:\\Users\\rijhw\\Downloads\\Nilesh-Rijhwanis-Untitled-project\\Assignment_2.ipynb Cell 47\u001B[0m line \u001B[0;36m9\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rijhw/Downloads/Nilesh-Rijhwanis-Untitled-project/Assignment_2.ipynb#X64sZmlsZQ%3D%3D?line=3'>4</a>\u001B[0m \u001B[39mprint\u001B[39m(\u001B[39m\"\u001B[39m\u001B[39mpad_idx:\u001B[39m\u001B[39m\"\u001B[39m, hyper_parameters[\u001B[39m'\u001B[39m\u001B[39mpad_idx\u001B[39m\u001B[39m'\u001B[39m])\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rijhw/Downloads/Nilesh-Rijhwanis-Untitled-project/Assignment_2.ipynb#X64sZmlsZQ%3D%3D?line=6'>7</a>\u001B[0m \u001B[39mfor\u001B[39;00m epoch \u001B[39min\u001B[39;00m \u001B[39mrange\u001B[39m(epochs):\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rijhw/Downloads/Nilesh-Rijhwanis-Untitled-project/Assignment_2.ipynb#X64sZmlsZQ%3D%3D?line=7'>8</a>\u001B[0m     \u001B[39m# Training\u001B[39;00m\n\u001B[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rijhw/Downloads/Nilesh-Rijhwanis-Untitled-project/Assignment_2.ipynb#X64sZmlsZQ%3D%3D?line=8'>9</a>\u001B[0m     train_loss, train_acc, epoch_time \u001B[39m=\u001B[39m train(model, train_dataloader, optimizer, loss_function, hyper_parameters[\u001B[39m'\u001B[39;49m\u001B[39mpad_idx\u001B[39;49m\u001B[39m'\u001B[39;49m])\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rijhw/Downloads/Nilesh-Rijhwanis-Untitled-project/Assignment_2.ipynb#X64sZmlsZQ%3D%3D?line=10'>11</a>\u001B[0m     \u001B[39m# Validation\u001B[39;00m\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rijhw/Downloads/Nilesh-Rijhwanis-Untitled-project/Assignment_2.ipynb#X64sZmlsZQ%3D%3D?line=11'>12</a>\u001B[0m     valid_loss, valid_acc \u001B[39m=\u001B[39m evaluate(model, valid_dataloader, loss_function, hyper_parameters[\u001B[39m'\u001B[39m\u001B[39mpad_idx\u001B[39m\u001B[39m'\u001B[39m])\n",
      "\u001B[1;32mc:\\Users\\rijhw\\Downloads\\Nilesh-Rijhwanis-Untitled-project\\Assignment_2.ipynb Cell 47\u001B[0m line \u001B[0;36m2\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rijhw/Downloads/Nilesh-Rijhwanis-Untitled-project/Assignment_2.ipynb#X64sZmlsZQ%3D%3D?line=24'>25</a>\u001B[0m optimizer\u001B[39m.\u001B[39mzero_grad()\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rijhw/Downloads/Nilesh-Rijhwanis-Untitled-project/Assignment_2.ipynb#X64sZmlsZQ%3D%3D?line=26'>27</a>\u001B[0m \u001B[39m# Forward pass\u001B[39;00m\n\u001B[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rijhw/Downloads/Nilesh-Rijhwanis-Untitled-project/Assignment_2.ipynb#X64sZmlsZQ%3D%3D?line=27'>28</a>\u001B[0m predictions \u001B[39m=\u001B[39m model(packed_text)\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rijhw/Downloads/Nilesh-Rijhwanis-Untitled-project/Assignment_2.ipynb#X64sZmlsZQ%3D%3D?line=29'>30</a>\u001B[0m \u001B[39m# Unpack sequences\u001B[39;00m\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rijhw/Downloads/Nilesh-Rijhwanis-Untitled-project/Assignment_2.ipynb#X64sZmlsZQ%3D%3D?line=30'>31</a>\u001B[0m outputs, _ \u001B[39m=\u001B[39m rnn_utils\u001B[39m.\u001B[39mpad_packed_sequence(predictions, batch_first\u001B[39m=\u001B[39m\u001B[39mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_compiled_call_impl(\u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs)  \u001B[39m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_call_impl(\u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[39m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_pre_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[39mor\u001B[39;00m _global_backward_pre_hooks \u001B[39mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[39mor\u001B[39;00m _global_forward_hooks \u001B[39mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[39mreturn\u001B[39;00m forward_call(\u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[39m=\u001B[39m \u001B[39mNone\u001B[39;00m\n",
      "\u001B[1;32mc:\\Users\\rijhw\\Downloads\\Nilesh-Rijhwanis-Untitled-project\\Assignment_2.ipynb Cell 47\u001B[0m line \u001B[0;36m5\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rijhw/Downloads/Nilesh-Rijhwanis-Untitled-project/Assignment_2.ipynb#X64sZmlsZQ%3D%3D?line=44'>45</a>\u001B[0m \u001B[39m\u001B[39m\u001B[39m'''\u001B[39;00m\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rijhw/Downloads/Nilesh-Rijhwanis-Untitled-project/Assignment_2.ipynb#X64sZmlsZQ%3D%3D?line=45'>46</a>\u001B[0m \u001B[39mS: sentence len\u001B[39;00m\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rijhw/Downloads/Nilesh-Rijhwanis-Untitled-project/Assignment_2.ipynb#X64sZmlsZQ%3D%3D?line=46'>47</a>\u001B[0m \u001B[39mB: batch size\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rijhw/Downloads/Nilesh-Rijhwanis-Untitled-project/Assignment_2.ipynb#X64sZmlsZQ%3D%3D?line=50'>51</a>\u001B[0m \u001B[39mL: number of layers\u001B[39;00m\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rijhw/Downloads/Nilesh-Rijhwanis-Untitled-project/Assignment_2.ipynb#X64sZmlsZQ%3D%3D?line=51'>52</a>\u001B[0m \u001B[39m'''\u001B[39;00m\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rijhw/Downloads/Nilesh-Rijhwanis-Untitled-project/Assignment_2.ipynb#X64sZmlsZQ%3D%3D?line=53'>54</a>\u001B[0m \u001B[39m# Pass text through the embedding layer and a dropout layer\u001B[39;00m\n\u001B[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rijhw/Downloads/Nilesh-Rijhwanis-Untitled-project/Assignment_2.ipynb#X64sZmlsZQ%3D%3D?line=54'>55</a>\u001B[0m embedded \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mdropout(\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49membedding(text))\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rijhw/Downloads/Nilesh-Rijhwanis-Untitled-project/Assignment_2.ipynb#X64sZmlsZQ%3D%3D?line=56'>57</a>\u001B[0m \u001B[39m# Pass embeddings into Bi-LSTM\u001B[39;00m\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rijhw/Downloads/Nilesh-Rijhwanis-Untitled-project/Assignment_2.ipynb#X64sZmlsZQ%3D%3D?line=57'>58</a>\u001B[0m outputs, (hidden, cell) \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mlstm(embedded)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_compiled_call_impl(\u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs)  \u001B[39m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_call_impl(\u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[39m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m (\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backward_pre_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_hooks \u001B[39mor\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[39mor\u001B[39;00m _global_backward_pre_hooks \u001B[39mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[39mor\u001B[39;00m _global_forward_hooks \u001B[39mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[39mreturn\u001B[39;00m forward_call(\u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[39m=\u001B[39m \u001B[39mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    161\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mforward\u001B[39m(\u001B[39mself\u001B[39m, \u001B[39minput\u001B[39m: Tensor) \u001B[39m-\u001B[39m\u001B[39m>\u001B[39m Tensor:\n\u001B[1;32m--> 162\u001B[0m     \u001B[39mreturn\u001B[39;00m F\u001B[39m.\u001B[39;49membedding(\n\u001B[0;32m    163\u001B[0m         \u001B[39minput\u001B[39;49m, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mweight, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mpadding_idx, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mmax_norm,\n\u001B[0;32m    164\u001B[0m         \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mnorm_type, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mscale_grad_by_freq, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49msparse)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\functional.py:2233\u001B[0m, in \u001B[0;36membedding\u001B[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[0;32m   2227\u001B[0m     \u001B[39m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[0;32m   2228\u001B[0m     \u001B[39m# XXX: equivalent to\u001B[39;00m\n\u001B[0;32m   2229\u001B[0m     \u001B[39m# with torch.no_grad():\u001B[39;00m\n\u001B[0;32m   2230\u001B[0m     \u001B[39m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[0;32m   2231\u001B[0m     \u001B[39m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[0;32m   2232\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[39minput\u001B[39m, max_norm, norm_type)\n\u001B[1;32m-> 2233\u001B[0m \u001B[39mreturn\u001B[39;00m torch\u001B[39m.\u001B[39;49membedding(weight, \u001B[39minput\u001B[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001B[1;31mTypeError\u001B[0m: embedding(): argument 'indices' (position 2) must be Tensor, not PackedSequence"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "print(\"pad_idx:\", hyper_parameters['pad_idx'])\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    train_loss, train_acc, epoch_time = train(model, train_dataloader, optimizer, loss_function, hyper_parameters['pad_idx'])\n",
    "\n",
    "    # Validation\n",
    "    valid_loss, valid_acc = evaluate(model, valid_dataloader, loss_function, hyper_parameters['pad_idx'])\n",
    "\n",
    "    # Save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model_checkpoint.pth')\n",
    "    elapsed_mins = int(epoch_time / 60)\n",
    "    elapsed_secs = int(epoch_time - (elapsed_mins * 60))\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {elapsed_mins}m {elapsed_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Validation Loss: {valid_loss:.3f} |  Validation Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "\n",
    "print(\"pad_idx:\", hyper_parameters['pad_idx'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bddf0372c4c249268ea4bf2db1a3a13e",
    "deepnote_cell_type": "markdown",
    "id": "7dXTYFEbHzNL"
   },
   "source": [
    "Question:\n",
    "\n",
    "1. Does overfitting occur? If so, after which epochs?\n",
    "\n",
    "2. How do you detect overfitting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f95d6c65f32144c2b2f586dc3d1594f8",
    "deepnote_cell_type": "markdown",
    "id": "9qSbr8CrH9hD"
   },
   "source": [
    "**Answer:**\n",
    "```\n",
    "Write your answer here.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fec53c90123142189ecd7e74c76a7df8",
    "deepnote_cell_type": "markdown",
    "id": "GmqNesqiMISb"
   },
   "source": [
    "Let's see how well our model is doing on the test set. Load the best checkpoint and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6bb72c495aa243cdbe91715e81f98431",
    "deepnote_cell_type": "code",
    "id": "SgCFqby0m8sf"
   },
   "outputs": [],
   "source": [
    "#### you code ####\n",
    "\n",
    "#### you code ####\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5ee3dafdb8b04e13ac5f39737b2cdef1",
    "deepnote_cell_type": "markdown",
    "id": "tMIDBeSFyqJr"
   },
   "source": [
    "####${\\color{red}{Comments\\ 1.3}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "cross-feedback comment section\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "da21f4ca36ee4e06a2f086ef29cf6f47",
    "deepnote_cell_type": "markdown",
    "id": "7dPfzmVGMeSO"
   },
   "source": [
    "### Subtask 4: Inference\n",
    "\n",
    "Let's use the model we trained to tag some actual sentences. We have the preprocessing pipeline ready from Subtask 1, now we need to map the predictions back to label texts for each token.\n",
    "\n",
    "Implement the `tag_sequence` function that takes a model and a sentence as input and generates POS tags. Keep in mind that you need to divide the sentence into tokens first. For this purpose, we just split each sentence on whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "9d8b114d33e84535890d763ad8fa2e8f",
    "deepnote_cell_type": "code",
    "id": "Xe7_ROCyEslD"
   },
   "outputs": [],
   "source": [
    "def tag_sentence(model, sentence):\n",
    "    ### your code ###\n",
    "\n",
    "\n",
    "    predictions = # make predictions on th esentence\n",
    "\n",
    "\n",
    "    predicted_tags = # get the tags\n",
    "    ### your code ###\n",
    "    return predicted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "8afdb0a5b24a4b88a6c7156f9e60c25f",
    "deepnote_cell_type": "code",
    "id": "JtiR75PFRR61"
   },
   "outputs": [],
   "source": [
    "text=\" \".join(list(UDPOS(split='test'))[0][0])\n",
    "label= list(UDPOS(split='test'))[0][1]\n",
    "predicted_tag=tag_sentence(model,text)\n",
    "print(\"Text: \",text)\n",
    "print(\"Predicted Tags: \",predicted_tag)\n",
    "print(\"True Tags: \",label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c307874d9e3b4573b648257116b7101e",
    "deepnote_cell_type": "markdown",
    "id": "7bPriv78yuyQ"
   },
   "source": [
    "####${\\color{red}{Comments\\ 1.4}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "cross-feedback comment section\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6935ae9bea414697993006e2a366a59e",
    "deepnote_cell_type": "markdown",
    "id": "X4n7Xi4TT_Ww"
   },
   "source": [
    "## **Task 2: Theoretical Questions** (0.5+1.5+1+3=6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "202e970f306a436285609e77413284a9",
    "deepnote_cell_type": "markdown",
    "id": "QbQJ3baXUHkQ"
   },
   "source": [
    "### Subtask 1:\n",
    "In beam search, if you increase the beam width, what will happen to a) the runtime and memory and b) the quality of results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "876efc4921ec4039bb260d2063210afd",
    "deepnote_cell_type": "markdown",
    "id": "Ei5A-D_djZup"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "\n",
    "```\n",
    "In beam search, if you increase the beam width, what will happen to a) the runtime and memory and b) the quality of results?\n",
    "\n",
    "a) runtime: \n",
    "Increasing the beam width in beam search leads to a linear rise in runtime, as it requires evaluating more potential paths per iteration. The computational effort and runtime grow with the number of candidates proportional to the beam size.\n",
    "a)Memory:\n",
    "A wider beam in beam search increases memory usage, as it stores more candidate sequences along with their associated data like scores and states, potentially straining memory in limited-capacity systems.\n",
    "b) the quality of results:\n",
    "Increasing the beam width in beam search can lead to better results by exploring more options. However, after a certain point, the improvement in results doesn't make up for the extra time and resources needed. Also, if the beam is too wide, the search might focus on less important sequences, especially if it's not good at picking the best ones from a larger group.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "2759650a8d334a9aa684d095549e808c",
    "deepnote_cell_type": "markdown",
    "id": "RuzX2q7TywyW"
   },
   "source": [
    "####${\\color{red}{Comments\\ 2.1}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "cross-feedback comment section\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e3e84afe54074de0953a1678ec45ce25",
    "deepnote_cell_type": "markdown",
    "id": "N7KlRPnhknbl"
   },
   "source": [
    "### Subtask 2:\n",
    "Except for beam search, there are other ways to create a more coherent output for generation tasks, one of which is adding a temperature to the softmax over the vocabulary. Temperature is a hyperparameter that is applied to the input of a softmax to affect the final probabilities. All values in the input are divided by the temperature before going through the softmax. What do you think will happen in these cases:\n",
    "\n",
    "1. A low temperature - below 1\n",
    "2. A high temperature - above 1\n",
    "3. Really small temperature - temperature $→$ 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "73a5bffaa5044b24aad9ed2be4e96985",
    "deepnote_cell_type": "markdown",
    "id": "uxuh5LOFnF3D"
   },
   "source": [
    "**Answer:**\n",
    "```\n",
    "1. A low temperature - below 1:\n",
    "Below 1, the softmax function produces a \"sharper\" probability distribution, which means the most likely tokens have a higher probability and the least likely tokens have a lower probability. In turn, this can lead to more conservative and deterministic outputs. In the context of text generation, a low temperature may lead to more predictable and repetitive sequences, as the model becomes more confident and selects the most likely tokens with greater assurance.\n",
    "2.High Temperature (Above 1):\n",
    "On the other hand, a high temperature, above 1, results in an even distribution of probabilities across the vocabulary.\n",
    "It can also lead to more diverse and exploratory outcomes since the model becomes less certain about its predictions and assigns more comparable probabilities to different tokens. Text generation is encouraged by a high temperature because the model becomes less constrained and more open to exploring different options, resulting in more creative sequences.\n",
    "3. Really small temperature - temperature $→$ 0\n",
    "At low temperatures, the softmax function tends toward a \"hard\" distribution, where the probability mass is increasingly concentrated on the most likely token, making the distribution more deterministic. In the context of text generation, an extremely small temperature can lead to highly repetitive and predictable outputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3e276e51cf374e5aae15313b9c4d1b1e",
    "deepnote_cell_type": "markdown",
    "id": "GmW4NNb4yyZF"
   },
   "source": [
    "####${\\color{red}{Comments\\ 2.2}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "cross-feedback comment section\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bcb26ecefdc144b8a307524d620be487",
    "deepnote_cell_type": "markdown",
    "id": "tM2DmNOwod4r"
   },
   "source": [
    "### Subtask 3:\n",
    "Explain what the “bottleneck” of an encoder-decoder RNN is and how attention provides a way to get around this bottleneck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "960e471978e8432089bd9719c18bd1d0",
    "deepnote_cell_type": "markdown",
    "id": "DNJ3f_MtdzrM"
   },
   "source": [
    "**Answer:**\n",
    "```\n",
    "The bottleneck of an encoder-decoder RNN stems from the fixed-length vector representation of the input sequence, which can limit the model's ability to process long sequences effectively. Attention provides a solution to this bottleneck by enabling the model to dynamically focus on different parts of the input sequence during the decoding process, thereby enhancing its ability to handle longer sequences and capture relevant information more effectively\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4de219e3f4a14bc58d5e1245704f9f3c",
    "deepnote_cell_type": "markdown",
    "id": "JFUHVF40y0j-"
   },
   "source": [
    "####${\\color{red}{Comments\\ 2.3}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "cross-feedback comment section\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ba948b977bba435fb8c30514b0b7a31d",
    "deepnote_cell_type": "markdown",
    "id": "HaHhdLdJehL3"
   },
   "source": [
    "### Subtask 4:\n",
    "As mentioned, there are various way to remedy the repetitiveness and incoherence of generation outputs. One of the widely used methods is Nucleus sampling described the paper \"[ The Curious Case of Neural Text DeGeneration](https://arxiv.org/pdf/1904.09751.pdf)\". Read the model section and introduction of the paper and use it as reference to answer the following questions:\n",
    "\n",
    "1. Describe top-k sampling in your own words, no need for mathematical notation.\n",
    "2. Describe Nucleus sampling in your own words, there is not need for mathematical notation.\n",
    "3.  Why is beam search not a good strategy for human-like text generation and why don't these methods suffer from the problem of the beam search?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c7d293512a8a49a38d5bbfb8dfebb111",
    "deepnote_cell_type": "markdown",
    "id": "9Ojtbn1kgaYE"
   },
   "source": [
    "**Answer:**\n",
    "```\n",
    "1.Describe top-k sampling in your own words, no need for mathematical notation.\n",
    "In text generation, a useful technique is the top-k sampling method. To implement this technique, first, k words are chosen from the model's probability distribution at each time step. The selected value of k never changes throughout the process. Next, the probability distribution is altered according to the total probabilities of the top k words. Lastly, a word is randomly chosen from the adjusted distribution, and this process is conducted repeatedly until the desired length of text is achieved. Compared to beam search, top-k sampling gives superior control over the variety of the produced text. The challenge lies in finding the perfect k value, where a low k might bring about unexciting or repeated text, whereas a high k may present improper choices.\n",
    "2.Describe Nucleus sampling in your own words, there is not need for mathematical notation.\n",
    "Selective text creation, also known as nucleus sampling, is a novel strategy that addresses the shortcomings of commonly used sampling methods such as pure sampling and top-k sampling. Rather than selecting a specified number of likely words, nucleus sampling extracts a component of the lexicon that represents a specific percentage of the total probability mass. This unit, termed \"the nucleus,\" can change in size in accordance with the probability distribution of each time step. Nucleus sampling focuses on the most likely words by eliminating lower probability words that can result in nonsensical or poorly connected phrases. High k values must be avoided when using nucleus sampling to introduce diversity while avoiding incoherence concerns. The proportion of the probability mass determined by a parameter called p determines the size of the nucleus.\n",
    "3.Why is beam search not a good strategy for human-like text generation and why don't these methods suffer from the problem of the beam search?\n",
    "Beam search is an ineffective technique for human-like text generation since it favors the most likely terms at each time step, resulting in repetitious and generic content. This is because language models reward well-formed content, but the top results for longer texts are frequently generic, repetitive, and clunky. Human-written text, on the other hand, tends to be more varied and instructive, with lower probabilities allocated to more informative but less predictable terms.\n",
    "By incorporating randomness into the selection of words, nucleus sampling and other stochastic decoding approaches such as top-k sampling overcome this problem, allowing for more variation in output text. Nucleus sampling, in particular, concentrates on the most likely words while avoiding the untrustworthy tail of low probability words, which might result in incoherent or unrelated material. Nucleus sampling ensures that the generated text is both high-quality and diversified, akin to human-written text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ee946366c36848fa8817fbef0c158da7",
    "deepnote_cell_type": "markdown",
    "id": "0wU67Iqpy2rg"
   },
   "source": [
    "####${\\color{red}{Comments\\ 2.4}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "cross-feedback comment section\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "58484791be204641bd8f9f4af2b65dda",
    "deepnote_cell_type": "markdown",
    "id": "IZl0-MMY6MWs"
   },
   "source": [
    "## **Task 3: Scaled Dot-Product Attention** (4+1=5 points)\n",
    "In class, you learned about attention and Transformers as described in the 2017 paper\n",
    "[Attention Is All You Need](https://arxiv.org/abs/1706.03762).\n",
    "The base of the attention module is a scaled dot product with Queries, Keys, and Values.\n",
    "In this task, you will implement a simplified version of scaled dot-product attention and inspired by the translation task, aim to replicate word alignment between English and French.\n",
    "You will not be training the embedding from scratch, we provide you with pre-trained embedding for both languages.\n",
    "However, you need to know the details of scaled dot product attention, which mainly consists of two matrix multiplications and a softmax scaling.\n",
    "Refer to Figure 2 of the [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper.\n",
    "\n",
    "The inputs of the attention module are Queries, Keys, and Values. Mathematically, attention is defined as follows:\n",
    "\n",
    "$$\n",
    "\\large \\mathrm{Attention}\\left(Q, K, V\\right) = \\mathrm{softmax}\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "\n",
    "*   $Q$, $K$, and $V$ are the Queries, Keys, and Values matrices.\n",
    "* $d_k$ is the dimension of the Keys (in practice dimensions of all matrices are the same).\n",
    "*   $QK^{\\top}$ is a measure of the similarity between the Queries and the Keys\n",
    "* softmax transforms the similarity into weights.\n",
    "* Weights multiplied by the Values are the output of the attention, defining how much importance should be given to each token of the input.\n",
    "\n",
    "In the case of self-attention, both Queries and Keys come from the encoder, however, for cross attention between encoder and decoder, decoder states are used as the queries while encoder states are the Keys and Values.\n",
    "In our case, we need the cross attention between one language to another to find the correct alignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "5fc11da9656146ccb5f2bfbd59f70def",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4113,
    "execution_start": 1701022940711,
    "id": "gOP3Fa8IfkMK",
    "source_hash": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /shared-libs/python3.9/py/lib/python3.9/site-packages (3.6.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib) (4.37.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.19 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib) (1.23.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.1\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython -m pip install --upgrade pip\u001B[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "1fb8ef8d682c4450a0297c52798dd68b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2785,
    "execution_start": 1701022948214,
    "id": "tB6RD3wssHe9",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#load the dictionaries (dictionary of words to ids)\n",
    "with open(\"./word2int_en.pkl\", \"rb\") as f:\n",
    "    en_dict = pickle.load(f)\n",
    "\n",
    "with open(\"./word2int_fr.pkl\", \"rb\") as f:\n",
    "    fr_dict = pickle.load(f)\n",
    "\n",
    "# load word embeddings (dictionary of token ids to embeddings)\n",
    "en_embeddings = np.load(\"./embeddings_en.npz\")[\"embeddings\"]\n",
    "fr_embeddings = np.load(\"./embeddings_fr.npz\")[\"embeddings\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "37af9b34bac54df9ad73d386b3896763",
    "deepnote_cell_type": "markdown",
    "id": "7VYjN1xHsbYj"
   },
   "source": [
    "### Subtask 1: Attention Weights\n",
    "Fill the blanks in `tokenize` to tokenize a sentence and convert it to ids and `embed` function to create an embedding of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ea23613a215f4cceb88b4933df36927e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 11,
    "execution_start": 1701022974860,
    "id": "cwS_M10nwOBH",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "def tokenize(sentence, token_mapping):\n",
    "   # we stick to simple blank space tokenization\n",
    "   tokenized = []\n",
    "   for word in sentence.lower().split(\" \"):\n",
    "      token_id = token_mapping.get(word,-1)\n",
    "      tokenized.append(token_id)\n",
    "   return tokenized\n",
    "\n",
    "def embed(tokens, embeddings):\n",
    "    \"\"\" get the embedding for the tokens in a sentence stacked in a simple matrix (sequence length, embedding size)\n",
    "        tokens: tokenized sentence\n",
    "        embeddings: dictionary of token to embeddings.\n",
    "    \"\"\"\n",
    "    embed_size = embeddings.shape[1]\n",
    "    output = np.zeros((len(tokens), embed_size))\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token != -1:\n",
    "            output[i] = embeddings[token]\n",
    "        else:\n",
    "            output[i] = np.zeros(embed_size)\n",
    "    # Note that the return statement is outside the for loop\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ca906500c38f4575ba20fb52541d792f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 12,
    "execution_start": 1701022982412,
    "id": "dkAulXarueT_",
    "source_hash": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized english: [59, 40, 9355, 6, 158, -1]\n",
      "embedding english: (6, 300)\n",
      "Tokenized french: [21, 73, 192, 16, 8652, -1, 558, -1]\n",
      "embedding french: (8, 300)\n"
     ]
    }
   ],
   "source": [
    "sentence_en = \"there were clouds in my coffeeeeee.\"\n",
    "tokenized_en = tokenize(sentence_en, en_dict)\n",
    "embedded_en = embed(tokenized_en, en_embeddings)\n",
    "print(\"Tokenized english:\",tokenized_en)\n",
    "print(\"embedding english:\",embedded_en.shape)\n",
    "\n",
    "sentence_fr = \"il y avait des nuages ​​dans mon ccafé.\"\n",
    "tokenized_fr = tokenize(sentence_fr, fr_dict)\n",
    "embedded_fr = embed(tokenized_fr, fr_embeddings)\n",
    "print(\"Tokenized french:\",tokenized_fr)\n",
    "print(\"embedding french:\",embedded_fr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "✅ Point distribution ✅\n",
    "- 0.5 if the tokenizations are [59, 40, 9355, 6, 158, -1] and [21, 73, 192, 16, 8652, -1, 558, -1], check the tokenizers code if the values differ to see if the logic is correct to assign partial points. If the value of the unknown token is not exactly -1 then it is ok, but it should be considered.\n",
    "- 0.5 if the shape of the embeddings are (6, 300) and (8, 300), if loaded properly the shapes should match, otherwise check the code."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b4cca51e5b3d4323a6cce529ae9a9df8",
    "deepnote_cell_type": "markdown",
    "id": "WJ58FCMpsHJP"
   },
   "source": [
    "\n",
    "Implement the `softmax` function with `Numpy`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "901b142440094f0c8e9a9ee4251df828",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7,
    "execution_start": 1701022994851,
    "id": "caOjXfbxon9h",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "def softmax(x, axis=0):\n",
    "    \"\"\"\n",
    "    x: input matrix\n",
    "    axis: defines which axis to compute the softmax over 0 for rows and 1 for columns\n",
    "        axis=0 calculates softmax across rows which means each column sums to 1\n",
    "        axis=1 calculates softmax across columns which means each row sums to 1\n",
    "    \"\"\"\n",
    "\n",
    "    #### your code ####\n",
    "    e_x = np.exp(x-np.max(x,axis=axis,keepdims=True))\n",
    "    \n",
    "    softmax_x= e_x/np.sum(e_x,axis=axis,keepdims=True)\n",
    "    #### your code ####\n",
    "\n",
    "    return softmax_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "658ddd4f45784a38946e53d5093f3bfb",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1701022997709,
    "id": "kCbjmG49p-v0",
    "source_hash": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=softmax(np.array([[1,3,4,1], [24,3,2,3]]),axis=0)\n",
    "w.sum(axis=0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "✅ Point distribution ✅\n",
    "- 0.5 if the sum is 1.0, if not check the code.\n",
    "- 0.5 if the axis=axis, keepdims=True are also set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "75b30fe7a51049a6b7e4a06d396c0bdc",
    "deepnote_cell_type": "markdown",
    "id": "oNqtNPB9orLr"
   },
   "source": [
    "Use the `softmax` function to calculate the weights.\n",
    "$$ \\mathrm{softmax}\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)$$\n",
    "Assume the queries and keys are 2D matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "8c43de46d34e45448f7a7d7f9d1aad80",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 44,
    "execution_start": 1701023014072,
    "id": "ysRdVCnP7Qir",
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "def calc_weights(queries, keys):\n",
    "    \"\"\"\n",
    "    queries: queries matrix\n",
    "    keys: keys matrix\n",
    "    \"\"\"\n",
    "    #### your code ####\n",
    "    #calculate the dimensions of the keys\n",
    "    d_k = keys.shape[1]  # Assuming keys is a 2D matrix of shape [num_keys, key_dimension]\n",
    "    raw_scores = np.matmul(queries, keys.T)\n",
    "    scaled_scores = raw_scores / np.sqrt(d_k)\n",
    "    print(\"Raw scores:\")\n",
    "    print(raw_scores)\n",
    "    print(\"Scaled scores:\")\n",
    "    print(scaled_scores)\n",
    "    return softmax(scaled_scores, axis=1)\n",
    "    #### your code ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "56c312a73de54dae85aeee03ee4e0b5c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 19,
    "execution_start": 1701023021050,
    "id": "LVYGelEsqsoo",
    "source_hash": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw scores:\n",
      "[[ 9 11]\n",
      " [ 9 11]]\n",
      "Scaled scores:\n",
      "[[6.36396103 7.77817459]\n",
      " [6.36396103 7.77817459]]\n",
      "[[0.19557032 0.80442968]\n",
      " [0.19557032 0.80442968]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights=calc_weights(np.array([[1,3],[1,3]]),np.array([[0,3],[2,3]]))\n",
    "print(weights)\n",
    "weights.sum(axis=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "✅ Point distribution ✅\n",
    "- 0.5 if the sum is 1.0, if not check the code.\n",
    "- 0.5 if the weights are :\n",
    "                            [[0.19557032 0.80442968]\n",
    "                            [0.19557032 0.80442968]]."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ef15904e3b2c4a7d9a0dd3b8bafb88d4",
    "deepnote_cell_type": "markdown",
    "id": "16V5yFAew1Hx"
   },
   "source": [
    "Use the `calcu_weights` to compute the attention matrix between two sentences from English and French and visualize the weights to check for alignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "2d744086fc01473da6d6776e7375fd55",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 639,
    "execution_start": 1701023046044,
    "id": "miW865arw9Fv",
    "source_hash": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw scores:\n",
      "[[ 0.11143156 -0.11918585 -0.11104156  0.18105114 -0.03099746 -0.06891545\n",
      "   0.16896826  0.          0.18105114  0.17189078  0.        ]\n",
      " [ 0.23277421 -0.02150457 -0.11164849  0.14258683 -0.0749176   0.00824908\n",
      "   0.14113042  0.          0.14258683  0.04867125  0.        ]\n",
      " [-0.02766669  0.18549792 -0.13051698  0.02532783 -0.02940436 -0.05491569\n",
      "  -0.02074198  0.          0.02532783  0.03975966  0.        ]\n",
      " [ 0.1332272   0.13820109 -0.02096431  0.17196022 -0.09233489 -0.02274998\n",
      "   0.28538581  0.          0.17196022  0.31356981  0.        ]\n",
      " [-0.08841272 -0.09345124  0.56232713 -0.00463369  0.00131161 -0.01447119\n",
      "   0.03417505  0.         -0.00463369  0.01371714  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [-0.00854627 -0.08358519  0.00771695 -0.02131187  0.39898962  0.0087569\n",
      "   0.05460894  0.         -0.02131187  0.07041253  0.        ]\n",
      " [-0.04822296 -0.11248157 -0.03778477  0.0395954  -0.02321789  0.46743281\n",
      "   0.01368038  0.          0.0395954   0.02704021  0.        ]\n",
      " [ 0.08120773  0.0951387  -0.00420785  0.20112103 -0.03427692  0.04095905\n",
      "   0.54909342  0.          0.20112103  0.19802181  0.        ]\n",
      " [ 0.1332272   0.13820109 -0.02096431  0.17196022 -0.09233489 -0.02274998\n",
      "   0.28538581  0.          0.17196022  0.31356981  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [ 0.12980945  0.00123662  0.00987953  0.4101532  -0.04402428  0.07958043\n",
      "   0.23575377  0.          0.4101532   0.2660732   0.        ]\n",
      " [ 0.05786075 -0.01844692  0.00269111  0.25106747 -0.06843535  0.04558987\n",
      "   0.2766422   0.          0.25106747  0.32839157  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]]\n",
      "Scaled scores:\n",
      "[[ 6.43350412e-03 -6.88119826e-03 -6.41098746e-03  1.04529924e-02\n",
      "  -1.78963919e-03 -3.97883536e-03  9.75538704e-03  0.00000000e+00\n",
      "   1.04529924e-02  9.92411881e-03  0.00000000e+00]\n",
      " [ 1.34392253e-02 -1.24156693e-03 -6.44602858e-03  8.23225447e-03\n",
      "  -4.32536965e-03  4.76260856e-04  8.14816860e-03  0.00000000e+00\n",
      "   8.23225447e-03  2.81003593e-03  0.00000000e+00]\n",
      " [-1.59733709e-03  1.07097274e-02 -7.53540135e-03  1.46230295e-03\n",
      "  -1.69766152e-03 -3.17055884e-03 -1.19753877e-03  0.00000000e+00\n",
      "   1.46230295e-03  2.29552504e-03  0.00000000e+00]\n",
      " [ 7.69187598e-03  7.97904365e-03 -1.21037500e-03  9.92812793e-03\n",
      "  -5.33095736e-03 -1.31347071e-03  1.64767574e-02  0.00000000e+00\n",
      "   9.92812793e-03  1.81039614e-02  0.00000000e+00]\n",
      " [-5.10451077e-03 -5.39540986e-03  3.24659720e-02 -2.67526217e-04\n",
      "   7.57258387e-05 -8.35494544e-04  1.97309743e-03  0.00000000e+00\n",
      "  -2.67526217e-04  7.91959447e-04  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-4.93419129e-04 -4.82579319e-03  4.45538316e-04 -1.23044139e-03\n",
      "   2.30356765e-02  5.05579857e-04  3.15284862e-03  0.00000000e+00\n",
      "  -1.23044139e-03  4.06526931e-03  0.00000000e+00]\n",
      " [-2.78415389e-03 -6.49412647e-03 -2.18150471e-03  2.28604148e-03\n",
      "  -1.34048550e-03  2.69872459e-02  7.89837108e-04  0.00000000e+00\n",
      "   2.28604148e-03  1.56116725e-03  0.00000000e+00]\n",
      " [ 4.68853048e-03  5.49283541e-03 -2.42940333e-04  1.16117281e-02\n",
      "  -1.97897890e-03  2.36477185e-03  3.17019234e-02  0.00000000e+00\n",
      "   1.16117281e-02  1.14327945e-02  0.00000000e+00]\n",
      " [ 7.69187598e-03  7.97904365e-03 -1.21037500e-03  9.92812793e-03\n",
      "  -5.33095736e-03 -1.31347071e-03  1.64767574e-02  0.00000000e+00\n",
      "   9.92812793e-03  1.81039614e-02  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 7.49455209e-03  7.13962890e-05  5.70394930e-04  2.36802060e-02\n",
      "  -2.54174299e-03  4.59457827e-03  1.36112503e-02  0.00000000e+00\n",
      "   2.36802060e-02  1.53617434e-02  0.00000000e+00]\n",
      " [ 3.34059196e-03 -1.06503342e-03  1.55371308e-04  1.44953871e-02\n",
      "  -3.95111677e-03  2.63213237e-03  1.59719449e-02  0.00000000e+00\n",
      "   1.44953871e-02  1.89596961e-02  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "Alignment matrix:\n",
      "[[0.09126155 0.09005448 0.09009684 0.09162911 0.09051417 0.09031623\n",
      "  0.09156522 0.0906763  0.09162911 0.09158067 0.0906763 ]\n",
      " [0.09189221 0.09055302 0.09008296 0.09141498 0.0902742  0.0907087\n",
      "  0.09140729 0.09066551 0.09141498 0.09092064 0.09066551]\n",
      " [0.09075714 0.091881   0.09021982 0.09103525 0.09074804 0.09061447\n",
      "  0.09079343 0.09090223 0.09103525 0.09111114 0.09090223]\n",
      " [0.09109157 0.09111773 0.09028425 0.0912955  0.08991299 0.09027494\n",
      "  0.09189533 0.0903936  0.0912955  0.09204498 0.0903936 ]\n",
      " [0.09024933 0.09022308 0.09370454 0.09068692 0.09071805 0.09063542\n",
      "  0.09089034 0.09071118 0.09068692 0.09078305 0.09071118]\n",
      " [0.09090909 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909\n",
      "  0.09090909 0.09090909 0.09090909 0.09090909 0.09090909]\n",
      " [0.09066874 0.09027678 0.09075391 0.09060194 0.09282739 0.09075936\n",
      "  0.09099995 0.09071349 0.09060194 0.09108301 0.09071349]\n",
      " [0.09047939 0.09014434 0.09053393 0.0909393  0.09061011 0.09321359\n",
      "  0.09080334 0.09073165 0.0909393  0.09087341 0.09073165]\n",
      " [0.09069796 0.09077093 0.09025178 0.09132805 0.09009524 0.09048744\n",
      "  0.09318141 0.09027371 0.09132805 0.09131171 0.09027371]\n",
      " [0.09109157 0.09111773 0.09028425 0.0912955  0.08991299 0.09027494\n",
      "  0.09189533 0.0903936  0.0912955  0.09204498 0.0903936 ]\n",
      " [0.09090909 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909\n",
      "  0.09090909 0.09090909 0.09090909 0.09090909 0.09090909]\n",
      " [0.09087144 0.09019939 0.09024441 0.09235422 0.08996399 0.0906083\n",
      "  0.09142898 0.09019295 0.09235422 0.09158916 0.09019295]\n",
      " [0.09067278 0.09027419 0.09038443 0.09168988 0.09001403 0.09060856\n",
      "  0.09182536 0.09037038 0.09168988 0.09210013 0.09037038]\n",
      " [0.09090909 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909\n",
      "  0.09090909 0.09090909 0.09090909 0.09090909 0.09090909]]\n",
      "Alignment matrix value range:  0.08991299289898745 0.09370453641488403\n",
      "The top 2 alignments (French to English):\n",
      "French word 'il': Top English words: ['in', 'in']\n",
      "French word 'y': Top English words: ['there', 'in']\n",
      "French word 'avait': Top English words: ['were', 'the']\n",
      "French word 'des': Top English words: ['the', 'and']\n",
      "French word 'nuages': Top English words: ['clouds', 'and']\n",
      "French word '​​dans': Top English words: ['there', 'were']\n",
      "French word 'mon': Top English words: ['my', 'the']\n",
      "French word 'café': Top English words: ['coffee', 'in']\n",
      "French word 'et': Top English words: ['and', 'in']\n",
      "French word 'des': Top English words: ['the', 'and']\n",
      "French word 'licornes': Top English words: ['there', 'were']\n",
      "French word 'dans': Top English words: ['in', 'in']\n",
      "French word 'le': Top English words: ['the', 'and']\n",
      "French word 'ciel.': Top English words: ['there', 'were']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 704,
       "width": 695
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence_en = \"there were clouds in my coffee and unicorns in the sky.\"\n",
    "tokenized_en = tokenize(sentence_en, en_dict)\n",
    "embedded_en = embed(tokenized_en, en_embeddings)\n",
    "\n",
    "sentence_fr = \"il y avait des nuages ​​dans mon café et des licornes dans le ciel.\"\n",
    "tokenized_fr = tokenize(sentence_fr, fr_dict)\n",
    "embedded_fr = embed(tokenized_fr, fr_embeddings)\n",
    "# Compute the range of the alignment matrix for visualization scaling\n",
    "alignment = calc_weights(embedded_fr, embedded_en)\n",
    "\n",
    "# Inspect the alignment matrix\n",
    "print(\"Alignment matrix:\")\n",
    "print(alignment)\n",
    "\n",
    "# Check the range of the alignment matrix\n",
    "alignment_min = np.min(alignment)\n",
    "alignment_max = np.max(alignment)\n",
    "\n",
    "\n",
    "# Visualize the weights with adjusted color scale\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cax = ax.imshow(alignment, cmap='gray', aspect='equal', vmin=alignment_min, vmax=alignment_max)\n",
    "fig.colorbar(cax)\n",
    "ax.xaxis.tick_top()\n",
    "ax.set_xticks(np.arange(len(sentence_en.split(\" \"))))\n",
    "ax.set_xticklabels(sentence_en.split(\" \"), rotation=90)\n",
    "ax.set_yticks(np.arange(len(sentence_fr.split(\" \"))))\n",
    "ax.set_yticklabels(sentence_fr.split(\" \"))\n",
    "\n",
    "# Print the range of values in the alignment matrix\n",
    "print(\"Alignment matrix value range: \", alignment_min, alignment_max)\n",
    "\n",
    "# Compute and print the top-2 alignments\n",
    "top_2 = np.argsort(-alignment, axis=1)[:, :2]\n",
    "print('The top 2 alignments (French to English):')\n",
    "for i, row in enumerate(top_2):\n",
    "    french_word = sentence_fr.split(\" \")[i]\n",
    "    english_words_alignment = [sentence_en.split(\" \")[index] for index in row]\n",
    "    print(f\"French word '{french_word}': Top English words: {english_words_alignment}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6ce92b5ef0bf46ab844fc68b566763c2",
    "deepnote_cell_type": "markdown",
    "id": "0BxnOJJ2xhIZ"
   },
   "source": [
    "1. What are the top 2 alignments shown in the figure?\n",
    "2. On the rows you see flat lines for `ciel` and `dans` and `licornes`, why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d153c49b8a284aa2835dc7a11d3f0367",
    "deepnote_cell_type": "markdown",
    "id": "QUwdKXhZxqUP"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "```\n",
    "1.What are the top 2 alignments shown in the figure?\n",
    "French word 'licornes': Top English words: ['there', 'were']\n",
    "French word 'dans': Top English words: ['in', 'in']\n",
    "French word 'le': Top English words: ['the', 'and']\n",
    "French word 'ciel.': Top English words: ['there', 'were']\n",
    "2.On the rows you see flat lines for ciel and dans and licornes, why do you think that is?\n",
    "The flat lines in the heatmap rows for \"ciel,\" \"dans,\" and \"licornes\" indicate that these French words are being given nearly equal attention weights across all English words. This can be for teh following reasons:\n",
    "1. If the embeddings do not capture the nuances between different words well, the model may not be able to distinguish between them effectively.\n",
    "2.The attention model might be too simple or not trained appropriately to capture the complexity of the word alignments between the two languages.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "✅ Point distribution ✅\n",
    "- 0.5 if the sum is 1.0, if not check the code.\n",
    "- 0.5 if the weights are :\n",
    "                            [[0.19557032 0.80442968]\n",
    "                            [0.19557032 0.80442968]]."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "af80ead2403c49298774af031408a2bc",
    "deepnote_cell_type": "markdown",
    "id": "C2TnQmsxy70i"
   },
   "source": [
    "####${\\color{red}{Comments\\ 3.1}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "The implementation of the tokenize and embed functions works exactly the same as the one in the solutions.\n",
    "\n",
    "The softmax function in the solutions is numerically more stable than your implementation because they used a trick for numerical stability by subtracting the maximum value in the input array (np.max(x, axis=axis, keepdims=True)) from x before computing the exponential. \n",
    "Of course, your implementation is correct too.\n",
    "\n",
    "Unfortunately, you did not correctly find the top 2 alignments. \n",
    "The main problem here is that you have performed row-wise sorting. For example, if your -alignments are as follows:\n",
    "-alignments = [[-1, -3, -2],\n",
    "               [-4, -6, -5]]\n",
    "For the first row [-1, -3, -2], the sorted order is [-3, -2, -1]. \n",
    "The indices to achieve this order in the original row are [1, 2, 0]. \n",
    "For the second row [-4, -6, -5], the sorted order is [-6, -5, -4]. \n",
    "The indices to achieve this order in the original row are [1, 2, 0]. \n",
    "This means your top 2 word (row-col) indices are as follows:\n",
    "- For the first word: (the id of the column with the highest value in the row 0, the id of the column with the second highest value in the row 1)\n",
    "- For the second word: (the id of the column with the highest value in the row 0, the id of the column with the second highest value in the row 1)\n",
    "(-0.5)\n",
    "\n",
    "Unfortunately, your explanation for the reason of seeing a flat line for ciel, dans, and licornes is not correct. \n",
    "The reason is that these three words are out of vocabulary, and we used np.zeros() as the embedding of these words. \n",
    "Every dot product with a vector full of zeros is always 0. To sum up, we have no embeddings for these words and used np.zeros() which is the reason for the flat lines.\n",
    "(-0.5)\n",
    "\n",
    "All other things are almost identical and have an exact match with the ones in the solutions.\n",
    "\n",
    "Total Points: 3 / 4\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ffa6d115b1cd43b3ab5d8b9fba5705b6",
    "deepnote_cell_type": "markdown",
    "id": "eqXwL4MpzW--"
   },
   "source": [
    "### Subtask 2: Scaled Dot-product\n",
    "Implement the scaled dot-product attention using the functions from above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "1968f7988c80421c82f7c3ad8d8e628a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 20,
    "execution_start": 1701023121799,
    "id": "8iQTr4KCzqZk",
    "source_hash": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw scores:\n",
      "[[ 0.11143156 -0.11918585 -0.11104156  0.18105114 -0.03099746 -0.06891545\n",
      "   0.16896826  0.          0.18105114  0.17189078  0.        ]\n",
      " [ 0.23277421 -0.02150457 -0.11164849  0.14258683 -0.0749176   0.00824908\n",
      "   0.14113042  0.          0.14258683  0.04867125  0.        ]\n",
      " [-0.02766669  0.18549792 -0.13051698  0.02532783 -0.02940436 -0.05491569\n",
      "  -0.02074198  0.          0.02532783  0.03975966  0.        ]\n",
      " [ 0.1332272   0.13820109 -0.02096431  0.17196022 -0.09233489 -0.02274998\n",
      "   0.28538581  0.          0.17196022  0.31356981  0.        ]\n",
      " [-0.08841272 -0.09345124  0.56232713 -0.00463369  0.00131161 -0.01447119\n",
      "   0.03417505  0.         -0.00463369  0.01371714  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [-0.00854627 -0.08358519  0.00771695 -0.02131187  0.39898962  0.0087569\n",
      "   0.05460894  0.         -0.02131187  0.07041253  0.        ]\n",
      " [-0.04822296 -0.11248157 -0.03778477  0.0395954  -0.02321789  0.46743281\n",
      "   0.01368038  0.          0.0395954   0.02704021  0.        ]\n",
      " [ 0.08120773  0.0951387  -0.00420785  0.20112103 -0.03427692  0.04095905\n",
      "   0.54909342  0.          0.20112103  0.19802181  0.        ]\n",
      " [ 0.1332272   0.13820109 -0.02096431  0.17196022 -0.09233489 -0.02274998\n",
      "   0.28538581  0.          0.17196022  0.31356981  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]\n",
      " [ 0.12980945  0.00123662  0.00987953  0.4101532  -0.04402428  0.07958043\n",
      "   0.23575377  0.          0.4101532   0.2660732   0.        ]\n",
      " [ 0.05786075 -0.01844692  0.00269111  0.25106747 -0.06843535  0.04558987\n",
      "   0.2766422   0.          0.25106747  0.32839157  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.        ]]\n",
      "Scaled scores:\n",
      "[[ 6.43350412e-03 -6.88119826e-03 -6.41098746e-03  1.04529924e-02\n",
      "  -1.78963919e-03 -3.97883536e-03  9.75538704e-03  0.00000000e+00\n",
      "   1.04529924e-02  9.92411881e-03  0.00000000e+00]\n",
      " [ 1.34392253e-02 -1.24156693e-03 -6.44602858e-03  8.23225447e-03\n",
      "  -4.32536965e-03  4.76260856e-04  8.14816860e-03  0.00000000e+00\n",
      "   8.23225447e-03  2.81003593e-03  0.00000000e+00]\n",
      " [-1.59733709e-03  1.07097274e-02 -7.53540135e-03  1.46230295e-03\n",
      "  -1.69766152e-03 -3.17055884e-03 -1.19753877e-03  0.00000000e+00\n",
      "   1.46230295e-03  2.29552504e-03  0.00000000e+00]\n",
      " [ 7.69187598e-03  7.97904365e-03 -1.21037500e-03  9.92812793e-03\n",
      "  -5.33095736e-03 -1.31347071e-03  1.64767574e-02  0.00000000e+00\n",
      "   9.92812793e-03  1.81039614e-02  0.00000000e+00]\n",
      " [-5.10451077e-03 -5.39540986e-03  3.24659720e-02 -2.67526217e-04\n",
      "   7.57258387e-05 -8.35494544e-04  1.97309743e-03  0.00000000e+00\n",
      "  -2.67526217e-04  7.91959447e-04  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-4.93419129e-04 -4.82579319e-03  4.45538316e-04 -1.23044139e-03\n",
      "   2.30356765e-02  5.05579857e-04  3.15284862e-03  0.00000000e+00\n",
      "  -1.23044139e-03  4.06526931e-03  0.00000000e+00]\n",
      " [-2.78415389e-03 -6.49412647e-03 -2.18150471e-03  2.28604148e-03\n",
      "  -1.34048550e-03  2.69872459e-02  7.89837108e-04  0.00000000e+00\n",
      "   2.28604148e-03  1.56116725e-03  0.00000000e+00]\n",
      " [ 4.68853048e-03  5.49283541e-03 -2.42940333e-04  1.16117281e-02\n",
      "  -1.97897890e-03  2.36477185e-03  3.17019234e-02  0.00000000e+00\n",
      "   1.16117281e-02  1.14327945e-02  0.00000000e+00]\n",
      " [ 7.69187598e-03  7.97904365e-03 -1.21037500e-03  9.92812793e-03\n",
      "  -5.33095736e-03 -1.31347071e-03  1.64767574e-02  0.00000000e+00\n",
      "   9.92812793e-03  1.81039614e-02  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 7.49455209e-03  7.13962890e-05  5.70394930e-04  2.36802060e-02\n",
      "  -2.54174299e-03  4.59457827e-03  1.36112503e-02  0.00000000e+00\n",
      "   2.36802060e-02  1.53617434e-02  0.00000000e+00]\n",
      " [ 3.34059196e-03 -1.06503342e-03  1.55371308e-04  1.44953871e-02\n",
      "  -3.95111677e-03  2.63213237e-03  1.59719449e-02  0.00000000e+00\n",
      "   1.44953871e-02  1.89596961e-02  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "(14, 300)\n"
     ]
    }
   ],
   "source": [
    "def attention(queries, keys, values):\n",
    "    \"\"\"  scaled dot-product attention\n",
    "    queries: query matrix\n",
    "    keys: key matrix\n",
    "    value: value matrix\n",
    "    \"\"\"\n",
    "\n",
    "    #### your code ####\n",
    "    weights = calc_weights(queries,keys)\n",
    "    attention = np.matmul(weights,values)\n",
    "    #### your code ####\n",
    "    return attention\n",
    "\n",
    "\n",
    "attention_result = attention(embedded_fr, embedded_en, embedded_en)\n",
    "print(attention_result.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b54b3c2a0aa143d1aba4027c33d7b799",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 30,
    "execution_start": 1701023136827,
    "source_hash": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0077802  -0.00345834 -0.01493142  0.05936939 -0.02286767 -0.00835552\n",
      "   0.00079745 -0.03737862  0.0270674   0.05374082]\n",
      " [-0.00777829 -0.00336843 -0.01498988  0.05937157 -0.02285948 -0.00835695\n",
      "   0.00085725 -0.03740797  0.02706951  0.05377672]]\n"
     ]
    }
   ],
   "source": [
    "print(attention_result[0:2,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "✅ Point distribution ✅\n",
    "- 0.5 if the shape is (14, 300).\n",
    "- 0.5 if the selected rows are:\n",
    "\n",
    "```\n",
    "[[-0.0077802  -0.00345834 -0.01493142  0.05936939 -0.02286767 -0.00835552\n",
    "   0.00079745 -0.03737862  0.0270674   0.05374082]\n",
    " [-0.00777829 -0.00336843 -0.01498988  0.05937157 -0.02285948 -0.00835695\n",
    "   0.00085725 -0.03740797  0.02706951  0.05377672]]\n",
    "   ```\n",
    "\n",
    "If these checks do not match, check the code."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "15a9aa6ecf6b4107bb6fae0577bd293b",
    "deepnote_cell_type": "markdown",
    "id": "vlwGjAWMy_OX"
   },
   "source": [
    "####${\\color{red}{Comments\\ 3.2}}$\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Good job! There is an exact match, and the implementation is correct!\n",
    "\n",
    "Total Points: 1 / 1\n",
    "```\n",
    "\n",
    "\n",
    "${\\color{red}{⚠️Comments\\ end⚠️}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "0dfc535908be40e798840ed2c44ac4bd",
    "deepnote_cell_type": "code",
    "id": "qDx8DrnVzAR1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=a1bcdaf6-328f-4516-bd85-cdc48d4f60b9' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "98f3bb85d881458798404ad49d8505ce",
  "deepnote_persisted_session": {
   "createdAt": "2023-11-26T18:45:41.034Z"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
